[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "",
    "text": "Preface\nWelcome to the age of big Earth observation data! Petabytes of images are now openly accessible in cloud services. Having free access to massive data sets, we need new methods to measure change on our planet using image data. An essential contribution of big EO data has been to provide access to image time series that capture signals from the same locations continually. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Better measurement of natural resources depletion caused by deforestation, forest degradation, and desertification is possible. Experts improve the production of agricultural statistics. Using image time series, analysts can use large data collections to detect subtle changes in ecosystem health and distinguish between various land classes more effectively. Time series analysis is an innovative way to address global challenges like climate change, biodiversity preservation, and sustainable agriculture.\nThis book introduces sits, an open-source R package of big Earth observation data analysis using satellite image time series. Users build regular data cubes from cloud services such as Amazon Web Services, Microsoft Planetary Computer, Copernicus Data Space Ecosystem, NASA Harmonized Landsat-Sentinel, Brazil Data Cube, Swiss Data Cube, Digital Earth Australia, and Digital Earth Africa. The sits API includes training sample quality measures, machine learning and deep learning classification algorithms, and Bayesian post-processing methods for smoothing and uncertainty assessment. To evaluate results, sits supports best practice accuracy assessments."
  },
  {
    "objectID": "index.html#how-much-r-knowledge-is-required",
    "href": "index.html#how-much-r-knowledge-is-required",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "How much R knowledge is required?",
    "text": "How much R knowledge is required?\nThe sits package is designed for remote sensing experts in the Earth Sciences field who want to use advanced data analysis techniques with basic programming knowledge. The package provides a clear and direct set of functions that are easy to learn and master. To quickly master what is needed to run sits, please read Parts 1 and 2 of Garrett Golemund’s book, Hands-On Programming with R. Although not needed to run sits, your R skills will benefit from the book by Hadley Wickham and Gareth Golemund, R for Data Science (2nd edition). Important concepts of spatial analysis are presented by Edzer Pebesma and Roger Bivand in their book Spatial Data Science."
  },
  {
    "objectID": "index.html#software-version-described-in-this-book",
    "href": "index.html#software-version-described-in-this-book",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Software version described in this book",
    "text": "Software version described in this book\nThe version of the sits package described in this book is 1.5.3."
  },
  {
    "objectID": "index.html#main-reference-for-sits",
    "href": "index.html#main-reference-for-sits",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Main reference for sits",
    "text": "Main reference for sits\nIf you use sits in your work, please cite the following paper:\nRolf Simoes, Gilberto Camara, Gilberto Queiroz, Felipe Souza, Pedro R. Andrade, Lorena Santos, Alexandre Carvalho, and Karine Ferreira. Satellite Image Time Series Analysis for Big Earth Observation Data. Remote Sensing, 13, p. 2428, 2021."
  },
  {
    "objectID": "index.html#intellectual-property-rights",
    "href": "index.html#intellectual-property-rights",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Intellectual property rights",
    "text": "Intellectual property rights\nThis book is licensed as Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) by Creative Commons. The sits package is licensed under the GNU General Public License, version 3.0."
  },
  {
    "objectID": "01-setup.html#how-to-use-this-on-line-book",
    "href": "01-setup.html#how-to-use-this-on-line-book",
    "title": "Setup",
    "section": "How to use this on-line book",
    "text": "How to use this on-line book\nThis book contains reproducible code that can be run on an R environment. There are three options to setup your working environment:\n\nInstall R and RStudio and the packages required by sits, with specific procedures for each type of operating systems.\nUse a Docker image provided by the Brazil Data Cube.\nInstall sits and all its dependencies using conda."
  },
  {
    "objectID": "01-setup.html#how-to-install-sits-using-r-and-rstudio",
    "href": "01-setup.html#how-to-install-sits-using-r-and-rstudio",
    "title": "Setup",
    "section": "How to install sits using R and RStudio",
    "text": "How to install sits using R and RStudio\nWe suggest a staged installation, as follows:\n\nGet and install base R from CRAN.\nInstall RStudio from the Posit website.\n\nInstalling sits from CRAN\nThe Comprehensive R Archive Network (CRAN), a network of servers (also known as mirrors) from around the world that store up-to-date versions of basic code and packages for R. In what follows, we describe how to use CRAN to sits on Windows, Linux and MacOS.\nInstalling in Microsoft Windows and MacOS environments\nWindows and MacOS users are strongly encouraged to install binary packages from CRAN. The sits package relies on the sf and terra packages, which require the GDAL and PROJ libraries. Run RStudio and install binary packages sf and terra, in this order:\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\n\nAfter installing the binaries for sf and terra, install sits as follows;\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\nTo run the examples in the book, please also install sitsdata package, which is available from GitHub. It is necessary to use package devtools to install sitsdata.\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sitsdata\")\n\nTo install sits from source, please install Rtools for Windows to have access to the compiling environment. For Mac, please follow the instructions available here.\nInstalling in Ubuntu environments\nFor Ubuntu, the first step should be to install the latest version of the GDAL, GEOS, and PROJ4 libraries and binaries. To do so, use the repository ubuntugis-unstable, which should be done as follows:\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt-get update\nsudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev \nsudo apt-get install gdal-bin\nsudo apt-get install proj-bin\nGetting an error while adding this PPA repository could be due to the absence of the package software-properties-common. After installing GDAL, GEOS, and PROJ4, please install packages sf and terra:\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\n\nThen please proceed to install sits, which can be installed as a regular R package.\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\nInstalling in Debian environments\nFor Debian, use the rocker geospatial dockerfiles.\nInstalling in Fedora environments\nIn the case of Fedora, the following command installs all required dependencies:\n\nsudo dnf install gdal-devel proj-devel geos-devel sqlite-devel udunits2-devel"
  },
  {
    "objectID": "01-setup.html#using-docker-images",
    "href": "01-setup.html#using-docker-images",
    "title": "Setup",
    "section": "Using Docker images",
    "text": "Using Docker images\nIf you are familiar with Docker, there are images for sits available with RStudio or Jupyter notebook. Such images are provided by the Brazil Data Cube team:\n\n\nVersion for R and RStudio.\n\nVersion for Jupyter Notebooks.\n\nOn a Windows or Mac platform, install Docker and then obtain one of the two images listed above from the Brazil Data Cube. Both images contain the full sits running environment. When GDAL is running in docker containers, please add the security flag --security-opt seccomp=unconfined on start."
  },
  {
    "objectID": "01-setup.html#install-sits-from-conda",
    "href": "01-setup.html#install-sits-from-conda",
    "title": "Setup",
    "section": "Install sits from CONDA",
    "text": "Install sits from CONDA\nConda is an open-source, cross-platform package manager. It is a convenient way to installl Python and R packages. To use conda, first download the software from the CONDA website. After installation, use conda to install sits from the terminal as follows:\n\n# add conda-forge to the download channels \nconda config --add channels conda-forge\nconda config --set channel_priority strict\n# install sits using conda\nconda install conda-forge::r-sits\n\nThe conda installer will download all packages and libraries required to run sits. This is the easiest way to install sits on Windows."
  },
  {
    "objectID": "01-setup.html#accessing-the-development-version",
    "href": "01-setup.html#accessing-the-development-version",
    "title": "Setup",
    "section": "Accessing the development version",
    "text": "Accessing the development version\nThe source code repository of sits is on GitHub. There are two versions available on GitHub: master and dev. The master contains the current stable version, which is either the same code available in CRAN or a minor update with bug fixes. To install the master version, install devtools (if not already available) and do as follows:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sits\", dependencies = TRUE)\n\nTo install the dev (development) version, which contains the latest updates but might be unstable, install devtools (if not already available), and then install sits as follows:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sits@dev\", dependencies = TRUE)"
  },
  {
    "objectID": "01-setup.html#additional-requirements",
    "href": "01-setup.html#additional-requirements",
    "title": "Setup",
    "section": "Additional requirements",
    "text": "Additional requirements\nTo run the examples in the book, please also install the sitsdata package. We recommend installing it using wget. See instructions in the GNU Wget site.\n\noptions(download.file.method = \"wget\")\ndevtools::install_github(\"e-sensing/sitsdata\")"
  },
  {
    "objectID": "01-setup.html#using-gpus-with-sits",
    "href": "01-setup.html#using-gpus-with-sits",
    "title": "Setup",
    "section": "Using GPUs with sits\n",
    "text": "Using GPUs with sits\n\nThe torch package automatically recognizes if a GPU is available on the machine and uses it for training and classification. There is a significant performance gain when GPUs are used instead of CPUs for deep learning models. There is no need for specific adjustments to torch scripts. To use GPUs, torch requires version 11.6 of the CUDA library, which is available for Ubuntu 18.04 and 20.04. Please follow the detailed instructions for setting up torch available here.\n\ninstall.packages(\"torch\")"
  },
  {
    "objectID": "02-acknowledgements.html#funding-sources",
    "href": "02-acknowledgements.html#funding-sources",
    "title": "Acknowledgements",
    "section": "Funding Sources",
    "text": "Funding Sources\nThe authors acknowledge the funders that supported the development of sits:\n\nAmazon Fund, established by Brazil with financial contribution from Norway, through contract 17.2.0536.1. between the Brazilian Development Bank (BNDES) and the Foundation for Science, Technology, and Space Applications (FUNCATE), for the establishment of the Brazil Data Cube.\nCoordenação de Aperfeiçoamento de Pessoal de Nível Superior-Brasil (CAPES) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) for grants 312151/2014-4 and 140684/2016-6.\nSao Paulo Research Foundation (FAPESP) under eScience Program grant 2014/08398-6, for providing MSc, PhD, and post-doc scholarships, equipment, and travel support.\nInternational Climate Initiative of the Germany Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (IKI) under grant 17-III-084-Global-A-RESTORE+ (“RESTORE+: Addressing Landscape Restoration on Degraded Land in Indonesia and Brazil”).\nMicrosoft Planetary Computer initiative under the GEO-Microsoft Cloud Computer Grants Programme.\nInstituto Clima e Sociedade, under the project grant “Modernization of PRODES and DETER Amazon monitoring systems”.\nOpen-Earth-Monitor Cyberinfrastructure project, which has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No. 101059548.\nFAO-EOSTAT initiative, which uses next generation Earth observation tools to produce land cover and land use statistics."
  },
  {
    "objectID": "02-acknowledgements.html#community-contributions",
    "href": "02-acknowledgements.html#community-contributions",
    "title": "Acknowledgements",
    "section": "Community Contributions",
    "text": "Community Contributions\nThe authors thank the R-spatial community for their foundational work, including Marius Appel, Tim Appelhans, Robert Hijmans, Jakub Nowosad, Edzer Pebesma, and Martijn Tennekes for their R packages gdalcubes, leafem, terra, supercells, sf/stars, and tmap. We are grateful for the work of Dirk Eddelbuettel on Rcpp and RcppArmadillo and Ron Wehrens in package kohonen. We are much indebted to Hadley Wickham for the tidyverse, Daniel Falbel for the torch and luz packages, and the RStudio team for package leaflet. The multiple authors of machine learning packages randomForest, e1071, and xgboost provided robust algorithms. We would like to thank Python developers who shared their deep learning algorithms for image time series classification: Vivien Sainte Fare Garnot, Zhiguang Wang, Maja Schneider, and Marc Rußwurm. The first author also thanks Roger Bivand for his benign influence in all things related to R."
  },
  {
    "objectID": "02-acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "href": "02-acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "title": "Acknowledgements",
    "section": "Reproducible papers and books used in building sits",
    "text": "Reproducible papers and books used in building sits\nWe thank the authors of the following papers for making their code and papers open and reusable. Their contribution has been essential to build sits.\n\nEdzer Pebesma, Simple Features for R: Standardized Support for Spatial Vector Data. R Journal, 10(1), 2018.\nMartin Tennekes, tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39, 2018.\nRon Wehrens and Johannes Kruisselbrink, Flexible Self-Organising Maps in kohonen 3.0. Journal of Statistical Software, 87, 7, 2018.\nHassan Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller, Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4): 917–963, 2019.\nCharlotte Pelletier, Geoffrey Webb, and Francois Petitjean. Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series. Remote Sensing 11 (5), 2019.\nMarc Rußwurm, Charlotte Pelletier, Maximilian Zollner, Sèbastien Lefèvre, and Marco Körner, Breizhcrops: a Time Series Dataset for Crop Type Mapping. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences ISPRS, 2020.\nMarius Appel and Edzer Pebesma, On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library. Data 4 (3): 1–16, 2020.\nVivien Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata, Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention, Conference on Computer Vision and Pattern Recognition, 2020.\nVivien Garnot and Loic Landrieu, Lightweight Temporal Self-Attention for Classifying Satellite Images Time Series, 2020.\nMaja Schneider, Marco Körner, Re: Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention ReScience C 7 (2), 2021.\nRolf Simoes, Felipe Souza, Mateus Zaglia, Gilberto Queiroz, Rafael dos Santos and Karine Ferreira, Rstac: An R Package to Access Spatiotemporal Asset Catalog Satellite Imagery. IGARSS, 2021, pp. 7674-7677.\nJakub Nowosad, Tomasz Stepinksi, Extended SLIC superpixels algorithm for applications to non-imagery geospatial rasters. International Journal of Applied Earth Observations and Geoinformation, 2022.\nSigrid Keydana, Deep Learning and Scientific Computing with R torch, Chapman and Hall/CRC, London, 2023.\nRobin Lovelace, Jakub Nowosad, Jannes Münchow, Geocomputation with R. Chapman and Hall/CRC, London, 2023.\nEdzer Pebesma, Roger Bivand, Spatial Data Science: With applications in R. Chapman and Hall/CRC, London, 2023."
  },
  {
    "objectID": "02-acknowledgements.html#publications-using-sits",
    "href": "02-acknowledgements.html#publications-using-sits",
    "title": "Acknowledgements",
    "section": "Publications using sits",
    "text": "Publications using sits\nThis section gathers the publications that have used sits to generate their results.\n2024\n\nGiuliani, Gregory. Time-First Approach for Land Cover Mapping Using Big Earth Observation Data Time-Series in a Data Cube – a Case Study from the Lake Geneva Region (Switzerland). Big Earth Data, 2024.\nWerner, João, Mariana Belgiu et al., Mapping Integrated Crop–Livestock Systems Using Fused Sentinel-2 and PlanetScope Time Series and Deep Learning. Remote Sensing 16, no. 8 (January 2024): 1421.\n\n2023\n\nHadi, Firman, Laode Muhammad Sabri, Yudo Prasetyo, and Bambang Sudarsono. Leveraging Time-Series Imageries and Open Source Tools for Enhanced Land Cover Classification. In IOP Conference Series: Earth and Environmental Science, 1276:012035. IOP Publishing, 2023.\nBruno Adorno, Thales Körting, and Silvana Amaral, Contribution of time-series data cubes to classify urban vegetation types by remote sensing. Urban Forest & Urban Greening, 79, 127817, 2023.\n\n2021\n\nLorena Santos, Karine R. Ferreira, Gilberto Camara, Michelle Picoli, and Rolf Simoes, Quality control and class noise reduction of satellite image time series. ISPRS Journal of Photogrammetry and Remote Sensing, 177, 75–88, 2021.\nLorena Santos, Karine Ferreira, Michelle Picoli, Gilberto Camara, Raul Zurita-Milla and Ellen-Wien Augustijn, Identifying Spatiotemporal Patterns in Land Use and Cover Samples from Satellite Image Time Series. Remote Sensing, 13(5), 974, 2021.\n\n2020\n\nRolf Simoes, Michelle Picoli, Gilberto Camara, Adeline Maciel, Lorena Santos, Pedro Andrade, Alber Sánchez, Karine Ferreira, and Alexandre Carvalho, Land use and cover maps for Mato Grosso State in Brazil from 2001 to 2017. Nature Scientific Data, 7, article 34, 2020.\nMichelle Picoli, Ana Rorato, Pedro Leitão, Gilberto Camara, Adeline Maciel, Patrick Hostert, and Ieda Sanches, Impacts of Public and Private Sector Policies on Soybean and Pasture Expansion in Mato Grosso—Brazil from 2001 to 2017. Land, 9(1), 2020.\nKarine Ferreira, Gilberto Queiroz et al., Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products. Remote Sensing, 12, 4033, 2020.\nAdeline Maciel, Lubia Vinhas, Michelle Picoli, and Gilberto Camara, Identifying Land Use Change Trajectories in Brazil’s Agricultural Frontier. Land, 9, 506, 2020.\n\n2018\n\nMichelle Picoli, Gilberto Camara, et al., Big Earth Observation Time Series Analysis for Monitoring Brazilian Agriculture. ISPRS Journal of Photogrammetry and Remote Sensing, 145, 328–339, 2018."
  },
  {
    "objectID": "02-acknowledgements.html#ai-support-in-preparing-the-book",
    "href": "02-acknowledgements.html#ai-support-in-preparing-the-book",
    "title": "Acknowledgements",
    "section": "AI support in preparing the book",
    "text": "AI support in preparing the book\nThe authors have use Generative AI tools (Chat-GPT, Grammarly and ProWritingAid) to improve readability and language of the work. The core technical and scientific content of the book has been prepared exclusively by the authors. Assistance from Generative AI has been limited to improving definitions and making the text easier to follow."
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Preface",
    "text": "Preface\n\nWelcome to the age of big Earth observation data! Petabytes of images are now openly accessible in cloud services. Having free access to massive data sets, we need new methods to measure change on our planet using image data. An essential contribution of big EO data has been to provide access to image time series that capture signals from the same locations continually. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Better measurement of natural resources depletion caused by deforestation, forest degradation, and desertification is possible. Experts improve the production of agricultural statistics. Using image time series, analysts can use large data collections to detect subtle changes in ecosystem health and distinguish between various land classes more effectively. Time series analysis is an innovative way to address global challenges like climate change, biodiversity preservation, and sustainable agriculture.\nThis book introduces sits, an open-source R package of big Earth observation data analysis using satellite image time series. Users build regular data cubes from cloud services such as Amazon Web Services, Microsoft Planetary Computer, Copernicus Data Space Ecosystem, NASA Harmonized Landsat-Sentinel, Brazil Data Cube, Swiss Data Cube, Digital Earth Australia, and Digital Earth Africa. The sits API includes training sample quality measures, machine learning and deep learning classification algorithms, and Bayesian post-processing methods for smoothing and uncertainty assessment. To evaluate results, sits supports best practice accuracy assessments."
  },
  {
    "objectID": "03-intro.html#who-is-this-book-for",
    "href": "03-intro.html#who-is-this-book-for",
    "title": "\n1  Introduction\n",
    "section": "\n1.1 Who is this book for?",
    "text": "1.1 Who is this book for?\nThis book, tailored for land use change experts and researchers, is a practical guide that enables them to analyze big Earth observation data sets. It provides readers with the means of producing high-quality maps of land use and land cover, guiding them through all the steps to achieve good results. Given the natural world’s complexity and huge variations in human-nature interactions, only local experts who know their countries and ecosystems can extract full information from big EO data.\nOne group of readers that we are keen to engage with is the national authorities on forest, agriculture, and statistics in developing countries. We aim to foster a collaborative environment where they can use EO data to enhance their national land use and cover estimates, supporting sustainable development policies. To achieve this goal, sits has strong backing from the FAO Expert Group on the Use of Earth Observation data (FAO-EOSTAT). FAO-EOSTAT is at the forefront of using advanced EO data analysis methods for agricultural statistics in developing countries [1], [2]."
  },
  {
    "objectID": "03-intro.html#why-work-with-satellite-image-time-series",
    "href": "03-intro.html#why-work-with-satellite-image-time-series",
    "title": "\n1  Introduction\n",
    "section": "Why work with satellite image time series?",
    "text": "Why work with satellite image time series?\nSatellite imagery provides the most extensive data on our environment. By encompassing vast areas of the Earth’s surface, images enable researchers to analyze local and worldwide transformations. By observing the same location multiple times, satellites provide data on environmental changes and survey areas that are difficult to observe from the ground. Given its unique features, images offer essential information for many applications, including deforestation, crop production, food security, urban footprints, water scarcity, and land degradation. Using time series, experts improve their understanding of ecological patterns and processes. Instead of selecting individual images from specific dates and comparing them, researchers track change continuously [3]."
  },
  {
    "objectID": "03-intro.html#time-first-space-later",
    "href": "03-intro.html#time-first-space-later",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 Time-first, space-later",
    "text": "1.2 Time-first, space-later\n“Time-first, space-later” is a concept in satellite image classification that takes time series analysis as the first step for analyzing remote sensing data, with spatial information being considered after all time series are classified. The time-first part brings a better understanding of changes in landscapes. Detecting and tracking seasonal and long-term trends becomes feasible, as well as identifying anomalous events or patterns in the data, such as wildfires, floods, or droughts. Each pixel in a data cube is treated as a time series, using information available in the temporal instances of the case. Time series classification is pixel-based, producing a set of labeled pixels. This result is then used as input to the space-later part of the method. In this phase, a smoothing algorithm improves the results of time-first classification by considering the spatial neighborhood of each pixel. The resulting map thus combines both spatial and temporal information."
  },
  {
    "objectID": "03-intro.html#land-use-and-land-cover",
    "href": "03-intro.html#land-use-and-land-cover",
    "title": "\n1  Introduction\n",
    "section": "\n1.3 Land use and land cover",
    "text": "1.3 Land use and land cover\nThe UN Food and Agriculture Organization defines land cover as “the observed biophysical cover on the Earth’s surface” [4]. Land cover can be observed and mapped directly through remote sensing images. In FAO’s guidelines and reports, land use is described as “the human activities or purposes for which land is managed or exploited”. Although land cover and land use denote different approaches for describing the Earth’s landscape, in practice there is considerable overlap between these concepts [5]. When classifying remote sensing images, natural areas are classified using land cover types (e.g, forest), while human-modified areas are described with land use classes (e.g., pasture).\nOne of the advantages of using image time series for land classification is its capacity of measuring changes in the landscape related to agricultural practices. For example, the time series of a vegetation index in an area of crop production will show a pattern of minima (planting and sowing stages) and maxima (flowering stage). Thus, classification schemas based on image time series data can be richer and more detailed than those associated only with land cover. In what follows, we use the term “land classification” to refer to image classification representing both land cover and land use classes."
  },
  {
    "objectID": "03-intro.html#how-sits-works",
    "href": "03-intro.html#how-sits-works",
    "title": "\n1  Introduction\n",
    "section": "\n1.4 How sits works",
    "text": "1.4 How sits works\nThe sits package uses satellite image time series for land classification, using a time-first, space-later approach. In the data preparation part, collections of big Earth observation images are organized as data cubes. Each spatial location of a data cube is associated with a time series. Locations with known labels train a machine learning algorithm, which classifies all time series of a data cube, as shown in Figure @ref(fig:gview).\n\n\n\n\nUsing time series for land classification (source: authors).\n\n\n\nThe package provides tools for analysis, visualization, and classification of satellite image time series. Users follow a typical workflow for a pixel-based classification:\n\nSelect an analysis-ready data image collection from a cloud provider such as AWS, Microsoft Planetary Computer, Digital Earth Africa, or Brazil Data Cube.\nBuild a regular data cube using the chosen image collection.\nObtain new bands and indices with operations on data cubes.\nExtract time series samples from the data cube to be used as training data.\nPerform quality control and filtering on the time series samples.\nTrain a machine learning model using the time series samples.\nClassify the data cube using the model to get class probabilities for each pixel.\nPost-process the probability cube to remove outliers.\nProduce a labeled map from the post-processed probability cube.\nEvaluate the accuracy of the classification using best practices.\n\nEach workflow step corresponds to a function of the sits API, as shown in the Table below and Figure @ref(fig:api). These functions have convenient default parameters and behaviors. A single function builds machine learning (ML) models. The classification function processes big data cubes with efficient parallel processing. Since the sits API is simple to learn, achieving good results do not require in-depth knowledge about machine learning and parallel processing.\n\n\n\nThe sits API workflow for land classification.\n\nAPI_function\nInputs\nOutput\n\n\n\nsits_cube()\nARD image collection\nIrregular data cube\n\n\nsits_regularize()\nIrregular data cube\nRegular data cube\n\n\nsits_apply()\nRegular data cube\nRegular data cube with new bands and indices\n\n\nsits_get_data()\nData cube and sample locations\nTime series\n\n\nsits_train()\nTime series and ML method\nML classification model\n\n\nsits_classify()\nML classification model and regular data cube\nProbability cube\n\n\nsits_smooth()\nProbability cube\nPost-processed probability cube\n\n\nsits_uncertainty()\nPost-processed probability cube\nUncertainty cube\n\n\nsits_label_classification()\nPost-processed probability cube\nClassified map\n\n\nsits_accuracy()\nClassified map and validation samples\nAccuracy assessment\n\n\n\n\n\n\n\n\n\nFigure 1.1: Main functions of the sits API (source: authors).\n\n\n\nAdditionally, experts can perform object-based image analysis (OBIA) with sits. In this case, before classifying the time series, one can use sits_segments() to create a set of closed polygons. These polygons are classified using a subset of the time series contained inside each segment. For details, see Chapter Object-based time series image analysis."
  },
  {
    "objectID": "03-intro.html#creating-a-data-cube",
    "href": "03-intro.html#creating-a-data-cube",
    "title": "\n1  Introduction\n",
    "section": "\n1.5 Creating a data cube",
    "text": "1.5 Creating a data cube\nThere are two kinds of data cubes in sits: (a) irregular data cubes generated by selecting image collections on cloud providers such as AWS and Planetary Computer; (b) regular data cubes with images fully covering a chosen area, where each image has the same spectral bands and spatial resolution, and images follow a set of adjacent and regular time intervals. Machine learning applications need regular data cubes. Please refer to Chapter Earth observation data cubes for further details.\nThe first steps in using sits are: (a) select an analysis-ready data image collection available in a cloud provider or stored locally using sits_cube(); (b) if the collection is not regular, use sits_regularize() to build a regular data cube.\nThis section shows how to build a data cube from local images already organized as a regular data cube. The data cube is composed of MODIS MOD13Q1 images for the region close to the city of Sinop in Mato Grosso, Brazil. This region is one of the world’s largest producers of soybeans. All images have indexes NDVI and EVI covering a one-year period from 2013-09-14 to 2014-08-29 (we use “year-month-day” for dates). There are 23 time instances, each covering a 16-day period. This data is available in the package sitsdata.\nTo build a data cube from local files, users must provide information about the original source from which the data was obtained. In this case, sits_cube() needs the parameters:\n\n\nsource, the cloud provider from where the data has been obtained (in this case, the Brazil Data Cube “BDC”);\n\ncollection, the collection of the cloud provider from where the images have been extracted. In this case, data comes from the MOD13Q1 collection 6;\n\ndata_dir, the local directory where the image files are stored;\n\nparse_info, a vector of strings stating how file names store information on “tile”, “band”, and “date”. In this case, local images are stored in files whose names are similar to TERRA_MODIS_012010_EVI_2014-07-28.tif. This file represents an image obtained by the MODIS sensor onboard the TERRA satellite, covering part of tile 012010 in the EVI band for date 2014-07-28.\n\n\n# load package \"tibble\"\nlibrary(tibble)\n# load packages \"sits\" and \"sitsdata\"\nlibrary(sits)\nlibrary(sitsdata)\n# Create a data cube using local files\nsinop_cube &lt;- sits_cube(\n  source = \"BDC\", \n  collection  = \"MOD13Q1-6.1\",\n  bands = c(\"NDVI\", \"EVI\"),\n  data_dir = system.file(\"extdata/sinop\", package = \"sitsdata\"),  \n  parse_info = c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\")\n)\n# Plot the NDVI for the first date (2013-09-14)\nplot(sinop_cube, \n     band = \"NDVI\", \n     dates = \"2013-09-14\",\n     palette = \"RdYlGn\")\n\n\n\nFigure 1.2: Main functions of the sits API (source: authors).\n\n\n\nThe aim of the parse_info parameter is to extract tile, band, and date information from the file name. Given the large variation in image file names generated by different produces, it includes designators such as X1 and X2; these are place holders for parts of the file name that is not relevant to sits_cube().\nThe R object returned by sits_cube() contains the metadata describing the contents of the data cube. It includes data source and collection, satellite, sensor, tile in the collection, bounding box, projection, and list of files. Each file refers to one band of an image at one of the temporal instances of the cube.\n\n# Show the description of the data cube\nsinop_cube\n\n#&gt; # A tibble: 1 × 11\n#&gt;   source collection satellite sensor tile     xmin    xmax    ymin    ymax crs  \n#&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 BDC    MOD13Q1-6… TERRA     MODIS  0120… -6.18e6 -5.96e6 -1.35e6 -1.23e6 \"PRO…\n#&gt; # ℹ 1 more variable: file_info &lt;list&gt;\n\n\nThe list of image files which make up the data cube is stored as a data frame in the column file_info. For each file, sits stores information about spectral band, reference date, size, spatial resolution, coordinate reference system, bounding box, path to file location and cloud cover information (when available).\n\n# Show information on the images files which are part of a data cube\nsinop_cube$file_info[[1]]\n\n#&gt; # A tibble: 69 × 13\n#&gt;    fid   band  date       nrows ncols  xres  yres      xmin      ymin      xmax\n#&gt;    &lt;chr&gt; &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 1     CLOUD 2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  2 1     EVI   2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  3 1     NDVI  2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  4 2     CLOUD 2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  5 2     EVI   2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  6 2     NDVI  2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  7 3     CLOUD 2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  8 3     EVI   2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  9 3     NDVI  2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt; 10 4     CLOUD 2013-11-01   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt; # ℹ 59 more rows\n#&gt; # ℹ 3 more variables: ymax &lt;dbl&gt;, crs &lt;chr&gt;, path &lt;chr&gt;\n\n\nA key attribute of a data cube is its timeline, as shown below. The command sits_timeline() lists the temporal references associated to sits objects, including samples, data cubes and models.\n\n# Show the R object that describes the data cube\nsits_timeline(sinop_cube)\n\n#&gt;  [1] \"2013-09-14\" \"2013-09-30\" \"2013-10-16\" \"2013-11-01\" \"2013-11-17\"\n#&gt;  [6] \"2013-12-03\" \"2013-12-19\" \"2014-01-01\" \"2014-01-17\" \"2014-02-02\"\n#&gt; [11] \"2014-02-18\" \"2014-03-06\" \"2014-03-22\" \"2014-04-07\" \"2014-04-23\"\n#&gt; [16] \"2014-05-09\" \"2014-05-25\" \"2014-06-10\" \"2014-06-26\" \"2014-07-12\"\n#&gt; [21] \"2014-07-28\" \"2014-08-13\" \"2014-08-29\"\n\n\nThe timeline of the sinop_cube data cube has 23 intervals with a temporal difference of 16 days. The chosen dates capture the agricultural calendar in Mato Grosso, Brazil. The agricultural year starts in September-October with the sowing of the summer crop (usually soybeans) which is harvested in February-March. Then the winter crop (mostly Corn, Cotton or Millet) is planted in March and harvested in June-July. For LULC classification, the training samples and the date cube should share a timeline with the same number of intervals and similar start and end dates."
  },
  {
    "objectID": "03-intro.html#the-time-series-tibble",
    "href": "03-intro.html#the-time-series-tibble",
    "title": "\n1  Introduction\n",
    "section": "The time series tibble",
    "text": "The time series tibble\nTo handle time series information, sits uses a tibble. Tibbles are extensions of the data.frame tabular data structures provided by the tidyverse set of packages. The example below shows a tibble with 1,837 time series obtained from MODIS MOD13Q1 images. Each series has four attributes: two bands (NIR and MIR) and two indexes (NDVI and EVI). This dataset is available in package sitsdata.\nThe time series tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band.\n\n# Load the MODIS samples for Mato Grosso from the \"sitsdata\" package\nlibrary(tibble)\nlibrary(sitsdata)\ndata(\"samples_matogrosso_mod13q1\", package = \"sitsdata\")\nsamples_matogrosso_mod13q1\n\n#&gt; # A tibble: 1,837 × 7\n#&gt;    longitude latitude start_date end_date   label   cube     time_series      \n#&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;list&gt;           \n#&gt;  1     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  2     -59.4    -9.31 2014-09-14 2015-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  3     -59.4    -9.31 2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  4     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  5     -55.2   -10.8  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  6     -51.9   -13.4  2014-09-14 2015-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  7     -56.0   -10.1  2005-09-14 2006-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  8     -54.6   -10.4  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  9     -52.5   -11.0  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt; 10     -52.1   -14.0  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt; # ℹ 1,827 more rows\n\n\nThe timeline for all time series associated with the samples follows the same agricultural calendar, starting in September 14th and ending in August 28th. All samples contain 23 values, corresponding to the same temporal interval as those of the sinop data cube. Notice that that although the years for the samples are different, the samples for a given year follow the same agricultural calendar.\nThe time series can be displayed by showing the time_series column.\n\n# Load the time series for MODIS samples for Mato Grosso\nsamples_matogrosso_mod13q1[1,]$time_series[[1]]\n\n#&gt; # A tibble: 23 × 5\n#&gt;    Index       NDVI   EVI   NIR    MIR\n#&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 2006-09-14 0.500 0.263 0.230 0.139 \n#&gt;  2 2006-09-30 0.485 0.330 0.359 0.161 \n#&gt;  3 2006-10-16 0.716 0.397 0.264 0.0757\n#&gt;  4 2006-11-01 0.654 0.415 0.332 0.124 \n#&gt;  5 2006-11-17 0.591 0.433 0.400 0.172 \n#&gt;  6 2006-12-03 0.662 0.439 0.348 0.125 \n#&gt;  7 2006-12-19 0.734 0.444 0.295 0.0784\n#&gt;  8 2007-01-01 0.739 0.502 0.348 0.0887\n#&gt;  9 2007-01-17 0.768 0.526 0.351 0.0761\n#&gt; 10 2007-02-02 0.797 0.550 0.355 0.0634\n#&gt; # ℹ 13 more rows\n\n\nThe distribution of samples per class can be obtained using the summary() command. The classification schema uses nine labels, four associated to crops (Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet), two with natural vegetation (Cerrado, Forest) and one to Pasture.\n\n# Load the MODIS samples for Mato Grosso from the \"sitsdata\" package\nsummary(samples_matogrosso_mod13q1)\n\n#&gt; # A tibble: 7 × 3\n#&gt;   label      count   prop\n#&gt;   &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1 Cerrado      379 0.206 \n#&gt; 2 Forest       131 0.0713\n#&gt; 3 Pasture      344 0.187 \n#&gt; 4 Soy_Corn     364 0.198 \n#&gt; 5 Soy_Cotton   352 0.192 \n#&gt; 6 Soy_Fallow    87 0.0474\n#&gt; 7 Soy_Millet   180 0.0980\n\n\nIt is helpful to plot the dispersion of the time series. In what follows, for brevity, we will filter only one label (Forest) and select one index (NDVI). Note that for filtering the label we use a function from dplyr package, while for selecting the index we use sits_select(). We use two different functions for selection because of they way metadata is stored in a samples files. The labels for the samples are listed in column label in the samples tibble, as shown above. In this case, one can use functions from the dplyr package to extract subsets. In particular, the function dplyr::filter retaining all rows that satisfy a given condition. In the above example, the result of dplyr::filter is the set of samples associated to the “Forest” label. The second selection involves obtaining only the values for the NDVI band. This operation requires access to the time_series column, which is stored as a list. In this case, selection with dplyr::filter will not work. To handle such cases, sits provides sits_select() to select subsets inside the time_series list.\n\n# select all samples with label \"Forest\"\nsamples_forest &lt;- dplyr::filter(\n    samples_matogrosso_mod13q1, \n    label == \"Forest\"\n)\n# select the NDVI band for all samples with label \"Forest\"\nsamples_forest_ndvi &lt;- sits_select(\n    samples_forest, \n    band = \"NDVI\"\n)\nplot(samples_forest_ndvi)\n\n\n\nFigure 1.3: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.4: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.5: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.6: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.7: Joint plot of all samples in band NDVI for label Forest.\n\n\n\nThe above figure shows all the time series associated with label Forest and band NDVI (in light blue), highlighting the median (shown in dark red) and the first and third quartiles (shown in brown). The spikes are noise caused by the presence of clouds."
  },
  {
    "objectID": "03-intro.html#training-a-machine-learning-model",
    "href": "03-intro.html#training-a-machine-learning-model",
    "title": "\n1  Introduction\n",
    "section": "Training a machine learning model",
    "text": "Training a machine learning model\nThe next step is to train a machine learning (ML) model using sits_train(). It takes two inputs, samples (a time series tibble) and ml_method (a function that implements a machine learning algorithm). The result is a model that is used for classification. Each ML algorithm requires specific parameters that are user-controllable. For novice users, sits provides default parameters that produce good results. Please see Chapter Machine learning for data cubes for more details.\nSince the time series data has four attributes (EVI, NDVI, NIR, and MIR) and the data cube images have only two, we select the NDVI and EVI values and use the resulting data for training. To build the classification model, we use a random forest model called by sits_rfor(). Results from the random forest model can vary between different runs, due to the stochastic nature of the algorithm, For this reason, in the code fragment below, we set the seed of R’s pseudo-random number generation explicitly to ensure the same results are produced for documentation purposes.\n\nset.seed(03022024)\n# Select the bands NDVI and EVI\nsamples_2bands &lt;- sits_select(\n    data = samples_matogrosso_mod13q1, \n    bands = c(\"NDVI\", \"EVI\")\n)\n# Train a random forest model\nrf_model &lt;- sits_train(\n    samples = samples_2bands, \n    ml_method = sits_rfor()\n)\n# Plot the most important variables of the model\nplot(rf_model)\n\n\n\nMost relevant variables of trained random forest model (source: authors)."
  },
  {
    "objectID": "03-intro.html#data-cube-classification",
    "href": "03-intro.html#data-cube-classification",
    "title": "\n1  Introduction\n",
    "section": "Data cube classification",
    "text": "Data cube classification\nAfter training the machine learning model, the next step is to classify the data cube using sits_classify(). This function produces a set of raster probability maps, one for each class. For each of these maps, the value of a pixel is proportional to the probability that it belongs to the class. This function has two mandatory parameters: data, the data cube or time series tibble to be classified; and ml_model, the trained ML model. Optional parameters include: (a) multicores, number of cores to be used; (b) memsize, RAM used in the classification; (c) output_dir, the directory where the classified raster files will be written. Details of the classification process are available in “Image classification in data cubes”.\n\n# Classify the raster image\nsinop_probs &lt;- sits_classify(\n    data = sinop_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = \"./tempdir/chp3\"\n)\n# Plot the probability cube for class Forest\nplot(sinop_probs, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nProbability map for class Forest (source: authors).\n\n\n\nAfter completing the classification, we plot the probability maps for class Forest. Probability maps are helpful to visualize the degree of confidence the classifier assigns to the labels for each pixel. They can be used to produce uncertainty information and support active learning, as described in Chapter Image classification in data cubes."
  },
  {
    "objectID": "03-intro.html#spatial-smoothing",
    "href": "03-intro.html#spatial-smoothing",
    "title": "\n1  Introduction\n",
    "section": "Spatial smoothing",
    "text": "Spatial smoothing\nWhen working with big Earth observation data, there is much variability in each class. As a result, some pixels will be misclassified. These errors are more likely to occur in transition areas between classes. To address these problems, sits_smooth() takes a probability cube as input and uses the class probabilities of each pixel’s neighborhood to reduce labeling uncertainty. Plotting the smoothed probability map for class Forest shows that most outliers have been removed.\n\n# Perform spatial smoothing\nsinop_bayes &lt;- sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = \"./tempdir/chp3\"\n)\nplot(sinop_bayes, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nSmoothed probability map for class Forest (source: authors)."
  },
  {
    "objectID": "03-intro.html#labeling-a-probability-data-cube",
    "href": "03-intro.html#labeling-a-probability-data-cube",
    "title": "\n1  Introduction\n",
    "section": "Labeling a probability data cube",
    "text": "Labeling a probability data cube\nAfter removing outliers using local smoothing, the final classification map can be obtained using sits_label_classification(). This function assigns each pixel to the class with the highest probability.\n\n# Label the probability file \nsinop_map &lt;- sits_label_classification(\n    cube = sinop_bayes, \n    output_dir = \"./tempdir/chp3\"\n)\nplot(sinop_map)\n\n\n\nClassification map for Sinop (source: authors).\n\n\n\nThe resulting classification files can be read by QGIS. Links to the associated files are available in the sinop_map object in the nested table file_info.\n\n# Show the location of the classification file\nsinop_map$file_info[[1]]\n\n#&gt; # A tibble: 1 × 12\n#&gt;   band  start_date end_date   ncols nrows  xres  yres      xmin     xmax    ymin\n#&gt;   &lt;chr&gt; &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 class 2013-09-14 2014-08-29   944   551  232.  232. -6181982.  -5.96e6 -1.35e6\n#&gt; # ℹ 2 more variables: ymax &lt;dbl&gt;, path &lt;chr&gt;\n\n\n\n\n\n\n[1] \nL. De Simone and P. Gennari, “Earth observations for official crop statistics in the context of scarcity of in-situ data,” Statistical Journal of the IAOS, vol. 38, no. 3, pp. 1009–1019, 2022, doi: 10.3233/SJI-220054.\n\n\n[2] \nL. De Simone, W. Ouellette, and P. Gennari, “Operational Use of EO Data for National Land Cover Official Statistics in Lesotho,” Remote Sensing, vol. 14, no. 14, p. 3294, 2022, doi: 10.3390/rs14143294.\n\n\n[3] \nC. E. Woodcock, T. R. Loveland, M. Herold, and M. E. Bauer, “Transitioning from change detection to monitoring with remote sensing: A paradigm shift,” Remote Sensing of Environment, vol. 238, p. 111558, 2020, doi: 10.1016/j.rse.2019.111558.\n\n\n[4] \nA. Di Gregorio, “Land Cover Classification System - Classification concepts Software version 3,” FAO, 2016.\n\n\n[5] \nA. J. Comber, R. A. Wadsworth, and P. F. Fisher, “Using semantics to clarify the conceptual confusion between land cover and land use: The example of forest,” Journal of Land Use Science, vol. 3, no. 2–3, pp. 185–198, 2008."
  },
  {
    "objectID": "introduction.html#who-is-this-book-for",
    "href": "introduction.html#who-is-this-book-for",
    "title": "Introduction",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book, tailored for land use change experts and researchers, is a practical guide that enables them to analyze big Earth observation data sets. It provides readers with the means of producing high-quality maps of land use and land cover, guiding them through all the steps to achieve good results. Given the natural world’s complexity and huge variations in human-nature interactions, only local experts who know their countries and ecosystems can extract full information from big EO data.\nOne group of readers that we are keen to engage with is the national authorities on forest, agriculture, and statistics in developing countries. We aim to foster a collaborative environment where they can use EO data to enhance their national land use and cover estimates, supporting sustainable development policies. To achieve this goal, sits has strong backing from the FAO Expert Group on the Use of Earth Observation data (FAO-EOSTAT). FAO-EOSTAT is at the forefront of using advanced EO data analysis methods for agricultural statistics in developing countries [1], [2]."
  },
  {
    "objectID": "introduction.html#why-work-with-satellite-image-time-series",
    "href": "introduction.html#why-work-with-satellite-image-time-series",
    "title": "Introduction",
    "section": "Why work with satellite image time series?",
    "text": "Why work with satellite image time series?\nSatellite imagery provides the most extensive data on our environment. By encompassing vast areas of the Earth’s surface, images enable researchers to analyze local and worldwide transformations. By observing the same location multiple times, satellites provide data on environmental changes and survey areas that are difficult to observe from the ground. Given its unique features, images offer essential information for many applications, including deforestation, crop production, food security, urban footprints, water scarcity, and land degradation. Using time series, experts improve their understanding of ecological patterns and processes. Instead of selecting individual images from specific dates and comparing them, researchers track change continuously [3]."
  },
  {
    "objectID": "introduction.html#time-first-space-later",
    "href": "introduction.html#time-first-space-later",
    "title": "Introduction",
    "section": "Time-first, space-later",
    "text": "Time-first, space-later\n“Time-first, space-later” is a concept in satellite image classification that takes time series analysis as the first step for analyzing remote sensing data, with spatial information being considered after all time series are classified. The time-first part brings a better understanding of changes in landscapes. Detecting and tracking seasonal and long-term trends becomes feasible, as well as identifying anomalous events or patterns in the data, such as wildfires, floods, or droughts. Each pixel in a data cube is treated as a time series, using information available in the temporal instances of the case. Time series classification is pixel-based, producing a set of labeled pixels. This result is then used as input to the space-later part of the method. In this phase, a smoothing algorithm improves the results of time-first classification by considering the spatial neighborhood of each pixel. The resulting map thus combines both spatial and temporal information.\n\n\n\n\nFigure 1: Satellite image time series classification (source: [4])."
  },
  {
    "objectID": "introduction.html#land-use-and-land-cover",
    "href": "introduction.html#land-use-and-land-cover",
    "title": "Introduction",
    "section": "Land use and land cover",
    "text": "Land use and land cover\nThe UN Food and Agriculture Organization defines land cover as “the observed biophysical cover on the Earth’s surface” [5]. Land cover can be observed and mapped directly through remote sensing images. In FAO’s guidelines and reports, land use is described as “the human activities or purposes for which land is managed or exploited”. Although land cover and land use denote different approaches for describing the Earth’s landscape, in practice there is considerable overlap between these concepts [6]. When classifying remote sensing images, natural areas are classified using land cover types (e.g, forest), while human-modified areas are described with land use classes (e.g., pasture).\nOne of the advantages of using image time series for land classification is its capacity of measuring changes in the landscape related to agricultural practices. For example, the time series of a vegetation index in an area of crop production will show a pattern of minima (planting and sowing stages) and maxima (flowering stage). Thus, classification schemas based on image time series data can be richer and more detailed than those associated only with land cover. In what follows, we use the term “land classification” to refer to image classification representing both land cover and land use classes."
  },
  {
    "objectID": "introduction.html#how-sits-works",
    "href": "introduction.html#how-sits-works",
    "title": "Introduction",
    "section": "How SITS works",
    "text": "How SITS works\nThe sits package uses satellite image time series for land classification, using a time-first, space-later approach. In the data preparation part, collections of big Earth observation images are organized as data cubes. Each spatial location of a data cube is associated with a time series. Locations with known labels train a machine learning algorithm, which classifies all time series of a data cube, as shown in Figure 2.\n\n\n\n\nFigure 2: General view of sits.\n\n\n\nThe sits API is a set of functions that can be chained to create a workflow for land classification. At its heart, the sits package has eight functions as shown in Figure 3:\n\nExtract data from an analysis-ready data (ARD) collection using sits_cube(), producing a data cube object.\nFrom an irregular data_cube create a regular one, using sits_regularize(). Regular data cubes are required by machine learning algorithms.\nObtain new bands and indices with operations on regular data cubes with sits_apply().\nGiven a set of ground truth values in formats such as CSV or SHP and a regular data cube, use sits_get_data() to obtain training samples containing time series for selected locations in the training area.\nSelect a machine learning algorithm and use sits_train() to produce a classification model.\nGiven a classification model and a regular data cube, use sits_classify() to get a probability data cube, which contains the probabilities for class allocation for each pixel.\nRemove outliers in a probability data cube using sits_smooth().\nUse sits_label_classification to produce a thematic map from a smoothed probability cube.\n\n\n\n\n\nFigure 3: Main functions of the sits API (source: authors).\n\n\n\nEach workflow step corresponds to a function of the sits API, as shown in the Table below. These functions have convenient default parameters and behaviors. A single function builds machine learning (ML) models. The classification function processes big data cubes with efficient parallel processing. Since the sits API is simple to learn, achieving good results do not require in-depth knowledge about machine learning and parallel processing.\n\n\n\nThe sits API workflow for land classification.\n\nAPI_function\nInputs\nOutput\n\n\n\nsits_cube()\nARD image collection\nIrregular data cube\n\n\nsits_regularize()\nIrregular data cube\nRegular data cube\n\n\nsits_apply()\nRegular data cube\nRegular data cube with new bands and indices\n\n\nsits_get_data()\nRegular data cube and sample locations\nTime series samples\n\n\nsits_train()\nTime series and ML method\nML classification model\n\n\nsits_classify()\nML classification model and regular data cube\nProbability cube\n\n\nsits_smooth()\nProbability cube\nSmoothed probability cube\n\n\nsits_label_classification()\nSmoothed probability cube\nClassified map"
  },
  {
    "objectID": "introduction.html#creating-a-data-cube",
    "href": "introduction.html#creating-a-data-cube",
    "title": "\n1  Introduction\n",
    "section": "\n1.6 Creating a data cube",
    "text": "1.6 Creating a data cube\nThere are two kinds of data cubes in sits: (a) irregular data cubes generated by selecting image collections on cloud providers such as AWS and Planetary Computer; (b) regular data cubes with images fully covering a chosen area, where each image has the same spectral bands and spatial resolution, and images follow a set of adjacent and regular time intervals. Machine learning applications need regular data cubes. Please refer to Chapter Earth observation data cubes for further details.\nThe first steps in using sits are: (a) select an analysis-ready data image collection available in a cloud provider or stored locally using sits_cube(); (b) if the collection is not regular, use sits_regularize() to build a regular data cube.\nThis section shows how to build a data cube from local images already organized as a regular data cube. The data cube is composed of MODIS MOD13Q1 images for the region close to the city of Sinop in Mato Grosso, Brazil. This region is one of the world’s largest producers of soybeans. All images have indexes NDVI and EVI covering a one-year period from 2013-09-14 to 2014-08-29 (we use “year-month-day” for dates). There are 23 time instances, each covering a 16-day period. This data is available in the package sitsdata.\nTo build a data cube from local files, users must provide information about the original source from which the data was obtained. In this case, sits_cube() needs the parameters:\n\n\nsource, the cloud provider from where the data has been obtained (in this case, the Brazil Data Cube “BDC”);\n\ncollection, the collection of the cloud provider from where the images have been extracted. In this case, data comes from the MOD13Q1 collection 6;\n\ndata_dir, the local directory where the image files are stored;\n\nparse_info, a vector of strings stating how file names store information on “tile”, “band”, and “date”. In this case, local images are stored in files whose names are similar to TERRA_MODIS_012010_EVI_2014-07-28.tif. This file represents an image obtained by the MODIS sensor onboard the TERRA satellite, covering part of tile 012010 in the EVI band for date 2014-07-28.\n\n\n# load package \"tibble\"\nlibrary(tibble)\n# load packages \"sits\" and \"sitsdata\"\nlibrary(sits)\nlibrary(sitsdata)\n# Create a data cube using local files\nsinop_cube &lt;- sits_cube(\n  source = \"BDC\", \n  collection  = \"MOD13Q1-6.1\",\n  bands = c(\"NDVI\", \"EVI\"),\n  data_dir = system.file(\"extdata/sinop\", package = \"sitsdata\"),  \n  parse_info = c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\")\n)\n# Plot the NDVI for the first date (2013-09-14)\nplot(sinop_cube, \n     band = \"NDVI\", \n     dates = \"2013-09-14\",\n     palette = \"RdYlGn\")\n\n\n\nFigure 1.2: False color MODIS image for NDVI band in 2013-09-14.\n\n\n\nThe aim of the parse_info parameter is to extract tile, band, and date information from the file name. Given the large variation in image file names generated by different produces, it includes designators such as X1 and X2; these are place holders for parts of the file name that is not relevant to sits_cube().\nThe R object returned by sits_cube() contains the metadata describing the contents of the data cube. It includes data source and collection, satellite, sensor, tile in the collection, bounding box, projection, and list of files. Each file refers to one band of an image at one of the temporal instances of the cube.\n\n# Show the description of the data cube\nsinop_cube\n\n#&gt; # A tibble: 1 × 11\n#&gt;   source collection satellite sensor tile     xmin    xmax    ymin    ymax crs  \n#&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1 BDC    MOD13Q1-6… TERRA     MODIS  0120… -6.18e6 -5.96e6 -1.35e6 -1.23e6 \"PRO…\n#&gt; # ℹ 1 more variable: file_info &lt;list&gt;\n\n\nThe list of image files which make up the data cube is stored as a data frame in the column file_info. For each file, sits stores information about spectral band, reference date, size, spatial resolution, coordinate reference system, bounding box, path to file location and cloud cover information (when available).\n\n# Show information on the images files which are part of a data cube\nsinop_cube$file_info[[1]]\n\n#&gt; # A tibble: 69 × 13\n#&gt;    fid   band  date       nrows ncols  xres  yres      xmin      ymin      xmax\n#&gt;    &lt;chr&gt; &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 1     CLOUD 2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  2 1     EVI   2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  3 1     NDVI  2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  4 2     CLOUD 2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  5 2     EVI   2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  6 2     NDVI  2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  7 3     CLOUD 2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  8 3     EVI   2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt;  9 3     NDVI  2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt; 10 4     CLOUD 2013-11-01   551   944  232.  232. -6181982. -1353336. -5963298.\n#&gt; # ℹ 59 more rows\n#&gt; # ℹ 3 more variables: ymax &lt;dbl&gt;, crs &lt;chr&gt;, path &lt;chr&gt;\n\n\nA key attribute of a data cube is its timeline, as shown below. The command sits_timeline() lists the temporal references associated to sits objects, including samples, data cubes and models.\n\n# Show the R object that describes the data cube\nsits_timeline(sinop_cube)\n\n#&gt;  [1] \"2013-09-14\" \"2013-09-30\" \"2013-10-16\" \"2013-11-01\" \"2013-11-17\"\n#&gt;  [6] \"2013-12-03\" \"2013-12-19\" \"2014-01-01\" \"2014-01-17\" \"2014-02-02\"\n#&gt; [11] \"2014-02-18\" \"2014-03-06\" \"2014-03-22\" \"2014-04-07\" \"2014-04-23\"\n#&gt; [16] \"2014-05-09\" \"2014-05-25\" \"2014-06-10\" \"2014-06-26\" \"2014-07-12\"\n#&gt; [21] \"2014-07-28\" \"2014-08-13\" \"2014-08-29\"\n\n\nThe timeline of the sinop_cube data cube has 23 intervals with a temporal difference of 16 days. The chosen dates capture the agricultural calendar in Mato Grosso, Brazil. The agricultural year starts in September-October with the sowing of the summer crop (usually soybeans) which is harvested in February-March. Then the winter crop (mostly Corn, Cotton or Millet) is planted in March and harvested in June-July. For LULC classification, the training samples and the date cube should share a timeline with the same number of intervals and similar start and end dates."
  },
  {
    "objectID": "introduction.html#the-time-series-tibble",
    "href": "introduction.html#the-time-series-tibble",
    "title": "\n1  Introduction\n",
    "section": "\n1.7 The time series tibble",
    "text": "1.7 The time series tibble\nTo handle time series information, sits uses a tibble. Tibbles are extensions of the data.frame tabular data structures provided by the tidyverse set of packages. The example below shows a tibble with 1,837 time series obtained from MODIS MOD13Q1 images. Each series has four attributes: two bands (NIR and MIR) and two indexes (NDVI and EVI). This dataset is available in package sitsdata.\nThe time series tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band.\n\n# Load the MODIS samples for Mato Grosso from the \"sitsdata\" package\nlibrary(tibble)\nlibrary(sitsdata)\ndata(\"samples_matogrosso_mod13q1\", package = \"sitsdata\")\nsamples_matogrosso_mod13q1\n\n#&gt; # A tibble: 1,837 × 7\n#&gt;    longitude latitude start_date end_date   label   cube     time_series      \n#&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;list&gt;           \n#&gt;  1     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  2     -59.4    -9.31 2014-09-14 2015-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  3     -59.4    -9.31 2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  4     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  5     -55.2   -10.8  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  6     -51.9   -13.4  2014-09-14 2015-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  7     -56.0   -10.1  2005-09-14 2006-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  8     -54.6   -10.4  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt;  9     -52.5   -11.0  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt; 10     -52.1   -14.0  2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n#&gt; # ℹ 1,827 more rows\n\n\nThe timeline for all time series associated with the samples follows the same agricultural calendar, starting in September 14th and ending in August 28th. All samples contain 23 values, corresponding to the same temporal interval as those of the sinop data cube. Notice that that although the years for the samples are different, the samples for a given year follow the same agricultural calendar.\nThe time series can be displayed by showing the time_series column.\n\n# Load the time series for MODIS samples for Mato Grosso\nsamples_matogrosso_mod13q1[1,]$time_series[[1]]\n\n#&gt; # A tibble: 23 × 5\n#&gt;    Index       NDVI   EVI   NIR    MIR\n#&gt;    &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 2006-09-14 0.500 0.263 0.230 0.139 \n#&gt;  2 2006-09-30 0.485 0.330 0.359 0.161 \n#&gt;  3 2006-10-16 0.716 0.397 0.264 0.0757\n#&gt;  4 2006-11-01 0.654 0.415 0.332 0.124 \n#&gt;  5 2006-11-17 0.591 0.433 0.400 0.172 \n#&gt;  6 2006-12-03 0.662 0.439 0.348 0.125 \n#&gt;  7 2006-12-19 0.734 0.444 0.295 0.0784\n#&gt;  8 2007-01-01 0.739 0.502 0.348 0.0887\n#&gt;  9 2007-01-17 0.768 0.526 0.351 0.0761\n#&gt; 10 2007-02-02 0.797 0.550 0.355 0.0634\n#&gt; # ℹ 13 more rows\n\n\nThe distribution of samples per class can be obtained using the summary() command. The classification schema uses nine labels, four associated to crops (Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet), two with natural vegetation (Cerrado, Forest) and one to Pasture.\n\n# Load the MODIS samples for Mato Grosso from the \"sitsdata\" package\nsummary(samples_matogrosso_mod13q1)\n\n#&gt; # A tibble: 7 × 3\n#&gt;   label      count   prop\n#&gt;   &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1 Cerrado      379 0.206 \n#&gt; 2 Forest       131 0.0713\n#&gt; 3 Pasture      344 0.187 \n#&gt; 4 Soy_Corn     364 0.198 \n#&gt; 5 Soy_Cotton   352 0.192 \n#&gt; 6 Soy_Fallow    87 0.0474\n#&gt; 7 Soy_Millet   180 0.0980\n\n\nIt is helpful to plot the dispersion of the time series. In what follows, for brevity, we will filter only one label (Forest) and select one index (NDVI). Note that for filtering the label we use a function from dplyr package, while for selecting the index we use sits_select(). We use two different functions for selection because of they way metadata is stored in a samples files. The labels for the samples are listed in column label in the samples tibble, as shown above. In this case, one can use functions from the dplyr package to extract subsets. In particular, the function dplyr::filter retaining all rows that satisfy a given condition. In the above example, the result of dplyr::filter is the set of samples associated to the “Forest” label. The second selection involves obtaining only the values for the NDVI band. This operation requires access to the time_series column, which is stored as a list. In this case, selection with dplyr::filter will not work. To handle such cases, sits provides sits_select() to select subsets inside the time_series list.\n\n# select all samples with label \"Forest\"\nsamples_forest &lt;- dplyr::filter(\n    samples_matogrosso_mod13q1, \n    label == \"Forest\"\n) |&gt; \n# select the NDVI band for all samples with label \"Forest\"\nsits_select(\n    band = \"NDVI\"\n) |&gt; \nplot()\n\n\n\nFigure 1.3: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.4: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.5: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.6: Joint plot of all samples in band NDVI for label Forest.\n\n\n\nThe above figure shows all the time series associated with label Forest and band NDVI (in light blue), highlighting the median (shown in dark red) and the first and third quartiles (shown in brown). The spikes are noise caused by the presence of clouds."
  },
  {
    "objectID": "introduction.html#training-a-machine-learning-model",
    "href": "introduction.html#training-a-machine-learning-model",
    "title": "\n1  Introduction\n",
    "section": "\n1.8 Training a machine learning model",
    "text": "1.8 Training a machine learning model\nThe next step is to train a machine learning (ML) model using sits_train(). It takes two inputs, samples (a time series tibble) and ml_method (a function that implements a machine learning algorithm). The result is a model that is used for classification. Each ML algorithm requires specific parameters that are user-controllable. For novice users, sits provides default parameters that produce good results. Please see Chapter Machine learning for data cubes for more details.\nSince the time series data has four attributes (EVI, NDVI, NIR, and MIR) and the data cube images have only two, we select the NDVI and EVI values and use the resulting data for training. To build the classification model, we use a random forest model called by sits_rfor(). Results from the random forest model can vary between different runs, due to the stochastic nature of the algorithm, For this reason, in the code fragment below, we set the seed of R’s pseudo-random number generation explicitly to ensure the same results are produced for documentation purposes.\n\nset.seed(03022024)\n# Select the bands NDVI and EVI\nsamples_2bands &lt;- sits_select(\n    data = samples_matogrosso_mod13q1, \n    bands = c(\"NDVI\", \"EVI\")\n)\n# Train a random forest model\nrf_model &lt;- sits_train(\n    samples = samples_2bands, \n    ml_method = sits_rfor()\n)\n# Plot the most important variables of the model\nplot(rf_model)\n\n\n\nFigure 1.7: Most relevant variables of trained random forest model."
  },
  {
    "objectID": "introduction.html#data-cube-classification",
    "href": "introduction.html#data-cube-classification",
    "title": "\n1  Introduction\n",
    "section": "\n1.9 Data cube classification",
    "text": "1.9 Data cube classification\nAfter training the machine learning model, the next step is to classify the data cube using sits_classify(). This function produces a set of raster probability maps, one for each class. For each of these maps, the value of a pixel is proportional to the probability that it belongs to the class. This function has two mandatory parameters: data, the data cube or time series tibble to be classified; and ml_model, the trained ML model. Optional parameters include: (a) multicores, number of cores to be used; (b) memsize, RAM used in the classification; (c) output_dir, the directory where the classified raster files will be written. Details of the classification process are available in Chapter Image classification in data cubes.\n\n# Classify the raster image\nsinop_probs &lt;- sits_classify(\n    data = sinop_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = \"./tempdir/chp3\"\n)\n# Plot the probability cube for class Forest\nplot(sinop_probs, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nFigure 1.8: Probability map for class Forest.\n\n\n\nAfter completing the classification, we plot the probability maps for class Forest. Probability maps are helpful to visualize the degree of confidence the classifier assigns to the labels for each pixel. They can be used to produce uncertainty information and support active learning, as described in Chapter Image classification in data cubes."
  },
  {
    "objectID": "introduction.html#spatial-smoothing",
    "href": "introduction.html#spatial-smoothing",
    "title": "\n1  Introduction\n",
    "section": "\n1.10 Spatial smoothing",
    "text": "1.10 Spatial smoothing\nWhen working with big Earth observation data, there is much variability in each class. As a result, some pixels will be misclassified. These errors are more likely to occur in transition areas between classes. To address these problems, sits_smooth() takes a probability cube as input and uses the class probabilities of each pixel’s neighborhood to reduce labeling uncertainty. Plotting the smoothed probability map for class Forest shows that most outliers have been removed.\n\n# Perform spatial smoothing\nsinop_bayes &lt;- sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = \"./tempdir/chp3\"\n)\nplot(sinop_bayes, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nFigure 1.9: Smoothed probability map for class Forest."
  },
  {
    "objectID": "introduction.html#labeling-a-probability-data-cube",
    "href": "introduction.html#labeling-a-probability-data-cube",
    "title": "\n1  Introduction\n",
    "section": "\n1.11 Labeling a probability data cube",
    "text": "1.11 Labeling a probability data cube\nAfter removing outliers using local smoothing, the final classification map can be obtained using sits_label_classification(). This function assigns each pixel to the class with the highest probability.\n\n# Label the probability file \nsinop_map &lt;- sits_label_classification(\n    cube = sinop_bayes, \n    output_dir = \"./tempdir/chp3\"\n)\nplot(sinop_map)\n\n\n\nFigure 1.10: Classification map for Sinop.\n\n\n\nThe resulting classification files can be read by QGIS. Links to the associated files are available in the sinop_map object in the nested table file_info.\n\n# Show the location of the classification file\nsinop_map$file_info[[1]]\n\n#&gt; # A tibble: 1 × 12\n#&gt;   band  start_date end_date   ncols nrows  xres  yres      xmin     xmax    ymin\n#&gt;   &lt;chr&gt; &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 class 2013-09-14 2014-08-29   944   551  232.  232. -6181982.  -5.96e6 -1.35e6\n#&gt; # ℹ 2 more variables: ymax &lt;dbl&gt;, path &lt;chr&gt;"
  },
  {
    "objectID": "introduction.html#references",
    "href": "introduction.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nL. De Simone and P. Gennari, “Earth observations for official crop statistics in the context of scarcity of in-situ data,” Statistical Journal of the IAOS, vol. 38, no. 3, pp. 1009–1019, 2022, doi: 10.3233/SJI-220054.\n\n\n[2] \nL. De Simone, W. Ouellette, and P. Gennari, “Operational Use of EO Data for National Land Cover Official Statistics in Lesotho,” Remote Sensing, vol. 14, no. 14, p. 3294, 2022, doi: 10.3390/rs14143294.\n\n\n[3] \nC. E. Woodcock, T. R. Loveland, M. Herold, and M. E. Bauer, “Transitioning from change detection to monitoring with remote sensing: A paradigm shift,” Remote Sensing of Environment, vol. 238, p. 111558, 2020, doi: 10.1016/j.rse.2019.111558.\n\n\n[4] \nC. W. Tan, G. I. Webb, and F. Petitjean, “Indexing and classifying gigabytes of time series under time warping,” in Proceedings of the 2017 SIAM International Conference on Data Mining (SDM), Society for Industrial and Applied Mathematics, 2017, pp. 282–290.\n\n\n[5] \nA. Di Gregorio, “Land Cover Classification System - Classification concepts Software version 3,” FAO, 2016.\n\n\n[6] \nA. J. Comber, R. A. Wadsworth, and P. F. Fisher, “Using semantics to clarify the conceptual confusion between land cover and land use: The example of forest,” Journal of Land Use Science, vol. 3, no. 2–3, pp. 185–198, 2008."
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Authors",
    "text": "Authors\n\nGilberto Camara, INPE, Brazil\nRolf Simoes, Open Geo Hub, Netherlands\nFelipe Souza, INPE, Brazil\nFelipe Carlos, INPE, Brazil\nPedro Andrade, INPE, Brazil\nKarine Ferreira, INPE, Brazil\nLorena Santos, INPE, Brazil\nCharlotte Pelletier, Université Bretagne-Sud, France"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Introduction",
    "text": "Introduction\n\nWelcome to the age of big Earth observation data! Petabytes of images are now openly accessible in cloud services. Having free access to massive data sets, we need new methods to measure change on our planet using image data. An essential contribution of big EO data has been to provide access to image time series that capture signals from the same locations continually. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Better measurement of natural resources depletion caused by deforestation, forest degradation, and desertification is possible. Experts improve the production of agricultural statistics. Using image time series, analysts can use large data collections to detect subtle changes in ecosystem health and distinguish between various land classes more effectively. Time series analysis is an innovative way to address global challenges like climate change, biodiversity preservation, and sustainable agriculture.\nThis book introduces sits, an open-source R package of big Earth observation data analysis using satellite image time series. Users build regular data cubes from cloud services such as Amazon Web Services, Microsoft Planetary Computer, Copernicus Data Space Ecosystem, NASA Harmonized Landsat-Sentinel, Brazil Data Cube, Swiss Data Cube, Digital Earth Australia, and Digital Earth Africa. The sits API includes training sample quality measures, machine learning and deep learning classification algorithms, and Bayesian post-processing methods for smoothing and uncertainty assessment. To evaluate results, sits supports best practice accuracy assessments."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Welcome",
    "text": "Welcome\n\nWelcome to the age of big Earth observation data! Petabytes of images are now openly accessible in cloud services. Having free access to massive data sets, we need new methods to measure change on our planet using image data. An essential contribution of big EO data has been to provide access to image time series that capture signals from the same locations continually. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Better measurement of natural resources depletion caused by deforestation, forest degradation, and desertification is possible. Experts improve the production of agricultural statistics. Using image time series, analysts can use large data collections to detect subtle changes in ecosystem health and distinguish between various land classes more effectively. Time series analysis is an innovative way to address global challenges like climate change, biodiversity preservation, and sustainable agriculture.\nThis book introduces sits, an open-source R package of big Earth observation data analysis using satellite image time series. Users build regular data cubes from cloud services such as Amazon Web Services, Microsoft Planetary Computer, Copernicus Data Space Ecosystem, NASA Harmonized Landsat-Sentinel, Brazil Data Cube, Swiss Data Cube, Digital Earth Australia, and Digital Earth Africa. The sits API includes training sample quality measures, machine learning and deep learning classification algorithms, and Bayesian post-processing methods for smoothing and uncertainty assessment. To evaluate results, sits supports best practice accuracy assessments. The authors also provide a Python API that interfaces to the R API, and thus allow Python users to directly run sits and convert sits data structures to Python data.frames and Xarrays."
  },
  {
    "objectID": "setup.html#how-to-use-this-on-line-book",
    "href": "setup.html#how-to-use-this-on-line-book",
    "title": "Setup",
    "section": "How to use this on-line book",
    "text": "How to use this on-line book\nThis book contains reproducible code that can be run on an R environment. There are three options to setup your working environment:\n\nInstall R and RStudio and the packages required by sits, with specific procedures for each type of operating systems.\nUse a Docker image provided by the Brazil Data Cube.\nInstall sits and all its dependencies using conda."
  },
  {
    "objectID": "setup.html#how-to-install-sits-using-r-and-rstudio",
    "href": "setup.html#how-to-install-sits-using-r-and-rstudio",
    "title": "Setup",
    "section": "How to install sits using R and RStudio",
    "text": "How to install sits using R and RStudio\nWe suggest a staged installation, as follows:\n\nGet and install base R from CRAN.\nInstall RStudio from the Posit website.\n\nInstalling sits from CRAN\nThe Comprehensive R Archive Network (CRAN), a network of servers (also known as mirrors) from around the world that store up-to-date versions of basic code and packages for R. In what follows, we describe how to use CRAN to sits on Windows, Linux and MacOS.\nInstalling in Microsoft Windows and MacOS environments\nWindows and MacOS users are strongly encouraged to install binary packages from CRAN. The sits package relies on the sf and terra packages, which require the GDAL and PROJ libraries. Run RStudio and install binary packages sf and terra, in this order:\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\n\nAfter installing the binaries for sf and terra, install sits as follows;\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\nTo run the examples in the book, please also install sitsdata package, which is available from GitHub. It is necessary to use package devtools to install sitsdata.\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sitsdata\")\n\nTo install sits from source, please install Rtools for Windows to have access to the compiling environment. For Mac, please follow the instructions available here.\nInstalling in Ubuntu environments\nFor Ubuntu, the first step should be to install the latest version of the GDAL, GEOS, and PROJ4 libraries and binaries. To do so, use the repository ubuntugis-unstable, which should be done as follows:\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt-get update\nsudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev \nsudo apt-get install gdal-bin\nsudo apt-get install proj-bin\nGetting an error while adding this PPA repository could be due to the absence of the package software-properties-common. After installing GDAL, GEOS, and PROJ4, please install packages sf and terra:\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\n\nThen please proceed to install sits, which can be installed as a regular R package.\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\nInstalling in Debian environments\nFor Debian, use the rocker geospatial dockerfiles.\nInstalling in Fedora environments\nIn the case of Fedora, the following command installs all required dependencies:\n\nsudo dnf install gdal-devel proj-devel geos-devel sqlite-devel udunits2-devel"
  },
  {
    "objectID": "setup.html#using-docker-images",
    "href": "setup.html#using-docker-images",
    "title": "Setup",
    "section": "Using Docker images",
    "text": "Using Docker images\nIf you are familiar with Docker, there are images for sits available with RStudio or Jupyter notebook. Such images are provided by the Brazil Data Cube team:\n\n\nVersion for R and RStudio.\n\nVersion for Jupyter Notebooks.\n\nOn a Windows or Mac platform, install Docker and then obtain one of the two images listed above from the Brazil Data Cube. Both images contain the full sits running environment. When GDAL is running in docker containers, please add the security flag --security-opt seccomp=unconfined on start."
  },
  {
    "objectID": "setup.html#install-sits-from-conda",
    "href": "setup.html#install-sits-from-conda",
    "title": "Setup",
    "section": "Install sits from CONDA",
    "text": "Install sits from CONDA\nConda is an open-source, cross-platform package manager. It is a convenient way to installl Python and R packages. To use conda, first download the software from the CONDA website. After installation, use conda to install sits from the terminal as follows:\n\n# add conda-forge to the download channels \nconda config --add channels conda-forge\nconda config --set channel_priority strict\n# install sits using conda\nconda install conda-forge::r-sits\n\nThe conda installer will download all packages and libraries required to run sits. This is the easiest way to install sits on Windows."
  },
  {
    "objectID": "setup.html#accessing-the-development-version",
    "href": "setup.html#accessing-the-development-version",
    "title": "Setup",
    "section": "Accessing the development version",
    "text": "Accessing the development version\nThe source code repository of sits is on GitHub. There are two versions available on GitHub: master and dev. The master contains the current stable version, which is either the same code available in CRAN or a minor update with bug fixes. To install the master version, install devtools (if not already available) and do as follows:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sits\", dependencies = TRUE)\n\nTo install the dev (development) version, which contains the latest updates but might be unstable, install devtools (if not already available), and then install sits as follows:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sits@dev\", dependencies = TRUE)"
  },
  {
    "objectID": "setup.html#additional-requirements",
    "href": "setup.html#additional-requirements",
    "title": "Setup",
    "section": "Additional requirements",
    "text": "Additional requirements\nTo run the examples in the book, please also install the sitsdata package. We recommend installing it using wget. See instructions in the GNU Wget site.\n\noptions(download.file.method = \"wget\")\ndevtools::install_github(\"e-sensing/sitsdata\")"
  },
  {
    "objectID": "setup.html#using-gpus-with-sits",
    "href": "setup.html#using-gpus-with-sits",
    "title": "Setup",
    "section": "Using GPUs with sits\n",
    "text": "Using GPUs with sits\n\nThe torch package automatically recognizes if a GPU is available on the machine and uses it for training and classification. There is a significant performance gain when GPUs are used instead of CPUs for deep learning models. There is no need for specific adjustments to torch scripts. To use GPUs, torch requires version 11.6 of the CUDA library, which is available for Ubuntu 18.04 and 20.04. Please follow the detailed instructions for setting up torch available here.\n\ninstall.packages(\"torch\")"
  },
  {
    "objectID": "acknowledgements.html#funding-sources",
    "href": "acknowledgements.html#funding-sources",
    "title": "Acknowledgements",
    "section": "Funding Sources",
    "text": "Funding Sources\nThe authors acknowledge the funders that supported the development of sits:\n\nAmazon Fund, established by Brazil with financial contribution from Norway, through contract 17.2.0536.1. between the Brazilian Development Bank (BNDES) and the Foundation for Science, Technology, and Space Applications (FUNCATE), for the establishment of the Brazil Data Cube.\nCoordenação de Aperfeiçoamento de Pessoal de Nível Superior-Brasil (CAPES) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) for grants 312151/2014-4 and 140684/2016-6.\nSao Paulo Research Foundation (FAPESP) under eScience Program grant 2014/08398-6, for providing MSc, PhD, and post-doc scholarships, equipment, and travel support.\nInternational Climate Initiative of the Germany Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (IKI) under grant 17-III-084-Global-A-RESTORE+ (“RESTORE+: Addressing Landscape Restoration on Degraded Land in Indonesia and Brazil”).\nMicrosoft Planetary Computer initiative under the GEO-Microsoft Cloud Computer Grants Programme.\nInstituto Clima e Sociedade, under the project grant “Modernization of PRODES and DETER Amazon monitoring systems”.\nOpen-Earth-Monitor Cyberinfrastructure project, which has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No. 101059548.\nFAO-EOSTAT initiative, which uses next generation Earth observation tools to produce land cover and land use statistics."
  },
  {
    "objectID": "acknowledgements.html#community-contributions",
    "href": "acknowledgements.html#community-contributions",
    "title": "Acknowledgements",
    "section": "Community Contributions",
    "text": "Community Contributions\nThe authors thank the R-spatial community for their foundational work, including Marius Appel, Tim Appelhans, Robert Hijmans, Jakub Nowosad, Edzer Pebesma, and Martijn Tennekes for their R packages gdalcubes, leafem, terra, supercells, sf/stars, and tmap. We are grateful for the work of Dirk Eddelbuettel on Rcpp and RcppArmadillo and Ron Wehrens in package kohonen. We are much indebted to Hadley Wickham for the tidyverse, Daniel Falbel for the torch and luz packages, and the RStudio team for package leaflet. The multiple authors of machine learning packages randomForest, e1071, and xgboost provided robust algorithms. We would like to thank Python developers who shared their deep learning algorithms for image time series classification: Vivien Sainte Fare Garnot, Zhiguang Wang, Maja Schneider, and Marc Rußwurm. The first author also thanks Roger Bivand for his benign influence in all things related to R."
  },
  {
    "objectID": "acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "href": "acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "title": "Acknowledgements",
    "section": "Reproducible papers and books used in building sits",
    "text": "Reproducible papers and books used in building sits\nWe thank the authors of the following papers for making their code and papers open and reusable. Their contribution has been essential to build sits.\n\nEdzer Pebesma, Simple Features for R: Standardized Support for Spatial Vector Data. R Journal, 10(1), 2018.\nMartin Tennekes, tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39, 2018.\nRon Wehrens and Johannes Kruisselbrink, Flexible Self-Organising Maps in kohonen 3.0. Journal of Statistical Software, 87, 7, 2018.\nHassan Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller, Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4): 917–963, 2019.\nCharlotte Pelletier, Geoffrey Webb, and Francois Petitjean. Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series. Remote Sensing 11 (5), 2019.\nMarc Rußwurm, Charlotte Pelletier, Maximilian Zollner, Sèbastien Lefèvre, and Marco Körner, Breizhcrops: a Time Series Dataset for Crop Type Mapping. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences ISPRS, 2020.\nMarius Appel and Edzer Pebesma, On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library. Data 4 (3): 1–16, 2020.\nVivien Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata, Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention, Conference on Computer Vision and Pattern Recognition, 2020.\nVivien Garnot and Loic Landrieu, Lightweight Temporal Self-Attention for Classifying Satellite Images Time Series, 2020.\nMaja Schneider, Marco Körner, Re: Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention ReScience C 7 (2), 2021.\nRolf Simoes, Felipe Souza, Mateus Zaglia, Gilberto Queiroz, Rafael dos Santos and Karine Ferreira, Rstac: An R Package to Access Spatiotemporal Asset Catalog Satellite Imagery. IGARSS, 2021, pp. 7674-7677.\nJakub Nowosad, Tomasz Stepinksi, Extended SLIC superpixels algorithm for applications to non-imagery geospatial rasters. International Journal of Applied Earth Observations and Geoinformation, 2022.\nSigrid Keydana, Deep Learning and Scientific Computing with R torch, Chapman and Hall/CRC, London, 2023.\nRobin Lovelace, Jakub Nowosad, Jannes Münchow, Geocomputation with R. Chapman and Hall/CRC, London, 2023.\nEdzer Pebesma, Roger Bivand, Spatial Data Science: With applications in R. Chapman and Hall/CRC, London, 2023."
  },
  {
    "objectID": "acknowledgements.html#publications-using-sits",
    "href": "acknowledgements.html#publications-using-sits",
    "title": "Acknowledgements",
    "section": "Publications using sits",
    "text": "Publications using sits\nThis section gathers the publications that have used sits to generate their results.\n2024\n\nGiuliani, Gregory. Time-First Approach for Land Cover Mapping Using Big Earth Observation Data Time-Series in a Data Cube – a Case Study from the Lake Geneva Region (Switzerland). Big Earth Data, 2024.\nWerner, João, Mariana Belgiu et al., Mapping Integrated Crop–Livestock Systems Using Fused Sentinel-2 and PlanetScope Time Series and Deep Learning. Remote Sensing 16, no. 8 (January 2024): 1421.\n\n2023\n\nHadi, Firman, Laode Muhammad Sabri, Yudo Prasetyo, and Bambang Sudarsono. Leveraging Time-Series Imageries and Open Source Tools for Enhanced Land Cover Classification. In IOP Conference Series: Earth and Environmental Science, 1276:012035. IOP Publishing, 2023.\nBruno Adorno, Thales Körting, and Silvana Amaral, Contribution of time-series data cubes to classify urban vegetation types by remote sensing. Urban Forest & Urban Greening, 79, 127817, 2023.\n\n2021\n\nLorena Santos, Karine R. Ferreira, Gilberto Camara, Michelle Picoli, and Rolf Simoes, Quality control and class noise reduction of satellite image time series. ISPRS Journal of Photogrammetry and Remote Sensing, 177, 75–88, 2021.\nLorena Santos, Karine Ferreira, Michelle Picoli, Gilberto Camara, Raul Zurita-Milla and Ellen-Wien Augustijn, Identifying Spatiotemporal Patterns in Land Use and Cover Samples from Satellite Image Time Series. Remote Sensing, 13(5), 974, 2021.\n\n2020\n\nRolf Simoes, Michelle Picoli, Gilberto Camara, Adeline Maciel, Lorena Santos, Pedro Andrade, Alber Sánchez, Karine Ferreira, and Alexandre Carvalho, Land use and cover maps for Mato Grosso State in Brazil from 2001 to 2017. Nature Scientific Data, 7, article 34, 2020.\nMichelle Picoli, Ana Rorato, Pedro Leitão, Gilberto Camara, Adeline Maciel, Patrick Hostert, and Ieda Sanches, Impacts of Public and Private Sector Policies on Soybean and Pasture Expansion in Mato Grosso—Brazil from 2001 to 2017. Land, 9(1), 2020.\nKarine Ferreira, Gilberto Queiroz et al., Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products. Remote Sensing, 12, 4033, 2020.\nAdeline Maciel, Lubia Vinhas, Michelle Picoli, and Gilberto Camara, Identifying Land Use Change Trajectories in Brazil’s Agricultural Frontier. Land, 9, 506, 2020.\n\n2018\n\nMichelle Picoli, Gilberto Camara, et al., Big Earth Observation Time Series Analysis for Monitoring Brazilian Agriculture. ISPRS Journal of Photogrammetry and Remote Sensing, 145, 328–339, 2018."
  },
  {
    "objectID": "acknowledgements.html#ai-support-in-preparing-the-book",
    "href": "acknowledgements.html#ai-support-in-preparing-the-book",
    "title": "Acknowledgements",
    "section": "AI support in preparing the book",
    "text": "AI support in preparing the book\nThe authors have use Generative AI tools (Chat-GPT, Grammarly and ProWritingAid) to improve readability and language of the work. The core technical and scientific content of the book has been prepared exclusively by the authors. Assistance from Generative AI has been limited to improving definitions and making the text easier to follow."
  },
  {
    "objectID": "intro_quicktour.html#creating-a-data-cube",
    "href": "intro_quicktour.html#creating-a-data-cube",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.1 Creating a data cube",
    "text": "1.1 Creating a data cube\nThere are two kinds of data cubes in sits: (a) irregular data cubes generated by selecting image collections on cloud providers such as AWS and Planetary Computer; (b) regular data cubes with images fully covering a chosen area, where each image has the same spectral bands and spatial resolution, and images follow a set of adjacent and regular time intervals. Machine learning applications need regular data cubes. Please refer to Chapter Earth observation data cubes for further details.\nThe first step in using sits is configuring the environment to for running R and Python together.\n\n\nR\nPython\n\n\n\n\n# load package \"tibble\"\nlibrary(tibble)\n# load packages \"sits\" and \"sitsdata\"\nlibrary(sits)\nlibrary(sitsdata)\n# set tempdir if it does not exist \ntempdir_r &lt;- \"~/sitsbook/tempdir/R/intro_quicktour\"\ndir.create(tempdir_r, showWarnings = FALSE)\n\n\n\n\nfrom pysits import *\nimport pandas as pd\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.max_rows\", 4)\n# set data directory\nsitsdata_dir = \"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/sitsdata/\"\n# set tempdir if it does not exist \nfrom pathlib import Path\nhome = str(Path.home())\ntempdir_py = home + \"/sitsbook/tempdir/Python/intro_quicktour\"\nPath(tempdir_py).mkdir(parents=True, exist_ok=True)\n\n\n\n\nThe first steps in using sits are: (a) select an analysis-ready data image collection available in a cloud provider or stored locally using sits_cube(); (b) if the collection is not regular, use sits_regularize() to build a regular data cube.\nThis example builds a data cube from local images already organized as a regular data cube available in the Brazil Data Cube (“BDC”). The data cube is composed of MODIS MOD13Q1 images for the region close to the city of Sinop in Mato Grosso, Brazil. This region is one of the world’s largest producers of soybeans. All images have indexes NDVI and EVI covering a one-year period from 2013-09-14 to 2014-08-29 (we use “year-month-day” for dates). There are 23 time instances, each covering a 16-day period. We first define the region of interest for the Sinop city in Mato Grosso, Brazil, and then define a data cube in the BDC repository based on this region.\n\n\nR\nPython\n\n\n\n\n# Define a region of interest for the city of Sinop\nroi_sinop &lt;- c(\"lon_min\" = -56.87417,\n               \"lon_max\" = -54.63718,\n               \"lat_min\" = -12.17083,\n               \"lat_max\" = -11.02292)\n# Define a data cube in the BDC repository based on the ROI\nbdc_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands = c(\"NDVI\", \"EVI\"),\n    roi = roi_sinop,\n    start_date = \"2013-09-14\",\n    end_date = \"2014-08-29\"\n)\n\n\n\n\n# Define a region of interest for the city of Sinop\nroi_sinop = {  \"lon_min\" : -56.87417,\n               \"lon_max\" : -54.63718,\n               \"lat_min\" : -12.17083,\n               \"lat_max\" : -11.02292}\n# Define a data cube in the BDC repository based on the ROI\nbdc_cube = sits_cube(\n    source = \"BDC\", \n    collection  = \"MOD13Q1-6.1\",\n    bands = [\"NDVI\", \"EVI\"],\n    roi = roi_sinop,\n    start_date = \"2013-09-14\",\n    end_date = \"2014-08-29\"\n)\n\n\n\n\nThe next step is to copy the data cube to a local directory for further processing. When using a region of interest to select a part of an ARD collection, sits intersects the region with the tiles of such collection. Thus, when one wants to get a subset of a tile, it is better to copy this subset to the local computer.\n\n\nR\nPython\n\n\n\n\n# Copy the region of interest to a local directory\nsinop_cube &lt;- sits_cube_copy(\n    cube = bdc_cube,\n    roi = roi_sinop,\n    output_dir = tempdir_r\n)\n# Plot the NDVI for the first date (2013-09-14)\nplot(sinop_cube, \n     band = \"NDVI\", \n     dates = \"2013-09-14\",\n     palette = \"RdYlGn\")\n\n\n\nFigure 1.1: False color MODIS image for NDVI band in 2013-09-14.\n\n\n\n\n\n\n# Copy the region of interest to a local directory\nsinop_cube = sits_cube_copy(\n    cube = bdc_cube,\n    roi = roi_sinop,\n    output_dir = tempdir_py\n)\n# Plot the NDVI for the first date (2013-09-14)\nplot(sinop_cube, \n     band = \"NDVI\", \n     dates = \"2013-09-14\",\n     palette = \"RdYlGn\")\n\n\n\nFigure 1.2: False color MODIS image for NDVI band in 2013-09-14.\n\n\n\n\n\n\nThe object returned by sits_cube() and by sits_cube_copy contains the metadata describing the contents of the data cube. It includes data source and collection, satellite, sensor, tile in the collection, bounding box, projection, and list of files. Each file refers to one band of an image at one of the temporal instances of the cube.\n\n\nR\nPython\n\n\n\n\n# Show the description of the data cube\nsinop_cube\n\n# A tibble: 1 × 12\n  source collection satellite sensor tile     xmin    xmax    ymin    ymax crs  \n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n1 BDC    MOD13Q1-6… TERRA     MODIS  0120… -6.21e6 -5.94e6 -1.35e6 -1.23e6 \"PRO…\n# ℹ 2 more variables: labels &lt;list&gt;, file_info &lt;list&gt;\n\n\n\n\n\n# Show the description of the data cube\nsinop_cube\n\n  source   collection satellite sensor    tile          xmin          xmax  \\\n1    BDC  MOD13Q1-6.1     TERRA  MODIS  012010 -6.207232e+06 -5.938974e+06   \n\n           ymin          ymax  \\\n1 -1.353105e+06 -1.225925e+06   \n\n                                                 crs  \\\n1  PROJCRS[\"unnamed\",\\n    BASEGEOGCRS[\"Unknown d...   \n\n                    file_info  \n1  NestedDataFrame(size = 46)  \n\n\n\n\n\nThe list of image files which make up the data cube is stored as a data frame in the column file_info. For each file, sits stores information about spectral band, reference date, size, spatial resolution, coordinate reference system, bounding box, path to file location and cloud cover information (when available).\n\n\nR\nPython\n\n\n\n\n# Show information on the images files which are part of a data cube\nsinop_cube$file_info[[1]]\n\n# A tibble: 46 × 13\n   fid          band  date       ncols nrows  xres  yres    xmin    xmax    ymin\n   &lt;chr&gt;        &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 MOD13Q1.A20… EVI   2013-09-14  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 2 MOD13Q1.A20… NDVI  2013-09-14  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 3 MOD13Q1.A20… EVI   2013-09-30  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 4 MOD13Q1.A20… NDVI  2013-09-30  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 5 MOD13Q1.A20… EVI   2013-10-16  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 6 MOD13Q1.A20… NDVI  2013-10-16  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 7 MOD13Q1.A20… EVI   2013-11-01  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 8 MOD13Q1.A20… NDVI  2013-11-01  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n 9 MOD13Q1.A20… EVI   2013-11-17  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n10 MOD13Q1.A20… NDVI  2013-11-17  1158   549  232.  232. -6.21e6 -5.94e6 -1.35e6\n# ℹ 36 more rows\n# ℹ 3 more variables: ymax &lt;dbl&gt;, crs &lt;chr&gt;, path &lt;chr&gt;\n\n\n\n\n\n# Show information on the images files which are part of a data cube\nsinop_cube[\"file_info\"][0]\n\n&lt;string&gt;:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n                                          fid  band       date   ncols  nrows  \\\n1   MOD13Q1.A2013257.h12v10.061.2021240012413   EVI 2013-09-14  1158.0  549.0   \n2   MOD13Q1.A2013257.h12v10.061.2021240012413  NDVI 2013-09-14  1158.0  549.0   \n..                                        ...   ...        ...     ...    ...   \n45  MOD13Q1.A2014241.h12v10.061.2021262010833   EVI 2014-08-29  1158.0  549.0   \n46  MOD13Q1.A2014241.h12v10.061.2021262010833  NDVI 2014-08-29  1158.0  549.0   \n\n          xres        yres          xmin          xmax          ymin  \\\n1   231.656358  231.656358 -6.207232e+06 -5.938974e+06 -1.353105e+06   \n2   231.656358  231.656358 -6.207232e+06 -5.938974e+06 -1.353105e+06   \n..         ...         ...           ...           ...           ...   \n45  231.656358  231.656358 -6.207232e+06 -5.938974e+06 -1.353105e+06   \n46  231.656358  231.656358 -6.207232e+06 -5.938974e+06 -1.353105e+06   \n\n            ymax                                                crs  \\\n1  -1.225925e+06  PROJCRS[\"unnamed\",\\n    BASEGEOGCRS[\"Unknown d...   \n2  -1.225925e+06  PROJCRS[\"unnamed\",\\n    BASEGEOGCRS[\"Unknown d...   \n..           ...                                                ...   \n45 -1.225925e+06  PROJCRS[\"unnamed\",\\n    BASEGEOGCRS[\"Unknown d...   \n46 -1.225925e+06  PROJCRS[\"unnamed\",\\n    BASEGEOGCRS[\"Unknown d...   \n\n                                                 path  \n1   /Users/gilbertocamara/sitsbook/tempdir/Python/...  \n2   /Users/gilbertocamara/sitsbook/tempdir/Python/...  \n..                                                ...  \n45  /Users/gilbertocamara/sitsbook/tempdir/Python/...  \n46  /Users/gilbertocamara/sitsbook/tempdir/Python/...  \n\n[46 rows x 13 columns]\n\n\n\n\n\nA key attribute of a data cube is its timeline, as shown below. The command sits_timeline() lists the temporal references associated to sits objects, including samples, data cubes and models.\n\n\nR\nPython\n\n\n\n\n# Show the R object that describes the data cube\nsits_timeline(sinop_cube)\n\n [1] \"2013-09-14\" \"2013-09-30\" \"2013-10-16\" \"2013-11-01\" \"2013-11-17\"\n [6] \"2013-12-03\" \"2013-12-19\" \"2014-01-01\" \"2014-01-17\" \"2014-02-02\"\n[11] \"2014-02-18\" \"2014-03-06\" \"2014-03-22\" \"2014-04-07\" \"2014-04-23\"\n[16] \"2014-05-09\" \"2014-05-25\" \"2014-06-10\" \"2014-06-26\" \"2014-07-12\"\n[21] \"2014-07-28\" \"2014-08-13\" \"2014-08-29\"\n\n\n\n\n\n# Show the R object that describes the data cube\nsits_timeline(sinop_cube)\n\n['2013-09-14', '2013-09-30', '2013-10-16', '2013-11-01', '2013-11-17', '2013-12-03', '2013-12-19', '2014-01-01', '2014-01-17', '2014-02-02', '2014-02-18', '2014-03-06', '2014-03-22', '2014-04-07', '2014-04-23', '2014-05-09', '2014-05-25', '2014-06-10', '2014-06-26', '2014-07-12', '2014-07-28', '2014-08-13', '2014-08-29']\n\n\n\n\n\nThe timeline of the sinop_cube data cube has 23 intervals with a temporal difference of 16 days. The chosen dates capture the agricultural calendar in Mato Grosso, Brazil. The agricultural year starts in September-October with the sowing of the summer crop (usually soybeans) which is harvested in February-March. Then the winter crop (mostly Corn, Cotton or Millet) is planted in March and harvested in June-July. For LULC classification, the training samples and the date cube should share a timeline with the same number of intervals and similar start and end dates."
  },
  {
    "objectID": "intro_quicktour.html#the-time-series-tibble",
    "href": "intro_quicktour.html#the-time-series-tibble",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.2 The time series tibble",
    "text": "1.2 The time series tibble\nTo handle time series information, sits uses a tibble. Tibbles are extensions of the data.frame tabular data structures provided by the tidyverse set of packages. In this chapter, we will use a training set with 1,101 time series obtained from MODIS MOD13Q1 images. Each series has two indexes (NDVI and EVI). To obtain the time series, we need two inputs:\n\nA CSV file, shapefile or a data.frame with contains information on the location of the sample and date of validity. The following information is required: longitude, latitude, start_date, end_date, class. It can either be provided as columns of a data.frame or a CSV file, or as attributes of a shapefile. Please refer to Chapter “Working with time series” for more information.\nA regular data cube which covers the dates indicated in the sample file. Each samples will be located in the data cube based on its longitude and latitude, and the time series is extracted based on the start and end dates and on the available bands of the cube.\n\nIt what follows, we use a data frame with samples that was produced by the Brazilian Agricultural Research Agency (EMBRAPA). In general, it is necessary to regularize the data cube so that all time series have the same dates. In our case, we use a regular data cube provided by the BDC repository.\n\n\nR\nPython\n\n\n\n\n# Load the MODIS samples for Mato Grosso from the \"sitsdata\" package\nlibrary(tibble)\nlibrary(sitsdata)\n# select the directory for the samples \nsamples_dir &lt;- system.file(\"data\", package = \"sitsdata\")\n# retrieve a data.frame with the samples\ndf_samples_matogrosso_modis &lt;- readRDS(file.path(samples_dir, \"df_samples_matogrosso_modis.rds\"))\ndf_samples_matogrosso_modis\n\n# A tibble: 1,351 × 6\n      id longitude latitude start_date end_date   label  \n   &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;  \n 1     1     -57.8    -9.76 2013-09-14 2014-08-29 Pasture\n 2     2     -59.4    -9.31 2013-09-14 2014-08-29 Pasture\n 3     3     -59.4    -9.31 2013-09-14 2014-08-29 Pasture\n 4     4     -57.8    -9.76 2013-09-14 2014-08-29 Pasture\n 5     5     -55.2   -10.8  2013-09-14 2014-08-29 Pasture\n 6     6     -51.9   -13.4  2013-09-14 2014-08-29 Pasture\n 7     7     -56.0   -10.1  2013-09-14 2014-08-29 Pasture\n 8     8     -54.6   -10.4  2013-09-14 2014-08-29 Pasture\n 9     9     -52.5   -11.0  2013-09-14 2014-08-29 Pasture\n10    10     -52.1   -14.0  2013-09-14 2014-08-29 Pasture\n# ℹ 1,341 more rows\n\n\n\n\n\n# Load the MODIS samples for Mato Grosso from the \"sitsdata\" package\nsamples_file = sitsdata_dir + \"/data/samples_matogrosso_modis.rds\"\nsamples_matogrosso_modis = read_sits_rds(samples_file)\nsamples_matogrosso_modis\n\n\n\n\nThe data.frame contains spatial and temporal information and the label assigned to the sample. Bsaed on this information. we will retrieve the time series from the BDC cube using sits_get_data().\n\n\nR\nPython\n\n\n\n\n# Retrieve the time series for each samples based on a data.frame\nsamples_matogrosso_modis &lt;- sits_get_data(\n    cube = bdc_cube,\n    samples = df_samples_matogrosso_modis\n)\n\nalthough coordinates are longitude/latitude, st_union assumes that they are\nplanar\n\n\n\n\n\n# Retrieve the time series for each samples based on a data.frame\nsamples_matogrosso_modis = sits_get_data(\n    cube = bdc_cube,\n    samples = df_samples_matogrosso_modis\n)\n\n\n\n\nThe time series tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band. The time series can be displayed by showing the time_series nested column.\n\n\nR\nPython\n\n\n\n\n# Load the time series for the first MODIS sample for Mato Grosso\nsamples_matogrosso_modis[1,]$time_series[[1]]\n\n# A tibble: 23 × 3\n   Index        EVI  NDVI\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2013-09-14 0.277 0.484\n 2 2013-09-30 0.334 0.485\n 3 2013-10-16 0.361 0.610\n 4 2013-11-01 0.380 0.510\n 5 2013-11-17 0.464 0.630\n 6 2013-12-03 0.505 0.618\n 7 2013-12-19 0.482 0.605\n 8 2014-01-01 0.456 0.542\n 9 2014-01-17 0.466 0.682\n10 2014-02-02 0.425 0.464\n# ℹ 13 more rows\n\n\n\n\n\n# Load the time series for the first MODIS sample for Mato Grosso\nsamples_matogrosso_modis[\"time_series\"][1]\n\n\n\n\nThe distribution of samples per class can be obtained using the summary() command. The classification schema uses nine labels, four associated to crops (Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet), two with natural vegetation (Cerrado, Forest) and one to Pasture.\n\n\nR\nPython\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_matogrosso_modis)\n\n# A tibble: 7 × 3\n  label      count   prop\n  &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado       34 0.0307\n2 Forest        23 0.0208\n3 Pasture      237 0.214 \n4 Soy_Corn     285 0.258 \n5 Soy_Cotton   285 0.258 \n6 Soy_Fallow    87 0.0787\n7 Soy_Millet   155 0.140 \n\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_matogrosso_modis)\n\n\n\n\nIt is helpful to plot the dispersion of the time series. In what follows, for brevity, we will filter only one label (Forest) and select one index (NDVI).\n\n\nR\nPython\n\n\n\n\n# Select all samples with label \"Forest\" using `dplyr::filter`\n# since the label attribute is a column of the samples data.frame\nsamples_forest &lt;- dplyr::filter(\n    samples_matogrosso_modis, \n    label == \"Forest\"\n) |&gt; \n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsits_select(\n    bands = \"NDVI\"\n) |&gt; \nplot()\n\n\n\nFigure 1.3: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.4: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.5: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.6: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.7: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.8: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.9: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.10: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.11: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.12: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.13: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.14: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.15: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.16: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.17: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.18: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.19: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.20: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.21: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.22: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.23: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.24: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\nFigure 1.25: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\n\n# Select all samples with label \"Forest\" using `query` \n# since the label attribute is a column of the samples data.frame\nsamples_forest = samples_matogrosso_modis.query('label == \"Forest\"')\n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsamples_forest_ndvi = sits_select(samples_forest, bands = \"NDVI\")\n# plot the samples for label Forest and band NDVI\nplot(samples_forest_ndvi)\n\n\n\n\nThe above figure shows all the time series associated with label Forest and band NDVI (in light blue), highlighting the median (shown in dark red) and the first and third quartiles (shown in brown). The spikes are noise caused by the presence of clouds."
  },
  {
    "objectID": "intro_quicktour.html#training-a-machine-learning-model",
    "href": "intro_quicktour.html#training-a-machine-learning-model",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.3 Training a machine learning model",
    "text": "1.3 Training a machine learning model\nThe next step is to train a machine learning (ML) model using sits_train(). It takes two inputs, samples (a time series tibble) and ml_method (a function that implements a machine learning algorithm). The result is a model that is used for classification. Each ML algorithm requires specific parameters that are user-controllable. For novice users, sits provides default parameters that produce good results. Please see Chapter Machine learning for data cubes for more details.\nTo build the classification model, we use a random forest model called by sits_rfor(). Results from the random forest model can vary between different runs, due to the stochastic nature of the algorithm, For this reason, in the code fragment below, we set the seed of R’s pseudo-random number generation explicitly to ensure the same results are produced for documentation purposes.\n\n\nR\nPython\n\n\n\n\nset.seed(03022024)\n# Train a random forest model\nrf_model &lt;- sits_train(\n    samples = samples_matogrosso_modis, \n    ml_method = sits_rfor()\n)\n# Plot the most important variables of the model\nplot(rf_model)\n\n\n\nFigure 1.26: Most relevant variables of trained random forest model.\n\n\n\n\n\n\n# Train a random forest model\nrf_model = sits_train(\n    samples = samples_matogrosso_modis, \n    ml_method = sits_rfor()\n)\n# Plot the most important variables of the model\nplot(rf_model)"
  },
  {
    "objectID": "intro_quicktour.html#data-cube-classification",
    "href": "intro_quicktour.html#data-cube-classification",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.4 Data cube classification",
    "text": "1.4 Data cube classification\nAfter training the machine learning model, the next step is to classify the data cube using sits_classify(). This function produces a set of raster probability maps, one for each class. For each of these maps, the value of a pixel is proportional to the probability that it belongs to the class. This function has two mandatory parameters: data, the data cube or time series tibble to be classified; and ml_model, the trained ML model. Optional parameters include: (a) multicores, number of cores to be used; (b) memsize, RAM used in the classification; (c) output_dir, the directory where the classified raster files will be written. Details of the classification process are available in Chapter Image classification in data cubes.\n\n\nR\nPython\n\n\n\n\n# Classify the raster image\nsinop_probs &lt;- sits_classify(\n    data = sinop_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n# Plot the probability cube for class Forest\nplot(sinop_probs, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nFigure 1.27: Probability map for class Forest.\n\n\n\n\n\n\n# Classify the raster image\nsinop_probs = sits_classify(\n    data = sinop_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\n# Plot the probability cube for class Forest\nplot(sinop_probs, labels = \"Forest\", palette = \"BuGn\")\n\n\n\n\nAfter completing the classification, we plot the probability maps for class Forest. Probability maps are helpful to visualize the degree of confidence the classifier assigns to the labels for each pixel. They can be used to produce uncertainty information and support active learning, as described in Chapter Image classification in data cubes."
  },
  {
    "objectID": "intro_quicktour.html#spatial-smoothing",
    "href": "intro_quicktour.html#spatial-smoothing",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.5 Spatial smoothing",
    "text": "1.5 Spatial smoothing\nWhen working with big Earth observation data, there is much variability in each class. As a result, some pixels will be misclassified. These errors are more likely to occur in transition areas between classes. To address these problems, sits_smooth() takes a probability cube as input and uses the class probabilities of each pixel’s neighborhood to reduce labeling uncertainty. Plotting the smoothed probability map for class Forest shows that most outliers have been removed.\n\n\nR\nPython\n\n\n\n\n# Perform spatial smoothing\nsinop_bayes &lt;- sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\nplot(sinop_bayes, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nFigure 1.28: Smoothed probability map for class Forest.\n\n\n\n\n\n\n# Perform spatial smoothing\nsinop_bayes = sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\nplot(sinop_bayes, labels = \"Forest\", palette = \"BuGn\")"
  },
  {
    "objectID": "intro_quicktour.html#labeling-a-probability-data-cube",
    "href": "intro_quicktour.html#labeling-a-probability-data-cube",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.6 Labeling a probability data cube",
    "text": "1.6 Labeling a probability data cube\nAfter removing outliers using local smoothing, the final classification map can be obtained using sits_label_classification(). This function assigns each pixel to the class with the highest probability.\n\n\nR\nPython\n\n\n\n\n# Label the probability file \nsinop_map &lt;- sits_label_classification(\n    cube = sinop_bayes, \n    output_dir = tempdir_r\n)\nplot(sinop_map)\n\n\n\nFigure 1.29: Classification map for Sinop.\n\n\n\n\n\n\n# Label the probability file \nsinop_map = sits_label_classification(\n    cube = sinop_bayes, \n    output_dir = tempdir_py\n)\nplot(sinop_map)\n\n\n\n\nThe resulting classification files can be read by QGIS. Links to the associated files are available in the sinop_map data.frame in the nested table file_info.\n\n\nR\nPython\n\n\n\n\n# Show the location of the classification file\nsinop_map$file_info[[1]]\n\n# A tibble: 1 × 12\n  band  start_date end_date   ncols nrows  xres  yres      xmin     xmax    ymin\n  &lt;chr&gt; &lt;date&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 class 2013-09-14 2014-08-29   944   551  232.  232. -6181982.  -5.96e6 -1.35e6\n# ℹ 2 more variables: ymax &lt;dbl&gt;, path &lt;chr&gt;\n\n\n\n\n\n# Show the location of the classification file\nsinop_map[\"file_info\"][0]"
  },
  {
    "objectID": "intro_examples.html",
    "href": "intro_examples.html",
    "title": "2  Example scripts",
    "section": "",
    "text": "The following scripts show examples of how to use sits with increasing levels of complexity.\n\n3"
  },
  {
    "objectID": "intro_examples.html#cerrado-classification",
    "href": "intro_examples.html#cerrado-classification",
    "title": "2  Example scripts",
    "section": "2.1 Cerrado classification",
    "text": "2.1 Cerrado classification"
  },
  {
    "objectID": "intro_visualisation.html#plotting",
    "href": "intro_visualisation.html#plotting",
    "title": "\n3  Data visualisation\n",
    "section": "\n3.1 Plotting",
    "text": "3.1 Plotting\nThe plot() function produces a graphical display of data cubes, time series, models, and SOM maps. For each type of data, there is a dedicated version of the plot() function. See ?plot.sits for details. Plotting of time series, models and SOM outputs uses the ggplot2 package; maps are plotted using the tmap package. When plotting images and classified maps, users can control the output, which appropriate parameters for each type of image. In this chapter, we provide examples of the options available for plotting different types of maps.\nPlotting and visualisation function in sits use COG overview if available. COG overviews are reduced-resolution versions of the main image, stored within the same file. Overviews allow for quick rendering at lower zoom levels, improving performance when dealing with large images. Usually, a single GeoTIFF will have many overviews, to match different zoom levels.\n\n3.1.1 Plotting false color maps\nWe refer to false color maps as images which are plotted on a color scale. Usually these are single bands, indexes such as NDVI or DEMs. For these data sets, the parameters for plot() are:\n\n\nx: data cube containing data to be visualised;\n\nband: band or index to be plotted;\n\npallete: color scheme to be used for false color maps, which should be one of the RColorBrewer palettes. These palettes have been designed to be effective for map display by Prof Cynthia Brewer as described at the Brewer website. By default, optical images use the RdYlGn scheme, SAR images use Greys, and DEM cubes use Spectral.\n\nrev: whether the color palette should be reversed; TRUE for DEM cubes, and FALSE otherwise.\n\nscale: global scale parameter used by tmap. All font sizes, symbol sizes, border widths, and line widths are controlled by this value. Default is 0.75; users should vary this parameter and see the results.\n\nfirst_quantile: 1st quantile for stretching images (default = 0.05).\n\nlast_quantile: last quantile for stretching images (default = 0.95).\n\nmax_cog_size: for cloud-oriented geotiff files (COG), sets the maximum number of lines or columns of the COG overview to be used for plotting.\n\nThe following optional parameters are available to allow for detailed control over the plot output: - graticules_labels_size: size of coordinates labels (default = 0.8). - legend_title_size: relative size of legend title (default = 1.0). - legend_text_size: relative size of legend text (default = 1.0). - legend_bg_color: color of legend background (default = “white”). - legend_bg_alpha: legend opacity (default = 0.5). - legend_position: where to place the legend (options = “inside” or “outside” with “inside” as default).\nThe following example shows a plot of an NDVI index of a data cube. This data cube covers part of MGRS tile 20LMR and contains bands “B02”, “B03”, “B04”, “B05”, “B06”, “B07”, “B08”, “B11”, “B12”, “B8A”, “EVI”, “NBR”, and “NDVI” for the period 2022-01-05 to 2022-12-23. We will use parameters with other than their defaults.\n\n\nR\nPython\n\n\n\n\n# set the directory where the data is \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR\", package = \"sitsdata\")\n# read the data cube\nro_20LMR &lt;- sits_cube(\n  source = \"MPC\", \n  collection = \"SENTINEL-2-L2A\",\n  data_dir = data_dir\n)\n# plot the NDVI for date 2022-08-01\nplot(ro_20LMR, \n     band = \"NDVI\", \n     date = \"2022-08-01\", \n     palette = \"Greens\",\n     legend_position = \"outside\",\n     scale = 1.0)\n\n\n\nFigure 3.1: Sentinel-2 NDVI index covering tile 20LMR\n\n\n\n\n\n\n# set the directory where the data is \ndata_dir = sitsdata_dir + \"extdata/Rondonia-20LMR\"\n# read the data cube\nro_20LMR = sits_cube(\n  source = \"MPC\", \n  collection = \"SENTINEL-2-L2A\",\n  data_dir = data_dir\n)\n# plot the NDVI for date 2022-08-01\nplot(ro_20LMR, \n     band = \"NDVI\", \n     date = \"2022-08-01\", \n     palette = \"Greens\",\n     legend_position = \"outside\",\n     scale = 1.0)\n\n\n\n\n\n\n\n\n3.1.2 Plotting RGB color composite maps\nFor RGB color composite maps, the parameters for the plot function are:\n\n\nx: data cube containing data to be visualised;\n\nband: band or index to be plotted;\n\ndate: date to be plotted (must be part of the cube timeline);\n\nred: band or index associated to the red color;\n\ngreen: band or index associated to the green color;\n\nblue: band or index associated to the blue color;\n\nscale: global scale parameter used by tmap. All font sizes, symbol sizes, border widths, and line widths are controlled by this value. Default is 0.75; users should vary this parameter and see the results.\n\nfirst_quantile: 1st quantile for stretching images (default = 0.05).\n\nlast_quantile: last quantile for stretching images (default = 0.95).\n\nmax_cog_size: for cloud-oriented geotiff files (COG), sets the maximum number of lines or columns of the COG overview to be used for plotting.\n\nThe optional parameters listed in the previous section are also available. An example follows:\n\n\nR\nPython\n\n\n\n\n# plot a color composite for date 2022-08-01\nplot(ro_20LMR, \n     red = \"B11\", \n     green = \"B8A\",\n     blue = \"B02\",\n     date = \"2022-08-01\", \n     scale = 1.0)\n\n\n\nFigure 3.2: Sentinel-2 color composite covering tile 20LMR\n\n\n\n\n\n\n# plot a color composite for date 2022-08-01\nplot(ro_20LMR, \n     red = \"B11\", \n     green = \"B8A\",\n     blue = \"B02\",\n     date = \"2022-08-01\", \n     scale = 1.0)\n\n\n\n\n\n\n\nPlotting classified maps\nClassified maps pose an additional challenge for plotting because of the association between labels and colors. In this case, sits allows three alternatives:\n\nPredefined color scheme: sits includes some well-established color schemes such as IBGP, UMD, ESA_CCI_LC, and WORLDCOVER. There is a predefined color table with associates labels commonly used in LUCC classification to colors. Users can also create their color schemas. Please see section “How Colors Work on sits in this chapter.\nlegend: in this case, users provide a named vector with labels and colors, as shown in the example below.\npalette: an RColorBrewer categorical palette, which is assigned to labels which are not in the color table.\n\nThe parameters for plot() applied to a classified data cube are:\n\n\nx: data cube containing a classified map;\n\nlegend: legend which associated colors to the classes, which is NULL by default.\n\npalette: color palette used for undefined colors, which is Spectral by default.\n\nscale: global scale parameter used by tmap.\n\nThe optional parameters listed in the previous section are also available. For an example of plotting a classified data cube with default color scheme, please see the section “Reading classified images as local data cube” in the “Earth observation data cubes” chapter. In what follows we show a similar case using a legend.\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LLP\", \n                        package = \"sitsdata\")\n# Read the classified cube\nrondonia_class_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    bands = \"class\",\n    labels = c(\"1\" = \"Burned\", \"2\" = \"Cleared\", \n               \"3\" = \"Degraded\", \"4\" =  \"Natural_Forest\"),\n    data_dir = data_dir\n)\n# Plot the classified cube\nplot(rondonia_class_cube,\n  legend = c(\"Burned\" = \"#a93226\",\n             \"Cleared\" = \"#f9e79f\",\n             \"Degraded\" = \"#d4efdf\",\n             \"Natural_Forest\" = \"#1e8449\"\n             ),\n  scale = 1.0,\n  legend_position = \"outside\"\n)\n\n\n\nFigure 3.3: Sentinel-2 color composite covering tile 20LMR"
  },
  {
    "objectID": "intro_visualisation.html#visualization-of-data-cubes-in-interactive-maps",
    "href": "intro_visualisation.html#visualization-of-data-cubes-in-interactive-maps",
    "title": "\n3  Data visualisation\n",
    "section": "\n3.2 Visualization of data cubes in interactive maps",
    "text": "3.2 Visualization of data cubes in interactive maps\nData cubes and samples can also be shown as interactive maps using sits_view(). This function creates tiled overlays of different kinds of data cubes, allowing comparison between the original, intermediate and final results. It also includes background maps. The following example creates an interactive map combining the original data cube with the classified map.\n\nsits_view(rondonia_class_cube,\n            legend = c(\"Burned\" = \"#a93226\",\n             \"Cleared\" = \"#f9e79f\",\n             \"Degraded\" = \"#d4efdf\",\n             \"Natural_Forest\" = \"#1e8449\"\n             )\n)\n\n\n\n\n\nFigure 3.4: Leaflet visualization of classification of an area in Rondonia, Brasil"
  },
  {
    "objectID": "intro_visualisation.html#how-colors-work-in-sits",
    "href": "intro_visualisation.html#how-colors-work-in-sits",
    "title": "\n3  Data visualisation\n",
    "section": "\n3.3 How colors work in sits",
    "text": "3.3 How colors work in sits\nIn examples provided in the book, the color legend is taken from a predefined color palette provided by sits. The default color definition file used by sits has 220 class names, which can be shown using sits_colors().\n\n# Point default `sits` colors\nsits_colors()\n\n# A tibble: 238 × 2\n   name                             color  \n   &lt;chr&gt;                            &lt;chr&gt;  \n 1 Evergreen_Broadleaf_Forest       #1E8449\n 2 Evergreen_Broadleaf_Forests      #1E8449\n 3 Tree_Cover_Broadleaved_Evergreen #1E8449\n 4 Forest                           #1E8449\n 5 Forests                          #1E8449\n 6 Closed_Forest                    #1E8449\n 7 Closed_Forests                   #1E8449\n 8 Mountainside_Forest              #229C59\n 9 Mountainside_Forests             #229C59\n10 Open_Forest                      #53A145\n# ℹ 228 more rows\n\n\nThese colors are grouped by typical legends used by the Earth observation community, which include “IGBP”, “UMD”, “ESA_CCI_LC”, “WORLDCOVER”, “PRODES”, “PRODES_VISUAL”, “TERRA_CLASS”, “TERRA_CLASS_PT”. The following commands shows the colors associated with the IGBP legend [1].\n\n\n\n\nFigure 3.5: Colors used in the sits package to represeny IGBP legend.\n\n\n\nThe default color table can be extended using sits_colors_set(). As an example of a user-defined color table, consider a definition that covers level 1 of the Anderson Classification System used in the US National Land Cover Data, obtained by defining a set of colors associated to a new legend. The colors should be defined by HEX values and the color names should consist of a single string; multiple names need to be connected with an underscore(“_“).\n\n# Define a color table based on the Anderson Land Classification System\nus_nlcd &lt;- tibble::tibble(name = character(), color = character())\nus_nlcd &lt;- us_nlcd |&gt;  \n  tibble::add_row(name = \"Urban_Built_Up\", color =  \"#85929E\") |&gt; \n  tibble::add_row(name = \"Agricultural_Land\", color = \"#F0B27A\") |&gt;  \n  tibble::add_row(name = \"Rangeland\", color = \"#F1C40F\") |&gt; \n  tibble::add_row(name = \"Forest_Land\", color = \"#27AE60\") |&gt;  \n  tibble::add_row(name = \"Water\", color = \"#2980B9\") |&gt;  \n  tibble::add_row(name = \"Wetland\", color = \"#D4E6F1\") |&gt; \n  tibble::add_row(name = \"Barren_Land\", color = \"#FDEBD0\") |&gt; \n  tibble::add_row(name = \"Tundra\", color = \"#EBDEF0\") |&gt; \n  tibble::add_row(name = \"Snow_and_Ice\", color = \"#F7F9F9\")\n# Load the color table into `sits`\nsits_colors_set(colors = us_nlcd, legend = \"US_NLCD\")\n# Show the new legend\nsits_colors_show(legend = \"US_NLCD\")\n\n\n\nFigure 3.6: Example of defining colors for the Anderson Land Classification Scheme.\n\n\n\nThe original default sits color table can be restored using sits_colors_reset().\n\n# Reset the color table\nsits_colors_reset()"
  },
  {
    "objectID": "intro_visualisation.html#exporting-colors-to-qgis",
    "href": "intro_visualisation.html#exporting-colors-to-qgis",
    "title": "\n3  Data visualisation\n",
    "section": "\n3.4 Exporting colors to QGIS",
    "text": "3.4 Exporting colors to QGIS\nTo simplify the process of importing your data to QGIS, the color palette used to display classified maps in sits can be exported as a QGIS style using sits_colors_qgis. The function takes two parameters: (a) cube, a classified data cube; and (b) file, the file where the QGIS style in XML will be written to. In this case study, we first retrieve and plot a classified data cube and then export the colors to a QGIS XML style.\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-Class-2022-Mosaic\", \n                        package = \"sitsdata\")\n\n# labels of the classified image\nlabels &lt;- c(\"1\" = \"Clear_Cut_Bare_Soil\",\n            \"2\" =  \"Clear_Cut_Burned_Area\",\n            \"3\" =   \"Clear_Cut_Vegetation\",\n            \"4\" = \"Forest\",\n            \"5\" =  \"Mountainside_Forest\",\n            \"6\" = \"Riparian_Forest\",\n            \"7\" = \"Seasonally_Flooded\",\n            \"8\" = \"Water\",\n            \"9\" = \"Wetland\" \n)\n# read classified data cube\nro_class &lt;- sits_cube(\n    source = \"MPC\", \n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir, \n    bands = \"class\",\n    labels = labels,\n    version = \"mosaic\"\n)\n# Plot the classified cube\nplot(ro_class, scale = 1.0)\n\n\n\nFigure 3.7: Classified data cube for the year 2022 for state of Rondonia, Brazil.\n\n\n\nThe file to be read by QGIS is a TIFF file whose location is informed by the data cube, as follows.\n\n# Show the location of the classified map\nro_class[[\"file_info\"]][[1]]$path\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/sitsdata/extdata/Rondonia-Class-2022-Mosaic/SENTINEL-2_MSI_MOSAIC_2022-01-05_2022-12-23_class_mosaic.tif\"\n\n\nThe color schema can be exported to QGIS as follows.\n\n# Export the color schema to QGIS\nsits_colors_qgis(ro_class, file = \"./tempdir/R/visualisation/qgis_style.xml\")"
  },
  {
    "objectID": "datacubes.html#regular-image-data-cubes",
    "href": "datacubes.html#regular-image-data-cubes",
    "title": "Earth observation data cubes",
    "section": "Regular image data cubes",
    "text": "Regular image data cubes\nMachine learning and deep learning (ML/DL) classification algorithms require the input data to be consistent. The dimensionality of the data used for training the model has to be the same as that of the data to be classified. There should be no gaps and no missing values. Thus, to use ML/DL algorithms for remote sensing data, ARD image collections should be converted to regular data cubes. Adapting a previous definition by Appel and Pebesma [1], we consider a regular data cube has the following definition and properties:\n\nA regular data cube is a four-dimensional data structure with explicit dimensions x (longitude or easting), y (latitude or northing), time, and bands. The spatial, temporal, and attribute dimensions are independent and not interchangeable.\nThe spatial dimensions refer to a coordinate system, such as the grids defined by UTM (Universal Transverse Mercator) or MGRS (Military Grid Reference System). A grid (or tile) of the grid corresponds to a unique zone of the coordinate system. A data cube may span various tiles and UTM zones.\nThe temporal dimension is a set of continuous and equally-spaced intervals.\nFor every combination of dimensions, a cell has a single value.\n\nAll cells of a data cube have the same spatiotemporal extent. The spatial resolution of each cell is the same in X and Y dimensions. All temporal intervals are the same. Each cell contains a valid set of measures. Each pixel is associated to a unique coordinate in a zone of the coordinate system. For each position in space, the data cube should provide a set of valid time series. For each time interval, the regular data cube should provide a valid 2D image (see Figure Figure 2).\n\n\n\n\nFigure 2: Conceptual view of a data cube.\n\n\n\nCurrently, the only cloud service that provides regular data cubes by default is the Brazil Data Cube (BDC). ARD collections available in other cloud services are not regular in space and time. Bands may have different resolutions, images may not cover the entire time, and time intervals may be irregular. For this reason, subsets of these collections need to be converted to regular data cubes before further processing. To produce data cubes for machine-learning data analysis, this part of the book describes the steps involved in producing an using regular data cubes:\n\nObtaining data from ARD image collections\nProducing regular data cubes from single and multi-source data\nRecovering data cubes from local files\nPerforming operations in data cubes"
  },
  {
    "objectID": "intro_visualisation.html#references",
    "href": "intro_visualisation.html#references",
    "title": "\n3  Data visualisation\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Herold, R. Hubald, and A. Di Gregorio, “Translating and evaluating land cover legends using the UN Land Cover Classification System (LCCS),” GOFC-GOLD Florence, Italy, 2009."
  },
  {
    "objectID": "datacubes.html#references",
    "href": "datacubes.html#references",
    "title": "Earth observation data cubes",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Appel and E. Pebesma, “On-Demand Processing of Data Cubes from Satellite Image Collections with the gdalcubes Library,” Data, vol. 4, no. 3, 2019, doi: 10.3390/data4030092."
  },
  {
    "objectID": "dc_ardcollections.html#what-is-analysis-ready-data",
    "href": "dc_ardcollections.html#what-is-analysis-ready-data",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.1 What is Analysis-Ready Data?",
    "text": "4.1 What is Analysis-Ready Data?\nAnalysis Ready Data (CEOS-ARD) are satellite data that have been processed to meet the ARD standards defined by the Committee on Earth Observation Satellites (CEOS). ARD data simplify and accelerate the analysis of Earth observation data by providing consistent and high-quality data that are standardized across different sensors and platforms.\nARD images processing includes geometric corrections, radiometric corrections, and sometimes atmospheric corrections. Images are georeferenced, meaning they are accurately aligned with a coordinate system. Optical ARD images include cloud and shadow masking information. These masks indicate which pixels affected by clouds or cloud shadows. For optical sensors, CEOS-ARD images have to be converted to surface reflectance values, which represent the fraction of light that is reflected by the surface. This makes the data more comparable across different times and locations.\nFor SAR images, CEOS-ARD specification require images to undergo Radiometric Terrain Correction (RTC) and are provided in the GammaNought (\\(\\gamma_0\\)) backscatter values. This value which mitigates the variations from diverse observation geometries and is recommended for most land applications.\nARD images are available from various satellite platforms, including Landsat, Sentinel, and commercial satellites. This provides a wide range of spatial, spectral, and temporal resolutions to suit different applications. They are organised as a collection of files, where each pixel contains a single value for each spectral band for a given date. These collections are available in cloud services such as Brazil Data Cube, Digital Earth Africa, and Microsoft’s Planetary Computer. In general, the timelines of the images of an ARD collection are different. Images still contain cloudy or missing pixels; bands for the images in the collection may have different resolutions. Figure Figure 4.1 shows an example of the Landsat ARD image collection.\n\n\n\n\nFigure 4.1: ARD image collection (source: USGS).\n\n\n\nARD image collections are organized in spatial partitions. Sentinel-2/2A images follow the Military Grid Reference System (MGRS) tiling system, which divides the world into 60 UTM zones of 8 degrees of longitude. Each zone has blocks of 6 degrees of latitude. Blocks are split into tiles of 110 \\(\\times\\) 110 km\\(^2\\) with a 10 km overlap. Figure Figure 4.2 shows the MGRS tiling system for a part of the Northeastern coast of Brazil, contained in UTM zone 24, block M.\n\n\n\n\nFigure 4.2: MGRS tiling system used by Sentinel-2 images (source: US Army).\n\n\n\nThe Landsat-4/5/7/8/9 satellites use the Worldwide Reference System (WRS-2), which breaks the coverage of Landsat satellites into images identified by path and row (see Figure @ref(fig:wrs)). The path is the descending orbit of the satellite; the WRS-2 system has 233 paths per orbit, and each path has 119 rows, where each row refers to a latitudinal center line of a frame of imagery. Images in WRS-2 are geometrically corrected to the UTM projection.\n\n\n\n\nFigure 4.3: MGRS tiling system used by Sentinel-2 images (source: US Army)."
  },
  {
    "objectID": "dc_ardcollections.html#image-collections-handled-by-sits",
    "href": "dc_ardcollections.html#image-collections-handled-by-sits",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.2 Image collections handled by sits\n",
    "text": "4.2 Image collections handled by sits\n\nIn version 1.5.3,sits supports access to the following ARD image cloud providers:\n\nAmazon Web Services (AWS): Open data Sentinel-2/2A level 2A collections for the Earth’s land surface.\nBrazil Data Cube (BDC): Open data collections of Sentinel-2/2A, Landsat-8, CBERS-4/4A, and MOD13Q1 products for Brazil. These collections are organized as regular data cubes.\nCopernicus Data Space Ecosystem (CDSE): Open data collections of Sentinel-1 RTC and Sentinel-2/2A images.\nDigital Earth Africa (DEAFRICA): Open data collections of Sentinel-1 RTC, Sentinel-2/2A, Landsat-5/7/8/9 for Africa. Additional products available include ALOS_PALSAR mosaics, DEM_COP_30, NDVI_ANOMALY based on Landsat data, and monthly and daily rainfall data from CHIRPS.\nDigital Earth Australia (DEAUSTRALIA): Open data ARD collections of Sentinel-2A/2B and Landsat-5/7/8/9 images, yearly geomedian of Landsat 5/7/8 images; yearly fractional land cover from 1986 to 2024.\nHarmonized Landsat-Sentinel (HLS): HLS, provided by NASA, is an open data collection that processes Landsat 8 and Sentinel-2 imagery to a common standard.\nMicrosoft Planetary Computer (MPC): Open data collections of Sentinel-1 GRD, Sentinel-1 RTC, Sentinel-2/2A, Landsat-4/5/7/8/9 images for the Earth’s land areas. Also supported are Copernicus DEM-30 and MOD13Q1, MOD10A1 and MOD09A1 products, and the Harmonized Landsat-Sentinel collections (HLSL30 and HLSS30).\nSwiss Data Cube (SDC): Collection of Sentinel-2/2A and Landsat-8 images for Switzerland.\nTerrascope: Cloud service with EO products which includes the ESA World Cover map.\nUSGS: Landsat-4/5/7/8/9 collections available in AWS, which require access payment.\n\nIn addition, sits supports the use of Planet monthly mosaics stored as local files. For a detailed description of the providers and collections supported by sits, please run sits_list_collections()."
  },
  {
    "objectID": "dc_ardcollections.html#accessing-ard-image-collections-in-cloud-providers",
    "href": "dc_ardcollections.html#accessing-ard-image-collections-in-cloud-providers",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.3 Accessing ARD image collections in cloud providers",
    "text": "4.3 Accessing ARD image collections in cloud providers\n\nTo obtain information on ARD image collection from cloud providers, sits uses the SpatioTemporal Asset Catalogue (STAC) protocol, a specification of geospatial information which many large image collection providers have adopted. A ‘spatiotemporal asset’ is any file that represents information about the Earth captured in a specific space and time. To access STAC endpoints, sits uses the rstac R package.\nThe function sits_cube() supports access to image collections in cloud services; it has the following parameters:\n\n\nsource: Name of the provider.\n\ncollection: A collection available in the provider and supported by sits. To find out which collections are supported by sits, see sits_list_collections().\n\nplatform: Optional parameter specifying the platform in collections with multiple satellites.\n\ntiles: Set of tiles of image collection reference system. Either tiles or roi should be specified.\n\nroi: A region of interest. Either: (a) a named vector (lon_min, lon_max, lat_min, lat_max) in WGS 84 coordinates; (b) an sf object; (c) a path to a shapefile polygon; (d) A named vector (xmin, xmax, ymin, ymax) with XY coordinates. All images intersecting the convex hull of the roi are selected.\n\nbands: Optional parameter with the bands to be used. If missing, all bands from the collection are used.\n\norbit: Optional parameter required only for Sentinel-1 images (default = “descending”).\n\nstart_date: The initial date for the temporal interval containing the time series of images.\n\nend_date: The final date for the temporal interval containing the time series of images.\n\nThe result of sits_cube() is a tibble with a description of the selected images required for further processing. It does not contain the actual data, but only pointers to the images. The attributes of individual image files can be assessed by listing the file_info column of the tibble."
  },
  {
    "objectID": "dc_ardcollections.html#amazon-web-services",
    "href": "dc_ardcollections.html#amazon-web-services",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.4 Amazon Web Services",
    "text": "4.4 Amazon Web Services\nAmazon Web Services (AWS) holds two kinds of collections: open-data and requester-pays. Open data collections can be accessed without cost. Requester-pays collections require payment from an AWS account. Currently, sits supports collection SENTINEL-2-L2A which is open data. The bands in 10 m resolution are B02, B03, B04, and B08. The 20 m bands are B05, B06, B07, B8A, B11, and B12. Bands B01 and B09 are available at 60 m resolution. A CLOUD band is also available. The example below shows how to access one tile of the open data SENTINEL-2-L2A collection. The tiles parameter allows selecting the desired area according to the MGRS reference system.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n\n\n\n\nFigure 4.4: Sentinel-2 image in an area of the Northeastern coast of Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#microsoft-planetary-computer",
    "href": "dc_ardcollections.html#microsoft-planetary-computer",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.5 Microsoft Planetary Computer",
    "text": "4.5 Microsoft Planetary Computer\nThe sits package supports access to open data collection from Microsoft’s Planetary Computer (MPC), including SENTINEL-1-GRD, SENTINEL-1-RTC, SENTINEL-2-L2A, LANDSAT-C2-L2, COP-DEM-GLO-30 (Copernicus Global DEM at 30 meter resolution) and MOD13Q1-6.1(version 6.1 of the MODIS MOD13Q1 product). Access to the non-open data collection is available for users that have registration in MPC.\n\n4.5.1 SENTINEL-2/2A images in MPC\nThe SENTINEL-2/2A ARD images available in MPC have the same bands and resolutions as those available in AWS (see above). The example below shows how to access the SENTINEL-2-L2A collection.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC &lt;- sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC = sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n\n\n\n\nFigure 4.5: Sentinel-2 image in an area of the state of Rondonia, Brazil.\n\n\n\n\n4.5.2 LANDSAT-C2-L2 images in MPC\nThe LANDSAT-C2-L2 collection provides access to data from Landsat-4/5/7/8/9 satellites. Images from these satellites have been intercalibrated to ensure data consistency. For compatibility between the different Landsat sensors, the band names are BLUE, GREEN, RED, NIR08, SWIR16, and SWIR22. All images have 30 m resolution. For this collection, tile search is not supported; the roi parameter should be used. The example below shows how to retrieve data from a region of interest covering the city of Brasilia in Brazil.\n\n\nR\nPython\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi &lt;- c(lon_min = -43.5526, lat_min = -2.9644, \n         lon_max = -42.5124, lat_max = -2.1671)\n# Select the cube\ns2_L8_cube_MPC &lt;- sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = c(\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi = {\"lon_min\" : -43.5526, \"lat_min\" : -2.9644, \n         \"lon_max\" : -42.5124, \"lat_max\" : -2.1671}\n# Select the cube\ns2_L8_cube_MPC = sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = [\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"],\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n\n\n\n\nFigure 4.6: Landsat-8 image in an area in Northeast Brazil.\n\n\n\n\n4.5.3 SENTINEL-1-GRD images in MPC\nSentinel-1 GRD products consist of focused SAR data that has been detected, multi-looked and projected to ground range using the WGS84 Earth ellipsoid model. GRD images are subject for variations in the radar signal’s intensity due to topographic effects, antenna pattern, range spreading loss, and other radiometric distortions. The most common types of distortions include foreshortening, layover and shadowing.\nForeshortening occurs when the radar signal strikes a steep terrain slope facing the radar, causing the slope to appear compressed in the image. Features like mountains can appear much steeper than they are, and their true heights can be difficult to interpret. Layover happens when the radar signal reaches the top of a tall feature (like a mountain or building) before it reaches the base. As a result, the top of the feature is displaced towards the radar and appears in front of its base. This results in a reversal of the order of features along the radar line-of-sight, making the image interpretation challenging. Shadowing occurs when a radar signal is obstructed by a tall object, casting a shadow on the area behind it that the radar cannot illuminate. The shadowed areas appear dark in SAR images, and no information is available from these regions, similar to optical shadows.\nAccess to Sentinel-1 GRD images can be done either by MGRS tiles (tiles) or by region of interest (roi). We recommend using the MGRS tiling system for specifying the area of interest, since when these images are regularized, they will be re-projected into MGRS tiles. By default, only images in descending orbit are selected.\nThe following example shows how to create a data cube of S1 GRD images over a region in Mato Grosso Brazil that is an area of the Amazon forest that has been deforested. The resulting cube will not follow any specific projection and its coordinates will be stated as EPSG 4326 (latitude/longitude). Its geometry is derived from the SAR slant-range perspective; thus, it will appear included in relation to the Earth’s longitude.\n\n\nR\nPython\n\n\n\n\ncube_s1_grd &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = c(\"VV\"),\n  orbit = \"descending\",\n  tiles = c(\"21LUJ\",\"21LVJ\"),\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_grd =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = [\"VV\"],\n  orbit = \"descending\",\n  tiles = [\"21LUJ\",\"21LVJ\"],\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.7: Sentinel-1 image in an area in Mato Grosso, Brazil.\n\n\n\nAs explained earlier in this chapter, in areas with areas with large elevation differences, Sentinel-1 GRD images will have geometric distortions. For this reason, whenever possible, we recommend the use of RTC (radiometrically terrain corrected) images as described in the next session.\n\n4.5.4 SENTINEL-1-RTC images in MPC\nAn RTC SAR image has undergone corrections for both geometric distortions and radiometric distortions caused by the terrain. The purpose of RTC processing is to enhance the interpretability and usability of SAR images for various applications by providing a more accurate representation of the Earth’s surface. The radar backscatter values are normalized to account for these variations, ensuring that the image accurately represents the reflectivity of the surface features.\nThe terrain correction addresses geometric distortions caused by the side-looking geometry of SAR imaging, such as foreshortening, layover, and shadowing. It uses a Digital Elevation Model (DEM) to model the terrain and re-project the SAR image from the slant range (radar line-of-sight) to the ground range (true geographic coordinates). This process aligns the SAR image with the actual topography, providing a more accurate spatial representation.\n\n\nR\nPython\n\n\n\n\ncube_s1_rtc &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = c(\"VV\", \"VH\"),\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_rtc =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = [\"VV\", \"VH\"],\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.8: Sentinel-1-RTC image of an area in Colombia.\n\n\n\nThe above image is from the central region of Colombia, a country with large variations in altitude due to the Andes mountains. Users are invited to compare this images with the one from the SENTINEL-1-GRD collection and see the significant geometrical distortions of the GRD image compared with the RTC one.\n\n4.5.5 Copernicus DEM 30 meter images in MPC\nThe Copernicus digital elevation model 30-meter global dataset (COP-DEM-GLO-30) is a high-resolution topographic data product provided by the European Space Agency (ESA) under the Copernicus Program. The vertical accuracy of the Copernicus DEM 30-meter dataset is typically within a few meters, but this can vary depending on the region and the original data sources. The primary data source for the Copernicus DEM is data from the TanDEM-X mission, designed by the German Aerospace Center (DLR). TanDEM-X provides high-resolution radar data through interferometric synthetic aperture radar (InSAR) techniques.\nThe Copernicus DEM 30 meter is organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. In sits, access to COP-DEM-GLO-30 images can be done either by MGRS tiles (tiles) or by region of interest (roi). In both case, the cube is retrieved based on the parts of the grid that intersect the region of interest or the chosen tiles.\n\n\nR\nPython\n\n\n\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\n\n\n\n\nFigure 4.9: Copernicus 30-meter DEM of an area in Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#brazil-data-cube",
    "href": "dc_ardcollections.html#brazil-data-cube",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.6 Brazil Data Cube",
    "text": "4.6 Brazil Data Cube\nThe Brazil Data Cube (BDC) is built by Brazil’s National Institute for Space Research (INPE), to provide regular EO data cubes from CBERS, LANDSAT, SENTINEL-2, and TERRA/MODIS satellites for environmental applications. The collections available in the BDC are: LANDSAT-OLI-16D (Landsat-8 OLI, 30 m resolution, 16-day intervals), SENTINEL-2-16D (Sentinel-2A and 2B MSI images at 10 m resolution, 16-day intervals), CBERS-WFI-16D (CBERS 4 WFI, 64 m resolution, 16-day intervals), CBERS-WFI-8D(CBERS 4 and 4A WFI images, 64m resolution, 8-day intervals), and MOD13Q1-6.1 (MODIS MOD13SQ1 product, collection 6, 250 m resolution, 16-day intervals). For more details, use sits_list_collections(source = \"BDC\").\nThe BDC uses three hierarchical grids based on the Albers Equal Area projection and SIRGAS 2000 datum. The large grid has tiles of 4224.4 \\(\\times4\\) 224.4 km2 and is used for CBERS-4 AWFI collections at 64 m resolution; each CBERS-4 AWFI tile contains images of 6600 \\(\\times\\) 6600 pixels. The medium grid is used for Landsat-8 OLI collections at 30 m resolution; tiles have an extension of 211.2 \\(\\times\\) 211.2 km2, and each image has 7040 \\(\\times\\) 7040 pixels. The small grid covers 105.6 \\(\\times\\) 105.6 km2 and is used for Sentinel-2 MSI collections at 10 m resolutions; each image has 10560 \\(\\times\\) 10560 pixels. The data cubes in the BDC are regularly spaced in time and cloud-corrected [1].\n\n\n\n\nFigure 4.10: Hierarchical BDC tiling system showing (a) large BDC grid overlayed on Brazilian biomes, (b) one learge tile from the grid used for CBERS-4 AWFI data, (c) four medium tiles from the grid used for LANDSAT data; and (d) sixteen small tiles from the grid used for SENTINEL-2 data. Tiles in (b), (c), and (d) are nested.\n\n\n\nTo access the BDC, users must provide their credentials using environment variables, as shown below. Obtaining a BDC access key is free. Users must register at the BDC site to obtain a key. Please include your BDC access key in your “.Rprofile”.\n\nSys.setenv(BDC_ACCESS_KEY = \"&lt;your_bdc_access_key&gt;\")\n\nIn the example below, the data cube is defined as one tile (“005004”) of CBERS-WFI-16D collection, which holds CBERS AWFI images at 16 days resolution.\n\n\nR\nPython\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = c(\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"),\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile = sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = [\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"],\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n\n\n\n\nFigure 4.11: CBERS-4 WFI image in a Cerrado area in Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#copernicus-data-space-ecosystem-cdse",
    "href": "dc_ardcollections.html#copernicus-data-space-ecosystem-cdse",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.7 Copernicus Data Space Ecosystem (CDSE)",
    "text": "4.7 Copernicus Data Space Ecosystem (CDSE)\nThe Copernicus Data Space Ecosystem (CDSE) is a cloud service designed to support access to Earth observation data from the Copernicus Sentinel missions and other sources. It is designed and maintained by the European Space Agency (ESA) with support from the European Commission.\nConfiguring user access to CDSE involves several steps to ensure proper registration, access to data, and utilization of the platform’s tools and services. Visit the Copernicus Data Space Ecosystem registration page. Complete the registration form with your details, including name, email address, organization, and sector. Confirm your email address through the verification link sent to your inbox.\nAfter registration, you will need to obtain access credentials to the S3 service implemented by CDSE, which can be obtained using the CSDE S3 credentials site. The site will request you to add a new credential. You will receive two keys: an an S3 access key and a secret access key. Take note of both and include the following lines in your .Rprofile.\n\nSys.setenv(\n    AWS_ACCESS_KEY_ID = \"your access key\",\n    AWS_SECRET_ACCESS_KEY = \"your secret access key\",\n      AWS_S3_ENDPOINT = \"eodata.dataspace.copernicus.eu\",\n      AWS_VIRTUAL_HOSTING = \"FALSE\"\n)\n\nAfter including these lines in your .Rprofile, restart R for the changes to take effect. By following these steps, users will have access to the Copernicus Data Space Ecosystem.\n\n4.7.1 SENTINEL-2/2A images in CDSE\nCDSE hosts a global collection of Sentinel-2 Level-2A images, which are processed according to the CEOS Analysis-Ready Data specifications. One example is provided below, where we present a Sentinel-2 image of the Lena river delta in Siberia in summertime.\n\n\nR\nPython\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = c(\"52XDF\")\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = [\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = \"52XDF\"\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 4.12: Sentinel-2 image of the Lena river delta in summertime.\n\n\n\n\n4.7.2 SENTINEL-1-RTC images in CDSE\nAn important product under development at CDSE are the radiometric terrain corrected (RTC) Sentinel-1 images. in CDSE, this product is referred to as normalized terrain backscater (NRB). The S1-NRB product contains radiometrically terrain corrected (RTC) gamma nought backscatter (γ0) processed from Single Look Complex (SLC) Level-1A data. Each acquired polarization is stored in an individual binary image file.\nAll images are projected and gridded into the United States Military Grid Reference System (US-MGRS). The use of the US-MGRS tile grid ensures a very high level of interoperability with Sentinel-2 Level-2A ARD products making it easy to also set-up complex analysis systems that exploit both SAR and optical data. While speckle is inherent in SAR acquisitions, speckle filtering is not applied to the S1-NRB product in order to preserve spatial resolution. Some applications (or processing methods) may require spatial or temporal filtering for stationary backscatter estimates.\nFor more details, please refer to the S1-NRB product website. Global coverage is expected to grow as ESA expands the S1-RTC archive. The following example shows an S1-RTC image for the Rift valley in Ethiopia.\n\n\nR\nPython\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = c(\"37NCH\")\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = [\"VV\", \"VH\"],\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = \"37NCH\"\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.13: Sentinel-1-RTC image of the Rift Valley in Ethiopia."
  },
  {
    "objectID": "dc_ardcollections.html#digital-earth-africa",
    "href": "dc_ardcollections.html#digital-earth-africa",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.8 Digital Earth Africa",
    "text": "4.8 Digital Earth Africa\nDigital Earth Africa (DEAFRICA) is a cloud service that provides open-access Earth observation data for the African continent. The ARD image collections in sits are:\n\nSentinel-2 level 2A (SENTINEL-2-L2A), organised as MGRS tiles.\nSentinel-1 radiometrically terrain corrected (SENTINEL-1-RTC)\nLandsat-5 (LS5-SR), Landsat-7 (LS7-SR), Landsat-8 (LS8-SR) and Landat-9 (LS9-SR). All Landsat collections are ARD data and are organized as WRS-2 tiles.\nSAR L-band images produced by PALSAR sensor onboard the Japanese ALOS satellite(ALOS-PALSAR-MOSAIC). Data is organized in a 5\\(^\\circ\\) by 5\\(^\\circ\\) grid with a spatial resolution of 25 meters. Images are available annually from 2007 to 2010 (ALOS/PALSAR) and from 2015 to 2022 (ALOS-2/PALSAR-2).\nEstimates of vegetation condition using NDVI anomalies (NDVI-ANOMALY) compared with the long-term baseline condition. The available measurements are “NDVI_MEAN” (mean NDVI for a month) and “NDVI-STD-ANOMALY” (standardised NDVI anomaly for a month).\nRainfall information provided by Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) from University of California in Santa Barbara. There are monthly (RAINFALL-CHIRPS-MONTHLY) and daily (RAINFALL-CHIRPS-DAILY) products over Africa.\nDigital elevation model provided by the EC Copernicus program (COP-DEM-30) in 30 meter resolution organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid.\nAnnual geomedian images for Landsat 8 and Landsat 9 (GM-LS8-LS9-ANNUAL (LANDSAT/OLI)`) in grid system WRS-2.\nAnnual geomedian images for Sentinel-2 (GM-S2-ANNUAL) in MGRS grid.\nRolling three-month geomedian images for Sentinel-2 (GM-S2-ROLLING) in MGRS grid.\nSemestral geomedian images for Sentinel-2 (GM-S2-SEMIANNUAL) in MGRS grid.\n\nAccess to DEAFRICA Sentinel-2 images can be done wither using tiles or roi parameter. In this example, the requested roi produces a cube that contains one MGRS tiles (“35LPH”) covering an area of Madagascar that includes the Betsiboka Estuary.\n\n\nR\nPython\n\n\n\n\ndea_s2_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = c(\n    lon_min = 46.1, lat_min = -15.6,\n    lon_max = 46.6, lat_max = -16.1\n  ),\n    bands = c(\"B02\", \"B04\", \"B08\"),\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\ndea_s2_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = {\n    \"lon_min\" : 46.1, \"lat_min\" : -15.6,\n    \"lon_max\" : 46.6, \"lat_max\" : -16.1\n  },\n    bands = [\"B02\", \"B04\", \"B08\"],\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\n\n\n\n\nFigure 4.14: Sentinel-2 image in an area over Madagascar.\n\n\n\nThe next example retrieves a set of ARD Landsat-9 data, covering the Serengeti plain in Tanzania.\n\n\nR\nPython\n\n\n\n\ndea_l9_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = c(\n        lon_min = 33.0, lat_min = -3.60, \n        lon_max = 33.6, lat_max = -3.00\n    ),\n    bands = c(\"B04\", \"B05\", \"B06\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\ndea_l9_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = {\n        \"lon_min\" : 33.0, \"lat_min\" : -3.60, \n        \"lon_max\" : 33.6, \"lat_max\" : -3.00\n    },\n    bands = [\"B04\", \"B05\", \"B06\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 4.15: Landsat-9 image in an area over the Serengeti in Tanzania.\n\n\n\nThe following example shows how to retrieve a subset of the ALOS-PALSAR mosaic for year 2020, for an area near the border between Congo and Rwanda.\n\n\nR\nPython\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = c(\n        lon_min = 28.69, lat_min = -2.35, \n        lon_max = 29.35, lat_max = -1.56\n    ),\n    bands = c(\"HH\", \"HV\"),\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = {\n       \"lon_min\" : 28.69, \"lat_min\" : -2.35, \n       \"lon_max\" : 29.35, \"lat_max\" : -1.56\n    },\n    bands = [\"HH\", \"HV\"],\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\n\n\n\n\nFigure 4.16: ALOS-PALSAR mosaic in the Congo forest area."
  },
  {
    "objectID": "dc_ardcollections.html#digital-earth-australia",
    "href": "dc_ardcollections.html#digital-earth-australia",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.9 Digital Earth Australia",
    "text": "4.9 Digital Earth Australia\nDigital Earth Australia (DEAUSTRALIA) is an initiative by Geoscience Australia that uses satellite data to monitor and analyze environmental changes and resources across the Australian continent. It provides many datasets that offer detailed information on phenomena such as droughts, agriculture, water availability, floods, coastal erosion, and urban development. The DEAUSTRALIA image collections in sits are:\n\nGA_LS5T_ARD_3: ARD images from Landsat-5 satellite, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, and “CLOUD”.\nGA_LS7E_ARD_3: ARD images from Landsat-7 satellite, with the same bands as Landsat-5.\nGA_LS8C_ARD_3: ARD images from Landsat-8 satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, “PANCHROMATIC”, and “CLOUD”.\nGA_LS9C_ARD_3: ARD images from Landsat-9 satellite, with the same bands as Landsat-8.\nGA_S2AM_ARD_3: ARD images from Sentinel-2A satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “RED-EDGE-1”, “RED-EDGE-2”, “RED-EDGE-3”, “NIR-1”, “NIR-2”, “SWIR-2”, “SWIR-3”, and “CLOUD”.\nGA_S2BM_ARD_3: ARD images from Sentinel-2B satellite, with the same bands as Sentinel-2A.\nGA_LS5T_GM_CYEAR_3: Landsat-5 geomedian images, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR1”, “SWIR2”, “EDEV”, “SDEV”, “BCDEV”.\nGA_LS7E_GM_CYEAR_3: Landsat-7 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS8CLS9C_GM_CYEAR_3: Landsat-8/9 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS_FC_3: Landsat fractional land cover, with bands “BS”, “PV”, “NPV”.\nGA_S2LS_INTERTIDAL_CYEAR_3: Landsat/Sentinel intertidal data, with bands “ELEVATION”, “ELEVATION-UNCERTAINTY”, “EXPOSURE”, “TA-HAT”, “TA-HOT”, “TA-LOT”, “TA-LAT” “TA-OFFSET-HIGH”, “TA-OFFSET-LOW”, “TA-SPREAD”, “QA-NDWI-CORR”and “QA-NDWI-FREQ”.\n\nThe following code retrieves an image from Sentinel-2A.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_55KGR &lt;- sits_mgrs_to_roi(\"55KGR\")\n# retrieve the world cover map for the chosen roi\ns2_56KKV &lt;- sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"),\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n# retrieve the world cover map for the chosen tile\ns2_56KKV = sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"],\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n\n\n\n\nFigure 4.17: Sentinel-2A image from the DEAUSTRALIA collection showing MGRS tile 56KKV."
  },
  {
    "objectID": "dc_ardcollections.html#harmonized-landsat-sentinel",
    "href": "dc_ardcollections.html#harmonized-landsat-sentinel",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.10 Harmonized Landsat-Sentinel",
    "text": "4.10 Harmonized Landsat-Sentinel\nHarmonized Landsat Sentinel (HLS) is a NASA initiative that processes and harmonizes Landsat 8 and Sentinel-2 imagery to a common standard, including atmospheric correction, alignment, resampling, and corrections for BRDF (bidirectional reflectance distribution function). The purpose of the HLS project is to create a unified and consistent dataset that integrates the advantages of both systems, making it easier to work with the data.\nThe NASA Harmonized Landsat and Sentinel (HLS) service provides two image collections:\n\nLandsat 8 OLI Surface Reflectance HLS (HLSL30) – The HLSL30 product includes atmospherically corrected surface reflectance from the Landsat 8 OLI sensors at 30 m resolution. The dataset includes 11 spectral bands.\nSentinel-2 MultiSpectral Instrument Surface Reflectance HLS (HLSS30) – The HLSS30 product includes atmospherically corrected surface reflectance from the Sentinel-2 MSI sensors at 30 m resolution. The dataset includes 12 spectral bands.\n\nThe HLS tiling system is identical as the one used for Sentinel-2 (MGRS). The tiles dimension is 109.8 km and there is an overlap of 4,900 m on each side.\nTo access NASA HLS, users need to registed at NASA EarthData, and save their login and password in a ~/.netrc plain text file in Unix (or %HOME%_netrc in Windows). The file must contain the following fields:\n\nmachine urs.earthdata.nasa.gov\nlogin &lt;username&gt;\npassword &lt;password&gt;\n\nWe recommend using the earthdatalogin package to create a .netrc file with the earthdatalogin::edl_netrc. This function creates a properly configured .netrc file in the user’s home directory and an environment variable GDAL_HTTP_NETRC_FILE, as shown in the example. As an alternative, we recommend using the HLS collections which are available in Microsoft Planetary Computer, which are a copy of the NASA collections and are faster to access.\n\nlibrary(earthdatalogin)\n\nearthdatalogin::edl_netrc( \nusername = \"&lt;your user name&gt;\", \npassword = \"&lt;your password&gt;\" \n) \n\nAccess to images in NASA HLS is done by region of interest or by tiles. The following example shows an HLS Sentinel-2 image over the Brazilian coast.\n\n\nR\nPython\n\n\n\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n# define a region of interest\nroi = { \"lon_min\" : -45.6422, \"lat_min\" : -24.0335,\n        \"lon_max\" : -45.0840, \"lat_max\" : -23.6178 }\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 = sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"],\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n\n\n\n\nFigure 4.18: Sentinel-2 image from NASA HLSS30 collection showing the island of Ilhabela in the coast of Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#eo-products-from-terrascope",
    "href": "dc_ardcollections.html#eo-products-from-terrascope",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.11 EO products from TERRASCOPE",
    "text": "4.11 EO products from TERRASCOPE\nTerrascope is online platform for accessing open-source satellite images. This service, operated by VITO, offers a range of Earth observation data and processing services that are accessible free of charge. Currently, sits supports the World Cover 2021 maps, produced by VITO with support form the European Commission and ESA. The following code shows how to access the World Cover 2021 convering tile “22LBL”. The first step is to use sits_mgrs_to_roi() to get the region of interest expressed as a bounding box; this box is then entered as the roi parameter in the sits_cube() function. Since the World Cover data is available as a 3\\(^\\circ\\) by 3\\(^\\circ\\) grid, it is necessary to use sits_cube_copy() to extract the exact MGRS tile.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL &lt;- sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 &lt;- sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL &lt;- sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = \"~/sitsbook/tempdir/R/dc_ardcollections\"\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL = sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 = sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL = sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = \"~/sitsbook/tempdir/Python/dc_ardcollections\"\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\n\n\n\nFigure 4.19: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "intro_visualisation.html#references-unnamed",
    "href": "intro_visualisation.html#references-unnamed",
    "title": "\n3  Data visualisation\n",
    "section": "\n3.4 References {unnamed}",
    "text": "3.4 References {unnamed}\n\n\n\n\n[1] \nM. Herold, R. Hubald, and A. Di Gregorio, “Translating and evaluating land cover legends using the UN Land Cover Classification System (LCCS),” GOFC-GOLD Florence, Italy, 2009."
  },
  {
    "objectID": "dc_ardcollections.html#references",
    "href": "dc_ardcollections.html#references",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nK. R. Ferreira et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products,” Remote Sensing, vol. 12, no. 24, p. 4033, 2020, doi: 10.3390/rs12244033."
  },
  {
    "objectID": "intro_quicktour.html#python-1",
    "href": "intro_quicktour.html#python-1",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.2 Python",
    "text": "1.2 Python\n\n# Create a data cube using local files\nsinop_cube = sits_cube(\n  source = \"BDC\", \n  collection  = \"MOD13Q1-6.1\",\n  bands = [\"NDVI\", \"EVI\"],\n  data_dir = \"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/sitsdata/extdata/sinop\",  \n  parse_info = [\"satellite\", \"sensor\", \"tile\", \"band\", \"date\"]\n)\n# Plot the NDVI for the first date (2013-09-14)\nplot(sinop_cube, \n     band = \"NDVI\", \n     dates = \"2013-09-14\",\n     palette = \"RdYlGn\")\n\n\n\nFigure 1.2: False color MODIS image for NDVI band in 2013-09-14.\n\n\n\nThe aim of the parse_info parameter is to extract tile, band, and date information from the file name. Given the large variation in image file names generated by different produces, it includes designators such as X1 and X2; these are place holders for parts of the file name that is not relevant to sits_cube().\nThe R object returned by sits_cube() contains the metadata describing the contents of the data cube. It includes data source and collection, satellite, sensor, tile in the collection, bounding box, projection, and list of files. Each file refers to one band of an image at one of the temporal instances of the cube.\n\n# Show the description of the data cube\nsinop_cube\n\n# A tibble: 1 × 11\n  source collection satellite sensor tile     xmin    xmax    ymin    ymax crs  \n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n1 BDC    MOD13Q1-6… TERRA     MODIS  0120… -6.18e6 -5.96e6 -1.35e6 -1.23e6 \"PRO…\n# ℹ 1 more variable: file_info &lt;list&gt;\n\n\nThe list of image files which make up the data cube is stored as a data frame in the column file_info. For each file, sits stores information about spectral band, reference date, size, spatial resolution, coordinate reference system, bounding box, path to file location and cloud cover information (when available).\n\n# Show information on the images files which are part of a data cube\nsinop_cube$file_info[[1]]\n\n# A tibble: 69 × 13\n   fid   band  date       nrows ncols  xres  yres      xmin      ymin      xmax\n   &lt;chr&gt; &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 1     CLOUD 2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n 2 1     EVI   2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n 3 1     NDVI  2013-09-14   551   944  232.  232. -6181982. -1353336. -5963298.\n 4 2     CLOUD 2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n 5 2     EVI   2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n 6 2     NDVI  2013-09-30   551   944  232.  232. -6181982. -1353336. -5963298.\n 7 3     CLOUD 2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n 8 3     EVI   2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n 9 3     NDVI  2013-10-16   551   944  232.  232. -6181982. -1353336. -5963298.\n10 4     CLOUD 2013-11-01   551   944  232.  232. -6181982. -1353336. -5963298.\n# ℹ 59 more rows\n# ℹ 3 more variables: ymax &lt;dbl&gt;, crs &lt;chr&gt;, path &lt;chr&gt;\n\n\nA key attribute of a data cube is its timeline, as shown below. The command sits_timeline() lists the temporal references associated to sits objects, including samples, data cubes and models.\n\n# Show the R object that describes the data cube\nsits_timeline(sinop_cube)\n\n [1] \"2013-09-14\" \"2013-09-30\" \"2013-10-16\" \"2013-11-01\" \"2013-11-17\"\n [6] \"2013-12-03\" \"2013-12-19\" \"2014-01-01\" \"2014-01-17\" \"2014-02-02\"\n[11] \"2014-02-18\" \"2014-03-06\" \"2014-03-22\" \"2014-04-07\" \"2014-04-23\"\n[16] \"2014-05-09\" \"2014-05-25\" \"2014-06-10\" \"2014-06-26\" \"2014-07-12\"\n[21] \"2014-07-28\" \"2014-08-13\" \"2014-08-29\"\n\n\nThe timeline of the sinop_cube data cube has 23 intervals with a temporal difference of 16 days. The chosen dates capture the agricultural calendar in Mato Grosso, Brazil. The agricultural year starts in September-October with the sowing of the summer crop (usually soybeans) which is harvested in February-March. Then the winter crop (mostly Corn, Cotton or Millet) is planted in March and harvested in June-July. For LULC classification, the training samples and the date cube should share a timeline with the same number of intervals and similar start and end dates."
  },
  {
    "objectID": "intro_quicktour.html#python-10",
    "href": "intro_quicktour.html#python-10",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.7 Python",
    "text": "1.7 Python\n\n# Perform spatial smoothing\nsinop_bayes = sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\nplot(sinop_bayes, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nFigure 1.7: Smoothed probability map for class Forest.\n\n\n\n:::"
  },
  {
    "objectID": "intro_quicktour.html#python-12",
    "href": "intro_quicktour.html#python-12",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.7 Python",
    "text": "1.7 Python\n\n# Show the location of the classification file\nsinop_map[\"file_info\"][0]\n\n&lt;string&gt;:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n    band start_date   end_date  ncols  nrows        xres        yres  \\\n1  class 2013-09-14 2014-08-29  944.0  551.0  231.656358  231.656358   \n\n           xmin          xmax          ymin          ymax  \\\n1 -6.181982e+06 -5.963298e+06 -1.353336e+06 -1.225694e+06   \n\n                                                path  \n1  /Users/gilbertocamara/sitsbook/tempdir/Python/..."
  },
  {
    "objectID": "intro_quicktour.html#python-5",
    "href": "intro_quicktour.html#python-5",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.3 Python",
    "text": "1.3 Python\n\n# Load the time series for the first MODIS sample for Mato Grosso\nsamples_matogrosso_modis[\"time_series\"][1]\n\n&lt;string&gt;:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n        Index    NDVI     EVI\n1  2014-09-14  0.3635  0.2127\n2  2014-09-30  0.4844  0.2692\n..        ...     ...     ...\n22 2015-08-13  0.4522  0.2955\n23 2015-08-29  0.4166  0.2662\n\n[23 rows x 3 columns]\n\n\n::: The distribution of samples per class can be obtained using the summary() command. The classification schema uses nine labels, four associated to crops (Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet), two with natural vegetation (Cerrado, Forest) and one to Pasture.\n\n\nR\nPython\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_matogrosso_modis)\n\n# A tibble: 7 × 3\n  label      count   prop\n  &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado      379 0.206 \n2 Forest       131 0.0713\n3 Pasture      344 0.187 \n4 Soy_Corn     364 0.198 \n5 Soy_Cotton   352 0.192 \n6 Soy_Fallow    87 0.0474\n7 Soy_Millet   180 0.0980\n\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_matogrosso_modis)\n\n\n\n\nIt is helpful to plot the dispersion of the time series. In what follows, for brevity, we will filter only one label (Forest) and select one index (NDVI).\n\n\nR\nPython\n\n\n\n\n# Select all samples with label \"Forest\" using `dplyr::filter`\n# since the label attribute is a column of the samples data.frame\nsamples_forest &lt;- dplyr::filter(\n    samples_matogrosso_modis, \n    label == \"Forest\"\n) |&gt; \n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsits_select(\n    bands = \"NDVI\"\n) |&gt; \nplot()\n\n\n\nFigure 1.3: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\n\n# Select all samples with label \"Forest\" using `query` \n# since the label attribute is a column of the samples data.frame\nsamples_forest = samples_matogrosso_modis.query('label == \"Forest\"')\n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsamples_forest_ndvi = sits_select(samples_forest, bands = \"NDVI\")\n# plot the samples for label Forest and band NDVI\nplot(samples_forest_ndvi)\n\n\n\n\nThe above figure shows all the time series associated with label Forest and band NDVI (in light blue), highlighting the median (shown in dark red) and the first and third quartiles (shown in brown). The spikes are noise caused by the presence of clouds."
  },
  {
    "objectID": "intro_quicktour.html#python-11",
    "href": "intro_quicktour.html#python-11",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.6 Python",
    "text": "1.6 Python\n\n# Perform spatial smoothing\nsinop_bayes = sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\nplot(sinop_bayes, labels = \"Forest\", palette = \"BuGn\")\n\n\n\nFigure 1.8: Smoothed probability map for class Forest.\n\n\n\n:::"
  },
  {
    "objectID": "dc_ardcollections.html#python-15",
    "href": "dc_ardcollections.html#python-15",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.13 Python",
    "text": "4.13 Python\n\n# get roi for an MGRS tile\nbbox_22LBL = sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 = sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL = sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = \"~/sitsbook/tempdir/Python/dc_ardcollections\"\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\nFigure 4.19: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "dc_ardcollections.html#python",
    "href": "dc_ardcollections.html#python",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.1 Python",
    "text": "4.1 Python"
  },
  {
    "objectID": "dc_ardcollections.html#python-14",
    "href": "dc_ardcollections.html#python-14",
    "title": "\n4  Access to ARD Image collections\n",
    "section": "\n4.12 Python",
    "text": "4.12 Python\n\n# get roi for an MGRS tile\nbbox_22LBL = sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 = sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL = sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = \"~/sitsbook/tempdir/Python/dc_ardcollections\"\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\nFigure 4.19: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "index.html#how-does-one-run-sits-in-python",
    "href": "index.html#how-does-one-run-sits-in-python",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "How does one run SITS in Python?",
    "text": "How does one run SITS in Python?\nIt is quite easy to run sits in Python. Follow the instructions on the “Setup” chapter on how to set you Python environment to interface with R. Then follow the examples provided in this book on using sits in Python. The book provides code both in R and in Python. Therefore, after correctly setting up their working environment, Python experts can run sits functions in their favorite tools, such as Jupyter notebooks."
  },
  {
    "objectID": "introduction.html#additional-functions-in-sits",
    "href": "introduction.html#additional-functions-in-sits",
    "title": "Introduction",
    "section": "Additional functions in SITS",
    "text": "Additional functions in SITS\nIn addition to the eight basic functions of its API, sits supports additional tools for improving training data quality and evaluating classification results. They include:\n\nPerforming quality control and filtering on the time series samples.\nMerge multi-source data to capture responses from different sensors.\nMeasure classification uncertainty to support active learning.\nSupport vector data cubes and object-based time series image analysis.\nEvaluate the accuracy of the classification using best practices.\n\nThese functions are also described in this book."
  },
  {
    "objectID": "datacubes.html#analysis-ready-data-collections",
    "href": "datacubes.html#analysis-ready-data-collections",
    "title": "Earth observation data cubes",
    "section": "Analysis-ready data collections",
    "text": "Analysis-ready data collections\nAnalysis Ready Data (CEOS-ARD) are satellite data that have been processed to meet the ARD standards defined by the Committee on Earth Observation Satellites (CEOS). ARD data simplify and accelerate the analysis of Earth observation data by providing consistent and high-quality data that are standardized across different sensors and platforms. ARD images processing includes geometric corrections, radiometric corrections, and sometimes atmospheric corrections. Images are georeferenced, meaning they are accurately aligned with a coordinate system. Optical ARD images include cloud and shadow masking information. These masks indicate which pixels affected by clouds or cloud shadows. For optical sensors, CEOS-ARD images have to be converted to surface reflectance values, which represent the fraction of light that is reflected by the surface. This makes the data more comparable across different times and locations. For SAR images, CEOS-ARD specification require images to undergo Radiometric Terrain Correction (RTC) and are provided in the GammaNought (\\(\\gamma_0\\)) backscatter values. This value which mitigates the variations from diverse observation geometries and is recommended for most land applications.\nARD images are available from various satellite platforms, including Landsat, Sentinel, and commercial satellites. This provides a wide range of spatial, spectral, and temporal resolutions to suit different applications. They are organised as a collection of files, where each pixel contains a single value for each spectral band for a given date. These collections are available in cloud services such as Brazil Data Cube, Digital Earth Africa, and Microsoft’s Planetary Computer. In general, the timelines of the images of an ARD collection are different. Images still contain cloudy or missing pixels; bands for the images in the collection may have different resolutions. Figure Figure 4.1 shows an example of the Landsat ARD image collection.\n\n\n\n\nFigure 1: ARD image collection (source: USGS)."
  },
  {
    "objectID": "datacubes.html#regular-earth-observation-data-cubes",
    "href": "datacubes.html#regular-earth-observation-data-cubes",
    "title": "Earth observation data cubes",
    "section": "Regular Earth observation data cubes",
    "text": "Regular Earth observation data cubes\nA regular EO data cube is a multidimensional array—typically x (longitude), y (latitude), time, and spectral band—containing “analysis-ready” satellite observations that have already been geometrically aligned, radiometrically calibrated and stored on a common grid. In other words, every pixel in the cube represents the same ground location over a sequence of dates, so the user can retrieve a complete, consistent time series with a single query."
  }
]