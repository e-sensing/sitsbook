[
  {
    "objectID": "intro_examples.html",
    "href": "intro_examples.html",
    "title": "\n2  How to use SITS with real examples\n",
    "section": "",
    "text": "3 Conclusion\nThese examples demonstrate the complete workflow for generating a classified land use and land cover map using the SITS package. The process includes data cube creation, sample extraction, model training with machine and deep learning approaches, classification, and post-processing steps such as smoothing and labeling. By following these steps, users can apply the SITS package to produce accurate and spatially coherent maps from satellite image time series, supporting environmental monitoring and land change analysis."
  },
  {
    "objectID": "intro_examples.html#land-use-and-land-cover-map-for-brazilian-cerrado",
    "href": "intro_examples.html#land-use-and-land-cover-map-for-brazilian-cerrado",
    "title": "\n2  How to use SITS with real examples\n",
    "section": "\n2.2 Land Use and Land cover Map for Brazilian Cerrado",
    "text": "2.2 Land Use and Land cover Map for Brazilian Cerrado\nThe Cerrado is the second largest biome in Brazil, covering approximately 1.9 million \\(km^2\\). This tropical savanna ecoregion is characterized by a rich and diverse ecosystem that ranges from grasslands to woodlands. It is home to over 7,000 plant species, many of which are endemic [4]. The biome comprises three major types of natural vegetation: Open Cerrado, predominantly composed of grasses and small shrubs with occasional small trees; Cerrado Sensu Stricto, a typical savanna formation characterized by low, thin-trunked trees with irregular branching; and Cerradão, a dry forest composed of medium-sized trees reaching up to 10–12 meters in height [5], [6]. Despite its ecological importance, large portions of the Cerrado are being rapidly converted to agricultural land, making it one of the world’s most dynamic agricultural frontiers [7]. The primary agricultural activities include cattle ranching, crop farming, and the establishment of planted forests.\nIn this example, we replicated the results presented in the article “Satellite Image Time Series Analysis for Big Earth Observation Data” [8], using Landsat imagery accessed through the Microsoft Planetary Computer (MPC) platform. We have selected a small region to illustrate all the steps involved in generating a land use and land cover map. The classification period ranges from September 2017 to August 2018, following the agricultural calendar. A temporal resolution of 16 days was used throughout the analysis.\n\n2.2.1 Cerrado samples\nAccurate classification requires a high-quality sample set. To obtain reliable training data, we conducted a systematic sampling using a 5 × 5 km grid across the entire Cerrado biome, resulting in the collection of 85,026 samples. The class labels for the training data were derived from three authoritative sources: the 2018 pastureland map provided by Pastagem.org [9], MapBiomas Collection 5 for the year 2018 [10], and land use maps from Brazil’s national mapping agency, IBGE, covering the period from 2016 to 2018 [11]. From the initial set of 85,026 samples, we retained only those for which all three sources agreed on the assigned label. This filtering process resulted in a final dataset of 48,850 sample points. Time series data for these points were extracted from the Landsat-8 data cube. The distribution of samples by class is as follows: Annual Crop (6,887), Cerradão (4,211), Cerrado (16,251), Natural Non-Vegetated (38), Open Cerrado (5,658), Pasture (12,894), Perennial Crop (68), Silviculture (805), Sugarcane (1,775), and Water (263).\n\n\nR\nPython\n\n\n\n\n# Read the Cerrado samples\ndata(\"samples_cerrado_lc8_examples\")\n\n\n\n\n# Read the Cerrado samples\nsamples_cerrado_lc8_examples = load_samples_dataset(\n    name    = \"samples_cerrado_lc8_examples\", \n    package = \"sitsdata\"\n)\n\n\n\n\nSITS package offers a range of functions to support a deeper understanding of the temporal behavior of sample data. One such function is sits_patterns(). In the example below, we showed the temporal patterns of each class.\n\n\nR\nPython\n\n\n\n\n# Generate the temporal patterns\nsamples_patterns &lt;- sits_patterns(samples_cerrado_lc8_examples)\n\n# Plot samples patterns\nplot(samples_patterns)\n\n\n\n\n# Generate the temporal patterns\nsamples_patterns = sits_patterns(samples_cerrado_lc8_examples)\n\n# Plot samples patterns\nplot(samples_patterns)\n\n\n\n\n\n\n\n\nFigure 2.6: Temporal patterns of each class.\n\n\n\nThe plot presents the temporal patterns of the sample classes. The pattern observed for the Annual Crop class reflects the typical agricultural dynamics in cropping regions in Cerrado, characterized by the beginning of the season, followed by a maximum in vegetation vigor and then a senescence phase. This pattern repeats twice, indicating the presence of a double-cropping system. In contrast, the Sugarcane class exhibits a semi-perennial pattern. These distinct temporal behaviors can be derived through analysis of the pattern plot.\n\n2.2.2 Acessing Landsat images\nLet us start by accessing the Landsat collection through the Microsoft Planetary Computer provider. First, we need to define the region of interest (ROI) where the example will take place. This ROI is then used to select the tiles that intersect with it, as shown below:\n\n\nR\nPython\n\n\n\n\n# Define a roi for the example\nroi &lt;- c(\n    lon_min = -50.780,\n    lat_min = -13.392,\n    lon_max = -50.540,\n    lat_max = -13.249\n)\n\n# Access the Landsat through the MPC\nmpc_cube &lt;- sits_cube(\n    source = \"MPC\",\n    collection  = \"LANDSAT-C2-L2\",\n    bands = c(\"BLUE\", \"GREEN\", \"RED\", \"NIR08\", \"SWIR16\", \"SWIR22\", \"CLOUD\"),\n    roi = roi,\n    start_date = \"2017-08-29\",\n    end_date = \"2018-08-29\"\n)\n\n\n\n\n# Define a roi for the example\nroi = dict(\n    lon_min = -50.780,\n    lat_min = -13.392,\n    lon_max = -50.540,\n    lat_max = -13.249\n)\n\n# Access the Landsat through the MPC\nmpc_cube = sits_cube(\n    source = \"MPC\",\n    collection  = \"LANDSAT-C2-L2\",\n    bands = (\"BLUE\", \"GREEN\", \"RED\", \"NIR08\", \"SWIR16\", \"SWIR22\", \"CLOUD\"),\n    roi = roi,\n    start_date = \"2017-08-29\",\n    end_date = \"2018-08-29\"\n)\n\n\n\n\nThe created data cube is not regular (for more details, see Regular Earth observation data cubes), which means that it needs to be regularized before proceeding with the analysis. We recommend copying the images locally before applying the regularization process. Although not mandatory, this is considered a good practice. In cases where the machine does not have sufficient storage space, regularization can be performed directly on the remote data.\nIn this experiment, we use the sits_cube_copy() function to copy the remote images locally. This function supports the roi parameter, allowing the images to be cropped during the download process. It also supports the multicores parameter, which can significantly improve download speed.\n\n\nR\nPython\n\n\n\n\n# Copy images locally\nmpc_cube &lt;- sits_cube_copy(\n  cube = mpc_cube,\n  roi = roi,\n  multicores = 5,\n  output_dir = tempdir_cerrado_cube\n)\n\n\n\n\n# Copy images locally\nmpc_cube = sits_cube_copy(\n  cube = mpc_cube,\n  roi = roi,\n  multicores = 5,\n  output_dir = tempdir_cerrado_cube\n)\n\n\n\n\nAs discussed in the Earth observation data cubes chapter, the SITS package can access both local and remote images. As shown in the code below, we read the images locally after copying them. The only difference between accessing remote and local images is the data_dir parameter, which specifies the directory where the images are stored.\n\n\nR\nPython\n\n\n\n\n# Access local data cube\nmpc_cube &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"LANDSAT-C2-L2\",\n    data_dir = tempdir_cerrado_cube\n)\n\n\n\n\n# Access local data cube\nmpc_cube = sits_cube(\n    source = \"MPC\",\n    collection = \"LANDSAT-C2-L2\",\n    data_dir = tempdir_cerrado_cube\n)\n\n\n\n\nAfter creating the local data cube, we can proceed with the regularization step using the sits_regularize() function. In this case, we provide the timeline parameter to define a custom timeline, specifically the timeline of the reference samples. This ensures consistency between the data cube and the samples. For more details on the regularization process, please refer to the corresponding chapter in Building regular data cubes.\n\n\nR\nPython\n\n\n\n\n# Samples timeline\ntimeline &lt;- sits_timeline(samples_cerrado_lc8_examples)\n\n# Regularize the local cube\ncube_reg &lt;- sits_regularize(\n    cube = mpc_cube,\n    period = \"P16D\",\n    res = 30,\n    roi = roi,\n    multicores = 5,\n    output_dir = tempdir_cerrado,\n    timeline = timeline\n)\n\n\n\n\n# Samples timeline\ntimeline = sits_timeline(samples_cerrado_lc8_examples)\n\n# Regularize the local cube\ncube_reg = sits_regularize(\n    cube = mpc_cube,\n    period = \"P16D\",\n    res = 30,\n    roi = roi,\n    multicores = 5,\n    output_dir = tempdir_cerrado,\n    timeline = timeline\n)\n\n\n\n\n\n2.2.3 Computing vegetation indices\nSITS provides a range of functions to perform various operations on a data cube, as highlighted in the Computing NDVI and other spectral indices chapter. One of these functions is sits_apply(), which allows users to generate indices using R expressions. In the example below, we compute NDVI and EVI indices to illustrate how this function can be used for generating vegetation indices.\n\n\nR\nPython\n\n\n\n\n# Calculate NDVI index using bands NIR08 and RED\ncube_reg &lt;- sits_apply(\n    data = cube_reg,\n    NDVI = (NIR08 - RED) / (NIR08 + RED),\n    output_dir = tempdir_cerrado,\n    multicores = 5,\n    memsize    = 8\n)\n\n# Calculate EVI index using bands NIR08 and RED\ncube_reg &lt;- sits_apply(\n    data       = cube_reg,\n    EVI        = 2.5 * ((NIR08 - RED) / (NIR08 + 6 * RED - 7.5 * BLUE + 1)),\n    output_dir = tempdir_cerrado,\n    multicores = 5,\n    memsize    = 8\n)\n\n# Plot NDVI for the second date (2017-09-14)\nplot(cube_reg,\n  band = \"NDVI\",\n  dates = \"2017-09-14\",\n  palette = \"RdYlGn\"\n)\n\n\n\n\n# Calculate NDVI index using bands NIR08 and RED\ncube_reg = sits_apply(\n    data = cube_reg,\n    NDVI = \"(NIR08 - RED) / (NIR08 + RED)\",\n    output_dir = tempdir_cerrado,\n    multicores = 5,\n    memsize    = 8\n)\n\n# Calculate EVI index using bands NIR08 and RED\ncube_reg = sits_apply(\n    data       = cube_reg,\n    EVI        = \"2.5 * ((NIR08 - RED) / (NIR08 + 6 * RED - 7.5 * BLUE + 1))\",\n    output_dir = tempdir_cerrado,\n    multicores = 5,\n    memsize    = 8\n)\n\n# Plot NDVI for the second date (2017-09-14)\nplot(cube_reg,\n  band = \"NDVI\",\n  dates = \"2017-09-14\",\n  palette = \"RdYlGn\"\n)\n\n\n\n\n\n\n\n\nFigure 2.7: NDVI image generated by sits_apply().\n\n\n\n\n2.2.4 Getting time series\nAfter creating the data cube, the next step is to extract the time series for the sample locations. Since we already have the samples time series for the entire Cerrado, the code below serves only as an example to demonstrate how to extract sample time series from the data cube.\n\n\nR\nPython\n\n\n\n\n# Extract the time series\nsamples &lt;- sits_get_data(\n    cube = cube_reg,\n    samples = samples_cerrado_lc8_examples,\n    multicores = 5\n)\n\n# Show the tibble with the first three points\nprint(samples[1:3,])\n\n\n\n# A tibble: 3 × 7\n  longitude latitude start_date end_date   label        cube         time_series\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;        &lt;chr&gt;        &lt;list&gt;     \n1     -50.8    -13.4 2017-08-29 2018-08-29 Open_Cerrado LANDSAT-C2-… &lt;tibble&gt;   \n2     -50.8    -13.3 2017-08-29 2018-08-29 Open_Cerrado LANDSAT-C2-… &lt;tibble&gt;   \n3     -50.8    -13.3 2017-08-29 2018-08-29 Open_Cerrado LANDSAT-C2-… &lt;tibble&gt;   \n\n\n\n\n\n# Extract the time series\nsamples = sits_get_data(\n    cube = cube_reg,\n    samples = samples_cerrado_lc8_examples,\n    multicores = 5\n)\n\n# Show the tibble with the first three points\nsamples[0:2]\n\n\n\n   longitude   latitude  start_date    end_date         label           cube                                        time_series\n0 -50.777271 -13.362440  2017-08-29  2018-08-29  Open_Cerrado  LANDSAT-C2-L2           Index      BLUE     EVI  ...       RE...\n1 -50.777271 -13.317524  2017-08-29  2018-08-29  Open_Cerrado  LANDSAT-C2-L2           Index      BLUE     EVI  ...       RE...\n\n\n\n\n\n\n2.2.5 Training a Deep Learning model\nThe next step in producing a classified map is training a model. This step consists of two phases: tuning the hyperparameters and training the model. Although hyperparameter tuning is not mandatory, it is highly recommended, as it helps identify the most suitable configuration that best fits the sample data. In this case, we used the Temporal Convolutional Neural Network (TempCNN) model [12], which was also used by the original authors of the study. The TempCNN architecture contains three layers of 1D convolutions, using kernels to capture local temporal patterns from time series.\nIn the code below, we tune the TempCNN model by adjusting the number of convolutional layers, kernel sizes, learning rate, and weight decay. This function may take several minutes to execute, depending on the hardware. The execution time can be significantly reduced by leveraging GPU acceleration. For further details, please refer to the training and running deep learning models section.\n\n\nR\nPython\n\n\n\n\ntuned_tempcnn &lt;- sits_tuning(\n    samples = samples_cerrado_lc8_examples,\n    ml_method = sits_tempcnn(),\n    params = sits_tuning_hparams(\n        optimizer = torch::optim_adamw,\n        cnn_layers = choice(\n            c(32, 32, 32), c(64, 64, 64), c(128, 128, 128)\n        ),\n        cnn_kernels = choice(\n            c(5, 5, 5), c(7, 7, 7), c(9, 9, 9)\n        ),\n        cnn_dropout_rates = choice(\n            c(0.2, 0.2, 0.2), c(0.3, 0.3, 0.3), c(0.4, 0.4, 0.4)\n        ),\n        opt_hparams = list(\n            lr = uniform(10^-4, 10^-2),\n            weight_decay = uniform(10^-6, 10^-3)\n        )\n    ),\n    trials = 10,\n    multicores = 8,\n    progress = TRUE\n)\n\n# Printing the best parameters:\ntuned_tempcnn\n\n\n\n# A tibble: 10 × 19\n   accuracy kappa acc        samples_validation cnn_layers cnn_kernels\n      &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;             &lt;list&gt;     &lt;list&gt;     \n 1    0.885 0.852 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 2    0.880 0.845 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 3    0.877 0.842 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 4    0.876 0.841 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 5    0.875 0.838 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 6    0.871 0.833 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 7    0.869 0.831 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 8    0.868 0.830 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 9    0.868 0.829 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n10    0.867 0.828 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n# ℹ 13 more variables: cnn_dropout_rates &lt;list&gt;, dense_layer_nodes &lt;list&gt;,\n#   dense_layer_dropout_rate &lt;list&gt;, epochs &lt;list&gt;, batch_size &lt;list&gt;,\n#   validation_split &lt;list&gt;, optimizer &lt;list&gt;, opt_hparams &lt;list&gt;,\n#   lr_decay_epochs &lt;list&gt;, lr_decay_rate &lt;list&gt;, patience &lt;list&gt;,\n#   min_delta &lt;list&gt;, verbose &lt;list&gt;\n\n\n\n\n\ntuned_mt = sits_tuning(\n     samples = samples_matogrosso_mod13q1,\n     ml_method = sits_lighttae,\n     params = sits_tuning_hparams(\n         optimizer = \"torch::optim_adamw\",\n         cnn_layers = hparam(\n            \"choice\", (32, 32, 32), (64, 64, 64), (128, 128, 128)\n         ),\n         cnn_kernels = hparam(\n            \"choice\", (5, 5, 5), (7, 7, 7), (9, 9, 9)\n         ),\n         cnn_dropout_rates = hparam(\n            \"choice\", (0.15, 0.15, 0.15), (0.2, 0.2, 0.2), \n                        (0.3, 0.3, 0.3), (0.4, 0.4, 0.4)\n         ),\n         opt_hparams = dict(\n             lr = hparam(\"uniform\", 10**-4, 10**-2),\n             weight_decay = hparam(\"uniform\", 10**-6, 10**-3)\n         )\n     ),\n     trials = 40,\n     multicores = 6,\n     progress = False\n)\n\n# Printing the best parameters:\ntuned_tempcnn\n\n\n\n# A tibble: 10 × 19\n   accuracy kappa acc        samples_validation cnn_layers cnn_kernels\n      &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;             &lt;list&gt;     &lt;list&gt;     \n 1    0.885 0.852 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 2    0.880 0.845 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 3    0.877 0.842 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 4    0.876 0.841 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 5    0.875 0.838 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 6    0.871 0.833 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 7    0.869 0.831 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 8    0.868 0.830 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n 9    0.868 0.829 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n10    0.867 0.828 &lt;cnfsnMtr&gt; &lt;NULL&gt;             &lt;language&gt; &lt;language&gt; \n# ℹ 13 more variables: cnn_dropout_rates &lt;list&gt;, dense_layer_nodes &lt;list&gt;,\n#   dense_layer_dropout_rate &lt;list&gt;, epochs &lt;list&gt;, batch_size &lt;list&gt;,\n#   validation_split &lt;list&gt;, optimizer &lt;list&gt;, opt_hparams &lt;list&gt;,\n#   lr_decay_epochs &lt;list&gt;, lr_decay_rate &lt;list&gt;, patience &lt;list&gt;,\n#   min_delta &lt;list&gt;, verbose &lt;list&gt;\n\n\n\n\n\nWe can now take the best-fitting parameters and use them to train our deep learning model. The selected hyperparameters were 64 convolutional filters, a kernel size of 7, and a dropout rate of 0.3.\n\n2.2.6 Training a deep learning model\n\n\nR\nPython\n\n\n\n\nset.seed(03022024)\n# Train using tempCNN\ntempcnn_model &lt;- sits_train(\n  samples_cerrado_lc8_examples,\n  sits_tempcnn(\n    optimizer         = torch::optim_adamw,\n    cnn_layers        = c(64, 64, 64),\n    cnn_kernels       = c(7, 7, 7),\n    cnn_dropout_rates = c(0.3, 0.3, 0.3),\n    epochs            = 50,\n    batch_size        = 256,\n    validation_split  = 0.2,\n    opt_hparams = list(lr = 0.002024501,\n                       weight_decay = 0.0006641582),\n    verbose = TRUE\n  )\n)\n\n# Show training evolution\nplot(tempcnn_model)\n\n\n\n\nr_set_seed(03022024)\n\n# Train using tempCNN\ntempcnn_model = sits_train(\n  samples_cerrado_lc8_examples,\n  sits_tempcnn(\n    optimizer         = \"torch::optim_adamw\",\n    cnn_layers        = (64, 64, 64),\n    cnn_kernels       = (7, 7, 7),\n    cnn_dropout_rates = (0.3, 0.3, 0.3),\n    epochs            = 50,\n    batch_size        = 256,\n    validation_split  = 0.2,\n    opt_hparams = dict(lr = 0.002024501,\n                       weight_decay = 0.0006641582),\n    verbose = True\n  )\n)\n\n# Show training evolution\nplot(tempcnn_model)\n\n\n\n\n\n\n\n\nFigure 2.8: Training loss and validation accuracy over epochs during the TempCNN model training.\n\n\n\n\n2.2.7 Data cube classification\nThe final step is to produce the probability map, which involves applying the trained model to classify the time series within the data cube.\n\n\nR\nPython\n\n\n\n\n# Classify the raster image\ncerrado_probs &lt;- sits_classify(\n  data = cube_reg,\n  ml_model = tempcnn_model,\n  multicores = 5,\n  memsize = 8,\n  output_dir = tempdir_cerrado\n)\n\n# Plot the probability cube for class Forest\nplot(cerrado_probs, labels = \"Cerrado\", palette = \"BuGn\")\n\n\n\n\n# Classify the raster image\ncerrado_probs = sits_classify(\n  data = cube_reg,\n  ml_model = tempcnn_model,\n  multicores = 5,\n  memsize = 8,\n  output_dir = tempdir_cerrado\n)\n\n# Plot the probability cube for class Forest\nplot(cerrado_probs, labels = \"Cerrado\", palette = \"BuGn\")\n\n\n\n\n\n\n\n\nFigure 2.9: Probability map for the Cerrado class.\n\n\n\nAfter generating the probability map, we highly recommend applying a smoothing operation based on the spatial neighborhood. This step is essential for incorporating spatial context into the classification results. We used the default values for smoothness and window_size, which serve as a solid starting point. However, we encourage users to adjust these parameters according to the specific needs of their application. For more about the sits_smooth() function, please refer to Bayesian smoothing for classification post-processing chapter.\n\n\nR\nPython\n\n\n\n\n# Perform spatial smoothing\ncerrado_bayes &lt;- sits_smooth(\n  cube = cerrado_probs,\n  multicores = 5,\n  memsize = 8,\n  output_dir = tempdir_cerrado\n)\n\n# Plot the smooth cube for class Forest\nplot(cerrado_bayes, labels = \"Cerrado\", palette = \"BuGn\")\n\n\n\n\n# Perform spatial smoothing\ncerrado_bayes = sits_smooth(\n  cube = cerrado_probs,\n  multicores = 5,\n  memsize = 8,\n  output_dir = tempdir_cerrado\n)\n\n# Plot the smooth cube for class Forest\nplot(cerrado_bayes, labels = \"Cerrado\", palette = \"BuGn\")\n\n\n\n\n\n\n\n\nFigure 2.10: Smooth map for the Cerrado class.\n\n\n\nThe final step is labeling the probability map. This involves selecting the highest probability values and assigning class labels to them based on the sample classes.\n\n\nR\nPython\n\n\n\n\n# Label the probability file\ncerrado_map &lt;- sits_label_classification(\n  cube = cerrado_bayes,\n  output_dir = tempdir_cerrado\n)\n\n# Plot the land use and land cover map\nplot(cerrado_map)\n\n\n\n\n# Label the probability file\ncerrado_map = sits_label_classification(\n  cube = cerrado_bayes,\n  output_dir = tempdir_cerrado\n)\n\n# Plot the land use and land cover map\nplot(cerrado_map)\n\n\n\n\n\n\n\n\nFigure 2.11: Labeled land use and land cover map for the region of interest.\n\n\n\n\n2.2.8 Accuracy assessment of classified images\nSo far, we have generated the classification for a small region to demonstrate how the SITS package can be applied in a real-world scenario. For the validation step, we extended the classification to the entire Cerrado biome using the reference samples provided by Simoes et al. (2021) [8], in order to evaluate the results with real and independently collected data.\nwe used 5402 validation points, which are independent of the training set used in the classification. They were interpreted by five specialists using high resolution images from the same period of the classification. For the assessment, we merged the labels Cerradao, Cerrado, and Open Cerrado into one label called Cerrado. We also did additional sampling to reach a minimal number of samples for the classes Natural Non Vegetated, Perennial Crop, and Water.\n\n\nR\nPython\n\n\n\n\n# Cerrado classification file\ncerrado_path = system.file(\n    \"/extdata/Cerrado-Class-2017-2018-Mosaic\", package = \"sitsdata\"\n)\n\n# Read the probability file\ncerrado_map &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"LANDSAT-C2-L2\",\n    bands = \"class\",\n    labels = c(\"1\" = \"Annual_Crop\", \"3\" = \"Cerrado\", \"4\" = \"Nat_NonVeg\",\n               \"6\" = \"Pasture\", \"7\" = \"Perennial_Crop\", \"8\" = \"Silviculture\",\n               \"9\" = \"Sugarcane\", \"10\" = \"Water\"),\n    data_dir = cerrado_path\n)\n\n# Plot Cerrado map\nplot(cerrado_map)\n\n\n\n\n\n\n\n# Cerrado classification file\ncerrado_path = r_package_dir(\n    content_dir = \"/extdata/Cerrado-Class-2017-2018-Mosaic\", \n    package = \"sitsdata\"\n)\n\n# Read the probability file\ncerrado_map = sits_cube(\n    source = \"MPC\",\n    collection = \"LANDSAT-C2-L2\",\n    bands = \"class\",\n    labels = {\"1\": \"Annual_Crop\", \"3\": \"Cerrado\", \"4\": \"Nat_NonVeg\",\n               \"6\": \"Pasture\", \"7\":\"Perennial_Crop\", \"8\": \"Silviculture\",\n               \"9\": \"Sugarcane\", \"10\": \"Water\"},\n    data_dir = cerrado_path\n)\n\n# Plot Cerrado map\nplot(cerrado_map)\n\n\n\n\n\n\n\nWe used the sits implementation of the area-weighted technique [13] to provide an unbiased estimator for the overall accuracy and the total area of each class based on the reference samples (see Map accuracy assessment). The classification accuracies are shown below. The overall accuracy of the classification was 0.83.\n\n\nR\nPython\n\n\n\n\n# Get ground truth points\nvalid_csv &lt;- system.file(\"extdata/csv/cerrado_lc8_validation.csv\",\n  package = \"sitsdata\"\n)\n\n# Calculate accuracy according to Olofsson's method\narea_acc &lt;- sits_accuracy(\n  data = cerrado_map,\n  validation = valid_csv,\n  multicores = 4\n)\n\n# Print the area estimated accuracy\narea_acc\n\nArea Weighted Statistics\nOverall Accuracy = 0.83\n\nArea-Weighted Users and Producers Accuracy\n               User Producer\nAnnual_Crop    0.87     0.82\nCerrado        0.91     0.86\nNat_NonVeg     0.94     0.27\nPasture        0.69     0.79\nPerennial_Crop 0.71     0.38\nSilviculture   0.82     0.80\nSugarcane      0.72     0.89\nWater          0.94     0.71\n\nMapped Area x Estimated Area (ha)\n               Mapped Area (ha) Error-Adjusted Area (ha) Conf Interval (ha)\nAnnual_Crop          22007606.5               23336717.8          1111777.8\nCerrado             116681183.2              122786133.9          2085647.1\nNat_NonVeg             261234.6                 911153.3           353704.1\nPasture              64237851.8               56008536.3          2155334.6\nPerennial_Crop         650393.2                1215456.5           382474.6\nSilviculture          4120936.3                4179525.9           506675.9\nSugarcane             4660479.4                3738766.0           508507.4\nWater                 1357575.1                1800970.6           320731.0\n\n\n\n\n\n# Get ground truth points\nvalid_csv = r_package_dir(\n    content_dir = \"extdata/csv/cerrado_lc8_validation.csv\",\n    package = \"sitsdata\"\n)\n\n# Calculate accuracy according to Olofsson's method\narea_acc = sits_accuracy(\n  data = cerrado_map,\n  validation = valid_csv,\n  multicores = 4\n)\n\n# Print the area estimated accuracy\narea_acc\n\n\n\nArea Weighted Statistics\nOverall Accuracy = 0.83\n\nArea-Weighted Users and Producers Accuracy\n               User Producer\nAnnual_Crop    0.87     0.82\nCerrado        0.91     0.86\nNat_NonVeg     0.94     0.27\nPasture        0.69     0.79\nPerennial_Crop 0.71     0.38\nSilviculture   0.82     0.80\nSugarcane      0.72     0.89\nWater          0.94     0.71\n\nMapped Area x Estimated Area (ha)\n               Mapped Area (ha) Error-Adjusted Area (ha) Conf Interval (ha)\nAnnual_Crop          22007606.5               23336717.8          1111777.8\nCerrado             116681183.2              122786133.9          2085647.1\nNat_NonVeg             261234.6                 911153.3           353704.1\nPasture              64237851.8               56008536.3          2155334.6\nPerennial_Crop         650393.2                1215456.5           382474.6\nSilviculture          4120936.3                4179525.9           506675.9\nSugarcane             4660479.4                3738766.0           508507.4\nWater                 1357575.1                1800970.6           320731.0"
  },
  {
    "objectID": "intro_examples.html#classifying-deforestation-in-rondonia",
    "href": "intro_examples.html#classifying-deforestation-in-rondonia",
    "title": "\n2  How to use SITS with real examples\n",
    "section": "\n2.1 Classifying deforestation in Rondonia",
    "text": "2.1 Classifying deforestation in Rondonia\nThe Amazon rainforest has a global importance due to its vast biodiversity and the critical ecosystem services it provides. As one of the most biodiverse regions on Earth, it plays a central role in regulating the planet’s climate and ecological balance. Undisturbed natural forests absorb large amounts of carbon dioxide from the atmosphere, which is vital for mitigating climate change.\nOur study area is the state of Rondônia (RO), located in the Brazilian Amazon. Deforestation in Rondonia is fragmented, in part due to the region’s historical occupation by smallholder settlers. This high degree of fragmentation presents significant challenges for automated classification methods, particularly in distinguishing between clear-cut areas and those that are highly degraded. While visual interpreters can rely on their experience and contextual field knowledge, automated approaches must be carefully trained to reach similar levels of accuracy and discernment.\n\n2.1.1 Deforestation samples in the Brazilian Amazon\nWe used an event-based sample, where an “event” refers to a specific occurrence associated with changes in land cover. For example, deforestation is a process characterized by multiple sequential events. Initially, portions of the forest are removed through selective logging; subsequently, surface fires—often set by poachers—are used to degrade the remaining forest and facilitate the extraction of large trees [1]. The process continues with the manual or mechanized removal of timber, potentially followed by additional fires to clear the area completely [2]. These events were associated with distinct land cover classes to be detected through time series classification, allowing the model to recognize and differentiate between various stages and types of land cover change.\nThe rondonia20LMR repository provides a dataset for testing machine learning algorithms applied to the mapping and monitoring of deforestation in the Brazilian Amazon. The dataset consists of a regular time series of Sentinel-2 satellite images covering the MGRS 20LMR tile, with 23 temporal instances from January 5 to December 23, 2022, at 16-day intervals. Each instance includes the spectral bands B02, B03, B04, B05, B06, B07, B08, B8A, B11, and B12. The samples contain time series for 792 selected locations, each assigned to one of the following classes: Clear_Cut_Bare_Soil (944), Clear_Cut_Burned_Area (983), Clear_Cut_Vegetation (603), Forest (964), Mountainside_Forest (211), Riparian_Forest (1247), Seasonally_Flooded (731), Water (109) and Wetland (215).\nTo access the data, it is needed to clone the rondonia20LMR repository from the e-sensing GitHub repository into a local directory. To do this, open a terminal and execute the following commands:\n\ngit clone https://github.com/e-sensing/rondonia20LMR.git\n\nAfter cloning the repository, the local directory path can be used to proceed with the example. This path will serve as the reference location for accessing the dataset and continuing with the subsequent steps in the analysis workflow.\nIn the rondonia20LMR directory, the file deforestation_samples_v18.rds contains time series extracted from Sentinel-2 satellite images over tile 20LMR. These data are automatically made available when the package is loaded. Each time series includes the following columns: longitude, latitude, start date, end date, label, cube, and time series.\n\n\nR\nPython\n\n\n\n\n# Samples path\nsamples_path &lt;- \"rondonia20LMR/inst/extdata/samples/deforestation_samples_v18.rds\"\n\n# Read the Cerrado samples\ndeforestation_samples_v18 &lt;- readRDS(samples_path)\n\n\n\n\n# Samples path\nsamples_path = \"rondonia20LMR/inst/extdata/samples/deforestation_samples_v18.rds\"\n\n# Read the Cerrado samples\ndeforestation_samples_v18 = read_rds(samples_path)\n\n\n\n\nAs shown in the previous example, the sits_patterns() function from the sits package allows the visualization of the temporal patterns of the classes. In the example below, we show these patterns for each class:\n\n\nR\nPython\n\n\n\n\n# Generate the temporal patterns\ndeforestation_samples_v18  |&gt; \n     sits_select(bands = c(\"B02\", \"B8A\", \"B11\")) |&gt; \n     sits_patterns() |&gt; \n     plot()\n\n\n\n\n# Generate the temporal patterns\nplot(\n    sits_patterns(\n        sits_select(deforestation_samples_v18, bands = (\"B02\", \"B8A\", \"B11\"))\n    )\n)\n\n\n\n\n\n\n\n\nFigure 2.1: Temporal patterns for each class.\n\n\n\nThe plot shows the temporal patterns of the different classes throughout the period. The classes Forest, Mountainside_Forest and Riparian_Forest display high values in band B8A (near-infrared), indicating high photosynthetic activity throughout the year. This is a typical characteristic of dense and evergreen vegetation. In contrast, the classes Clear_Cut_Bare_Soil and Clear_Cut_Burned_Area exhibit lower values in band B8A and increasing values in band B11 (shortwave infrared) during the second half of the year, suggesting exposed soil and possible presence of burned or degraded areas. The Seasonally_Flooded class shows seasonal variations in bands B8A and B11, possibly related to the hydrological dynamics of these areas. The Water class shows very low values across all analyzed bands, especially in B8A and B11, reflecting the strong absorption of water in these spectral regions. Finally, the Wetland class shows increasing values in bands B8A and B11 over the course of the year, indicating a possible transition between periods of water saturation and greater vegetation presence.\n\n2.1.2 Build Data Cube and visualize band combination\nLet us access the Sentinel images available in the repository. First, it is necessary to define the provider, which in this case is the Microsoft Planetary Computer (MPC). Then, we specify the directory path, as shown below:\n\n\nR\nPython\n\n\n\n\n# Images path\nimages_path &lt;- \"rondonia20LMR/inst/extdata/images/\"\n\n# Access local data cube\ncube_20LMR &lt;- sits_cube(\n     source = \"MPC\",\n     collection = \"SENTINEL-2-L2A\",\n     data_dir = images_path\n)\n\n\n\n\n# Images path\nimages_path = \"rondonia20LMR/inst/extdata/images/\"\n\n# Access local data cube\ncube_20LMR = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = images_path\n)\n\n\n\n\nThe sits_cube() function creates the data cube cube_20LMR with the Sentinel-2 tile 20LMR images available in the repository. Each temporal instance includes the spectral bands B02, B03, B04, B05, B06, B07, B08, B8A, B11 and B12.\n\n\nR\nPython\n\n\n\n\n# Plot one temporal instance\nplot(cube_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", date = \"2022-07-16\")\n\n\n\n\n# Plot one temporal instance\nplot(cube_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", date = \"2022-07-16\")\n\n\n\n\n\n\n\n\nFigure 2.2: Visualization of band composition in Rondonia.\n\n\n\nSITS provides the sits_select() function, which filters only the selected bands and dates from a set of time series or a data cube. As shown in the code below:\n\n\nR\nPython\n\n\n\n\n# Select only bands B02, B8A and B11\nsamples_3bands &lt;- sits_select(\n        data = deforestation_samples_v18,\n        bands = c(\"B02\", \"B8A\", \"B11\")\n)\n\n\n\n\n# Select only bands B02, B8A and B11\nsamples_3bands = sits_select(\n    data = deforestation_samples_v18,\n    bands = (\"B02\", \"B8A\", \"B11\")\n)\n\n\n\n\n\n2.1.3 Training a Machine Learning model\nThe next step in producing a classified map is to train a model. In the code below, we use the sits_train() function to train a Random Forest classifier with the samples containing the B02, B8A and B11 bands:\n\n\nR\nPython\n\n\n\n\n# Train the selected samples set using Rfor\nrfor_model &lt;- sits_train(\n        samples = samples_3bands,\n        ml_method = sits_rfor()\n)\n\n\n\n\n# Train the selected samples set using Rfor\nrfor_model = sits_train(\n    samples = samples_3bands,\n    ml_method = sits_rfor()\n)\n\n\n\n\nRandom Forest is a classification algorithm based on multiple decision trees, each trained with random subsets of data and attributes. The final prediction is made by majority voting among the trees. This approach reduces overfitting and improves model accuracy. The method also provides the importance of each attribute, which can assist in feature selection. For more details on the use of Random Forest in time series classification of satellite images, see Pelletier et al., (2016) [3]. For more, refer to Random Forest section.\n\n2.1.4 Data cube classification\nThe next step in producing a classified map is to apply the trained model to the data cube. In the example below, we use the sits_classify() function to classify the cube_20LMR cube using the Random Forest model. The sits_classify() function creates one raster probability map per class, showing the model’s confidence for each pixel. It requires two main arguments: data (a data cube or time series) and ml_model (the trained model). Optional arguments include multicores (number of processing cores), memsize (available memory) and output_dir (output directory).\n\n\nR\nPython\n\n\n\n\n# Classify the data cube\ncube_20LMR_probs &lt;- sits_classify(\n        data = cube_20LMR,\n        ml_model = rfor_model,\n        multicores = 5,\n        memsize = 8,\n        output_dir = tempdir_rondonia\n)\n\n# Plot the probabilities\nplot(cube_20LMR_probs, labels = \"Forest\", palette = \"YlGn\")\n\n\n\n\n# Classify the data cube\ncube_20LMR_probs = sits_classify(\n    data = cube_20LMR,\n    ml_model = rfor_model,\n    multicores = 5,\n    memsize = 8,\n    output_dir = tempdir_rondonia\n)\n\n# Plot the probabilities\nplot(cube_20LMR_probs, labels = \"Forest\", palette = \"YlGn\")\n\n\n\n\n\n\n\n\nFigure 2.3: Visualization of forest probabilities in Rondonia.\n\n\n\nAfter classification, the probability maps can be visualized, such as for the Forest class. These maps help assess classification uncertainty and can support active learning approaches, as discussed in the section on image classification in data cubes.\nWe recommend smoothing the results based on the spatial neighborhood of the pixels. This step helps incorporate spatial context and reduce isolated errors, especially in transition areas. The sits_smooth() function performs this task by applying a spatial Bayesian model to adjust predictions using neighboring class probabilities. In the code below, we apply smoothing to the classified cube cube_20LMR_probs.\n\n\nR\nPython\n\n\n\n\n# Smoothing the probabilities\ncube_20LMR_bayes &lt;- sits_smooth(\n        cube = cube_20LMR_probs,\n        multicores = 5,\n        memsize = 8,\n        output_dir = tempdir_rondonia\n)\n\n# Plot the smoothed probabilities\nplot(cube_20LMR_bayes, labels = \"Forest\", palette = \"YlGn\")\n\n\n\n\n# Smoothing the probabilities\ncube_20LMR_bayes = sits_smooth(\n    cube = cube_20LMR_probs,\n    multicores = 5,\n    memsize = 8,\n    output_dir = tempdir_rondonia\n)\n\n# Plot the smoothed probabilities\nplot(cube_20LMR_bayes, labels = \"Forest\", palette = \"YlGn\")\n\n\n\n\n\n\n\n\nFigure 2.4: Visualization of forest smoothing in Rondonia.\n\n\n\nWe use the default values for smoothness and window_size, which generally yield good results. However, we recommend tuning these parameters based on the specific needs of your application. Smoothing reduces classification uncertainty by removing isolated low-confidence pixels. In the smoothed map for the Forest class, most outliers are corrected, resulting in a more spatially consistent classification.\nThe final step is to assign a class label to each pixel by selecting the one with the highest probability value. After spatial smoothing, we use the sits_label_classification() function to generate the final classification map:\n\n\nR\nPython\n\n\n\n\ncube_20LMR_map &lt;- sits_label_classification(\n        cube = cube_20LMR_bayes,\n        multicores = 5,\n        memsize = 8,\n        output_dir = tempdir_rondonia\n)\n\n# Plot the map\nplot(cube_20LMR_map)\n\n\n\n\ncube_20LMR_map = sits_label_classification(\n    cube = cube_20LMR_bayes,\n    multicores = 5,\n    memsize = 8,\n    output_dir = tempdir_rondonia\n)\n\n# Plot the map\nplot(cube_20LMR_map)\n\n\n\n\n\n\n\n\nFigure 2.5: Visualization of classification map in Rondonia."
  },
  {
    "objectID": "dc_merge.html#introduction",
    "href": "dc_merge.html#introduction",
    "title": "\n6  Merging multi-source EO data cubes\n",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nThis section describes the process of merging collections from different sources. There are many instances when users want to combine different data sources, including:\n\nCombine Sentinel-2A and Sentinel-2B when their collections are stored separately.\nMerge collections of different Landsat satellites.\nJoin Sentinel-1 and Sentinel-2 to increase the number of attributes of time series.\nCombine Landsat and Sentinel HLS (Harmonized Landsat-Sentinel) collections.\nCombine Sentinel-2 cubes with digital elevation models.\n\nExcept in the cases of combining DEMs with Sentinel-2 cubes, and joining Sentinel-1 and Sentinel-2, the other cases require a combination of two operations: sits_merge() and sits_regularize(). The first function combines the timelines of the data cubes, in most cases producing to a non-regular data cube. For this reason, users have to use sits_regularize() to produce a multi-source regular cube."
  },
  {
    "objectID": "dc_merge.html#merging-hls-landsat-and-sentinel-2-collections",
    "href": "dc_merge.html#merging-hls-landsat-and-sentinel-2-collections",
    "title": "\n6  Merging multi-source EO data cubes\n",
    "section": "\n6.2 Merging HLS Landsat and Sentinel-2 collections",
    "text": "6.2 Merging HLS Landsat and Sentinel-2 collections\nImages from the HLS Landsat and Sentinel-2 collections are accessed separately and can be combined with sits_merge(). We first create two ARD collections, one for HLS Sentinel-2 and one for HLS Landsat over the same area. The two cubes are then merged.\n\n\nR\nPython\n\n\n\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# Retrieve an HLS Sentinel-2 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n\n# create a cube from the HLS Landsat collection\nhls_cube_l8 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSL30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n\n# merge the Sentinel-2 and Landsat-8 cubes\nhls_cube_merged &lt;- sits_merge(hls_cube_s2, hls_cube_l8)\n\n\n\n\n# define a region of interest\nroi = dict(lon_min = -45.6422, lat_min = -24.0335,\n            lon_max = -45.0840, lat_max = -23.6178)\n\n# Retrieve an HLS Sentinel-2 collection\nhls_cube_s2 = sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = (\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = \"2020-06-01\",\n  end_date = \"2020-09-01\",\n  progress = False\n)\n\n# create a cube from the HLS Landsat collection\nhls_cube_l8 = sits_cube(\n  source = \"HLS\",\n  collection = \"HLSL30\",\n  roi = roi,\n  bands = (\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = \"2020-06-01\",\n  end_date = \"2020-09-01\",\n  progress = False\n)\n\n# merge the Sentinel-2 and Landsat-8 cubes\nhls_cube_merged = sits_merge(hls_cube_s2, hls_cube_l8)\n\n\n\n\nComparing the timelines of the original cubes and the merged one, one can see the benefits of the merged collection for time series data analysis.\nHLS Sentinel-2 timeline\n\n\nR\nPython\n\n\n\n\n# Timeline of the Sentinel-2 cube\nsits_timeline(hls_cube_s2)\n\n [1] \"2020-06-15\" \"2020-06-20\" \"2020-06-25\" \"2020-06-30\" \"2020-07-05\"\n [6] \"2020-07-10\" \"2020-07-20\" \"2020-07-25\" \"2020-08-04\" \"2020-08-09\"\n[11] \"2020-08-14\" \"2020-08-19\" \"2020-08-24\" \"2020-08-29\"\n\n\n\n\n\n# Timeline of the Sentinel-2 cube\nsits_timeline(hls_cube_s2)\n\n['2020-06-15', '2020-06-20', '2020-06-25', '2020-06-30', '2020-07-05', '2020-07-10', '2020-07-20', '2020-07-25', '2020-08-04', '2020-08-09', '2020-08-14', '2020-08-19', '2020-08-24', '2020-08-29']\n\n\n\n\n\nHLS Landsat-8 timeline\n\n\nR\nPython\n\n\n\n\n# Timeline of the Landsat-8 cube\nsits_timeline(hls_cube_l8)\n\n[1] \"2020-06-09\" \"2020-06-25\" \"2020-07-11\" \"2020-07-27\" \"2020-08-12\"\n[6] \"2020-08-28\"\n\n\n\n\n\n# Timeline of the Landsat-8 cube\nsits_timeline(hls_cube_l8)\n\n['2020-06-09', '2020-06-25', '2020-07-11', '2020-07-27', '2020-08-12', '2020-08-28']\n\n\n\n\n\nMerged timeline\n\n\nR\nPython\n\n\n\n\n# Timeline of the merged cube\nsits_timeline(hls_cube_merged)\n\n [1] \"2020-06-09\" \"2020-06-15\" \"2020-06-20\" \"2020-06-25\" \"2020-06-30\"\n [6] \"2020-07-05\" \"2020-07-10\" \"2020-07-11\" \"2020-07-20\" \"2020-07-25\"\n[11] \"2020-07-27\" \"2020-08-04\" \"2020-08-09\" \"2020-08-12\" \"2020-08-14\"\n[16] \"2020-08-19\" \"2020-08-24\" \"2020-08-28\" \"2020-08-29\"\n\n\n\n\n\n# Timeline of the merged cube\nsits_timeline(hls_cube_merged)\n\n['2020-06-09', '2020-06-15', '2020-06-20', '2020-06-25', '2020-06-30', '2020-07-05', '2020-07-10', '2020-07-11', '2020-07-20', '2020-07-25', '2020-07-27', '2020-08-04', '2020-08-09', '2020-08-12', '2020-08-14', '2020-08-19', '2020-08-24', '2020-08-28', '2020-08-29']\n\n\n\n\n\nThe merged cube contains a denser timeline. However, for further use in sits, we need to create a regularized cube to obtain a composite with less clouds. To make processing easier, we copy the original data to a local directory.\n\n\nR\nPython\n\n\n\n\n# define the output directory\ntempdir_r_hls &lt;- \"~/sitsbook/tempdir/R/dc_merge/hls\"\n\n# set output dir if it does not exist \ndir.create(tempdir_r_hls, showWarnings = FALSE)\n\n# copy the hls cube for a local directory for faster regularization\ncube_hls_local &lt;- sits_cube_copy(\n    cube = hls_cube_merged,\n    output_dir = tempdir_r_hls\n)\n\n\n\n\n# define the output directory\ntempdir_py_hls = tempdir_py / \"hls\"\n\n# set output dir if it does not exist \ntempdir_py_hls.mkdir(parents=True, exist_ok=True)\n\n# copy the hls cube for a local directory for faster regularization\ncube_hls_local = sits_cube_copy(\n    cube = hls_cube_merged,\n    output_dir = tempdir_py_hls\n)\n\n\n\n\nCopying the merged HLS data to a local directory is an optional procedure, which is recommended in the case of optical data because it speeds up regularization. After the regular data cube is built, these files can be removed. In this example, we select a period of 16-days and keep the original 30-meter resolution of the HLS data for the resulting regular cube.\n\n\nR\nPython\n\n\n\n\n# define the output directory for the regularized images\ntempdir_r_hls_reg &lt;- \"~/sitsbook/tempdir/R/dc_merge/hls_reg\"\n\n# set output dir if it does not exist \ndir.create(tempdir_r_hls_reg, showWarnings = FALSE)\n\n# regularizing a harmonized Landsat-Sentinel data cube \ncube_hls_reg &lt;- sits_regularize(\n    cube = cube_hls_local,\n    period = \"P16D\", \n    res = 30,\n    output_dir = tempdir_r_hls_reg\n)\n\nplot(cube_hls_local, date = \"2020-07-11\")\n\n\n\n\n# define the output directory for the regularized images\ntempdir_py_hls_reg = tempdir_py / \"hls_reg\"\n\n# set output dir if it does not exist \ntempdir_py_hls_reg.mkdir(parents=True, exist_ok=True)\n\n# regularizing a harmonized Landsat-Sentinel data cube \ncube_hls_reg = sits_regularize(\n    cube = cube_hls_local,\n    period = \"P16D\", \n    res = 30,\n    output_dir = tempdir_py_hls_reg\n)\n\n# plot the cube\nplot(cube_hls_local, date = \"2020-07-11\")\n\n\n\n\n\n\n\n\nFigure 6.1: Sentinel-2 image obtained from merging NASA HLS Landsat-8 and Sentinel-2 collection for date 2020-06-15 showing the island of Ilhabela in Brazil."
  },
  {
    "objectID": "dc_merge.html#merging-sentinel-1-and-sentinel-2-images",
    "href": "dc_merge.html#merging-sentinel-1-and-sentinel-2-images",
    "title": "\n6  Merging multi-source EO data cubes\n",
    "section": "\n6.3 Merging Sentinel-1 and Sentinel-2 images",
    "text": "6.3 Merging Sentinel-1 and Sentinel-2 images\nTo combine Sentinel-1 and Sentinel-2 data, the first step is to produce regular data cubes for the same MGRS tiles with compatible time steps. The timelines do not have to be exactly the same, but they need to be close enough so that matching is acceptable and have the same number of time steps. This example uses the regular Sentinel-1 cube for tile “22LBL” produced in the previous sections. The next step is to produce a regular Sentinel-2 data cube for the same tile and regularize it. We start by defining a non-regular data cube from Planetary Computer collections.\n\n\nR\nPython\n\n\n\n\n# define the output directory\ncube_s2 &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    tiles = c(\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-09-30\"\n)\n\nplot(cube_s2, red = \"B11\", green = \"B8A\", blue = \"B02\", date = \"2021-07-07\")\n\n\n\n\n# define the output directory\ncube_s2 = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = (\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    tiles = (\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-09-30\"\n)\n\nplot(cube_s2, red = \"B11\", green = \"B8A\", blue = \"B02\", date = \"2021-07-07\")\n\n\n\n\n\n\n\n\nFigure 6.2: Sentinel-2 image covering tile 22LBL.\n\n\n\nThe next step is to create a regular data cube for tile “20LBL” and Sentinel-2 data.\n\n\nR\nPython\n\n\n\n\nif (!file.exists(\"./tempdir/R/dc_merge/s2_opt\"))\n    dir.create(\"./tempdir/R/dc_merge/s2_opt\")\n\ntempdir_r_s2_opt &lt;- \"./tempdir/R/dc_merge/s2_opt\"\n\n# define the output directory\ncube_s2_reg &lt;-  sits_regularize(\n    cube = cube_s2,\n    period = \"P16D\",\n    res = 40,\n    tiles = c(\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_r_s2_opt\n)\n\n\n\n\ntempdir_py_s2_opt = tempdir_py / \"s2_opt\"\ntempdir_py_s2_opt.mkdir(parents=True, exist_ok=True)\n\n# define the output directory\ncube_s2_reg = sits_regularize(\n    cube = cube_s2,\n    period = \"P16D\",\n    res = 40,\n    tiles = (\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_py_s2_opt\n)\n\n\n\n\nWe then create a regular data cube of Sentinel-1 images covering the same MGRS tile\n\n\nR\nPython\n\n\n\n\n# create an RTC cube from MPC collection for a region in Mato Grosso, Brazil.\ncube_s1_rtc &lt;-  sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    tiles = c(\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-10-01\"\n)\n\nplot(cube_s1_rtc, band = \"VH\", palette = \"Greys\", scale = 0.7)\n\n\n\n\n# create an RTC cube from MPC collection for a region in Mato Grosso, Brazil.\ncube_s1_rtc = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = (\"VV\", \"VH\"),\n    orbit = \"descending\",\n    tiles = (\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-10-01\"\n)\n\nplot(cube_s1_rtc, band = \"VH\", palette = \"Greys\", scale = 0.7)\n\n\n\n\n\n\n\n\nFigure 6.3: “Original Sentinel-1 image covering tile 22LBL.\n\n\n\nThe next step is to create a regular Sentinel-1 data cube.\n\n\nR\nPython\n\n\n\n\n# define the output directory\ntempdir_r_sar &lt;- \"~/sitsbook/tempdir/R/dc_merge/sar\"\n\n# set output dir if it does not exist \ndir.create(tempdir_r_sar, showWarnings = FALSE)\n\n# create a regular RTC cube from MPC collection for a tile 22LBL.\ncube_s1_reg &lt;- sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P16D\",\n    res = 40,\n    tiles = c(\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_r_sar\n)\n\nplot(cube_s1_reg, band = \"VH\", palette = \"Greys\", scale = 0.7, \n     dates = c(\"2021-06-06\", \"2021-07-24\", \"2021-09-26\"))\n\n\n\n\n# define the output directory\ntempdir_py_sar = tempdir_py / \"sar\"\n\n# set output dir if it does not exist\ntempdir_py_sar.mkdir(parents=True, exist_ok=True)\n\n# create a regular RTC cube from MPC collection for a tile 22LBL.\ncube_s1_reg = sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P16D\",\n    res = 40,\n    tiles = (\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_py_sar\n)\n\nplot(cube_s1_reg, band = \"VH\", palette = \"Greys\", scale = 0.7, \n     dates = (\"2021-06-06\", \"2021-07-24\", \"2021-09-26\"))\n\n\n\n\nAfter creating the two regular cubes, we can merge them. Considering that the timelines are close enough so that the cubes can be combined, we can use the sits_merge function to produce a combined cube. As an example, we show a plot with both radar and optical bands.\n\n\nR\nPython\n\n\n\n\n# merge Sentinel-1 and Sentinel-2 cubes\ncube_s1_s2 &lt;- sits_merge(cube_s2_reg, cube_s1_reg)\n\n# plot a an image with both SAR and optical bands\nplot(cube_s1_s2, red = \"B11\", green = \"B8A\", blue = \"VH\")\n\n\n\n\n# merge Sentinel-1 and Sentinel-2 cubes\ncube_s1_s2 = sits_merge(cube_s2_reg, cube_s1_reg)\n\n# plot a an image with both SAR and optical bands\nplot(cube_s1_s2, red = \"B11\", green = \"B8A\", blue = \"VH\")\n\n\n\n\n\n\n\n\nFigure 6.4: Sentinel-2 and Sentinel-1 RGB composite for tile 22LBL."
  },
  {
    "objectID": "dc_merge.html#combining-multitemporal-data-cubes-with-digital-elevation-models",
    "href": "dc_merge.html#combining-multitemporal-data-cubes-with-digital-elevation-models",
    "title": "\n6  Merging multi-source EO data cubes\n",
    "section": "\n6.4 Combining multitemporal data cubes with digital elevation models",
    "text": "6.4 Combining multitemporal data cubes with digital elevation models\nIn many applications, especially in regions with large topographical, soil or climatic variations, is is useful to merge multitemporal data cubes with base information such as digital elevation models (DEM). Merging multitemporal satellite images with digital elevation models (DEMs) offers several advantages that enhance the analysis and interpretation of geospatial data. Elevation data provides an additional to the two-dimensional satellite images, which help to distinguish land use and land cover classes which are impacted by altitude gradients. One example is the capacity to distinguish between low-altitude and high-altitude forests. In case where topography changes significantly, DEM information can improve the accuracy of classification algorithms.\nAs an example of DEM integration in a data cube, we will consider an agricultural region of Chile which is located in a narrow area close to the Andes. There is a steep gradient so that the cube benefits from the inclusion of the DEM.\n\n\nR\nPython\n\n\n\n\ns2_cube_19HBA &lt;- sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-2-L2A\",\n  tiles = \"19HBA\",\n  bands = c(\"B04\", \"B8A\", \"B12\", \"CLOUD\"),\n  start_date = \"2021-01-01\",\n  end_date = \"2021-03-31\"\n)\n\nplot(s2_cube_19HBA, red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\ns2_cube_19HBA = sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-2-L2A\",\n  tiles = \"19HBA\",\n  bands = (\"B04\", \"B8A\", \"B12\", \"CLOUD\"),\n  start_date = \"2021-01-01\",\n  end_date = \"2021-03-31\"\n)\n\nplot(s2_cube_19HBA, red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n\n\n\n\nSentinel-2 image covering tile 19HBA.\n\n\n\nThen, we produce a regular data cube to use for classification. In this example, we will use a reduced resolution (30 meters) to expedite processing. In practice, a resolution of 10 meters is recommended.\n\n\nR\nPython\n\n\n\n\n# set output dir if it does not exist \ntempdir_r_s2_19HBA &lt;- \"~/sitsbook/tempdir/R/dc_merge/s2_19HBA\"\n\ndir.create(tempdir_r_s2_19HBA, showWarnings = FALSE)\n\n# Create a regular data cube\ns2_cube_19HBA_reg &lt;- sits_regularize(\n  cube = s2_cube_19HBA,\n  period = \"P16D\",\n  res = 30,\n  output_dir = tempdir_r_s2_19HBA\n)\n\n\n\n\n# set output dir if it does not exist\ntempdir_py_s2_19HBA = tempdir_py / \"s2_19HBA\"\ntempdir_py_s2_19HBA.mkdir(parents=True, exist_ok=True)\n\n# Create a regular data cube\ns2_cube_19HBA_reg = sits_regularize(\n  cube = s2_cube_19HBA,\n  period = \"P16D\",\n  res = 30,\n  output_dir = tempdir_py_s2_19HBA\n)\n\n\n\n\nThe next step is recover the DEM for the area. For this purpose, we will use the Copernicus Global DEM-30, and select the area covered by the tile. As explained in the MPC access section above, the Copernicus DEM tiles are stored as 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. For them to match an MGRS tile, they have to be regularized in a similar way as the Sentinel-1 images, as shown below. To select a DEM, no temporal information is required.\n\n\nR\nPython\n\n\n\n\n# obtain the DEM cube\ndem_cube_19HBA &lt;- sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  bands = \"ELEVATION\",\n  tiles = \"19HBA\"\n)\n\n\n\n\n# obtain the DEM cube\ndem_cube_19HBA = sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  bands = \"ELEVATION\",\n  tiles = \"19HBA\"\n)\n\n\n\n\nAfter obtaining the 1\\(^\\circ\\) by 1\\(^\\circ\\) data cube covering the selected tile, the next step is to regularize it. This is done using the sits_regularize() function. This function will produce a DEM which matches exactly the chosen tile.\n\n\nR\nPython\n\n\n\n\n# set output dir if it does not exist \ntempdir_r_dem_19HBA &lt;- \"~/sitsbook/tempdir/R/dc_merge/dem_19HBA\"\n\n# create directory\ndir.create(tempdir_r_dem_19HBA, showWarnings = FALSE)\n\n# regularize DEM cube\ndem_cube_19HBA_reg &lt;- sits_regularize(\n  cube = dem_cube_19HBA,\n  res = 30,\n  bands = \"ELEVATION\",\n  tiles = \"19HBA\",\n  output_dir = tempdir_r_dem_19HBA\n)\n\n# plot the DEM reversing the palette \nplot(dem_cube_19HBA_reg, band = \"ELEVATION\", palette = \"Spectral\", rev = TRUE)\n\n\n\n\n# set output dir if it does not exist \ntempdir_py_dem_19HBA = tempdir_py / \"dem_19HBA\"\ntempdir_py_dem_19HBA.mkdir(parents=True, exist_ok=True)\n\n# regularize DEM cube\ndem_cube_19HBA_reg = sits_regularize(\n  cube = dem_cube_19HBA,\n  res = 30,\n  bands = \"ELEVATION\",\n  tiles = \"19HBA\",\n  output_dir = tempdir_py_dem_19HBA\n)\n\n# plot the DEM reversing the palette \nplot(dem_cube_19HBA_reg, band = \"ELEVATION\", palette = \"Spectral\", rev = True)\n\n\n\n\n\n\n\n\nFigure 6.5: Copernicus DEM-30 covering tile 19HBA.\n\n\n\nThere are two ways to combine multitemporal data cubes with DEM data. The first method takes the DEM as a base information, which is used in combination with the multispectral time series. For exemple, consider a situation of a data cube with 10 bands and 23 time steps, which has a 230-dimensional space. Adding DEM as a base cube will include one dimension to the attribute space. This combination is supported by function sits_add_base_cube. In the resulting cube, the information on the image time series and that of the DEM are stored separately. The data cube metadata will now include a column called base_info.\n\n\nR\nPython\n\n\n\n\nmerged_cube_base &lt;- sits_add_base_cube(s2_cube_19HBA_reg, dem_cube_19HBA_reg)\nmerged_cube_base$base_info[[1]]\n\n# A tibble: 1 × 11\n  source collection     satellite sensor  tile    xmin   xmax   ymin  ymax crs  \n  &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 MPC    COP-DEM-GLO-30 TANDEM-X  X-band… 19HBA 199980 309780 5.99e6 6.1e6 EPSG…\n# ℹ 1 more variable: file_info &lt;list&gt;\n\n\n\n\n\nmerged_cube_base = sits_add_base_cube(s2_cube_19HBA_reg, dem_cube_19HBA_reg)\nmerged_cube_base[\"base_info\"][0]\n\n  source      collection satellite  ...       ymax         crs                                          file_info\n0    MPC  COP-DEM-GLO-30  TANDEM-X  ...  6100000.0  EPSG:32719    fid       band        date  ...       ymax  ...\n\n[1 rows x 11 columns]\n\n\n\n\n\nAlthough this combination is conceptually simple, it has drawbacks. Since the attribute space now mixes times series with fixed-time information, the only applicable classification method is Random Forest. Because of the way Random Forest works, not all attributes are used by every decision tree. During the training of each tree, at each node, a random subset of features is selected, and the best split is chosen based on this subset rather than all features. Thus, there may be a significant number of decision trees that do use the DEM attribute. As a result, the effect of the DEM information may be underestimated.\nThe alternative is to combine the image data cube and the DEM using sits_merge. In this case, the DEM becomes another band. Although it may look peculiar to replicate the DEM many time to build an artificial time series, there are many advantages in doing so. All classification algorithms available in sits (including the deep learning ones) can be used to classify the resulting cube. For cases where the DEM information is particularly important, such organisation places DEM data at a par with other spectral bands. Users are encouraged to compare the results obtained by direct merging of DEM with spectral bands with the method where DEM is taken as a base cube.\n\n\nR\nPython\n\n\n\n\nmerged_cube &lt;- sits_merge(s2_cube_19HBA_reg, dem_cube_19HBA_reg)\nmerged_cube$file_info[[1]]\n\n# A tibble: 24 × 13\n   fid   band      date       nrows ncols  xres  yres   xmin   ymin   xmax  ymax\n   &lt;chr&gt; &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 1     B04       2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 2 1     B12       2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 3 1     B8A       2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 4 1     ELEVATION 2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 5 2     B04       2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 6 2     B12       2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 7 2     B8A       2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 8 1     ELEVATION 2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 9 3     B04       2021-02-04  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n10 3     B12       2021-02-04  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n# ℹ 14 more rows\n# ℹ 2 more variables: crs &lt;chr&gt;, path &lt;chr&gt;\n\n\n\n\n\nmerged_cube = sits_merge(s2_cube_19HBA_reg, dem_cube_19HBA_reg)\nmerged_cube[\"file_info\"][0]\n\n   fid       band        date  ...       ymax         crs                                               path\n0    1        B04  2021-01-03  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n1    1        B12  2021-01-03  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n2    1        B8A  2021-01-03  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n3    1  ELEVATION  2021-01-03  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n4    2        B04  2021-01-19  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n..  ..        ...         ...  ...        ...         ...                                                ...\n19   1  ELEVATION  2021-03-08  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n20   6        B04  2021-03-24  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n21   6        B12  2021-03-24  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n22   6        B8A  2021-03-24  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n23   1  ELEVATION  2021-03-24  ...  6100000.0  EPSG:32719  /Users/gilbertocamara/sitsbook/tempdir/Python/...\n\n[24 rows x 13 columns]"
  },
  {
    "objectID": "dc_merge.html#summary",
    "href": "dc_merge.html#summary",
    "title": "\n6  Merging multi-source EO data cubes\n",
    "section": "\n6.5 Summary",
    "text": "6.5 Summary\nIn this chapter, we provide an overview of methods to merge multisource cubes. Combining multisource data is a powerful means of increasing the information available for classification. Users are encouraged to consider what data sets are available for their areas of interest and to try to merge them."
  },
  {
    "objectID": "dc_localcubes.html#introduction",
    "href": "dc_localcubes.html#introduction",
    "title": "\n7  Data cubes from local files\n",
    "section": "\n7.1 Introduction",
    "text": "7.1 Introduction\nIn many situations, users need to build data cubes from files installed in locally-accessible computers. The most common case is when one has obtained a data cube by downloading an ARD collection using sits_cube() and producing a regular cube using sits_regularize(). The result will be a data cube stored in a local computer. In this case, STAC services are not available and we need to rely on file-specific information.\nAll regular cubes produced by sits use the same convention for file naming. Images are stored as individual TIFF COGs and follow the structure &lt;satellite&gt;_&lt;sensor&gt;_&lt;tile&gt;_&lt;band&gt;_&lt;date&gt;.tif. When their files follow this convention, users only need to provide the names of the original cloud provider, the ARD collection from which the data was obtained, and the directory where data is stored. By default, sits scans the directory looking for files that are associated to a regular data cube. We require that users store each tile/band/date combination in a single file, since sits does not support multi-image files.\nA special situation is when users have obtained images by other means, such as downloading Planet collections. In this case, they have to organize the files so that each file corresponds to a single tile/date/band combination and that the file name contains information on tile, band and date. We show an example with Planet data in this chapter. Finally, users also may want to retrieve local files associated with processed data cubes, such as probability cubes or classified map. Information on how to proceed is also provided in what follows."
  },
  {
    "objectID": "dc_localcubes.html#building-data-cubes-from-local-files",
    "href": "dc_localcubes.html#building-data-cubes-from-local-files",
    "title": "\n7  Data cubes from local files\n",
    "section": "\n7.2 Building data cubes from local files",
    "text": "7.2 Building data cubes from local files\nTo build a data cube from local files, users must provide information about the original source from which the data was obtained. In this case, sits_cube() needs the parameters:\n\n\nsource, the cloud provider from where the data has been obtained (in this case, the Brazil Data Cube “BDC”);\n\ncollection, the collection of the cloud provider from where the images have been extracted. In this case, data comes from the MOD13Q1 collection 6;\n\ndata_dir, the local directory where the image files are stored;\n\nparse_info (optional), a vector of strings stating how file names store information on “tile”, “band”, and “date”. In this case, local images are stored in files whose names are similar to TERRA_MODIS_012010_EVI_2014-07-28.tif. This file represents an image obtained by the MODIS sensor onboard the TERRA satellite, covering part of tile 012010 in the EVI band for date 2014-07-28."
  },
  {
    "objectID": "dc_localcubes.html#planet-data-as-ard-local-files",
    "href": "dc_localcubes.html#planet-data-as-ard-local-files",
    "title": "\n7  Data cubes from local files\n",
    "section": "Planet data as ARD local files",
    "text": "Planet data as ARD local files\nARD images downloaded from cloud collections to a local computer are not associated with a STAC endpoint that describes them. They must be organized and named to allow sits to create a data cube from them. All local files have to be in the same directory and have the same spatial resolution and projection. Each file must contain a single image band for a single date. Each file name needs to include tile, date, and band information. Users must provide information about the original data source to allow sits to retrieve information about image attributes such as band names, missing values, etc.\nTo be able to read local files, they must belong to a collection registered by sits. All collections known to sits by default are shown using sits_list_collections(). To register a new collection, please see the information provided in the Chapter [Supporting new ARD collections and data cubes] (https://e-sensing.github.io/sitsbook/annex_stac.html).\nThe example shows how to define a data cube using Planet images from the sitsdata package. The dataset contains monthly PlanetScope mosaics for tile “604-1043” for August to October 2022, with bands B01, B02, B04, and B04. In general, sits users need to match the local file names to the values provided by the parse_info parameter. The file names of this dataset use the format PLANETSCOPE_MOSAIC_604-1043_B4_2022-10-01.tif, which fits the default value for parse_info which is c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\") and for delim which is “_“, it is not necessary to set these values when creating a data cube from the local files.\n\n\nR\nPython\n\n\n\n\n# Define the directory where Planet files are stored\ndata_dir &lt;- system.file(\"extdata/Planet\", package = \"sitsdata\")\n\n# Create a data cube from local files\nplanet_cube &lt;- sits_cube(\n    source = \"PLANET\",\n    collection = \"MOSAIC\",\n    data_dir = data_dir\n)\n\n# Plot the first instance of the Planet data in natural colors\nplot(planet_cube, red = \"B3\", green = \"B2\", blue = \"B1\")\n\n\n\n\n# Define the directory where Planet files are stored\ndata_dir = r_package_dir(\"extdata/Planet\", package = \"sitsdata\")\n\n# Create a data cube from local files\nplanet_cube = sits_cube(\n    source = \"PLANET\",\n    collection = \"MOSAIC\",\n    data_dir = data_dir\n)\n\n# Plot the first instance of the Planet data in natural colors\nplot(planet_cube, red = \"B3\", green = \"B2\", blue = \"B1\")\n\n\n\n\n\n\n\n\nFigure 7.1: Planet image over an area in Colombia."
  },
  {
    "objectID": "dc_localcubes.html#reading-classified-images-as-local-data-cube",
    "href": "dc_localcubes.html#reading-classified-images-as-local-data-cube",
    "title": "\n7  Data cubes from local files\n",
    "section": "Reading classified images as local data cube",
    "text": "Reading classified images as local data cube\nIt is also possible to create local cubes based on results that have been produced by classification or post-classification algorithms. In this case, more parameters are required, and the parameter parse_info is specified differently, as follows:\n\n\nsource: Name of the original data provider.\n\ncollection: Name of the collection from where the data was extracted.\n\ndata_dir: Local directory for the classified images.\n\nband: Band name associated with the type of result. Use: (a) probs for probability cubes produced by sits_classify(); (b) bayes, for cubes produced by sits_smooth(); (c) entropy, least, ratio or margin, according to the method selected when using sits_uncertainty(); (d) variance for those produced by sits_variance(); and (e) class for classified cubes.\n\nstart_date: starting date for the temporal scope of the processed cube, which matches to the start date of the EO data cube that was processed by sits.\n\nend_date: final date for the temporal scope of the processed cube, which matches to the end date of the EO data cube that was processed by sits.\n\nlabels: Labels associated with the names of the classes (not required for cubes produced by sits_uncertainty()).\n\nversion: Version of the result (default = v1).\n\nparse_info: File name parsing information to allow sits to deduce the values of tile, start_date, end_date, band, and version from the file name. Unlike non-classified image files, cubes produced by classification and post-classification have both start_date and end_date.\n\nData cube containing results of sits_classify(), sits_smooth() and sits_variance() are stored as one file per file, organized internally as three-dimensional matrices. The third dimension is associated with the classification labels. Cubes associated with sits_uncertainty() and sits_label_classification() are organized as TIFF COGs containing two-dimensional matrices, one per tile.\nThe following code creates a results cube based on the classification of deforestation in Brazil. This classified cube was obtained by a large data cube of Sentinel-2 images, covering the state of Rondonia, Brazil comprising 40 tiles, 10 spectral bands, and covering the period from 2020-06-01 to 2021-09-11. Samples of four classes were trained by a Random Forest classifier. Internally, classified images use integers to represent classes. Thus, labels have to be associated to the integers that represent each class name.\n\n\nR\nPython\n\n\n\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LLP\", \n                        package = \"sitsdata\")\n\n# File name  \"SENTINEL-2_MSI_20LLP_2020-06-04_2021-08-26_class_v1.tif\" \nRondonia_class_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    bands = \"class\",\n    labels = c(\"1\" = \"Burned_Area\", \"2\" = \"Cleared_Area\", \n               \"3\" = \"Highly_Degraded\", \"4\" =  \"Forest\"),\n    data_dir = data_dir,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \"start_date\", \"end_date\", \n                   \"band\", \"version\"))\n\n# Plot the classified cube\nplot(Rondonia_class_cube)\n\n\n\nFigure 7.2: Classified data cube for the year 2020/2021 in Rondonia, Brazil.\n\n\n\n\n\n\n# Create a cube based on a classified image \ndata_dir = r_package_dir(\"extdata/Rondonia-20LLP\", \n                        package = \"sitsdata\")\n\n# File name  \"SENTINEL-2_MSI_20LLP_2020-06-04_2021-08-26_class_v1.tif\" \nRondonia_class_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    bands = \"class\",\n    labels = {\n        \"1\": \"Burned_Area\",\n        \"2\": \"Cleared_Area\",\n        \"3\": \"Highly_Degraded\",\n        \"4\": \"Forest\"\n    },\n    data_dir = data_dir,\n    parse_info = (\"satellite\", \"sensor\", \"tile\", \"start_date\", \"end_date\", \n                   \"band\", \"version\"))\n\n# Plot the classified cube\nplot(Rondonia_class_cube)\n\n\n\nFigure 7.3: Classified data cube for the year 2020/2021 in Rondonia, Brazil."
  },
  {
    "objectID": "dc_localcubes.html#summary",
    "href": "dc_localcubes.html#summary",
    "title": "\n7  Data cubes from local files\n",
    "section": "\n7.3 Summary",
    "text": "7.3 Summary\nIn this chapter, we explain how to create cubes from local files. Sometimes, users stop a sits session after performing an operation and want to continue from that point onwards. In this case, they can use sits_cube() to retrieve data from local files. This feature is useful, for example, to retrieve data cubes that have been regularized or classified."
  },
  {
    "objectID": "dc_cubeoperations.html#pixel-based-and-neighborhood-based-operations",
    "href": "dc_cubeoperations.html#pixel-based-and-neighborhood-based-operations",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.1 Pixel-based and neighborhood-based operations",
    "text": "8.1 Pixel-based and neighborhood-based operations\nPixel-based operations in remote sensing images refer to image processing techniques that operate on individual pixels or cells in an image without considering their spatial relationships with neighboring pixels. These operations are typically applied to each pixel in the image independently; they are used to extract information on spectral, radiometric, or spatial properties. Pixel-based operations produce spectral indexes which combine data from multiple bands.\nNeighborhood-based operations are applied to groups of pixels in an image. The neighborhood is typically defined as a rectangular or circular region centered on a given pixel. These operations can be used for removing noise, detecting edges, and sharpening, among other uses.\nThe sits_apply() function computes new indices from a desired mathematical operation as a function of the bands available on the cube using any valid R expression. It applies the operation for all tiles and all temporal intervals. There are two types of operations in sits_apply():\n\nPixel-based operations that produce an index based on individual pixels of existing bands. The input bands and indexes should be part of the input data cube and have the same names used in the cube. The new index will be computed for every pixel of all images in the time series. Besides arithmetic operators, the function also accepts vectorized R functions that can be applied to matrices (e.g., sqrt(), log(), and sin()).\nNeighborhood-based operations that produce a derived value based on a window centered around each individual pixel. The available functions are w_median(), w_sum(), w_mean(), w_min(), w_max(), w_sd() (standard deviation), and w_var() (variance). Users set the window size (only odd values are allowed).\n\nThe following examples show how to use sits_apply()."
  },
  {
    "objectID": "dc_cubeoperations.html#computing-vegetation-indexes",
    "href": "dc_cubeoperations.html#computing-vegetation-indexes",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.2 Computing vegetation indexes",
    "text": "8.2 Computing vegetation indexes\nUsing vegetation indexes is an established practice in remote sensing. These indexes aim to improve the discrimination of vegetation structure by combining two wavebands, one where leaf pigments reflect incoming light with another where leaves absorb incoming radiation. Green leaves from natural vegetation such as forests have a strong emissivity rate in the near-infrared bands and low emissivity rates in the red bands of the electromagnetic spectrum. These spectral properties are used to calculate the Normalized Difference Vegetation Index (NDVI), a widely used index that is computed as the normalized difference between the values of infra-red and red bands. Including red-edge bands in Sentinel-2 images has broadened the scope of the bands used to calculate these indices [1], [2]. In what follows, we show examples of vegetation index calculation using a Sentinel-2 data cube.\nFirst, we define a data cube for a tile in the state of Rondonia, Brazil, including bands used to compute different vegetation indexes. We regularize the cube using a target resolution of 60-meters to reduce processing time.\n\n\nR\nPython\n\n\n\n\n# Create an non-regular data cube from AWS\ns2_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    tiles = \"20LKP\",\n    bands = c(\"B02\", \"B03\", \"B04\", \n              \"B05\", \"B06\", \"B07\", \n              \"B08\", \"B8A\", \"B11\", \n              \"B12\",\"CLOUD\"),\n    start_date = as.Date(\"2018-07-01\"),\n    end_date = as.Date(\"2018-08-31\"))\n\n# Regularize the cube to 15 day intervals\nreg_cube &lt;- sits_regularize(\n          cube       = s2_cube,\n          output_dir = tempdir_r,\n          res        = 60,\n          period     = \"P15D\",\n          multicores = 4)\n\n\n\n\n# Create an non-regular data cube from AWS\ns2_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    tiles = \"20LKP\",\n    bands = (\"B02\", \"B03\", \"B04\", \n              \"B05\", \"B06\", \"B07\", \n              \"B08\", \"B8A\", \"B11\", \n              \"B12\",\"CLOUD\"),\n    start_date = \"2018-07-01\",\n    end_date = \"2018-08-31\")\n    \n# Regularize the cube to 15 day intervals\nreg_cube = sits_regularize(\n          cube       = s2_cube,\n          output_dir = tempdir_py,\n          res        = 60,\n          period     = \"P15D\",\n          multicores = 4)\n\n\n\n\nThere are many options for calculating vegetation indexes using Sentinel-2 bands. The most widely used method combines band B08 (785-899 nm) and band B04 (650-680 nm). Recent works in the literature propose using the red-edge bands B05 (698-713 nm), B06 (733-748 nm), and B07 (773-793 nm) for capturing subtle variations in chlorophyll absorption producing indexes, which are called Normalized Difference Vegetation Red-edge indexes (NDRE) [1]. In a recent review, Chaves et al. argue that red-edge bands are important for distinguishing leaf structure and chlorophyll content of different vegetation species [3]. In the example below, we show how to include indexes in the regular data cube with the Sentinel-2 spectral bands.\nWe first calculate the NDVI in the usual way, using bands B08 and B04.\n\n\nR\nPython\n\n\n\n\n# Calculate NDVI index using bands B08 and B04\nreg_cube &lt;- sits_apply(reg_cube,\n    NDVI = (B08 - B04)/(B08 + B04),\n    output_dir = tempdir_r\n)\n\n# Plot\nplot(reg_cube, band = \"NDVI\", palette = \"RdYlGn\")\n\n\n\nFigure 8.1: NDVI using bands B08 and B04 of Sentinel-2.\n\n\n\n\n\n\n# Calculate NDVI index using bands B08 and B04\nreg_cube = sits_apply(reg_cube,\n    NDVI = \"(B08 - B04)/(B08 + B04)\",\n    output_dir = tempdir_py\n)\n\n# Plot\nplot(reg_cube, band = \"NDVI\", palette = \"RdYlGn\")\n\n\n\nFigure 8.2: NDVI using bands B08 and B04 of Sentinel-2.\n\n\n\n\n\n\nWe now compare the traditional NDVI with other vegetation index computed using red-edge bands. The example below such the NDRE1 index, obtained using bands B06 and B05. Sun et al. argue that a vegetation index built using bands B06 and B07 provides a better approximation to leaf area index estimates than NDVI [2]. Notice that the contrast between forests and deforested areas is more robust in the NDRE1 index than with NDVI.\n\n\nR\nPython\n\n\n\n\n# Calculate NDRE1 index using bands B06 and B05\nreg_cube &lt;- sits_apply(reg_cube,\n    NDRE1 = (B06 - B05)/(B06 + B05),\n    output_dir = tempdir_r\n)\n\n# Plot NDRE1 index\nplot(reg_cube, band = \"NDRE1\",  palette = \"RdYlGn\")\n\n\n\nNDRE1 using bands B06 and B05 of Sentinel-2.\n\n\n\n\n\n\n# Calculate NDRE1 index using bands B06 and B05\nreg_cube = sits_apply(reg_cube,\n    NDRE1 = \"(B06 - B05)/(B06 + B05)\",\n    output_dir = tempdir_py\n)\n\n# Plot NDRE1 index\nplot(reg_cube, band = \"NDRE1\",  palette = \"RdYlGn\")\n\n\n\nNDRE1 using bands B06 and B05 of Sentinel-2."
  },
  {
    "objectID": "dc_cubeoperations.html#spectral-indexes-for-identifying-burned-areas",
    "href": "dc_cubeoperations.html#spectral-indexes-for-identifying-burned-areas",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.3 Spectral indexes for identifying burned areas",
    "text": "8.3 Spectral indexes for identifying burned areas\nBand combinations can also generate spectral indices for detecting degradation by fires, which are an important element in environmental degradation. Forest fires significantly impact emissions and impoverish natural ecosystems [4]. Fires open the canopy, making the microclimate drier and increasing the amount of dry fuel [5]. One well-established technique for detecting burned areas with remote sensing images is the normalized burn ratio (NBR), the difference between the near-infrared and the short wave infrared band, calculated using bands B8A and B12.\n\n\nR\nPython\n\n\n\n\n# Calculate the NBR index\nreg_cube &lt;- sits_apply(reg_cube,\n    NBR = (B12 - B8A)/(B12 + B8A),\n    output_dir = tempdir_r\n)\n\n# Plot the NBR for the first date\nplot(reg_cube, band = \"NBR\", palette = \"Reds\")\n\n\n\nFigure 8.3: NBR ratio using Sentinel-2 B11 and B8A.\n\n\n\n\n\n\n# Calculate the NBR index\nreg_cube = sits_apply(reg_cube,\n    NBR = \"(B12 - B8A)/(B12 + B8A)\",\n    output_dir = tempdir_py\n)\n\n# Plot the NBR for the first date\nplot(reg_cube, band = \"NBR\", palette = \"Reds\")\n\n\n\nFigure 8.4: NBR ratio using Sentinel-2 B11 and B8A."
  },
  {
    "objectID": "dc_cubeoperations.html#support-for-non-normalized-indexes",
    "href": "dc_cubeoperations.html#support-for-non-normalized-indexes",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "Support for non-normalized indexes",
    "text": "Support for non-normalized indexes\nAll data cube operations discussed so far produce normalized indexes. By default, the indexes generated by the sits_apply() function are normalized between -1 and 1, scaled by a factor of 0.0001. Normalized indexes are saved as INT2S (Integer with sign). If the normalized parameter is FALSE, no scaling factor will be applied and the index will be saved as FLT4S (Float with sign). The code below shows an example of the non-normalized index, CVI - chlorophyll vegetation index. CVI is a spectral index used to estimate the chlorophyll content and overall health of vegetation. It combines bands in visible and near-infrared (NIR) regions to assess vegetation characteristics. Since CVI is not normalized, we have to set the parameter normalized to FALSE to inform sits_apply() to generate an FLT4S image.\n\n\nR\nPython\n\n\n\n\n# Calculate the NBR index\nreg_cube &lt;- sits_apply(reg_cube,\n    CVI = (B8A / B03) * (B05 / B03 ),\n    normalized = FALSE, \n    output_dir = tempdir_r\n)\n\n# Plot\nplot(reg_cube, band = \"CVI\", palette = \"Greens\")\n\n\n\nFigure 8.5: CVI index using bands B03, B05, and B8A.\n\n\n\n\n\n\n# Calculate the NBR index\nreg_cube = sits_apply(reg_cube,\n    CVI = \"(B8A / B03) * (B05 / B03 )\",\n    normalized = False, \n    output_dir = tempdir_py\n)\n\n# Plot\nplot(reg_cube, band = \"CVI\", palette = \"Greens\")\n\n\n\nFigure 8.6: CVI index using bands B03, B05, and B8A."
  },
  {
    "objectID": "dc_cubeoperations.html#summary",
    "href": "dc_cubeoperations.html#summary",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.4 Summary",
    "text": "8.4 Summary\nIn this chapter, we learned how to operate on data cubes, including how to compute spectral indexes, estimate mixture models, and do reducing operations. This chapter concludes our overview of how data cubes work in sits. In the next part, we will describe how to work with time series."
  },
  {
    "objectID": "dc_cubeoperations.html#references",
    "href": "dc_cubeoperations.html#references",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nQ. Xie et al., “Retrieval of crop biophysical parameters from Sentinel-2 remote sensing imagery,” International Journal of Applied Earth Observation and Geoinformation, vol. 80, pp. 187–195, 2019, doi: 10.1016/j.jag.2019.04.019.\n\n\n[2] \nY. Sun, Q. Qin, H. Ren, T. Zhang, and S. Chen, “Red-Edge Band Vegetation Indices for Leaf Area Index Estimation From Sentinel-2/MSI Imagery,” IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no. 2, pp. 826–840, 2020, doi: 10.1109/TGRS.2019.2940826.\n\n\n[3] \nM. Chaves, M. Picoli, and I. Sanches, “Recent Applications of Landsat 8/OLI and Sentinel-2/MSI for Land Use and Land Cover Mapping: A Systematic Review,” Remote Sensing, vol. 12, no. 18, p. 3062, 2020, doi: 10.3390/rs12183062.\n\n\n[4] \nD. C. Nepstad et al., “Large-scale impoverishment of Amazonian forests by logging and fire,” Nature, vol. 398, no. 6727, pp. 505–508, 1999, doi: 10.1038/19066.\n\n\n[5] \nY. Gao, M. Skutsch, J. Paneque-Gálvez, and A. Ghilardi, “Remote sensing of forest degradation: A review,” Environmental Research Letters, vol. 15, no. 10, p. 103001, 2020, doi: 10.1088/1748-9326/abaad7."
  },
  {
    "objectID": "dc_mixture.html#introduction",
    "href": "dc_mixture.html#introduction",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.1 Introduction",
    "text": "9.1 Introduction\nMany pixels in images of medium-resolution satellites such as Landsat or Sentinel-2 contain a mixture of spectral responses of different land cover types inside a resolution element [1]. In many applications, it is desirable to obtain the proportion of a given class inside a mixed pixel. For this purpose, the literature proposes mixture models; these models represent pixel values as a combination of multiple pure land cover types [2]. Assuming that the spectral response of pure land cover classes (called endmembers) is known, spectral mixture analysis derives new bands containing the proportion of each endmember inside a pixel.\nApplications of spectral mixture analysis in remote sensing include forest degradation [6], wetland surface dynamics [7], and urban area characterization [8]. These models providing valuable information for a wide range of applications, from land mapping and change detection to resource management and environmental monitoring."
  },
  {
    "objectID": "dc_mixture.html#methods",
    "href": "dc_mixture.html#methods",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.2 Methods",
    "text": "9.2 Methods\nThe most used method for spectral mixture analysis is the linear model [2]. The main idea behind the linear mixture model is that the observed pixel spectrum can be expressed as a linear combination of the spectra of the pure endmembers, weighted by their respective proportions (or abundances) within the pixel. Mathematically, the model can be represented as: \\[\nR_i = \\sum_{j=1}^N a_{i,j}*x_j + \\epsilon_i, i \\in {1,...M}, M &gt; N,\n\\] where \\(i=1,..M\\) is the set of spectral bands and \\(j=1,..N\\) is the set of land classes. For each pixel, \\(R_i\\) is the reflectance in the i-th spectral band, \\(x_j\\) is the reflectance value due to the j-th endmember, and \\(a_{i,j}\\) is the proportion between the j-th endmember and the i-th spectral band. To solve this system of equations and obtain the proportion of each endmember, sits uses a non-negative least squares (NNLS) regression algorithm, which is available in the R package RStoolbox and was developed by Jakob Schwalb-Willmann, based on the sequential coordinate-wise algorithm (SCA) proposed on Franc et al. [9]."
  },
  {
    "objectID": "dc_mixture.html#running-mixture-models-in-sits",
    "href": "dc_mixture.html#running-mixture-models-in-sits",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.3 Running mixture models in SITS",
    "text": "9.3 Running mixture models in SITS\nTo run the mixture model in sits, it is necessary to inform the values of pixels which represent spectral responses of a unique class. These are the so-called “pure” pixels. Because the quality of the resulting endmember images depends on the quality of the pure pixels, they should be chosen carefully and based on expert knowledge of the area. Since sits supports multiple endmember spectral mixture analysis [10], users can specify more than one pure pixel per endmember to account for natural variability.\nIn sits, spectral mixture analysis is done by sits_mixture_model(), which has two mandatory parameters: cube (a data cube) and endmembers, a named table (or equivalent) that defines the pure pixels. The endmembers table must have the following named columns: (a) type, which defines the class associated with an endmember; (b) names, the names of the bands. Each line of the table must contain the value of each endmember for all bands (see example).\nTo improve readability, we suggest that, in R, the endmembers parameters be defined as a tribble. A tribble is a tibble with an easier to read row-by-row layout. In the example below, we define three endmembers for classes Forest, Soil, and Water. Note that the values for each band are expressed as integers ranging from 0 to 10,000. We use the same data cube which was used in the previous chapter.\nIn Python, the endmembers are defined as a pandas data frame which encapsulates a dictionary. The first name of the dictionary should be class and its values will be the names of the endmember bands. The other names of the dictionary are the bands used to compute the mixture model, as shown in the example below.\nCreate Sentinel-2 Data Cube\n\n\nR\nPython\n\n\n\n\n# Create an non-regular data cube from AWS\ns2_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    tiles = \"20LKP\",\n    bands = c(\"B02\", \"B03\", \"B04\", \n              \"B05\", \"B06\", \"B07\", \n              \"B08\", \"B8A\", \"B11\", \n              \"B12\",\"CLOUD\"),\n    start_date = as.Date(\"2018-07-01\"),\n    end_date = as.Date(\"2018-08-31\"))\n\n# Regularize the cube to 15 day intervals\nreg_cube &lt;- sits_regularize(\n          cube       = s2_cube,\n          output_dir = tempdir_r,\n          res        = 60,\n          period     = \"P15D\",\n          multicores = 4)\n\n\n\n\n# Create an non-regular data cube from AWS\ns2_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    tiles = \"20LKP\",\n    bands = (\"B02\", \"B03\", \"B04\", \n              \"B05\", \"B06\", \"B07\", \n              \"B08\", \"B8A\", \"B11\", \n              \"B12\",\"CLOUD\"),\n    start_date = \"2018-07-01\",\n    end_date = \"2018-08-31\")\n    \n# Regularize the cube to 15 day intervals\nreg_cube = sits_regularize(\n          cube       = s2_cube,\n          output_dir = tempdir_py,\n          res        = 60,\n          period     = \"P15D\",\n          multicores = 4)\n\n\n\n\nGenerate the mixture model\n\n\nR\nPython\n\n\n\n\n# Define the endmembers for three classes and six bands\nem &lt;- tibble::tribble(\n    ~class,   ~B02, ~B03, ~B04, ~B8A, ~B11, ~B12,\n    \"forest\",  200,  352,  189, 2800, 1340,  546,\n    \"soil\",    400,  650,  700, 3600, 3500, 1800,\n    \"water\",   700, 1100, 1400,  850,   40,   26)\n\n# Generate the mixture model\nreg_cube &lt;- sits_mixture_model(\n    data = reg_cube,\n    endmembers = em,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r)\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n# Define the endmembers for three classes and six bands\nem = pd.DataFrame({\n    \"class\": [\"forest\", \"soil\", \"water\"],\n    \"B02\": [200, 400, 700],\n    \"B03\": [352, 650, 1100],\n    \"B04\": [189, 700, 1400],\n    \"B8A\": [2800, 3600, 850],\n    \"B11\": [1340, 3500, 40],\n    \"B12\": [546, 1800, 26]\n})\n\n# Generate the mixture model\nreg_cube = sits_mixture_model(\n    data = reg_cube,\n    endmembers = em,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_py\n)\n\n\n\n\nVisualize forest endmember\n\n\nR\nPython\n\n\n\n\n# Plot the FOREST for the first date using the Greens palette\nplot(reg_cube, band = \"FOREST\", palette = \"Greens\")\n\n\n\nFigure 9.1: Percentage of forest per pixel estimated by mixture model.\n\n\n\n\n\n\n# Plot the FOREST for the first date using the Greens palette\nplot(reg_cube, band = \"FOREST\", palette = \"Greens\")\n\n\n\nFigure 9.2: Percentage of forest per pixel estimated by mixture model.\n\n\n\n\n\n\nVisualize water endmember\n\n\nR\nPython\n\n\n\n\n# Plot the water endmember for the first date using the Blues palette\nplot(reg_cube, band = \"WATER\", palette = \"Blues\")\n\n\n\nFigure 9.3: Percentage of water per pixel estimated by mixture model.\n\n\n\n\n\n\n# Plot the water endmember for the first date using the Blues palette\nplot(reg_cube, band = \"WATER\", palette = \"Blues\")\n\n\n\nFigure 9.4: Percentage of water per pixel estimated by mixture model.\n\n\n\n\n\n\nVisualize soil endmember\n\n\nR\nPython\n\n\n\n\n# Plot the SOIL endmember for the first date using the orange red (OrRd) palette \nplot(reg_cube, band = \"SOIL\", palette = \"OrRd\")\n\n\n\nFigure 9.5: Percentage of soil per pixel estimated by mixture model.\n\n\n\n\n\n\n# Plot the SOIL endmember for the first date using the orange red (OrRd) palette \nplot(reg_cube, band = \"SOIL\", palette = \"OrRd\")\n\n\n\nFigure 9.6: Percentage of soil per pixel estimated by mixture model."
  },
  {
    "objectID": "dc_mixture.html#summary",
    "href": "dc_mixture.html#summary",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.4 Summary",
    "text": "9.4 Summary\nLinear mixture models (LMM) improve the interpretation of remote sensing images by accounting for mixed pixels and providing a more accurate representation of the Earth’s surface. LMMs provide a more accurate representation of mixed pixels by considering the contributions of multiple land classes within a single pixel. This can lead to improved land cover classification accuracy compared to conventional per-pixel classification methods, which may struggle to accurately classify mixed pixels.\nLMMs also allow for the estimation of the abundances of each land class within a pixel, providing valuable sub-pixel information. This can be especially useful in applications where the spatial resolution of the sensor is not fine enough to resolve individual land cover types, such as monitoring urban growth or studying vegetation dynamics. By considering the sub-pixel composition of land classes, LMMs can provide a more sensitive measure of changes in land cover over time. This can lead to more accurate and precise change detection, particularly in areas with complex land cover patterns or where subtle changes in land cover may occur."
  },
  {
    "objectID": "dc_mixture.html#references",
    "href": "dc_mixture.html#references",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nD. A. Roberts, M. O. Smith, and J. B. Adams, “Green vegetation, nonphotosynthetic vegetation, and soils in AVIRIS data,” Remote Sensing of Environment, vol. 44, no. 2, pp. 255–269, 1993, doi: 10.1016/0034-4257(93)90020-X.\n\n\n[2] \nY. E. Shimabukuro and F. J. Ponzoni, Spectral Mixture for Remote Sensing: Linear Model and Applications. Cham: Springer International Publishing, 2019.\n\n\n[3] \nM. A. Cochrane and C. Souza, “Linear mixture model classification of burned forests in the Eastern Amazon,” International Journal of Remote Sensing, vol. 19, no. 17, pp. 3433–3440, 1998, doi: 10.1080/014311698214109.\n\n\n[4] \nC. M. Souza Jr, D. A. Roberts, and M. A. Cochrane, “Combining spectral and spatial information to map canopy damage from selective logging and forest fires,” Remote Sensing of Environment, vol. 98, no. 2, pp. 329–343, 2005, doi: 10.1016/j.rse.2005.07.013.\n\n\n[5] \nE. L. Bullock, C. E. Woodcock, and C. E. Holden, “Improved change monitoring using an ensemble of time series algorithms,” Remote Sensing of Environment, vol. 238, p. 111165, 2020, doi: 10.1016/j.rse.2019.04.018.\n\n\n[6] \nS. Chen et al., “Monitoring temperate forest degradation on Google Earth Engine using Landsat time series analysis,” Remote Sensing of Environment, vol. 265, p. 112648, 2021, doi: 10.1016/j.rse.2021.112648.\n\n\n[7] \nM. Halabisky, L. M. Moskal, A. Gillespie, and M. Hannam, “Reconstructing semi-arid wetland surface water dynamics through spectral mixture analysis of a time series of Landsat satellite images (1984–2011),” Remote Sensing of Environment, vol. 177, pp. 171–183, 2016, doi: 10.1016/j.rse.2016.02.040.\n\n\n[8] \nC. Wu and A. T. Murray, “Estimating impervious surface distribution by spectral mixture analysis,” Remote sensing of Environment, vol. 84, no. 4, pp. 493–505, 2003.\n\n\n[9] \nV. Franc, V. Hlaváč, and M. Navara, “Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem,” in Computer Analysis of Images and Patterns, 2005, pp. 407–414, doi: 10.1007/11556121_50.\n\n\n[10] \nD. A. Roberts, M. Gardner, R. Church, S. Ustin, G. Scheer, and R. O. Green, “Mapping Chaparral in the Santa Monica Mountains Using Multiple Endmember Spectral Mixture Models,” Remote Sensing of Environment, vol. 65, no. 3, pp. 267–279, 1998, doi: 10.1016/S0034-4257(98)00037-6."
  },
  {
    "objectID": "dc_reduce.html#introduction",
    "href": "dc_reduce.html#introduction",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.1 Introduction",
    "text": "10.1 Introduction\nThere are cases when users want to combine the values of a time series associated to each pixel of a data cube using reduction operators. In time series analysis, a reduction operator is a function that combines a sequence of data points into a single value. This process involves summarizing or aggregating the information from the time series in a meaningful way. Reduction operators are often used to extract key statistics or features from the data, making it easier to analyze and interpret."
  },
  {
    "objectID": "dc_reduce.html#methods",
    "href": "dc_reduce.html#methods",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.2 Methods",
    "text": "10.2 Methods\nTo produce temporal combinations, sits provides sits_reduce, with associated functions:\n\n\nt_max(): maximum value of the series.\n\nt_min(): minimum value of the series\n\nt_mean(): mean of the series.\n\nt_median(): median of the series.\n\nt_std(): standard deviation of the series.\n\nt_skewness(): skewness of the series.\n\nt_kurtosis(): kurtosis of the series.\n\nt_amplitude(): difference between maximum and minimum values of the cycle. A small amplitude means a stable cycle.\n\nt_fslope(): maximum value of the first slope of the cycle. Indicates when the cycle presents an abrupt change in the curve. The slope between two values relates the speed of the growth or senescence phases\n\nt_mse(): average spectral energy density. The energy of the time series is distributed by frequency.\n\nt_fqr(): value of the first quartile of the series (0.25).\n\nt_tqr(): value of the third quartile of the series (0.75).\n\nt_iqr(): interquartile range (difference between the third and first quartiles).\n\nThe functions t_std(), t_skewness(), t_kurtosis(), and t_mse() produce values greater than the limit of a two-byte integer. Therefore, we save the images generated by them in floating point format."
  },
  {
    "objectID": "dc_reduce.html#example",
    "href": "dc_reduce.html#example",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.3 Example",
    "text": "10.3 Example\nThe following example shows how to execute a temporal reduction operation.\n\n\nR\nPython\n\n\n\n\n# Define a region of interest to be enclosed in the data cube\nroi &lt;- c(\n    \"lon_min\" = -55.80259, \n    \"lon_max\" = -55.199, \n    \"lat_min\" = -11.80208, \n    \"lat_max\" = -11.49583\n)\n\n# Define a data cube in the MPC repository using NDVI MODIS data\nndvi_cube &lt;- sits_cube(\n    source = \"MPC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands = c(\"NDVI\"),\n    roi = roi,\n    start_date =  \"2018-05-01\",\n    end_date = \"2018-09-30\"\n)\n\n# Copy the cube to a local file system\nndvi_cube_local &lt;- sits_cube_copy(\n    cube = ndvi_cube,\n    output_dir = tempdir_r,\n    multicores = 4\n)\n\n\n\n\n# Define a region of interest to be enclosed in the data cube\nroi = dict(\n    lon_min = -55.80259, \n    lon_max = -55.199, \n    lat_min = -11.80208, \n    lat_max = -11.49583\n)\n\n# Define a data cube in the MPC repository using NDVI MODIS data\nndvi_cube = sits_cube(\n    source = \"MPC\",\n    collection = \"MOD13Q1-6.1\",\n    bands = \"NDVI\",\n    roi = roi,\n    start_date =  \"2018-05-01\",\n    end_date = \"2018-09-30\"\n)\n\n# Copy the cube to a local file system\nndvi_cube_local = sits_cube_copy(\n    cube = ndvi_cube,\n    output_dir = tempdir_py,\n    multicores = 4\n)\n\n\n\n\nAfter creating a local data cube based on the contents of the MPC MODIS cube with the NDVI band, we can now compute the maximum NDVI values for each pixel for the images during the period from 2018-05-01 to 2018-09-30.\n\n\nR\nPython\n\n\n\n\n# create a local directory to store the result of the operation\ntempdir_r_ndvi_max &lt;- file.path(tempdir_r, \"ndvi_max\")\ndir.create(tempdir_r_ndvi_max, showWarnings = FALSE)\n\n# Calculate the NBR index\nmax_ndvi_cube &lt;- sits_reduce(ndvi_cube_local,\n    NDVIMAX = t_max(NDVI),\n    output_dir = tempdir_r_ndvi_max,\n    multicores = 4,\n    progress = TRUE\n)\nplot(max_ndvi_cube, band = \"NDVIMAX\")\n\n\n\n\n# create a local directory to store the result of the operation\ntempdir_py_ndvi_max = tempdir_py / \"ndvi_max\"\ntempdir_py_ndvi_max.mkdir(parents=True, exist_ok=True)\n\n# Calculate the NBR index\nmax_ndvi_cube = sits_reduce(ndvi_cube_local,\n    NDVIMAX = \"t_max(NDVI)\",\n    output_dir = tempdir_py_ndvi_max,\n    multicores = 4,\n    progress = True\n)\n\nplot(max_ndvi_cube, band = \"NDVIMAX\")\n\n\n\n\n\n\n\n\nFigure 10.1: Maximum NDVI for MPC MODIS cube for period 2018-05-01 to 2018-09-30."
  },
  {
    "objectID": "dc_reduce.html#summary",
    "href": "dc_reduce.html#summary",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.4 Summary",
    "text": "10.4 Summary\nTemporal reducing operations in Earth Observation (EO) data cubes are aggregations or summaries performed along the time dimension. These operations compress temporal information—typically multiple observations of the same location over time—into a single value or summary per pixel or area. They are useful for generating cloud-free composites, trend analyses, and preparing data for classification or modeling."
  },
  {
    "objectID": "dc_texture.html#introduction",
    "href": "dc_texture.html#introduction",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.1 Introduction",
    "text": "11.1 Introduction\nGLCM (Gray Level Co-occurrence Matrix) texture measures capture spatial relationships between pixel intensities. Important measures include contrast, dissimilarity, homogeneity, energy, entropy, correlation, variance, and cluster prominence. These measures are useful in image processing and remote sensing, helping with land cover classification, texture recognition, and medical imaging applications. One key benefit is their ability to capture information about pixel arrangement, even under some lighting changes, making them resilient for object classification tasks. Surfaces such as cropland, urban fabric, tree crowns or tumbling water can share similar reflectance values but differ in their arrangement. Including GLCM features into a classifier therefore improves separability of spectrally ambiguous classes."
  },
  {
    "objectID": "dc_texture.html#methods",
    "href": "dc_texture.html#methods",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.2 Methods",
    "text": "11.2 Methods\nA GLCM is a 2-D histogram that records how often a pixel with gray level i occurs at a fixed spatial offset (distance d, direction θ) from a pixel with gray level j. It is therefore a second-order statistic: instead of looking at single-pixel values, it captures pairwise relationships. These measures quantify spatial relationships between grey levels in an image and are used to describe texture—patterns of variation in pixel intensity that convey surface properties such as smoothness, roughness, regularity, or granularity. The idea was first formalised by Haralick and colleagues in 1973 [1] and remains one of the most widely used texture descriptors in computer vision, remote sensing and medical imaging.\nThe sits implementation of GLCM measures is follows the guidelines and equations described by Hall-Beyer [2] and uses the following parameters:\n\n\ncube : a data cube\n\nwindow_size : odd number with the size of the sliding window.\n\nangles : a vector indicating the direction angles in radians related to the central pixel and its neighbors (See details). Default is 0.\n\nmemsize: memory available for processing\n\nmulticores: number of cores available for processing\n\noutput_dir: output directory for the resulting cube\n\nprogress: show progress bar?\n\n... : GLCM function (see details).\n\nThe angles parameter captures the spatial relation between the central pixel and its neighbors in radians, where: - 0 : neighbor on right-side. - pi/4 : neighbor on the top-right diagonal - pi/2: neighbor on top of the current pixel - 3*pi/4: neighbor on the top-left diagonal\nOur implementation relies on a symmetric co-occurrence matrix, which considers the opposite directions of an angle. For example, the neighbor pixels based on angle 0 relies on the left and right direction. The neighbor pixels of pi/2 are above and below the central pixel, and so on. If more than one angle is provided, we compute their average.\n\n\nglcm_contrast() : measures the contrast or the amount of local variations present in an image. Low contrast values indicate regions with low spatial frequency.\n\nglcm_homogeneity(): also known as the Inverse Difference Moment, measures image homogeneity by assuming larger values for smaller gray tone differences in pairs.\n\nglcm_asm(): the Angular Second Moment (ASM) measures textural uniformity. High ASM values indicate a constant or a periodic form in the window values.\n\nglcm_energy(): measures textural uniformity. Energy is defined as the square root of the ASM.\n\nglcm_mean(): measures the mean of the probability of co-occurrence of specific pixel values within the neighborhood.\n\nglcm_variance(): measures the heterogeneity and is strongly correlated to first order statistical variables such as standard deviation. Variance values increase as the gray-level values deviate from their mean.\n\nglcm_std(): the standard deviation, which is the square root of the variance.\n\nglcm_correlation(): measures the gray-tone linear dependencies of the image. Low correlation values indicate homogeneous region edges."
  },
  {
    "objectID": "dc_texture.html#example",
    "href": "dc_texture.html#example",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.3 Example",
    "text": "11.3 Example\nAs an example of using GLCM texture measure, we will take an example of a SAR Sentinel-1 image over a small part of the MGRS tile “20LMR”. We will use a cube with a single date to make processing faster. In the case of a multi-date cube, the texture measure will be computed separately for each band and each date.\nWe first select a single date cube for a Sentinel-1 image (band “VH”) for a region in Rondonia, Brasil. We then regularize this cube to match a small region of interest inside tile “20LMR”. Using a small ROI allows a better understanding of the effect of texture measures.\n\n\nR\nPython\n\n\n\n\n# create an RTC cube from MPC collection for a region in Rondonia, Brazil.\ncube_s1_rtc &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VH\"),\n    orbit = \"descending\",\n    tiles = c(\"20LMR\"),\n    start_date = \"2024-08-01\",\n    end_date = \"2024-08-12\"\n)\n\n# define a ROI which is part of the cube to reduce processing\nroi &lt;- c(\n    \"lon_min\" = -63.45, \n    \"lon_max\" = -63.3,  \n    \"lat_min\" = -8.7, \n    \"lat_max\" = -8.55\n)\n\n# create an RTC cube regular\ncube_s1_rtc_20LMR &lt;-  sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P12D\",\n    res = 20,\n    output_dir = tempdir_r,\n    roi = roi,\n    multicores = 4\n)\nplot(cube_s1_rtc_20LMR, band = \"VH\")\n\n\n\n\n# create an RTC cube from MPC collection for a region in Rondonia, Brazil.\ncube_s1_rtc = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = \"VH\",\n    orbit = \"descending\",\n    tiles = \"20LMR\",\n    start_date = \"2024-08-01\",\n    end_date = \"2024-08-12\"\n)\n\n# define a ROI which is part of the cube to reduce processing\nroi = dict(\n    lon_min = -63.45, \n    lon_max = -63.3,  \n    lat_min = -8.7, \n    lat_max = -8.55\n)\n\n# create an RTC cube regular\ncube_s1_rtc_20LMR = sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P12D\",\n    res = 20,\n    output_dir = tempdir_py,\n    roi = roi,\n    multicores = 4\n)\n\nplot(cube_s1_rtc_20LMR, band = \"VH\")\n\n\n\n\n\n\n\n\nFigure 11.1: Original Sentinel-1 image covering part of tile 20LMR.\n\n\n\nThe plot on Figure 11.1 shows a situation of small forest patches inside an area with much deforestation. If also shows that the forest areas have a grainy and unequal texture. There are areas near the center of the image which show higly degraded forest areas. To try to capture these textural differences, we use GLCM Mean and GLCM Contrast, following recommendations by Hall-Beyer [2]. Each measure captures different aspects of image texture.\nThe GLCM Mean represents the average grey level of the pixel pairs in the co-occurrence matrix. For a normalized GLCM matrix \\(P(i,j)\\), the mean is calculated along rows or columns (since in symmetric matrices they are the same):\n\\[\n\\mu = \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} i \\cdot P(i, j)\n\\]\nThis measure detects brightness trends in local regions of the image. It tends to be high for brighter areas, and low for darker areas. The GLCM Mean can distinguish regions based on overall tone, e.g., water vs. dry soil, shadow vs. sunlit areas. It is useful for separating classes that have different average intensity values.\n\n\nR\nPython\n\n\n\n\n# measure GLMC texture and plot the result\ncube_s1_rtc_20LMR &lt;- sits_texture(\n    cube = cube_s1_rtc_20LMR,\n    VHMEAN = glcm_mean(VH),\n    window_size = 5,\n    output_dir = tempdir_r,\n    multicores = 4,\n    memsize = 16\n)\n\nplot(cube_s1_rtc_20LMR, band = \"VHMEAN\")\n\n\n\n\n# measure GLMC texture and plot the result\ncube_s1_rtc_20LMR = sits_texture(\n    cube = cube_s1_rtc_20LMR,\n    VHMEAN = \"glcm_mean(VH)\",\n    window_size = 5,\n    output_dir = tempdir_py,\n    multicores = 4,\n    memsize = 16\n)\n\nplot(cube_s1_rtc_20LMR, band = \"VHMEAN\")\n\n\n\n\n\n\n\n\nFigure 11.2: GLCM Mean measure for Sentinel-1 image covering part of tile 20LMR.\n\n\n\nFigure 11.2 shows that the GLCM Mean enhances the edges and increases the distinction between forest and non-forest areas. Areas of degraded forest, where trees and clear-cut areas are intermingled, are also stressed to produce an intermediate value between that of the trees and clear-cut areas. In theory, this distinction could help the detection of degraded forests.\nThe GLCM Contrast measures the weighted sum of squared intensity differences between pixel pairs. Contrast quantifies the intensity variation between a pixel and its neighbor, measuring local changes in grey levels. It produces high contrast areas when neighboring pixel pairs have very different grey levels, indicating rough, coarse, or edge-rich texture. Low contrast areas appear when neighboring pixels are similar, indicating smooth, homogeneous regions.\n\\[\n\\text{Contrast} = \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} (i - j)^2 \\cdot P(i, j)\n\\]\n\n\nR\nPython\n\n\n\n\n# measure GLMC texture and plot the result\ncube_s1_rtc_20LMR &lt;- sits_texture(\n    cube = cube_s1_rtc_20LMR,\n    VHCON = glcm_contrast(VH),\n    window_size = 5,\n    output_dir = tempdir_r,\n    multicores = 4,\n    memsize = 16\n)\nplot(cube_s1_rtc_20LMR, band = \"VHCON\")\n\n\n\n\n# measure GLMC texture and plot the result\ncube_s1_rtc_20LMR = sits_texture(\n    cube = cube_s1_rtc_20LMR,\n    VHCON = \"glcm_contrast(VH)\",\n    window_size = 5,\n    output_dir = tempdir_py,\n    multicores = 4,\n    memsize = 16\n)\n\nplot(cube_s1_rtc_20LMR, band = \"VHCON\")\n\n\n\n\n\n\n\n\nFigure 11.3: GLCM Contrast measure for Sentinel-1 image covering part of tile 20LMR.\n\n\n\nIn Figure 11.3, one can see that the GLCM Contrast improves our understanding of the internal structure of forest patches. In this region, due to high human activity, many of the forest patches show evidence of degradation. In a dense canopy area, areas of low contrast are to be expected. In low or moderate degradation areas, which is the case in parts of the image, one sees pixels of high and low contrast grouped together. Thus, the GLCM Contrast measure provides further insights on the structure of forests in this area of intense deforestation."
  },
  {
    "objectID": "dc_texture.html#summary",
    "href": "dc_texture.html#summary",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.4 Summary",
    "text": "11.4 Summary\nGLCM features are a way to include contextual texture cues into image analysis pipelines. When combined with spectral or deep-learning features they consistently boost classification and change-detection accuracy, particularly in heterogeneous landscapes where “what pixels are made of” is not enough—“how they are arranged” makes the difference."
  },
  {
    "objectID": "dc_texture.html#references",
    "href": "dc_texture.html#references",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nR. M. Haralick, K. Shanmugam, and I. Dinstein, “Textural Features for Image Classification,” IEEE Transactions on Systems, Man, and Cybernetics, vol. SMC–3, no. 6, pp. 610–621, 1973, doi: 10.1109/TSMC.1973.4309314.\n\n\n[2] \nM. Hall-Beyer, “Practical guidelines for choosing GLCM textures to use in landscape classification tasks over a range of moderate spatial scales,” International Journal of Remote Sensing, vol. 38, no. 5, pp. 1312–1338, 2017, doi: 10.1080/01431161.2016.1278314."
  },
  {
    "objectID": "timeseries.html#introduction",
    "href": "timeseries.html#introduction",
    "title": "Satellite image time series",
    "section": "Introduction",
    "text": "Introduction\nThe sits package uses sets of time series data describing properties in spatiotemporal locations of interest. For land classification, these sets consist of samples labeled by experts. The package can also be used for any type of classification, provided that the timeline and bands of the time series used for training match that of the data cubes. In the chapters that follow, we discuss how to work with time series and present methods for improving data quality."
  },
  {
    "objectID": "timeseries.html#the-importance-of-high-quality-training-sets",
    "href": "timeseries.html#the-importance-of-high-quality-training-sets",
    "title": "Satellite image time series",
    "section": "The importance of high-quality training sets",
    "text": "The importance of high-quality training sets\nSelecting good training samples for machine learning classification of satellite images is critical to achieving accurate results. Experience with machine learning methods has shown that the number and quality of training samples are crucial factors in obtaining accurate results [1]. Large and accurate datasets are preferable, regardless of the algorithm used, while noisy training samples can negatively impact classification performance [2]. Thus, it is beneficial to use pre-processing methods to improve the quality of samples and eliminate those that may have been incorrectly labeled or possess low discriminatory power.\nIt is necessary to distinguish between wrongly labeled samples and differences resulting from the natural variability of class signatures. When working in a large geographic region, the variability of vegetation phenology leads to different patterns being assigned to the same label. A related issue is the limitation of crisp boundaries to describe the natural world. Class definitions use idealized descriptions (e.g., “a savanna woodland has tree cover of 50% to 90% ranging from 8 to 15 m in height”). Class boundaries are fuzzy and sometimes overlap, making it hard to distinguish between them. To improve sample quality, sits provides methods for evaluating the training data. For large datasets, we recommend using both imbalance-reducing and SOM-based algorithms. The SOM-based method identifies potential mislabeled samples and outliers that require further investigation. The methods for balancing training samples reduce bias in favour of classes of high occurrences. The results demonstrate a positive impact on the overall classification accuracy."
  },
  {
    "objectID": "timeseries.html#general-guidance",
    "href": "timeseries.html#general-guidance",
    "title": "Satellite image time series",
    "section": "General guidance",
    "text": "General guidance\nThe complexity and diversity of our planet defy simple label names with hard boundaries. Due to representational and data handling issues, all classification systems have a limited number of categories, which inevitably fail to adequately describe the nuances of the planet’s landscapes. All representation systems are thus limited and application-dependent. As stated by Janowicz [3]: “geographical concepts are situated and context-dependent and can be described from different, equally valid, points of view; thus, ontological commitments are arbitrary to a large extent”.\nThe availability of big data and satellite image time series is a further challenge. In principle, image time series can capture more subtle changes for land classification. Experts must conceive classification systems and training data collections by understanding how time series information relates to actual land change. Methods for quality analysis, such as those presented in this Part, cannot replace user understanding and informed choices."
  },
  {
    "objectID": "timeseries.html#references",
    "href": "timeseries.html#references",
    "title": "Satellite image time series",
    "section": "References",
    "text": "References\n\n\n\n\n[1] A. E. Maxwell, T. A. Warner, and F. Fang, “Implementation of machine-learning classification in remote sensing: An applied review,” International Journal of Remote Sensing, vol. 39, no. 9, pp. 2784–2817, 2018.\n\n\n[2] B. Frenay and M. Verleysen, “Classification in the Presence of Label Noise: A Survey,” IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 5, pp. 845–869, 2014, doi: 10.1109/TNNLS.2013.2292894.\n\n\n[3] K. Janowicz, S. Scheider, T. Pehle, and G. Hart, “Geospatial semantics and linked spatiotemporal data – Past, present, and future,” Semantic Web, vol. 3, no. 4, pp. 321–332, 2012, doi: 10.3233/SW-2012-0077."
  },
  {
    "objectID": "ts_basics.html#data-structure-for-image-time-series",
    "href": "ts_basics.html#data-structure-for-image-time-series",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.1 Data structure for image time series",
    "text": "12.1 Data structure for image time series\n\nIn sits, time series are stored in a data frame data structure, both in R and Python. The following code shows the first three lines of a time series containing 1,882 labeled samples of land classes in Mato Grosso state of Brazil. The samples have time series extracted from the MODIS MOD13Q1 product from 2000 to 2016, provided every 16 days at 250 m resolution in the Sinusoidal projection. Based on ground surveys and high-resolution imagery, it includes samples of seven classes: Forest, Cerrado, Pasture, Soy_Fallow, Soy_Cotton, Soy_Corn, and Soy_Millet.\n\n\nR\nPython\n\n\n\n\n# Samples\ndata(\"samples_matogrosso_mod13q1\")\nsamples_matogrosso_mod13q1[1:4,]\n\n# A tibble: 4 × 7\n  longitude latitude start_date end_date   label   cube     time_series      \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;list&gt;           \n1     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n2     -59.4    -9.31 2014-09-14 2015-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n3     -59.4    -9.31 2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n4     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n\n\n\n\n\n# Samples\nsamples_matogrosso_mod13q1 = load_samples_dataset(\n    name = \"samples_matogrosso_mod13q1\", \n    package = \"sitsdata\"\n)\n\nsamples_matogrosso_mod13q1[0:4]\n\n   longitude  latitude  start_date    end_date    label      cube                                        time_series\n0    -57.794   -9.7573  2006-09-14  2007-08-29  Pasture  bdc_cube           Index    NDVI     EVI     NIR     MIR...\n1    -59.418   -9.3126  2014-09-14  2015-08-29  Pasture  bdc_cube           Index    NDVI     EVI     NIR     MIR...\n2    -59.403   -9.3146  2013-09-14  2014-08-29  Pasture  bdc_cube           Index    NDVI     EVI     NIR     MIR...\n3    -57.803   -9.7565  2006-09-14  2007-08-29  Pasture  bdc_cube           Index    NDVI     EVI     NIR     MIR...\n\n\n\n\n\nThe time series data frame contains data and metadata. The first six columns contain spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The first sample has been labeled Pasture at location (-58.5631, -13.8844), being valid for the period (2006-09-14, 2007-08-29). Informing the dates where the label is valid is crucial for correct classification. In this case, the researchers labeling the samples used the agricultural calendar in Brazil. The relevant dates for other applications and other countries will likely differ from those used in the example. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a data frame, with a column with the dates and the other columns with the values for each spectral band."
  },
  {
    "objectID": "ts_basics.html#utilities-for-handling-time-series",
    "href": "ts_basics.html#utilities-for-handling-time-series",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.2 Utilities for handling time series",
    "text": "12.2 Utilities for handling time series\nThe package provides functions for data manipulation and displaying information for time series data frames. For example, summary() shows the labels of the sample set and their frequencies.\n\n\nR\nPython\n\n\n\n\nsummary(samples_matogrosso_mod13q1)\n\n# A tibble: 7 × 3\n  label      count   prop\n  &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado      379 0.206 \n2 Forest       131 0.0713\n3 Pasture      344 0.187 \n4 Soy_Corn     364 0.198 \n5 Soy_Cotton   352 0.192 \n6 Soy_Fallow    87 0.0474\n7 Soy_Millet   180 0.0980\n\n\n\n\n\nsummary(samples_matogrosso_mod13q1)\n\n        label  count      prop\n1     Cerrado    379  0.206315\n2      Forest    131  0.071312\n3     Pasture    344  0.187262\n4    Soy_Corn    364  0.198149\n5  Soy_Cotton    352  0.191617\n6  Soy_Fallow     87  0.047360\n7  Soy_Millet    180  0.097986\n\n\n\n\n\nIn many cases, it is helpful to relabel the dataset. For example, there may be situations where using a smaller set of labels is desirable because samples in one label on the original set may not be distinguishable from samples with other labels. We then could use sits_labels()&lt;- to assign new labels. The example below shows how to do relabeling on a time series set shown above; all samples associated with crops are grouped in a single Cropland label.\n\n\nR\nPython\n\n\n\n\n# Copy the sample set for Mato Grosso \nsamples_new_labels &lt;- samples_matogrosso_mod13q1\n\n# Show the current labels\nsits_labels(samples_new_labels)\n\n[1] \"Cerrado\"    \"Forest\"     \"Pasture\"    \"Soy_Corn\"   \"Soy_Cotton\"\n[6] \"Soy_Fallow\" \"Soy_Millet\"\n\n# Update the labels\nsits_labels(samples_new_labels) &lt;- c(\"Cerrado\",   \"Forest\",    \n                                     \"Pasture\",   \"Cropland\", \n                                     \"Cropland\", \"Cropland\",\n                                     \"Cropland\")\n\nsummary(samples_new_labels)\n\n# A tibble: 4 × 3\n  label    count   prop\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado    379 0.206 \n2 Cropland   983 0.535 \n3 Forest     131 0.0713\n4 Pasture    344 0.187 \n\n\n\n\n\n# Copy the sample set for Mato Grosso \nsamples_new_labels = samples_matogrosso_mod13q1.copy(deep = True)\n\n# Get the current labels\nsits_labels(samples_new_labels)\n\n['Cerrado', 'Forest', 'Pasture', 'Soy_Corn', 'Soy_Cotton', 'Soy_Fallow', 'Soy_Millet']\n\n# Update the labels\nsamples_new_labels.sits.labels = (\n    \"Cerrado\", \"Forest\", \"Pasture\", \"Cropland\", \n    \"Cropland\", \"Cropland\",\"Cropland\"\n)\n\nsummary(samples_new_labels)\n\n      label  count      prop\n1   Cerrado    379  0.206315\n2  Cropland    983  0.535112\n3    Forest    131  0.071312\n4   Pasture    344  0.187262\n\n\n\n\n\nIn R, the functions from dplyr, tidyr, and purrr packages of the tidyverse [1] can be used to process the data. In Python, time series data frames can be processed using pandas. For example, the following code uses sits_select() to get a subset of the sample dataset with two bands (NDVI and EVI). In R, it then uses the dplyr::filter() to select the samples labeled as Cerrado, while in Python the code uses the query function from the pandas package.\n\n\nR\nPython\n\n\n\n\n# Select NDVI band\nsamples_ndvi &lt;- sits_select(samples_matogrosso_mod13q1, \n                            bands = \"NDVI\")\n\n# Select only samples with Cerrado label\nsamples_cerrado &lt;- dplyr::filter(samples_ndvi, \n                                 label == \"Cerrado\")\n\n\n\n\n# Select NDVI band\nsamples_ndvi = sits_select(samples_matogrosso_mod13q1, \n                            bands = \"NDVI\")\n\n# Select only samples with Cerrado label\nsamples_cerrado = samples_ndvi.query(\"label == 'Cerrado'\")"
  },
  {
    "objectID": "ts_basics.html#time-series-visualisation",
    "href": "ts_basics.html#time-series-visualisation",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.3 Time series visualisation",
    "text": "12.3 Time series visualisation\nThe default time series plot in sits combines all samples together in a single temporal interval, even if they belong to different years. This plot shows the spread of values for the time series of each band. The strong red line in the plot indicates the median of the values, while the two orange lines are the first and third interquartile ranges. See ?sits::plot for more details on data visualization in sits.\n\n\nR\nPython\n\n\n\n\n# Plot cerrado samples together\nplot(samples_cerrado)\n\n\n\nFigure 12.1: Plot of Cerrado samples for NDVI band.\n\n\n\n\n\n\n# Plot cerrado samples together\nplot(samples_cerrado)\n\n\n\nFigure 12.2: Plot of Cerrado samples for NDVI band.\n\n\n\n\n\n\nTo see the spatial distribution of the samples, use sits_view() to create an interactive plot. The spatial visulisation is useful to show where the data has been collected.\n\n\nR\nPython\n\n\n\n\nsits_view(samples_matogrosso_mod13q1)\n\n\n\n\nsits_view(samples_matogrosso_mod13q1)\n\n\n\n\n\n\n\n\nFigure 12.3: View location of training samples"
  },
  {
    "objectID": "ts_basics.html#patterns-of-training-samples",
    "href": "ts_basics.html#patterns-of-training-samples",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.4 Patterns of training samples",
    "text": "12.4 Patterns of training samples\nWhen dealing with large time series, it is useful to obtain a single plot that captures the essential temporal variability of each class. Following the work on the dtwSat R package [2], we use a generalized additive model (GAM) to obtain a single time series based on statistical approximation. In a GAM, the predictor depends linearly on a smooth function of the predictor variables.\n\\[\ny = \\beta_{i} + f(x) + \\epsilon, \\epsilon \\sim N(0, \\sigma^2).\n\\]\nThe function sits_patterns() uses a GAM to predict an idealized approximation to the time series associated with each class for all bands. The resulting patterns can be viewed using plot().\n\n\nR\nPython\n\n\n\n\n# Estimate the patterns for each class and plot them\nsamples_matogrosso_mod13q1 |&gt;  \n    sits_patterns() |&gt; \n    plot()\n\n\n\nFigure 12.4: Patterns for Mato Grosso MOD13Q1 samples.\n\n\n\n\n\n\n# Estimate the patterns for each class and plot them\nplot(\n    sits_patterns(\n        samples_matogrosso_mod13q1\n    )\n)\n\n\n\nFigure 12.5: Patterns for Mato Grosso MOD13Q1 samples.\n\n\n\n\n\n\nThe resulting patterns provide some insights over the time series behaviour of each class. The response of the Forest class is quite distinctive. They also show that it should be possible to separate between the single and double cropping classes. There are similarities between the double-cropping classes (Soy_Corn and Soy_Millet) and between the Cerrado and Pasture classes. The subtle differences between class signatures provide hints at possible ways by which machine learning algorithms might distinguish between classes. One example is the difference between the middle-infrared response during the dry season (May to September) to differentiate between Cerrado and Pasture."
  },
  {
    "objectID": "ts_basics.html#geographical-variability-of-training-data",
    "href": "ts_basics.html#geographical-variability-of-training-data",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.5 Geographical variability of training data",
    "text": "12.5 Geographical variability of training data\nWhen working with machine learning classification of Earth observation data, it is important to evaluate if the training samples are well distributed in the study area. Training data often comes from ground surveys made at chosen locations. In large areas, ideally representative samples need to capture spatial variability. In practice, however, ground surveys or other means of data collection are limited to selected areas. In many cases, the geographical distribution of the training data does not cover the study area equally. Such mismatch can be a problem for achieving a good quality classification. As stated by Meyer and Pebesma [3]: “large gaps in geographic space do not always imply large gaps in feature space”.\nMeyer and Pebesma propose using a spatial distance distribution plot, which displays two distributions of nearest-neighbor distances: sample-to-sample and prediction-location-to-sample [3]. The difference between the two distributions reflects the degree of spatial clustering in the reference data. Ideally, the two distributions should be similar. Cases where the sample-to-sample distance distribution does not match prediction-location-to-sample distribution indicate possible problems in training data collection.\nsits implements spatial distance distribution plots with the sits_geo_dist() function. This function gets a training data in the samples parameter, and the study area in the roi parameter expressed as an sf object. Additional parameters are n (maximum number of samples for each distribution) and crs (coordinate reference system for the samples). By default, n is 1000, and crs is “EPSG:4326”. The example below shows how to use sits_geo_dist().\n\n\nR\nPython\n\n\n\n\n# Read a shapefile for the state of Mato Grosso, Brazil\nmt_shp &lt;- system.file(\"extdata/shapefiles/mato_grosso/mt.shp\",\n                      package = \"sits\")\n\n# Convert to an sf object\nmt_sf &lt;- sf::read_sf(mt_shp)\n\n# Calculate sample-to-sample and sample-to-prediction distances\ndistances &lt;- sits_geo_dist(\n    samples = samples_modis_ndvi,\n    roi = mt_sf)\n\n# Plot sample-to-sample and sample-to-prediction distances\nplot(distances)\n\n\n\nFigure 12.6: Distribution of sample-to-sample and sample-to-prediction distances.\n\n\n\n\n\n\n# Import GeoPandas\nimport geopandas as gpd\n\n# Read a shapefile for the state of Mato Grosso, Brazil\nmt_shp = r_package_dir(\"extdata/shapefiles/mato_grosso/mt.shp\",\n                      package = \"sits\")\n\n# Convert to an GeoDataFrame object\nmt_sf = gpd.read_file(mt_shp)\n\n# Calculate sample-to-sample and sample-to-prediction distances\ndistances = sits_geo_dist(\n    samples = samples_modis_ndvi,\n    roi = mt_sf)\n\n# Plot sample-to-sample and sample-to-prediction distances\nplot(distances)\n\n\n\nFigure 12.7: Distribution of sample-to-sample and sample-to-prediction distances.\n\n\n\n\n\n\nThe plot shows a mismatch between the sample-to-sample and the sample-to-prediction distributions. Most samples are closer to each other than they are close to the location where values need to be predicted. In this case, there are many areas where few or no samples have been collected and where the prediction uncertainty will be higher. In this and similar cases, improving the distribution of training samples is always welcome. If that is not possible, areas with insufficient samples could have lower accuracy. This information must be reported to potential users of classification results."
  },
  {
    "objectID": "ts_basics.html#time-series-from-data-cubes",
    "href": "ts_basics.html#time-series-from-data-cubes",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.6 Time series from data cubes",
    "text": "12.6 Time series from data cubes\nTo get a set of time series in sits, first create a regular data cube and then request one or more time series from the cube using sits_get_data(). This function uses two mandatory parameters: cube and samples. The cube indicates the data cube from which the time series will be extracted. The samples parameter accepts the following data types:\n\nA data.frame with information on latitude and longitude (mandatory), start_date, end_date, and label for each sample point.\nA csv file with columns latitude, longitude, start_date, end_date, and label.\nA shapefile containing either POINTor POLYGON geometries. See details below.\nAn sf object (from the sf package in R) with POINT or POLYGON geometry.\nA geopandas object from the geopandas package in Python with POINT or POLYGON geometry.\n\nIn the example below, given a data cube, the user provides the latitude and longitude of the desired location. Since the bands, start date, and end date of the time series are missing, sits obtains them from the data cube. The result is a tibble with one time series that can be visualized using plot().\n\n\nR\nPython\n\n\n\n\n# Obtain a raster cube based on local files\ndata_dir &lt;- system.file(\"extdata/sinop\", package = \"sitsdata\")\nraster_cube &lt;- sits_cube(\n    source     = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir   = data_dir,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\"))\n\n# Obtain a time series from the raster cube from a point\nsample_latlong &lt;- tibble::tibble(\n    longitude = -55.57320, \n    latitude  = -11.50566)\n\nseries &lt;- sits_get_data(cube    = raster_cube,\n                        samples = sample_latlong)\n\nplot(series)\n\n\n\nFigure 12.8: NDVI and EVI time series fetched from local raster cube.\n\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n# Obtain a raster cube based on local files\ndata_dir = r_package_dir(\"extdata/sinop\", package = \"sitsdata\")\nraster_cube = sits_cube(\n    source     = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir   = data_dir,\n    parse_info = (\"satellite\", \"sensor\", \"tile\", \"band\", \"date\"))\n\n# Obtain a time series from the raster cube from a point\nsample_latlong = pd.DataFrame([dict(\n    longitude = -55.57320,\n    latitude  = -11.50566\n)])\n\nseries = sits_get_data(cube = raster_cube,\n                        samples = sample_latlong)\n\nplot(series)\n\n\n\nFigure 12.9: NDVI and EVI time series fetched from local raster cube.\n\n\n\n\n\n\nA useful case is when a set of labeled samples can be used as a training dataset. In this case, trusted observations are usually labeled and commonly stored in plain text files in comma-separated values (csv) or using shapefiles (shp).\n\n\nR\nPython\n\n\n\n\n# Retrieve a list of samples described by a csv file\nsamples_csv_file &lt;- system.file(\"extdata/samples/samples_sinop_crop.csv\",\n                                package = \"sits\")\n\n# Read the csv file into an R object\nsamples_csv &lt;- read.csv(samples_csv_file)\n\n# Print the first three samples\nsamples_csv[1:3,]\n\n  id longitude  latitude start_date   end_date   label\n1  1 -55.65931 -11.76267 2013-09-14 2014-08-29 Pasture\n2  2 -55.64833 -11.76385 2013-09-14 2014-08-29 Pasture\n3  3 -55.66738 -11.78032 2013-09-14 2014-08-29  Forest\n\n\n\n\n\n# Retrieve a list of samples described by a csv file\nsamples_csv_file = r_package_dir(\"extdata/samples/samples_sinop_crop.csv\",\n                                package = \"sits\")\n\n# Read the csv file into an R object\nsamples_csv = pd.read_csv(samples_csv_file)\n\n# Print the first three samples\nsamples_csv[0:3]\n\n   id  longitude  latitude  start_date    end_date    label\n0   1  -55.65931 -11.76267  2013-09-14  2014-08-29  Pasture\n1   2  -55.64833 -11.76385  2013-09-14  2014-08-29  Pasture\n2   3  -55.66738 -11.78032  2013-09-14  2014-08-29   Forest\n\n\n\n\n\nTo retrieve training samples for time series analysis, users must provide the temporal information (start_date and end_date). In the simplest case, all samples share the same dates. That is not a strict requirement. It is possible to specify different dates as long as they have a compatible duration. For example, the dataset samples_matogrosso_mod13q1 provided with the sitsdata package contains samples from different years covering the same duration. These samples are from the MOD13Q1 product, which contains the same number of images per year. Thus, all time series in the dataset samples_matogrosso_mod13q1 have the same number of dates.\nGiven a suitably built csv sample file, sits_get_data() requires two parameters: (a) cube, the name of the R object that describes the data cube; (b) samples, the name of the CSV file.\n\n\nR\nPython\n\n\n\n\n# Get the points from a data cube in raster brick format\npoints &lt;- sits_get_data(cube = raster_cube, \n                        samples = samples_csv_file)\n\n# Show the tibble with the first three points\npoints[1:3,]\n\n# A tibble: 3 × 7\n  longitude latitude start_date end_date   label    cube        time_series\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;    &lt;chr&gt;       &lt;list&gt;     \n1     -55.8    -11.7 2013-09-14 2014-08-29 Cerrado  MOD13Q1-6.1 &lt;tibble&gt;   \n2     -55.8    -11.7 2013-09-14 2014-08-29 Cerrado  MOD13Q1-6.1 &lt;tibble&gt;   \n3     -55.7    -11.7 2013-09-14 2014-08-29 Soy_Corn MOD13Q1-6.1 &lt;tibble&gt;   \n\n\n\n\n\n# Get the points from a data cube in raster brick format\npoints = sits_get_data(cube = raster_cube, \n                        samples = samples_csv_file)\n\n# Show the tibble with the first three points\npoints[1:3]\n\n   longitude  latitude  start_date    end_date     label         cube                                        time_series\n1  -55.75218 -11.68855  2013-09-14  2014-08-29   Cerrado  MOD13Q1-6.1           Index     EVI    NDVI\n0   2013-09-14 ...\n2  -55.69004 -11.73343  2013-09-14  2014-08-29  Soy_Corn  MOD13Q1-6.1           Index     EVI    NDVI\n0   2013-09-14 ...\n\n\n\n\n\nUsers can also specify samples by providing shapefiles, sf objects (in R), or geopandas objects (in Python) containing POINT or POLYGON geometries. The geographical location is inferred from the geometries associated with the shapefile or sf (or geopandas) object. For files containing points, the geographical location is obtained directly. For polygon geometries, the parameter n_sam_pol (defaults to 20) determines the number of samples to be extracted from each polygon. The temporal information can be provided explicitly by the user; if absent, it is inferred from the data cube. If label information is available in the shapefile or sf object, the parameter label_attr is indicates which column contains the label associated with each time series.\nIn what follows, we provide a shapefile with location of forested areas in the Cerrado biome in Brazil. The shapefile is first used to define the region of interest to retrieve a data cube. Then the point locations are used to retrieve the time series.\n\n\nR\nPython\n\n\n\n\n# Obtain a set of points inside the state of Mato Grosso, Brazil\nshp_file &lt;- system.file(\"extdata/shapefiles/cerrado/cerrado_forested.shp\", \n                        package = \"sits\")\n# Read the shapefile into an \"sf\" object\nsf_shape &lt;- sf::st_read(shp_file)\n\n# Create a data cube based on MOD13Q1 collection from BDC\nmodis_cube &lt;- sits_cube(\n    source      = \"BDC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands       = c(\"NDVI\", \"EVI\"),\n    roi         = sf_shape,\n    start_date  = \"2020-06-01\", \n    end_date    = \"2021-08-29\")\n\n# Read the points from the cube and produce a tibble with time series\nsamples_cerrado_forested &lt;- sits_get_data(\n    cube         = modis_cube, \n    samples      = shp_file, \n    start_date   = \"2020-06-01\",\n    end_date     = \"2021-08-29\", \n    label        = \"Woody Savanna\",\n    multicores   = 4)\n\n# Display the time series for the locations of Woody Savanna\nsamples_cerrado_forested\n\n\n\n\n# Obtain a set of points inside the state of Mato Grosso, Brazil\nshp_file = r_package_dir(\"extdata/shapefiles/cerrado/cerrado_forested.shp\", \n                        package = \"sits\")\n# Read the shapefile into an \"geopandas\" object\nsf_shape = gpd.read_file(shp_file)\n\n# Create a data cube based on MOD13Q1 collection from BDC\nmodis_cube = sits_cube(\n    source      = \"BDC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands       = (\"NDVI\", \"EVI\"),\n    roi         = sf_shape,\n    start_date  = \"2020-06-01\", \n    end_date    = \"2021-08-29\")\n\n# Read the points from the cube and produce a tibble with time series\nsamples_cerrado_forested = sits_get_data(\n    cube         = modis_cube, \n    samples      = shp_file, \n    start_date   = \"2020-06-01\",\n    end_date     = \"2021-08-29\", \n    label        = \"Woody Savanna\",\n    multicores   = 4)\n\n# Display the time series for the locations of Woody Savanna\nsamples_cerrado_forested\n\n\n\n\n\n\n# A tibble: 40 × 7\n   longitude latitude start_date end_date   label         cube       time_series\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;      &lt;list&gt;     \n 1     -51.9    -13.6 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 2     -51.5    -13.6 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 3     -51.4    -12.6 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 4     -51.3    -13.3 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 5     -51.2    -13.8 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 6     -51.1    -13.0 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 7     -50.7    -13.1 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 8     -50.2    -14.3 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 9     -50.1    -14.2 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n10     -47.3    -11.3 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n# ℹ 30 more rows"
  },
  {
    "objectID": "ts_basics.html#filtering-time-series",
    "href": "ts_basics.html#filtering-time-series",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.7 Filtering time series",
    "text": "12.7 Filtering time series\nSatellite image time series is generally contaminated by atmospheric influence, geolocation error, and directional effects [4]. Atmospheric noise, sun angle, interferences on observations or different equipment specifications, and the nature of the climate-land dynamics can be sources of variability [5]. Inter-annual climate variability also changes the phenological cycles of the vegetation, resulting in time series whose periods and intensities do not match on a year-to-year basis. To make the best use of available satellite data archives, methods for satellite image time series analysis need to deal with noisy and non-homogeneous datasets.\nThe literature on satellite image time series has several applications of filtering to correct or smooth vegetation index data. The package supports the well-known Savitzky–Golay (sits_sgolay()) and Whittaker (sits_whittaker()) filters. In an evaluation of NDVI time series filtering for estimating phenological parameters in India, Atkinson et al. found that the Whittaker filter provides good results [5]. Zhou et al. found that the Savitzky-Golay filter is suitable for reconstructing tropical evergreen broadleaf forests [6].\n\n12.7.1 Savitzky–Golay filter\nThe Savitzky-Golay filter fits a successive array of \\(2n+1\\) adjacent data points with a \\(d\\)-degree polynomial through linear least squares. The main parameters for the filter are the polynomial degree (\\(d\\)) and the length of the window data points (\\(n\\)). It generally produces smoother results for a larger value of \\(n\\) and/or a smaller value of \\(d\\) [7]. The optimal value for these two parameters can vary from case to case. In sits, the parameter order sets the order of the polynomial (default = 3), the parameter length sets the size of the temporal window (default = 5), and the parameter scaling sets the temporal expansion (default = 1). The following example shows the effect of Savitsky-Golay filter on a point extracted from the MOD13Q1 product, ranging from 2000-02-18 to 2018-01-01.\n\n\nR\nPython\n\n\n\n\n# Take NDVI band of the first sample dataset\npoint_ndvi &lt;- sits_select(point_mt_6bands, bands = \"NDVI\")\n\n# Apply Savitzky Golay filter\npoint_sg &lt;- sits_sgolay(point_ndvi, length = 5)\n\n# Merge the point and plot the series\nsits_merge(point_sg, point_ndvi) |&gt; plot()\n\n\n\nFigure 12.10: Savitzky-Golay filter applied on a multi-year NDVI time series.\n\n\n\n\n\n\n# Take NDVI band of the first sample dataset\npoint_ndvi = sits_select(point_mt_6bands, bands = \"NDVI\")\n\n# Apply Savitzky Golay filter\npoint_sg = sits_sgolay(point_ndvi, length = 5)\n\n# Merge the point and plot the series\nplot(\n    sits_merge(point_sg, point_ndvi)\n)\n\n\n\nFigure 12.11: Savitzky-Golay filter applied on a multi-year NDVI time series.\n\n\n\n\n\n\n\n12.7.2 Whittaker filter\nThe Whittaker smoother attempts to fit a curve representing the raw data, but is penalized if subsequent points vary too much [8]. The Whittaker filter balances the residual to the original data and the smoothness of the fitted curve. The filter has one parameter: \\(\\lambda{}\\) that works as a smoothing weight parameter. The following example shows the effect of the Whittaker filter on a point extracted from the MOD13Q1 product, ranging from 2000-02-18 to 2018-01-01. The lambda parameter controls the smoothing of the filter. By default, it is set to 0.5, a small value. The example shows the effect of a larger smoothing parameter.\n\n\nR\nPython\n\n\n\n\n# Take NDVI band of the first sample dataset\npoint_ndvi &lt;- sits_select(point_mt_6bands, bands = \"NDVI\")\n\n# Apply Whitakker filter\npoint_whit &lt;- sits_whittaker(point_ndvi, lambda = 0.5)\n\n# Merge the point and plot the series\nsits_merge(point_whit, point_ndvi) |&gt; plot()\n\n\n\nFigure 12.12: Whittaker filter applied on a one-year NDVI time series (source: authors)\n\n\n\n\n\n\n# Take NDVI band of the first sample dataset\npoint_ndvi = sits_select(point_mt_6bands, bands = \"NDVI\")\n\n# Apply Whitakker filter\npoint_whit = sits_whittaker(point_ndvi, lambda_ = 0.5)\n\n# Merge the point and plot the series\nplot(\n    sits_merge(point_whit, point_ndvi)\n)\n\n\n\nFigure 12.13: Whittaker filter applied on a one-year NDVI time series (source: authors)\n\n\n\n\n\n\nSimilar to what is observed in the Savitsky-Golay filter, high values of the smoothing parameter lambda produce an over-smoothed time series that reduces the capacity of the time series to represent natural variations in crop growth. For this reason, low smoothing values are recommended when using sits_whittaker()."
  },
  {
    "objectID": "ts_basics.html#summary",
    "href": "ts_basics.html#summary",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.8 Summary",
    "text": "12.8 Summary\nIn this chapter, we presented the data structure used by sits to store pixel-based image time series. The text also shows how to retrieve time series from data cubes, as well as utilities available for visualisation, filtering and extracting patterns and geographical distribution. In the next chapters, we discuss how to improve the quality of training samples."
  },
  {
    "objectID": "ts_basics.html#references",
    "href": "ts_basics.html#references",
    "title": "\n12  Basic operations on image time series\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nH. Wickham and G. Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc., 2017.\n\n\n[2] \nV. Maus, G. Câmara, M. Appel, and E. Pebesma, “dtwSat: Time-Weighted Dynamic Time Warping for Satellite Image Time Series Analysis in R,” Journal of Statistical Software, vol. 88, no. 5, pp. 1–31, 2019, doi: 10.18637/jss.v088.i05.\n\n\n[3] \nH. Meyer and E. Pebesma, “Machine learning-based global maps of ecological variables and the challenge of assessing them,” Nature Communications, vol. 13, no. 1, p. 2208, 2022, doi: 10.1038/s41467-022-29838-9.\n\n\n[4] \nE. F. Lambin and M. Linderman, “Time series of remote sensing data for land change science,” IEEE Transactions on Geoscience and Remote Sensing, vol. 44, no. 7, pp. 1926–1928, 2006.\n\n\n[5] \nP. M. Atkinson, C. Jeganathan, J. Dash, and C. Atzberger, “Inter-comparison of four models for smoothing satellite sensor time-series data to estimate vegetation phenology,” Remote Sensing of Environment, vol. 123, pp. 400–417, 2012.\n\n\n[6] \nJ. Zhou, L. Jia, M. Menenti, and B. Gorte, “On the performance of remote sensing time series reconstruction methods: A spatial comparison,” Remote Sensing of Environment, vol. 187, pp. 367–384, 2016.\n\n\n[7] \nJ. Chen, Per. Jönsson, M. Tamura, Z. Gu, B. Matsushita, and L. Eklundh, “A simple method for reconstructing a high-quality NDVI time-series data set based on the Savitzky–Golay filter,” Remote Sensing of Environment, vol. 91, no. 3, pp. 332–344, 2004, doi: 10.1016/j.rse.2004.03.014.\n\n\n[8] \nC. Atzberger and P. H. Eilers, “Evaluating the effectiveness of smoothing algorithms in the absence of ground reference measurements,” International Journal of Remote Sensing, vol. 32, no. 13, pp. 3689–3709, 2011."
  },
  {
    "objectID": "ts_cluster.html#introduction",
    "href": "ts_cluster.html#introduction",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.1 Introduction",
    "text": "13.1 Introduction\nGiven a set of training samples, experts should first cross-validate the training set to assess their inherent prediction error. The results show whether the data is internally consistent. Since cross-validation does not predict actual model performance, this chapter provides additional tools for improving the quality of training sets. More detailed information is available in Topic Validation and accuracy measurements."
  },
  {
    "objectID": "ts_cluster.html#dataset-used-in-this-chapter",
    "href": "ts_cluster.html#dataset-used-in-this-chapter",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.2 Dataset used in this chapter",
    "text": "13.2 Dataset used in this chapter\nThe examples of this chapter use the cerrado_2classes data, a set of time series for the Cerrado region of Brazil, the second largest biome in South America with an area of more than 2 million \\(km^2\\). The data contains 746 samples divided into 2 classes (Cerrado and Pasture). Each time series covers 12 months (23 data points) from MOD13Q1 product, and has 2 bands (EVI, and NDVI).\n\n\nR\nPython\n\n\n\n\n# Show the summary of the cerrado_2_classes dataset\nsummary(cerrado_2classes)\n\n# A tibble: 2 × 3\n  label   count  prop\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 Cerrado   400 0.536\n2 Pasture   346 0.464\n\n\n\n\n\n# Show the summary of the cerrado_2_classes dataset\nsummary(cerrado_2classes)\n\n     label  count      prop\n1  Cerrado    400  0.536193\n2  Pasture    346  0.463807"
  },
  {
    "objectID": "ts_cluster.html#hierarchical-clustering-for-sample-quality-control",
    "href": "ts_cluster.html#hierarchical-clustering-for-sample-quality-control",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.3 Hierarchical clustering for sample quality control",
    "text": "13.3 Hierarchical clustering for sample quality control\nThe package provides two clustering methods to assess sample quality: Agglomerative Hierarchical Clustering (AHC) and Self-organizing Maps (SOM). These methods have different computational complexities. AHC has a computational complexity of \\(\\mathcal{O}(n^2)\\), given the number of time series \\(n\\), whereas SOM complexity is linear. For large data, AHC requires substantial memory and running time; in these cases, SOM is recommended. This section describes how to run AHC in sits. The SOM-based technique is presented in the next section.\nAHC computes the dissimilarity between any two elements from a dataset. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. This approach is helpful for exploring samples due to its visualization power and ease of use [1]. In sits, AHC is implemented using sits_cluster_dendro().\n\n\nR\nPython\n\n\n\n\n# Take a set of patterns for 2 classes\n# Create a dendrogram, plot, and get the optimal cluster based on ARI index\nclusters &lt;- sits_cluster_dendro(\n    samples = cerrado_2classes, \n    bands = c(\"NDVI\", \"EVI\"),\n    dist_method = \"dtw_basic\",\n    linkage =  \"ward.D2\")\n\n\n\nFigure 13.1: Example of hierarchical clustering for a two class set.\n\n\n\n\n\n\n# Take a set of patterns for 2 classes\n# Create a dendrogram, plot, and get the optimal cluster based on ARI index\nclusters = sits_cluster_dendro(\n    samples = cerrado_2classes, \n    bands = (\"NDVI\", \"EVI\"),\n    dist_method = \"dtw_basic\",\n    linkage =  \"ward.D2\")\n\n\n\n\n\nFigure 13.2: Example of hierarchical clustering for a two class set.\n\n\n\n\n\n\nThe sits_cluster_dendro() function has one mandatory parameter (samples), with the samples to be evaluated. Optional parameters include bands, dist_method, and linkage. The dist_method parameter specifies how to calculate the distance between two time series. We recommend a metric that uses dynamic time warping (DTW) [2], as DTW is a reliable method for measuring differences between satellite image time series [3]. The options available in sits are based on those provided by package dtwclust, which include dtw_basic, dtw_lb, and dtw2. Please check ?dtwclust::tsclust for more information on DTW distances.\nThe linkage parameter defines the distance metric between clusters. The recommended linkage criteria are: complete or ward.D2. Complete linkage prioritizes the within-cluster dissimilarities, producing clusters with shorter distance samples, but results are sensitive to outliers. As an alternative, Ward proposes to use the sum-of-squares error to minimize data variance [4]; his method is available as ward.D2 option to the linkage parameter. To cut the dendrogram, the sits_cluster_dendro() function computes the adjusted rand index (ARI) [5], returning the height where the cut of the dendrogram maximizes the index. In the example, the ARI index indicates that there are six clusters. The result of sits_cluster_dendro() is a time series tibble with one additional column called “cluster”. The function sits_cluster_frequency() provides information on the composition of each cluster.\n\n\nR\nPython\n\n\n\n\n# Show clusters samples frequency\nsits_cluster_frequency(clusters)\n\n         \n            1   2   3   4   5   6 Total\n  Cerrado 203  13  23  80   1  80   400\n  Pasture   2 176  28   0 140   0   346\n  Total   205 189  51  80 141  80   746\n\n\n\n\n\n# Show clusters samples frequency\nsits_cluster_frequency(clusters)\n\n             1      2     3     4      5     6  Total\nCerrado  203.0   13.0  23.0  80.0    1.0  80.0  400.0\nPasture    2.0  176.0  28.0   0.0  140.0   0.0  346.0\nTotal    205.0  189.0  51.0  80.0  141.0  80.0  746.0\n\n\n\n\n\nThe cluster frequency table shows that each cluster has a predominance of either Cerrado or Pasture labels, except for cluster 3, which has a mix of samples from both labels. Such confusion may have resulted from incorrect labeling, inadequacy of selected bands and spatial resolution, or even a natural confusion due to the variability of the land classes. To remove cluster 3, use dplyr::filter(). The resulting clusters still contain mixed labels, possibly resulting from outliers. In this case, sits_cluster_clean() removes the outliers, leaving only the most frequent label. After cleaning the samples, the resulting set of samples is likely to improve the classification results.\n\n\nR\nPython\n\n\n\n\n# Remove cluster 3 from the samples\nclusters_new &lt;- dplyr::filter(clusters, cluster != 3)\n\n# Clear clusters, leaving only the majority label\nclean &lt;- sits_cluster_clean(clusters_new)\n\n# Show clusters samples frequency\nsits_cluster_frequency(clean)\n\n         \n            1   2   4   5   6 Total\n  Cerrado 203   0  80   0  80   363\n  Pasture   0 176   0 140   0   316\n  Total   203 176  80 140  80   679\n\n\n\n\n\n# Remove cluster 3 from the samples\nclusters_new = clusters.query(\"cluster != 3\")\n\n# Clear clusters, leaving only the majority label\nclean = sits_cluster_clean(clusters_new)\n\n# Show clusters samples frequency\nsits_cluster_frequency(clean)\n\n             1      2     4      5     6  Total\nCerrado  203.0    0.0  80.0    0.0  80.0  363.0\nPasture    0.0  176.0   0.0  140.0   0.0  316.0\nTotal    203.0  176.0  80.0  140.0  80.0  679.0"
  },
  {
    "objectID": "ts_cluster.html#summary",
    "href": "ts_cluster.html#summary",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.4 Summary",
    "text": "13.4 Summary\nIn this chapter, we present hierarchical clustering to improve the quality of training data. This method works well for up to four classes. Because of its quadratical computational complexity, it is not practical to use it for data sets with many classes. In this case, we suggest the use of self-organized maps (SOM) as shown in the next chapter."
  },
  {
    "objectID": "ts_cluster.html#references",
    "href": "ts_cluster.html#references",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nE. Keogh, J. Lin, and W. Truppel, “Clustering of time series subsequences is meaningless: Implications for previous and future research,” in Data Mining, 2003. ICDM 2003. Third IEEE International Conference on, 2003, pp. 115–122.\n\n\n[2] \nF. Petitjean, J. Inglada, and P. Gancarski, “Satellite Image Time Series Analysis Under Time Warping,” IEEE Transactions on Geoscience and Remote Sensing, vol. 50, no. 8, pp. 3081–3095, 2012, doi: 10.1109/TGRS.2011.2179050.\n\n\n[3] \nV. Maus, G. Camara, R. Cartaxo, A. Sanchez, F. M. Ramos, and G. R. Queiroz, “A Time-Weighted Dynamic Time Warping Method for Land-Use and Land-Cover Mapping,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 9, no. 8, pp. 3729–3739, 2016, doi: 10.1109/JSTARS.2016.2517118.\n\n\n[4] \nJ. H. Ward, “Hierarchical grouping to optimize an objective function,” Journal of the American statistical association, vol. 58, no. 301, pp. 236–244, 1963.\n\n\n[5] \nW. M. Rand, “Objective Criteria for the Evaluation of Clustering Methods,” Journal of the American Statistical Association, vol. 66, no. 336, pp. 846–850, 1971, doi: 10.1080/01621459.1971.10482356."
  },
  {
    "objectID": "ts_som.html#introduction",
    "href": "ts_som.html#introduction",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.1 Introduction",
    "text": "14.1 Introduction\nThe sits package provides a clustering technique based on self-organizing maps (SOM) as an alternative to hierarchical clustering for quality control of training samples. SOM is a dimensionality reduction technique [1], where high-dimensional data is mapped into a two-dimensional map, keeping the topological relations between data patterns. As shown in Figure 14.1, the SOM 2D map is composed of units called neurons. Each neuron has a weight vector, with the same dimension as the training samples. At the start, neurons are assigned a small random value and then trained by competitive learning. The algorithm computes the distances of each member of the training set to all neurons and finds the neuron closest to the input, called the best matching unit.\n\n\n\n\nFigure 14.1: SOM 2D map creation (source: [2]).\n\n\n\nThe input data for quality assessment is a set of training samples, which are high-dimensional data; for example, a time series with 25 instances of 4 spectral bands has 100 dimensions. When projecting a high-dimensional dataset into a 2D SOM map, the units of the map (called neurons) compete for each sample. Each time series will be mapped to one of the neurons. Since the number of neurons is smaller than the number of classes, each neuron will be associated with many time series. The resulting 2D map will be a set of clusters. Given that SOM preserves the topological structure of neighborhoods in multiple dimensions, clusters that contain training samples with a given label will usually be neighbors in 2D space. The neighbors of each neuron of a SOM map provide information on intraclass and interclass variability, which is used to detect noisy samples. The methodology of using SOM for sample quality assessment is discussed in detail in the reference paper [2].\n\n\n\n\nFigure 14.2: Using SOM for class noise reduction (source: [2])"
  },
  {
    "objectID": "ts_som.html#dataset-used-in-this-chapter",
    "href": "ts_som.html#dataset-used-in-this-chapter",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.2 Dataset used in this chapter",
    "text": "14.2 Dataset used in this chapter\nThe examples of this chapter use samples_cerrado_mod13q1, a set of time series from the Cerrado region of Brazil. The data ranges from 2000 to 2017 and includes 50,160 samples divided into 12 classes (Dense_Woodland, Dunes, Fallow_Cotton, Millet_Cotton, Pasture, Rocky_Savanna, Savanna, Savanna_Parkland, Silviculture, Soy_Corn, Soy_Cotton, and Soy_Fallow). Each time series covers 12 months (23 data points) from MOD13Q1 product, and has 4 bands (EVI, NDVI, MIR, and NIR). We use bands NDVI and EVI for faster processing.\n\n\nR\nPython\n\n\n\n\n# Take only the NDVI and EVI bands\nsamples_cerrado_mod13q1_2bands &lt;- sits_select(\n    data = samples_cerrado_mod13q1, \n    bands = c(\"NDVI\", \"EVI\"))\n\n# Show the summary of the samples\nsummary(samples_cerrado_mod13q1_2bands)\n\n# A tibble: 12 × 3\n   label            count    prop\n   &lt;chr&gt;            &lt;int&gt;   &lt;dbl&gt;\n 1 Dense_Woodland    9966 0.199  \n 2 Dunes              550 0.0110 \n 3 Fallow_Cotton      630 0.0126 \n 4 Millet_Cotton      316 0.00630\n 5 Pasture           7206 0.144  \n 6 Rocky_Savanna     8005 0.160  \n 7 Savanna           9172 0.183  \n 8 Savanna_Parkland  2699 0.0538 \n 9 Silviculture       423 0.00843\n10 Soy_Corn          4971 0.0991 \n11 Soy_Cotton        4124 0.0822 \n12 Soy_Fallow        2098 0.0418 \n\n\n\n\n\n# Load samples\nsamples_cerrado_mod13q1 = load_samples_dataset(\n    name = \"samples_cerrado_mod13q1\", \n    package = \"sitsdata\"\n)\n\n# Take only the NDVI and EVI bands\nsamples_cerrado_mod13q1_2bands = sits_select(\n    data = samples_cerrado_mod13q1, \n    bands = (\"NDVI\", \"EVI\"))\n\n# Show the summary of the samples\nsummary(samples_cerrado_mod13q1_2bands)\n\n               label  count      prop\n1     Dense_Woodland   9966  0.198684\n2              Dunes    550  0.010965\n3      Fallow_Cotton    630  0.012560\n4      Millet_Cotton    316  0.006300\n5            Pasture   7206  0.143660\n..               ...    ...       ...\n8   Savanna_Parkland   2699  0.053808\n9       Silviculture    423  0.008433\n10          Soy_Corn   4971  0.099103\n11        Soy_Cotton   4124  0.082217\n12        Soy_Fallow   2098  0.041826\n\n[12 rows x 3 columns]"
  },
  {
    "objectID": "ts_som.html#creating-the-som-map",
    "href": "ts_som.html#creating-the-som-map",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.3 Creating the SOM map",
    "text": "14.3 Creating the SOM map\nTo perform the SOM-based quality assessment, the first step is to run sits_som_map(), which uses the kohonen R package to compute a SOM grid [3], controlled by five parameters. The grid size is given by grid_xdim and grid_ydim. The starting learning rate is alpha, which decreases during the interactions. To measure the separation between samples, use distance (either “dtw” or “euclidean”). The number of iterations is set by rlen. When using sits_som_map() in machines which have multiprocessing support for the OpenMP protocol, setting the learning mode parameter mode to “patch” improves processing time. In Windows, please use “online”.\nWe suggest using the Dynamic Time Warping (“dtw”) metric as the distance measure. It is a technique used to measure the similarity between two temporal sequences that may vary in speed or timing [4]. The core idea of DTW is to find the optimal alignment between two sequences by allowing non-linear mapping of one sequence onto another. In time series analysis, DTW matches two series slightly out of sync. This property is useful in land use studies for matching time series of agricultural areas [5].\n\n\nR\nPython\n\n\n\n\n# Clustering time series using SOM\nsom_cluster &lt;- sits_som_map(samples_cerrado_mod13q1_2bands,\n    grid_xdim = 15,\n    grid_ydim = 15,\n    alpha = 1.0,\n    distance = \"dtw\",\n    rlen = 20\n)\n\n# Plot the SOM map\nplot(som_cluster)\n\n\n\n\n\nFigure 14.3: SOM map for the Cerrado samples.\n\n\n\n\n\n\n# Clustering time series using SOM\nsom_cluster = sits_som_map(samples_cerrado_mod13q1_2bands,\n    grid_xdim = 15,\n    grid_ydim = 15,\n    alpha = 1.0,\n    distance = \"dtw\",\n    rlen = 20\n)\n\n# Plot the SOM map\nplot(som_cluster)\n\n\n\n\n\nFigure 14.4: SOM map for the Cerrado samples.\n\n\n\n\n\n\nThe output of the sits_som_map() is a list with three elements: (a) data, the original set of time series with two additional columns for each time series: id_sample (the original id of each sample) and id_neuron (the id of the neuron to which it belongs); (b) labelled_neurons, a tibble with information on the neurons. For each neuron, it gives the prior and posterior probabilities of all labels which occur in the samples assigned to it; and (c) the SOM grid. To plot the SOM grid, use plot(). The neurons are labelled using majority voting.\nThe SOM grid shows that most classes are associated with neurons close to each other, although there are exceptions. Some Pasture neurons are far from the main cluster because the transition between open savanna and pasture areas is not always well defined and depends on climate and latitude. Also, the neurons associated with Soy_Fallow are dispersed in the map, indicating possible problems in distinguishing this class from the other agricultural classes. The SOM map can be used to remove outliers, as shown below."
  },
  {
    "objectID": "ts_som.html#measuring-confusion-between-labels-using-som",
    "href": "ts_som.html#measuring-confusion-between-labels-using-som",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.4 Measuring confusion between labels using SOM",
    "text": "14.4 Measuring confusion between labels using SOM\nThe second step in SOM-based quality assessment is understanding the confusion between labels. The function sits_som_evaluate_cluster() groups neurons by their majority label and produces a tibble. Neurons are grouped into clusters, and there will be as many clusters as there are labels. The results shows the percentage of samples of each label in each cluster. Ideally, all samples of each cluster would have the same label. In practice, cluster contain samples with different label. This information helps on measuring the confusion between samples.\n\n\nR\nPython\n\n\n\n\n# Produce a tibble with a summary of the mixed labels\nsom_eval &lt;- sits_som_evaluate_cluster(som_cluster)\n\n# Show the result\nsom_eval \n\n# A tibble: 66 × 4\n   id_cluster cluster        class          mixture_percentage\n        &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;                       &lt;dbl&gt;\n 1          1 Dense_Woodland Dense_Woodland            78.1   \n 2          1 Dense_Woodland Pasture                    5.56  \n 3          1 Dense_Woodland Rocky_Savanna              8.95  \n 4          1 Dense_Woodland Savanna                    3.88  \n 5          1 Dense_Woodland Silviculture               3.48  \n 6          1 Dense_Woodland Soy_Corn                   0.0249\n 7          2 Dunes          Dunes                    100     \n 8          3 Fallow_Cotton  Dense_Woodland             0.169 \n 9          3 Fallow_Cotton  Fallow_Cotton             49.5   \n10          3 Fallow_Cotton  Millet_Cotton             13.9   \n# ℹ 56 more rows\n\n\n\n\n\n# Produce a tibble with a summary of the mixed labels\nsom_eval = sits_som_evaluate_cluster(som_cluster)\n\n# Show the result\nsom_eval \n\n    id_cluster         cluster           class  mixture_percentage\n1            1  Dense_Woodland  Dense_Woodland           78.103950\n2            1  Dense_Woodland         Pasture            5.563410\n3            1  Dense_Woodland   Rocky_Savanna            8.948025\n4            1  Dense_Woodland         Savanna            3.875260\n5            1  Dense_Woodland    Silviculture            3.484407\n..         ...             ...             ...                 ...\n62          10      Soy_Fallow   Fallow_Cotton            1.002506\n63          10      Soy_Fallow         Pasture            3.258145\n64          10      Soy_Fallow         Savanna            0.250627\n65          10      Soy_Fallow        Soy_Corn            7.769424\n66          10      Soy_Fallow      Soy_Fallow           87.719298\n\n[66 rows x 4 columns]\n\n\n\n\n\nMany labels are associated with clusters where there are some samples with a different label. Such confusion between labels arises because sample labeling is subjective and can be biased. In many cases, interpreters use high-resolution data to identify samples. However, the actual images to be classified are captured by satellites with lower resolution. In our case study, a MOD13Q1 image has pixels with 250 m resolution. As such, the correspondence between labeled locations in high-resolution images and mid to low-resolution images is not direct. The confusion by sample label can be visualized in a bar plot using plot(), as shown below. The bar plot shows some confusion between the labels associated with the natural vegetation typical of the Brazilian Cerrado (Savanna, Savanna_Parkland, Rocky_Savanna). This mixture is due to the large variability of the natural vegetation of the Cerrado biome, which makes it difficult to draw sharp boundaries between classes. Some confusion is also visible between the agricultural classes. The Fallow_Cotton class is a particularly difficult one since many of the samples assigned to this class are confused with Soy_Cotton and Millet_Cotton.\n\n\nR\nPython\n\n\n\n\n# Plot the confusion between clusters\nplot(som_eval)\n\n\n\nFigure 14.5: Confusion between classes as measured by SOM.\n\n\n\n\n\n\n# Plot the confusion between clusters\nplot(som_eval)\n\n\n\nFigure 14.6: Confusion between classes as measured by SOM."
  },
  {
    "objectID": "ts_som.html#detecting-noisy-samples-using-som",
    "href": "ts_som.html#detecting-noisy-samples-using-som",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.5 Detecting noisy samples using SOM",
    "text": "14.5 Detecting noisy samples using SOM\nThe third step in the quality assessment uses the discrete probability distribution associated with each neuron, which is included in the labeled_neurons tibble produced by sits_som_map(). This approach associates probabilities with frequency of occurrence. More homogeneous neurons (those with one label has high frequency) are assumed to be composed of good quality samples. Heterogeneous neurons (those with two or more classes with significant frequencies) are likely to contain noisy samples. The algorithm computes two values for each sample:\n\nprior probability: the probability that the label assigned to the sample is correct, considering the frequency of samples in the same neuron. For example, if a neuron has 20 samples, of which 15 are labeled as Pasture and 5 as Forest, all samples labeled Forest are assigned a prior probability of 25%. This indicates that Forest samples in this neuron may not be of good quality.\nposterior probability: the probability that the label assigned to the sample is correct, considering the neighboring neurons. Take the case of the above-mentioned neuron whose samples labeled Pasture have a prior probability of 75%. What happens if all the neighboring neurons have Forest as a majority label? To answer this question, we use Bayesian inference to estimate if these samples are noisy based on the surrounding neurons [2].\n\nTo identify noisy samples, we take the result of the sits_som_map() function as the first argument to the function sits_som_clean_samples(). This function finds out which samples are noisy, which are clean, and which need to be further examined by the user. It requires the prior_threshold and posterior_threshold parameters according to the following rules:\n\nIf the prior probability of a sample is less than prior_threshold, the sample is assumed to be noisy and tagged as “remove”;\nIf the prior probability is greater or equal to prior_threshold and the posterior probability calculated by Bayesian inference is greater or equal to posterior_threshold, the sample is assumed not to be noisy and thus is tagged as “clean”;\nIf the prior probability is greater or equal to prior_threshold and the posterior probability is less than posterior_threshold, we have a situation when the sample is part of the majority level of those assigned to its neuron, but its label is not consistent with most of its neighbors. This is an anomalous condition and is tagged as “analyze”. Users are encouraged to inspect such samples to find out whether they are in fact noisy or not.\n\nThe default value for both prior_threshold and posterior_threshold is 60%. The sits_som_clean_samples() has an additional parameter (keep), which indicates which samples should be kept in the set based on their prior and posterior probabilities. The default for keep is c(\"clean\", \"analyze\"). As a result of the cleaning, about 900 samples have been considered to be noisy and thus to be possibly removed. We first show the complete distribution of the samples and later remove the noisy ones.\n\n\nR\nPython\n\n\n\n\nall_samples &lt;- sits_som_clean_samples(\n    som_map = som_cluster, \n    prior_threshold = 0.6,\n    posterior_threshold = 0.6,\n    keep = c(\"clean\", \"analyze\", \"remove\"))\n\n# Print the sample distribution based on evaluation\nplot(all_samples)\n\n\n\nFigure 14.7: Distribution of samples using som evaluation.\n\n\n\n\n\n\nall_samples = sits_som_clean_samples(\n    som_map = som_cluster, \n    prior_threshold = 0.6,\n    posterior_threshold = 0.6,\n    keep = (\"clean\", \"analyze\", \"remove\"))\n\n# Print the sample distribution based on evaluation\nplot(all_samples)\n\n\n\nFigure 14.8: Distribution of samples using som evaluation.\n\n\n\n\n\n\nWe now remove the noisy samples to improve the quality of the training set.\n\n\nR\nPython\n\n\n\n\nnew_samples &lt;- sits_som_clean_samples(\n    som_map = som_cluster, \n    prior_threshold = 0.6,\n    posterior_threshold = 0.6,\n    keep = c(\"clean\", \"analyze\"))\n\n# Print the new sample distribution\nsummary(new_samples)\n\n# A tibble: 9 × 3\n  label            count   prop\n  &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n1 Dense_Woodland    8519 0.220 \n2 Dunes              550 0.0142\n3 Pasture           5509 0.142 \n4 Rocky_Savanna     5508 0.142 \n5 Savanna           7651 0.197 \n6 Savanna_Parkland  1619 0.0418\n7 Soy_Corn          4595 0.119 \n8 Soy_Cotton        3515 0.0907\n9 Soy_Fallow        1309 0.0338\n\n\n\n\n\nnew_samples = sits_som_clean_samples(\n    som_map = som_cluster, \n    prior_threshold = 0.6,\n    posterior_threshold = 0.6,\n    keep = (\"clean\", \"analyze\"))\n\n# Print the new sample distribution\nsummary(new_samples)\n\n              label  count      prop\n1    Dense_Woodland   8519  0.219703\n2             Dunes    550  0.014184\n3           Pasture   5509  0.142076\n4     Rocky_Savanna   5508  0.142050\n5           Savanna   7651  0.197318\n6  Savanna_Parkland   1619  0.041754\n7          Soy_Corn   4595  0.118504\n8        Soy_Cotton   3515  0.090651\n9        Soy_Fallow   1309  0.033759\n\n\n\n\n\nAll samples of the classes which had the highest confusion with others(Fallow_Cotton, Silviculture, and Millet_Cotton) are marken as noisy been removed. Classes Fallow_Cotton and Millet_Cotton are not distinguishable from other crops. Samples of class Silviculture (planted forests) have removed since they have been confused with natural forests and woodlands in the SOM map. Further analysis includes calculating the SOM map and confusion matrix for the new set, as shown in the following example.\n\n\nR\nPython\n\n\n\n\n# Produce a new SOM map with the cleaned samples\nnew_cluster &lt;- sits_som_map(\n   data = new_samples,\n   grid_xdim = 15,\n   grid_ydim = 15,\n   alpha = 1.0,\n   rlen = 20,\n   distance = \"dtw\")\n\n# Evaluate the mixture in the new SOM clusters\nnew_cluster_mixture &lt;- sits_som_evaluate_cluster(new_cluster)\n\n# Plot the mixture information.\nplot(new_cluster_mixture)\n\n\n\n\n\n\n\n\n\n# Produce a new SOM map with the cleaned samples\nnew_cluster = sits_som_map(\n   data = new_samples,\n   grid_xdim = 15,\n   grid_ydim = 15,\n   alpha = 1.0,\n   rlen = 20,\n   distance = \"dtw\")\n   \n# Evaluate the mixture in the new SOM clusters\nnew_cluster_mixture = sits_som_evaluate_cluster(new_cluster)\n\n# Plot the mixture information.\nplot(new_cluster_mixture)\n\n\n\n\n\n\n\n\n\nAs expected, the new confusion map shows a significant improvement over the previous one. This result should be interpreted carefully since it may be due to different effects. The most direct interpretation is that Millet_Cotton and Silviculture cannot be easily separated from the other classes, given the current attributes (a time series of NDVI and EVI indices from MODIS images). In such situations, users should consider improving the number of samples from the less represented classes, including more MODIS bands, or working with higher resolution satellites. The results of the SOM method should be interpreted based on the users’ understanding of the ecosystems and agricultural practices of the study region.\nThe SOM-based analysis discards samples that can be confused with samples of other classes. After removing noisy samples or uncertain classes, the dataset obtains a better validation score since there is less confusion between classes. Users should analyse the results with care. Not all discarded samples are low-quality ones. Confusion between samples of different classes can result from inconsistent labeling or from the lack of capacity of satellite data to distinguish between chosen classes. When many samples are discarded, as in the current example, revising the whole classification schema is advisable. The aim of selecting training data should always be to match the reality on the ground to the power of remote sensing data to identify differences. No analysis procedure can replace actual user experience and knowledge of the study region."
  },
  {
    "objectID": "ts_som.html#summary",
    "href": "ts_som.html#summary",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.6 Summary",
    "text": "14.6 Summary\nIn this chapter, we discuss the use of SOM as a proven clustering method for removing noisy samples and those that cannot be easily distinguishable from other samples of other classes. Experience with sits indicates that using SOM is a good way to assess data quality. In the next section, we focus on a complementary method of removing sample imbalance."
  },
  {
    "objectID": "ts_som.html#references",
    "href": "ts_som.html#references",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nT. Kohonen, “The self-organizing map,” Proceedings of the IEEE, vol. 78, no. 9, pp. 1464–1480, 1990, doi: 10.1109/5.58325.\n\n\n[2] \nL. A. Santos, K. R. Ferreira, G. Camara, M. C. A. Picoli, and R. E. Simoes, “Quality control and class noise reduction of satellite image time series,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 177, pp. 75–88, 2021, doi: 10.1016/j.isprsjprs.2021.04.014.\n\n\n[3] \nR. Wehrens and J. Kruisselbrink, “Flexible Self-Organizing Maps in kohonen 3.0,” Journal of Statistical Software, vol. 87, no. 1, pp. 1–18, 2018, doi: 10.18637/jss.v087.i07.\n\n\n[4] \nD. Berndt and J. Clifford, “Using Dynamic Time Warping to Find Patterns in Time Series,” 1994, [Online]. Available: https://www.semanticscholar.org/paper/Using-Dynamic-Time-Warping-to-Find-Patterns-in-Time-Berndt-Clifford/1ac57524ba2d2a69c1bb6defed7352a06fd7050d.\n\n\n[5] \nV. Maus, G. Camara, R. Cartaxo, F. M. Ramos, A. Sanchez, and G. Q. Ribeiro, “Open boundary dynamic time warping for satellite image time series classification,” in 2015 IEEE international geoscience and remote sensing symposium (IGARSS), 2015, pp. 3349–3352, doi: 10.1109/IGARSS.2015.7326536."
  },
  {
    "objectID": "ts_balance.html#introduction",
    "href": "ts_balance.html#introduction",
    "title": "\n15  Reducing imbalances in training samples\n",
    "section": "\n15.1 Introduction",
    "text": "15.1 Introduction\nMany training samples for Earth observation data analysis are imbalanced. This situation arises when the distribution of samples associated with each label is uneven. Sample imbalance is an undesirable property of a training set since machine learning algorithms tend to be more accurate for classes with many samples. The instances belonging to the minority group are misclassified more often than those belonging to the majority group. Thus, reducing sample imbalance can positively affect classification accuracy [1]."
  },
  {
    "objectID": "ts_balance.html#dataset-using-in-this-chapter",
    "href": "ts_balance.html#dataset-using-in-this-chapter",
    "title": "\n15  Reducing imbalances in training samples\n",
    "section": "\n15.2 Dataset using in this chapter",
    "text": "15.2 Dataset using in this chapter\nThe examples of this chapter use samples_cerrado_mod13q1, a set of time series from the Cerrado region of Brazil. The data ranges from 2000 to 2017 and includes 50,160 samples divided into 12 classes (Dense_Woodland, Dunes, Fallow_Cotton, Millet_Cotton, Pasture, Rocky_Savanna, Savanna, Savanna_Parkland, Silviculture, Soy_Corn, Soy_Cotton, and Soy_Fallow). Each time series covers 12 months (23 data points) from MOD13Q1 product, and has 4 bands (EVI, NDVI, MIR, and NIR). We use bands NDVI and EVI for faster processing.\n\n\nR\nPython\n\n\n\n\n# Take only the NDVI and EVI bands\nsamples_cerrado_mod13q1_2bands &lt;- sits_select(\n    data = samples_cerrado_mod13q1, \n    bands = c(\"NDVI\", \"EVI\"))\n\n# Show the summary of the samples\nsummary(samples_cerrado_mod13q1_2bands)\n\n# A tibble: 12 × 3\n   label            count    prop\n   &lt;chr&gt;            &lt;int&gt;   &lt;dbl&gt;\n 1 Dense_Woodland    9966 0.199  \n 2 Dunes              550 0.0110 \n 3 Fallow_Cotton      630 0.0126 \n 4 Millet_Cotton      316 0.00630\n 5 Pasture           7206 0.144  \n 6 Rocky_Savanna     8005 0.160  \n 7 Savanna           9172 0.183  \n 8 Savanna_Parkland  2699 0.0538 \n 9 Silviculture       423 0.00843\n10 Soy_Corn          4971 0.0991 \n11 Soy_Cotton        4124 0.0822 \n12 Soy_Fallow        2098 0.0418 \n\n\n\n\n\n# Load samples\nsamples_cerrado_mod13q1 = load_samples_dataset(\n    name = \"samples_cerrado_mod13q1\",\n    package = \"sitsdata\"\n)\n\n# Take only the NDVI and EVI bands\nsamples_cerrado_mod13q1_2bands = sits_select(\n    data = samples_cerrado_mod13q1, \n    bands = (\"NDVI\", \"EVI\"))\n\n# Show the summary of the samples\nsummary(samples_cerrado_mod13q1_2bands)\n\n               label  count      prop\n1     Dense_Woodland   9966  0.198684\n2              Dunes    550  0.010965\n3      Fallow_Cotton    630  0.012560\n4      Millet_Cotton    316  0.006300\n5            Pasture   7206  0.143660\n..               ...    ...       ...\n8   Savanna_Parkland   2699  0.053808\n9       Silviculture    423  0.008433\n10          Soy_Corn   4971  0.099103\n11        Soy_Cotton   4124  0.082217\n12        Soy_Fallow   2098  0.041826\n\n[12 rows x 3 columns]\n\n\n\n\n\nThe Cerrado dataset is highly imbalanced. The three most frequent labels (Dense Woodland, Savanna, and Pasture) include 53% of all samples, while the three least frequent labels (Millet-Cotton, Silviculture, and Dunes) comprise only 2.5% of the dataset. This is a good dataset to investigate the impact of rebalancing."
  },
  {
    "objectID": "ts_balance.html#producing-a-balanced-training-set",
    "href": "ts_balance.html#producing-a-balanced-training-set",
    "title": "\n15  Reducing imbalances in training samples\n",
    "section": "\n15.3 Producing a balanced training set",
    "text": "15.3 Producing a balanced training set\nThe function sits_reduce_imbalance() deals with training set imbalance; it increases the number of samples of least frequent labels, and reduces the number of samples of most frequent labels. Oversampling requires generating synthetic samples. The package uses the SMOTE method that estimates new samples by considering the cluster formed by the nearest neighbors of each minority label. SMOTE takes two samples from this cluster and produces a new one by randomly interpolating them [2].\nTo perform undersampling, sits_reduce_imbalance() builds a SOM map for each majority label based on the required number of samples to be selected. Each dimension of the SOM is set to ceiling(sqrt(new_number_samples/4)) to allow a reasonable number of neurons to group similar samples. After calculating the SOM map, the algorithm extracts four samples per neuron to generate a reduced set of samples that approximates the variation of the original one.\nThe sits_reduce_imbalance() algorithm has two parameters: n_samples_over and n_samples_under. The first parameter indicates the minimum number of samples per class. All classes with samples less than its value are oversampled. The second parameter controls the maximum number of samples per class; all classes with more samples than its value are undersampled. The following example uses sits_reduce_imbalance() with the Cerrado samples used in the previous chapter. We generate a balanced dataset where all classes have a minimum of 1000 and and a maximum of 1500 samples.\n\n\nR\nPython\n\n\n\n\n# Reducing imbalances in the Cerrado dataset\nbalanced_samples &lt;- sits_reduce_imbalance(\n    samples = samples_cerrado_mod13q1_2bands,\n    n_samples_over = 1000,\n    n_samples_under = 1500,\n    multicores = 4)\n\n# Show summary of balanced samples\n# Some classes have more than 1500 samples due to the SOM map\n# Each label has between 10% and 6% of the full set\nsummary(balanced_samples)\n\n\n\n# A tibble: 12 × 3\n   label            count   prop\n   &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n 1 Dense_Woodland    1596 0.0974\n 2 Dunes             1000 0.0610\n 3 Fallow_Cotton     1000 0.0610\n 4 Millet_Cotton     1000 0.0610\n 5 Pasture           1592 0.0971\n 6 Rocky_Savanna     1476 0.0901\n 7 Savanna           1600 0.0976\n 8 Savanna_Parkland  1564 0.0954\n 9 Silviculture      1000 0.0610\n10 Soy_Corn          1588 0.0969\n11 Soy_Cotton        1568 0.0957\n12 Soy_Fallow        1404 0.0857\n\n\n\n\n\n# Reducing imbalances in the Cerrado dataset\nbalanced_samples = sits_reduce_imbalance(\n    samples = samples_cerrado_mod13q1_2bands,\n    n_samples_over = 1000,\n    n_samples_under = 1500,\n    multicores = 4)\n    \n# Show summary of balanced samples\n# Some classes have more than 1500 samples due to the SOM map\n# Each label has between 10% and 6% of the full set\nsummary(balanced_samples)\n\n\n\n               label  count      prop\n1     Dense_Woodland   1596  0.097388\n2              Dunes   1000  0.061020\n3      Fallow_Cotton   1000  0.061020\n4      Millet_Cotton   1000  0.061020\n5            Pasture   1592  0.097144\n..               ...    ...       ...\n8   Savanna_Parkland   1564  0.095436\n9       Silviculture   1000  0.061020\n10          Soy_Corn   1588  0.096900\n11        Soy_Cotton   1568  0.095680\n12        Soy_Fallow   1404  0.085672\n\n[12 rows x 3 columns]\n\n\n\n\n\nTo assess the impact of reducing imbalance, we use the SOM cluster technique described in the previous chapter. In synthesis, SOM builds clusters out of the training data. Ideally, each cluster would be composed by a samples of a single class. Mixed clusters indicate possible confusion between samples of different classes. We fist build a SOM using sits_som_map() and then assess the results with sits_som_evaluate_clusters().\n\n\nR\nPython\n\n\n\n\n# Clustering time series using SOM\nsom_cluster_bal &lt;- sits_som_map(\n    data = balanced_samples,\n    grid_xdim = 15,\n    grid_ydim = 15,\n    alpha = 1.0,\n    distance = \"dtw\",\n    rlen = 20)\n\n# Produce a tibble with a summary of the mixed labels\nsom_eval &lt;- sits_som_evaluate_cluster(som_cluster_bal)\n\n# Show the result\nplot(som_eval) \n\n\n\nWarning: Removed 38 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\nFigure 15.1: Confusion by cluster for the balanced dataset (source: authors).\n\n\n\n\n\n\n# Clustering time series using SOM\nsom_cluster_bal = sits_som_map(\n    data = balanced_samples,\n    grid_xdim = 15,\n    grid_ydim = 15,\n    alpha = 1.0,\n    distance = \"dtw\",\n    rlen = 20)\n    \n# Produce a tibble with a summary of the mixed labels\nsom_eval = sits_som_evaluate_cluster(som_cluster_bal)\n\n# Show the result\nplot(som_eval) \n\n\n\n\n\nFigure 15.2: Confusion by cluster for the balanced dataset (source: authors).\n\n\n\n\n\n\nAs shown in Figure 15.1, the balanced dataset shows less confusion per label than the unbalanced one. In this case, many classes that were confused with others in the original confusion map are now better represented. Reducing sample imbalance should be tried as an alternative to reducing the number of samples of the classes using SOM. In general, users should balance their training data for better performance."
  },
  {
    "objectID": "ts_balance.html#summary",
    "href": "ts_balance.html#summary",
    "title": "\n15  Reducing imbalances in training samples\n",
    "section": "\n15.4 Summary",
    "text": "15.4 Summary\nReducing imbalance is an important method to improve quality of training data. As a general rule, users should work with balanced sets, since experiments with sits show an improvement of classification accuracy in almost all cases."
  },
  {
    "objectID": "ts_balance.html#references",
    "href": "ts_balance.html#references",
    "title": "\n15  Reducing imbalances in training samples\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nJ. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with class imbalance,” Journal of Big Data, vol. 6, no. 1, p. 27, 2019, doi: 10.1186/s40537-019-0192-5.\n\n\n[2] \nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, no. 1, pp. 321–357, 2002."
  },
  {
    "objectID": "classification.html#considerations-on-model-choice",
    "href": "classification.html#considerations-on-model-choice",
    "title": "Machine learning for image time series",
    "section": "Considerations on model choice",
    "text": "Considerations on model choice\nThe results should not be taken as an indication of which method performs better. The most crucial factor for achieving a good result is the quality of the training data [2]. Experience shows that classification quality depends on the training samples and how well the model matches these samples. For examples of ML for classifying large areas, please see the papers by the authors [3]–[6].\nIn the specific case of satellite image time series, Russwurm et al. present a comparative study between seven deep neural networks for the classification of agricultural crops, using Random Forest as a baseline [7]. The data is composed of Sentinel-2 images over Britanny, France. Their results indicate a slight difference between the best model (attention-based transformer model) over TempCNN and random forest. Attention-based models obtain accuracy ranging from 80-81%, TempCNN gets 78-80%, and random forest obtains 78%. Based on this result and also on the authors’ experience, we make the following recommendations:\n\nRandom Forest provides a good baseline for image time series classification and should be included in users’ assessments.\nXGBoost is a worthy alternative to Random Forest. In principle, XGBoost is more sensitive to data variations at the cost of possible overfitting.\nTempCNN is a reliable model with reasonable training time, which is close to the state-of-the-art in deep learning classifiers for image time series.\nAttention-based models (TAE and LightTAE) can achieve the best overall performance with well-designed and balanced training sets and hyperparameter tuning.\n\nThe best means of improving classification performance is to provide an accurate and reliable training dataset. Accuracy improvements resulting from using deep learning methods instead of Random Forest or XGBoost are on the order of 3-5%, while gains when using good training data improve results by 10-30%. As a basic rule, make sure you have good quality samples before training and classification.\nIn the chapters that follow, we first present the sits machine learning algorithms, then show how to classify data cubes and how to smooth the classification results. We also discuss tuning of deep learning algorithms and methods for uncertainty assessment and ensemble prediction."
  },
  {
    "objectID": "classification.html#references",
    "href": "classification.html#references",
    "title": "Machine learning for image time series",
    "section": "References",
    "text": "References\n\n\n\n\n[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.\n\n\n[2] A. E. Maxwell, T. A. Warner, and F. Fang, “Implementation of machine-learning classification in remote sensing: An applied review,” International Journal of Remote Sensing, vol. 39, no. 9, pp. 2784–2817, 2018.\n\n\n[3] M. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007.\n\n\n[4] M. C. A. Picoli et al., “CBERS data cube: A powerful technology for mapping and monitoring Brazilian biomes.” in ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2020, vol. V–3–2020, pp. 533–539, doi: 10.5194/isprs-annals-V-3-2020-533-2020.\n\n\n[5] R. Simoes et al., “Land use and cover maps for Mato Grosso State in Brazil from 2001 to 2017,” Scientific Data, vol. 7, no. 1, p. 34, 2020, doi: 10.1038/s41597-020-0371-4.\n\n\n[6] K. R. Ferreira et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products,” Remote Sensing, vol. 12, no. 24, p. 4033, 2020, doi: 10.3390/rs12244033.\n\n\n[7] M. Rußwurm, C. Pelletier, M. Zollner, S. Lefèvre, and M. Körner, “BreizhCrops: A Time Series Dataset for Crop Type Mapping,” 2020, [Online]. Available: http://arxiv.org/abs/1905.11893."
  },
  {
    "objectID": "cl_machinelearning.html#data-used-in-this-chapter",
    "href": "cl_machinelearning.html#data-used-in-this-chapter",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.1 Data used in this chapter",
    "text": "16.1 Data used in this chapter\nThe following examples show how to train machine learning methods and apply them to classify a single time series. We use the set samples_matogrosso_mod13q1, containing time series samples from the Brazilian Mato Grosso state obtained from the MODIS MOD13Q1 product. It has 1,892 samples and nine classes (Cerrado, Forest, Pasture, Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet). Each time series covers 12 months (23 data points) with six bands (NDVI, EVI, BLUE, RED, NIR, MIR). The samples are arranged along an agricultural year, starting in September and ending in August. The dataset was used in the paper “Big Earth observation time series analysis for monitoring Brazilian agriculture” [1], and is available in the R package sitsdata."
  },
  {
    "objectID": "cl_machinelearning.html#common-interface-to-machine-learning-and-deep-learning-models",
    "href": "cl_machinelearning.html#common-interface-to-machine-learning-and-deep-learning-models",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.2 Common interface to machine learning and deep learning models",
    "text": "16.2 Common interface to machine learning and deep learning models\nThe sits_train() function provides a standard interface to all machine learning models. This function takes two mandatory parameters: the training data (samples) and the ML algorithm (ml_method). After the model is estimated, it can classify individual time series or data cubes with sits_classify(). In what follows, we show how to apply each method to classify a single time series. Then, in Chapter Image classification in data cubes, we discuss how to classify data cubes.\nSince sits is aimed at remote sensing users who are not machine learning experts, it provides a set of default values for all classification models. These settings have been chosen based on testing by the authors. Nevertheless, users can control all parameters for each model. Novice users can rely on the default values, while experienced ones can fine-tune model parameters to meet their needs. Model tuning is discussed at the end of this Chapter.\nWhen a set of time series organized as tibble is taken as input to the classifier, the result is the same tibble with one additional column (predicted), which contains the information on the labels assigned for each interval. The results can be shown in text format using the function sits_show_prediction() or graphically using plot()."
  },
  {
    "objectID": "cl_machinelearning.html#random-forest",
    "href": "cl_machinelearning.html#random-forest",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.3 Random Forest",
    "text": "16.3 Random Forest\nRandom Forest is a machine learning algorithm that uses an ensemble learning method for classification tasks. The algorithm consists of multiple decision trees, each trained on a different subset of the training data and with a different subset of features. To make a prediction, each decision tree in the forest independently classifies the input data. The final prediction is made based on the majority vote of all the decision trees. The randomness in the algorithm comes from the random subsets of data and features used to train each decision tree, which helps to reduce overfitting and improve the accuracy of the model. This classifier measures the importance of each feature in the classification task, which can be helpful in feature selection and data visualization. For an in-depth discussion of the robustness of Random Forest for satellite image time series classification, please see Pelletier et al [2].\n\n\n\n\nFigure 16.1: Random forest algorithm (source: Venkata Jagannath in Wikipedia).\n\n\n\nsits provides sits_rfor(), which uses the R randomForest package [3]; its main parameter is num_trees, which is the number of trees to grow with a default value of 100. The model can be visualized using plot().\n\n\nR\nPython\n\n\n\n\nset.seed(290356)\n\n# Train the Mato Grosso samples with Random Forest algorithm\nrfor_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_rfor(num_trees = 100))\n\n# Plot the most important variables of the model\nplot(rfor_model)\n\n\n\n\nr_set_seed(290356)\n\n# Load samples\nsamples_matogrosso_mod13q1 = load_samples_dataset(\n    name = \"samples_matogrosso_mod13q1\", \n    package = \"sitsdata\"\n)\n\n# Train the Mato Grosso samples with Random Forest algorithm\nrfor_model = sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_rfor(num_trees = 100))\n\n# Plot the most important variables of the model\nplot(rfor_model)\n\n\n\n\n\n\n\n\nFigure 16.2: Most important variables in Random Forest model.\n\n\n\nThe model plot shows the most important explanatory variables, which are the NIR (near infrared) band on date 17 (2007-05-25) and the MIR (middle infrared) band on date 22 (2007-08-13). The NIR value at the end of May captures the growth of the second crop for double cropping classes. Values of the MIR band at the end of the period (late July to late August) capture bare soil signatures to distinguish between agricultural and natural classes. This corresponds to summertime when the ground is drier after harvesting crops.\n\n\nR\nPython\n\n\n\n\n# Classify using Random Forest model and plot the result\npoint_class &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model  = rfor_model)\n\nplot(point_class, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n# Load samples\npoint_mt_mod13q1 = load_samples_dataset(\n    name = \"point_mt_mod13q1\", \n    package = \"sitsdata\"\n)\n\n# Classify using Random Forest model and plot the result\npoint_class = sits_classify(\n    data = point_mt_mod13q1, \n    ml_model  = rfor_model)\n\nplot(point_class, bands = (\"NDVI\", \"EVI\"))\n\n\n\n\n\n\n\n\nFigure 16.3: Classification of time series using Random Forest.\n\n\n\nThe result shows that the area started as a forest in 2000, was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2009 onwards. This behavior is consistent with expert evaluation of land change process in this region of Amazonia.\nRandom Forest is robust to outliers and can deal with irrelevant inputs [4]. The method tends to overemphasize some variables because its performance tends to stabilize after part of the trees is grown [4]. In cases where abrupt change occurs, such as deforestation mapping, Random Forest (if properly trained) will emphasize the temporal instances and bands that capture such quick change. Before using Random Forest, it is recommended that users balance their training samples as explained in “Reducing imbalances in training samples”"
  },
  {
    "objectID": "cl_machinelearning.html#support-vector-machine",
    "href": "cl_machinelearning.html#support-vector-machine",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.4 Support vector machine",
    "text": "16.4 Support vector machine\nThe support vector machine (SVM) classifier is a generalization of a linear classifier that finds an optimal separation hyperplane which minimizes misclassification [5]. Since a set of samples with \\(n\\) features defines an n-dimensional feature space, hyperplanes are linear \\({(n-1)}\\)-dimensional boundaries that define linear partitions in that space. If the classes are linearly separable on the feature space, there will be an optimal solution defined by the maximal margin hyperplane, which is the separating hyperplane that is farthest from the training observations [6]. The maximal margin is computed as the smallest distance from the observations to the hyperplane. The solution for the hyperplane coefficients depends only on the samples that define the maximum margin criteria, the so-called support vectors.\n\n\n\n\nFigure 16.4: Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. (source: Larhmam in Wikipedia).\n\n\n\nFor data that is not linearly separable, SVM provides kernel functions that map the original feature space into a higher dimensional space. Despite having a linear boundary on the enlarged feature space, the new classification model generally translates its hyperplane to a nonlinear boundary in the original attribute space. Kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space; thus, they improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been applied to classify remote sensing data [7].\nIn sits, SVM is implemented as a wrapper of e1071 R package that uses the LIBSVM implementation [8]. The sits package adopts the one-against-one method for multiclass classification. For a \\(q\\) class problem, this method creates \\({q(q-1)/2}\\) SVM binary models, one for each class pair combination, testing any unknown input vectors throughout all those models. A voting scheme computes the overall result.\nThe example below shows how to apply SVM to classify time series using default values. The main parameters are kernel, which controls whether to use a nonlinear transformation (default is radial), cost, which measures the punishment for wrongly-classified samples (default is 10), and cross, which sets the value of the k-fold cross validation (default is 10).\n\n\nR\nPython\n\n\n\n\nset.seed(290356)\n\n# Train an SVM model\nsvm_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_svm())\n\n# Classify using the SVM model and plot the result\npoint_class &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model = svm_model)\n\n# Plot the result\nplot(point_class, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\nr_set_seed(290356)\n\n# Train an SVM model\nsvm_model = sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_svm())\n\n# Classify using the SVM model and plot the result\npoint_class = sits_classify(\n    data = point_mt_mod13q1, \n    ml_model = svm_model)\n\n# Plot the result\nplot(point_class, bands = (\"NDVI\", \"EVI\"))\n\n\n\n\n\n\n\n\nFigure 16.5: Classification of time series using SVM.\n\n\n\nThe SVM classifier is less stable and less robust to outliers than the Random Forest method. In this example, it tends to misclassify some of the data. In 2008, it is likely that the correct land class was still Pasture rather than Soy_Millet as produced by the algorithm, while the Soy_Cotton class in 2012 is also inconsistent with the previous and latter classification of Soy_Corn."
  },
  {
    "objectID": "cl_machinelearning.html#extreme-gradient-boosting",
    "href": "cl_machinelearning.html#extreme-gradient-boosting",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.5 Extreme gradient boosting",
    "text": "16.5 Extreme gradient boosting\nXGBoost (eXtreme Gradient Boosting) [9] is an implementation of gradient boosted decision trees designed for speed and performance. It is an ensemble learning method, meaning it combines the predictions from multiple models to produce a final prediction. XGBoost builds trees one at a time, where each new tree helps to correct errors made by previously trained tree. Each tree builds a new model to correct the errors made by previous models. Using gradient descent, the algorithm iteratively adjusts the predictions of each tree by focusing on instances where previous trees made errors. Models are added sequentially until no further improvements can be made.\nAlthough Random Forest and boosting use trees for classification, there are significant differences. While Random Forest builds multiple decision trees in parallel and merges them together later, XGBoost builds trees one at a time. In XGBoost, each new tree helps to correct errors made by previously trained tree. XGBoost is often preferred for its speed and performance, particularly on large datasets and is well-suited for problems where precision is paramount. Random Forest, on the other hand, is simpler to implement, more interpretable, and can be more robust to overfitting, making it a good choice for general-purpose applications.\n\n\n\n\nFigure 16.6: Flow chart of XGBoost algorithm (source: Guo et al., Applied Sciences, 2020).\n\n\n\nThe boosting method starts from a weak predictor and then improves performance sequentially by fitting a better model at each iteration. It fits a simple classifier to the training data and uses the residuals of the fit to build a predictor. Typically, the base classifier is a regression tree. Although random forest and boosting use trees for classification, there are significant differences. The performance of Random Forest generally increases with the number of trees until it becomes stable. Boosting trees apply finer divisions over previous results to improve performance [4]. Some recent papers show that it outperforms Random Forest for remote sensing image classification [10]. However, this result is not generalizable since the quality of the training dataset controls actual performance.\nIn sits, the XGBoost method is implemented by the sits_xbgoost() function, based on XGBoost R package, and has five hyperparameters that require tuning. The sits_xbgoost() function takes the user choices as input to a cross-validation to determine suitable values for the predictor.\nThe learning rate eta varies from 0.0 to 1.0 and should be kept small (default is 0.3) to avoid overfitting. The minimum loss value gamma specifies the minimum reduction required to make a split. Its default is 0; increasing it makes the algorithm more conservative. The max_depth value controls the maximum depth of the trees. Increasing this value will make the model more complex and likely to overfit (default is 6). The subsample parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The nrounds parameter controls the maximum number of boosting interactions; its default is 100, which has proven to be enough in most cases. To follow the convergence of the algorithm, users can turn the verbose parameter on. In general, the results using the extreme gradient boosting algorithm are similar to the Random Forest method.\n\n\nR\nPython\n\n\n\n\nset.seed(290356)\n\n# Train using  XGBoost\nxgb_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_xgboost(verbose = FALSE))\n\n# Classify using SVM model and plot the result\npoint_class_xgb &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model = xgb_model)\n\n# View classification\nplot(point_class_xgb, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\nr_set_seed(290356)\n\n# Train using  XGBoost\nxgb_model = sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_xgboost(verbose = False))\n\n# Classify using SVM model and plot the result\npoint_class_xgb = sits_classify(\n    data = point_mt_mod13q1, \n    ml_model = xgb_model)\n\n# View classification\nplot(point_class_xgb, bands = (\"NDVI\", \"EVI\"))\n\n\n\n\n\n\n\n\nFigure 16.7: Classification of time series using XGBoost."
  },
  {
    "objectID": "cl_machinelearning.html#deep-learning-using-multilayer-perceptron",
    "href": "cl_machinelearning.html#deep-learning-using-multilayer-perceptron",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.6 Deep learning using multilayer perceptron",
    "text": "16.6 Deep learning using multilayer perceptron\nTo support deep learning methods, sits uses the torch R package, which takes the Facebook torch C++ library as a back-end. Machine learning algorithms that use the R torch package are similar to those developed using PyTorch. The simplest deep learning method is multilayer perceptron (MLP), which are feedforward artificial neural networks. An MLP consists of three kinds of nodes: an input layer, a set of hidden layers, and an output layer. The input layer has the same dimension as the number of features in the dataset. The hidden layers attempt to approximate the best classification function. The output layer decides which class should be assigned to the input. In an MLP, all inputs are treated equally at first; based on iterative matching of training and test data, the backpropagation technique feeds information back to the initial layers to identify the most suitable combination of inputs that produces the best output.\nIn sits, MLP models can be built using sits_mlp(). Since there is no established model for generic classification of satellite image time series, designing MLP models requires parameter customization. The most important decisions are the number of layers in the model and the number of neurons per layer. These values are set by the layers parameter, which is a list of integer values. The size of the list is the number of layers, and each element indicates the number of nodes per layer.\nThe choice of the number of layers depends on the inherent separability of the dataset to be classified. For datasets where the classes have different signatures, a shallow model (with three layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. Models with many hidden layers may take a long time to train and may not converge. We suggest to start with three layers and test different options for the number of neurons per layer before increasing the number of layers. In our experience, using three to five layers is a reasonable compromise if the training data has a good quality. Further increase in the number of layers will not improve the model.\nMLP models also need to include the activation function. The activation function of a node defines the output of that node given an input or set of inputs. Following standard practices [11], we use the relu activation function.\nThe optimization method (optimizer) represents the gradient descent algorithm to be used. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function [12]. Since gradient descent plays a key role in deep learning model fitting, developing optimizers is an important topic of research [13]. Many optimizers have been proposed in the literature, and recent results are reviewed by Schmidt et al. [14]. The Adamw optimizer provides a good baseline and reliable performance for general deep learning applications [15]. By default, all deep learning algorithms in sits use Adamw.\nAnother relevant parameter is the list of dropout rates (dropout). Dropout is a technique for randomly dropping units from the neural network during training [16]. By randomly discarding some neurons, dropout reduces overfitting. Since a cascade of neural nets aims to improve learning as more data is acquired, discarding some neurons may seem like a waste of resources. In practice, dropout prevents an early convergence to a local minimum [11]. We suggest users experiment with different dropout rates, starting from small values (10-30%) and increasing as required.\nThe following example shows how to use sits_mlp(). The default parameters have been chosen based on a modified version of [17], which proposes using multilayer perceptron as a baseline for time series classification. These parameters are: (a) Three layers with 512 neurons each, specified by the parameter layers; (b) Using the “relu” activation function; (c) dropout rates of 40%, 30%, and 20% for the layers; (d) the “optimizer_adamw” as optimizer (default value); (e) a number of training steps (epochs) of 100; (f) a batch_size of 64, which indicates how many time series are used for input at a given step; and (g) a validation percentage of 20%, which means 20% of the samples will be randomly set aside for validation.\nTo simplify the output, the verbose option has been turned off. After the model has been generated, we plot its training history.\n\n\nR\nPython\n\n\n\n\nset.seed(290356)\n\n# Train using an MLP model\n# This is an example of how to set parameters\n# First-time users should test default options first\nmlp_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_mlp(\n        optimizer        = torch::optim_adamw, \n        layers           = c(512, 512, 512),\n        dropout_rates    = c(0.40, 0.30, 0.20),\n        epochs           = 80,\n        batch_size       = 64,\n        verbose          = FALSE,\n        validation_split = 0.2))\n\n# Show training evolution\nplot(mlp_model)\n\n\n\n\nr_set_seed(290356)\n\n# Train using an MLP model\n# This is an example of how to set parameters\n# First-time users should test default options first\nmlp_model = sits_train(\n    samples = samples_matogrosso_mod13q1,\n    ml_method = sits_mlp(\n        optimizer        = \"torch::optim_adamw\",\n        layers           = (512, 512, 512),\n        dropout_rates    = (0.40, 0.30, 0.20),\n        epochs           = 80,\n        batch_size       = 64,\n        verbose          = False,\n        validation_split = 0.2))\n\n# Show training evolution\nplot(mlp_model)\n\n\n\n\n\n\n\n\nFigure 16.8: Evolution of training accuracy of MLP model.\n\n\n\nThen, we classify a 16-year time series using the multilayer perceptron model.\n\n\nR\nPython\n\n\n\n\n# Classify using MLP model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(mlp_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n# Classify using MLP model\npoint_class = sits_classify(point_mt_mod13q1, mlp_model)\n\n# Plot\nplot(\n    point_class, bands = (\"NDVI\", \"EVI\")\n)\n\n\n\n\n\n\n\n\nFigure 16.9: Classification of time series using MLP.\n\n\n\nIn theory, multilayer perceptron model can capture more subtle changes than Random Forest and XGBoost. In this specific case, the result is similar to theirs. Although the model mixes the Soy_Corn and Soy_Millet classes, the distinction between their temporal signatures is quite subtle. Also it suggests the need to improve the number of samples. In this example, the MLP model shows an increase in sensitivity compared to previous models. We recommend to compare different configurations since the MLP model is sensitive to changes in its parameters."
  },
  {
    "objectID": "cl_machinelearning.html#temporal-convolutional-neural-network-tempcnn",
    "href": "cl_machinelearning.html#temporal-convolutional-neural-network-tempcnn",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.7 Temporal Convolutional Neural Network (TempCNN)",
    "text": "16.7 Temporal Convolutional Neural Network (TempCNN)\nConvolutional neural networks (CNN) are deep learning methods that apply convolution filters (sliding windows) to the input data sequentially. The Temporal Convolutional Neural Network (TempCNN) is a neural network architecture specifically designed to process sequential data such as time series. In the case of time series, a 1D CNN applies a moving temporal window to the time series to produce another time series as the result of the convolution.\nThe TempCNN architecture for satellite image time series classification is proposed by Pelletier et al. [18]. It has three 1D convolutional layers and a final softmax layer for classification. The authors combine different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalization. In the TempCNN reference paper [18], the authors favourably compare their model with the Recurrent Neural Network proposed by Russwurm and Körner [19]. Figure 16.10 shows the architecture of the TempCNN model. TempCNN applies one-dimensional convolutions on the input sequence to capture temporal dependencies, allowing the network to learn long-term dependencies in the input sequence. Each layer of the model captures temporal dependencies at a different scale. Due to its multi-scale approach, TempCNN can capture complex temporal patterns in the data and produce accurate predictions.\n\n\n\n\nFigure 16.10: Structure of tempCNN architecture (source: [18]).\n\n\n\nThe function sits_tempcnn() implements the model. The first parameter is the optimizer used in the backpropagation phase for gradient descent. The default is adamw which is considered as a stable and reliable optimization function. The parameter cnn_layers controls the number of 1D-CNN layers and the size of the filters applied at each layer; the default values are three CNNs with 128 units. The parameter cnn_kernels indicates the size of the convolution kernels; the default is kernels of size 7. Activation for all 1D-CNN layers uses the “relu” function. The dropout rates for each 1D-CNN layer are controlled individually by the parameter cnn_dropout_rates. The validation_split controls the size of the test set relative to the full dataset. We recommend setting aside at least 20% of the samples for validation.\n\n\nR\nPython\n\n\n\n\nset.seed(290356)\nlibrary(torch)\n\n# Train using tempCNN\ntempcnn_model &lt;- sits_train(\n    samples_matogrosso_mod13q1, \n    sits_tempcnn(\n        optimizer            = torch::optim_adamw,\n        cnn_layers           = c(256, 256, 256),\n        cnn_kernels          = c(7, 7, 7),\n        cnn_dropout_rates    = c(0.2, 0.2, 0.2),\n        epochs               = 80,\n        batch_size           = 64,\n        validation_split     = 0.2,\n        verbose              = TRUE))\n\n# Show training evolution\nplot(tempcnn_model)\n\n\n\n\nr_set_seed(290356)\n\n# Train using tempCNN\ntempcnn_model = sits_train(\n    samples_matogrosso_mod13q1, \n    sits_tempcnn(\n        optimizer            = \"torch::optim_adamw\",\n        cnn_layers           = (256, 256, 256),\n        cnn_kernels          = (7, 7, 7),\n        cnn_dropout_rates    = (0.2, 0.2, 0.2),\n        epochs               = 80,\n        batch_size           = 64,\n        validation_split     = 0.2,\n        verbose              = False))\n\n# Show training evolution\nplot(tempcnn_model)\n\n\n\n\n\n\n\n\nFigure 16.11: Training evolution of TempCNN model.\n\n\n\nUsing the TempCNN model, we classify a 16-year time series.\n\n\nR\nPython\n\n\n\n\n# Classify using TempCNN model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(tempcnn_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n# Classify using TempCNN model\npoint_class = sits_classify(point_mt_mod13q1, tempcnn_model)\n\n# Plot\nplot(\n    point_class, bands = (\"NDVI\", \"EVI\")\n)\n\n\n\n\n\n\n\n\nFigure 16.12: Classification of time series using TempCNN\n\n\n\nThe result has important differences from the previous ones. The TempCNN model indicates the Soy_Cotton class as the most likely one in 2004. While this result is possibly wrong, it shows that the time series for 2004 is different from those of Forest and Pasture classes. One possible explanation is that there was forest degradation in 2004, leading to a signature that is a mix of forest and bare soil. In this case, including forest degradation samples could improve the training data. In our experience, TempCNN models are a reliable way of classifying image time series [20]. Recent work which compares different models also provides evidence that TempCNN models have satisfactory behavior, especially in the case of crop classes [21]."
  },
  {
    "objectID": "cl_machinelearning.html#attention-based-models",
    "href": "cl_machinelearning.html#attention-based-models",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.9 Attention-based models",
    "text": "16.9 Attention-based models\nAttention-based deep learning models are a class of models that use a mechanism inspired by human attention to focus on specific parts of input during processing. These models have been shown to be effective for various tasks such as machine translation, image captioning, and speech recognition.\nThe basic idea behind attention-based models is to allow the model to selectively focus on different input parts at different times. This can be done by introducing a mechanism that assigns weights to each element of the input, indicating the relative importance of that element to the current processing step. The model can then use them to compute a weighted sum of the input. The results capture the model’s attention on specific parts of the input.\nAttention-based models have become one of the most used deep learning architectures for problems that involve sequential data inputs, e.g., text recognition and automatic translation. The general idea is that not all inputs are alike in applications such as language translation. Consider the English sentence “Look at all the lonely people”. A sound translation system needs to relate the words “look” and “people” as the key parts of this sentence to ensure such link is captured in the translation. A specific type of attention models, called transformers, enables the recognition of such complex relationships between input and output sequences [25].\nThe basic structure of transformers is the same as other neural network algorithms. They have an encoder that transforms textual input values into numerical vectors and a decoder that processes these vectors to provide suitable answers. The difference is how the values are handled internally. The two main differences between transformer models and other algorithms are positional encoding and self-attention. Positional encoding assigns an index to each input value, ensuring that the relative locations of the inputs are maintained throughout the learning and processing phases. Self-attention compares every word in a sentence to every other word in the same sentence, including itself. In this way, it learns contextual information about the relation between the words. This conception has been validated in large language models such as BERT [26] and GPT-3 [27].\nThe application of attention-based models for satellite image time series analysis is proposed by Garnot et al. [28] and Russwurm and Körner [21]. A self-attention network can learn to focus on specific time steps and image features most relevant for distinguishing between different classes. The algorithm tries to identify which combination of individual temporal observations is most relevant to identify each class. For example, crop identification will use observations that capture the onset of the growing season, the date of maximum growth, and the end of the growing season. In the case of deforestation, the algorithm tries to identify the dates when the forest is being cut. Attention-based models are a means to identify events that characterize each land class.\nThe first model proposed by Garnot et al. is a full transformer-based model [28]. Considering that image time series classification is easier than natural language processing, Garnot et al. also propose a simplified version of the full transformer model [29]. This simpler model uses a reduced way to compute the attention matrix, reducing time for training and classification without loss of quality of the result.\nIn sits, the full transformer-based model proposed by Garnot et al. [28] is implemented using sits_tae(). The default parameters are those proposed by the authors. The default optimizer is optim_adamw, as also used in the sits_tempcnn() function.\n\n\nR\nPython\n\n\n\n\n# Train a machine learning model using TAE\ntae_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_tae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = torch::optim_adamw,\n                          validation_split     = 0.2,\n                          verbose              = FALSE))\n\n# Show training evolution\nplot(tae_model)\n\n\n\n\n# Train a machine learning model using TAE\ntae_model = sits_train(samples_matogrosso_mod13q1, \n                       sits_tae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = \"torch::optim_adamw\",\n                          validation_split     = 0.2,\n                          verbose              = False))\n\n# Show training evolution\nplot(tae_model)\n\n\n\n\n\n\n\n\nFigure 16.16: Training evolution of Temporal Self-Attention model.\n\n\n\nThen, we classify a 16-year time series using the TAE model.\n\n\nR\nPython\n\n\n\n\n# Classify using DL model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(tae_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n# Classify using DL model\npoint_class = sits_classify(point_mt_mod13q1, tae_model)\n\n# Plot\nplot(\n    point_class, bands = (\"NDVI\", \"EVI\")\n)\n\n\n\n\n\n\n\n\nFigure 16.17: Classification of time series using TAE.\n\n\n\nGarnot and co-authors also proposed the Lightweight Temporal Self-Attention Encoder (LTAE) [29], which the authors claim can achieve high classification accuracy with fewer parameters compared to other neural network models. It is a good choice for applications where computational resources are limited. The sits_lighttae() function implements this algorithm. The most important parameter to be set is the learning rate lr. Values ranging from 0.001 to 0.005 should produce good results. See also the section below on model tuning.\n\n\nR\nPython\n\n\n\n\n# Train a machine learning model using TAE\nltae_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_lighttae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = torch::optim_adamw,\n                          opt_hparams = list(lr = 0.001),\n                          validation_split     = 0.2))\n\n# Show training evolution\nplot(ltae_model)\n\n\n\n\n# Train a machine learning model using TAE\nltae_model = sits_train(samples_matogrosso_mod13q1, \n                       sits_lighttae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = \"torch::optim_adamw\",\n                          opt_hparams = dict(lr = 0.001),\n                          validation_split      = 0.2))\n\n# Show training evolution\nplot(ltae_model)\n\n\n\n\n\n\n\n\nFigure 16.18: Training evolution of Lightweight Temporal Self-Attention model.\n\n\n\nThen, we classify a 16-year time series using the LightTAE model.\n\n\nR\nPython\n\n\n\n\n# Classify using DL model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(ltae_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n# Classify using DL model\npoint_class = sits_classify(point_mt_mod13q1, ltae_model)\n\n# Plot\nplot(\n    point_class, bands = (\"NDVI\", \"EVI\")\n)\n\n\n\n\n\n\n\n\nFigure 16.19: Classification of time series using LightTAE.\n\n\n\nThe behaviour of both sits_tae() and sits_lighttae() is similar to that of sits_tempcnn(). It points out the possible need for more classes and training data to better represent the transition period between 2004 and 2010. One possibility is that the training data associated with the Pasture class is only consistent with the time series between the years 2005 to 2008. However, the transition from Forest to Pasture in 2004 and from Pasture to Agriculture in 2009-2010 is subject to uncertainty since the classifiers do not agree on the resulting classes. In general, deep learning temporal-aware models are more sensitive to class variability than Random Forest and extreme gradient boosters."
  },
  {
    "objectID": "cl_machinelearning.html#summary",
    "href": "cl_machinelearning.html#summary",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.10 Summary",
    "text": "16.10 Summary\nIn this chapter, we present a basic description of the machine learning algorithms available in sits. The basic distinction is between time-sensitive algorithms such as LightTAE and TempCNN and those who do not consider temporal order of values, such as Random Forest. In practice, we suggest that users take Random Forest as their baseline and then use LightTAE or TempCNN to try to improve classification accuracy."
  },
  {
    "objectID": "cl_machinelearning.html#references",
    "href": "cl_machinelearning.html#references",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007.\n\n\n[2] \nC. Pelletier, S. Valero, J. Inglada, N. Champion, and G. Dedieu, “Assessing the robustness of Random Forests to map land cover with high resolution satellite image time series over large areas,” Remote Sensing of Environment, vol. 187, pp. 156–168, 2016, doi: 10.1016/j.rse.2016.10.010.\n\n\n[3] \nJ. S. Wright et al., “Rainforest-initiated wet season onset over the southern Amazon,” Proceedings of the National Academy of Sciences, 2017, doi: 10.1073/pnas.1621516114.\n\n\n[4] \nT. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Data Mining, Inference, and Prediction. New York: Springer, 2009.\n\n\n[5] \nC. Cortes and V. Vapnik, “Support-vector networks,” Machine learning, vol. 20, no. 3, pp. 273–297, 1995.\n\n\n[6] \nG. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning: With Applications in R. New York, EUA: Springer, 2013.\n\n\n[7] \nG. Mountrakis, J. Im, and C. Ogole, “Support vector machines in remote sensing: A review,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 66, no. 3, pp. 247–259, 2011.\n\n\n[8] \nC.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector machines,” ACM transactions on intelligent systems and technology (TIST), vol. 2, no. 3, p. 27, 2011.\n\n\n[9] \nT. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 785–794, doi: 10.1145/2939672.2939785.\n\n\n[10] \nH. Jafarzadeh, M. Mahdianpari, E. Gill, F. Mohammadimanesh, and S. Homayouni, “Bagging and Boosting Ensemble Classifiers for Classification of Multispectral, Hyperspectral and PolSAR Data: A Comparative Evaluation,” Remote Sensing, vol. 13, no. 21, p. 4405, 2021, doi: 10.3390/rs13214405.\n\n\n[11] \nI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.\n\n\n[12] \nS. Ruder, “An overview of gradient descent optimization algorithms,” CoRR, vol. abs/1609.04747, 2016, [Online]. Available: http://arxiv.org/abs/1609.04747.\n\n\n[13] \nL. Bottou, F. E. Curtis, and J. Nocedal, “Optimization Methods for Large-Scale Machine Learning,” SIAM Review, vol. 60, no. 2, pp. 223–311, 2018, doi: 10.1137/16M1080173.\n\n\n[14] \nR. M. Schmidt, F. Schneider, and P. Hennig, “Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers,” in Proceedings of the 38th International Conference on Machine Learning, 2021, pp. 9367–9376, [Online]. Available: https://proceedings.mlr.press/v139/schmidt21a.html.\n\n\n[15] \nD. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization.” arXiv, 2017, doi: 10.48550/arXiv.1412.6980.\n\n\n[16] \nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n\n\n[17] \nZ. Wang, W. Yan, and T. Oates, “Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline,” 2017.\n\n\n[18] \nC. Pelletier, G. I. Webb, and F. Petitjean, “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,” Remote Sensing, vol. 11, no. 5, 2019.\n\n\n[19] \nM. Russwurm and M. Korner, “Multi-temporal land cover classification with sequential recurrent encoders,” ISPRS International Journal of Geo-Information, vol. 7, no. 4, p. 129, 2018.\n\n\n[20] \nR. Simoes et al., “Satellite Image Time Series Analysis for Big Earth Observation Data,” Remote Sensing, vol. 13, no. 13, p. 2428, 2021, doi: 10.3390/rs13132428.\n\n\n[21] \nM. Rußwurm, C. Pelletier, M. Zollner, S. Lefèvre, and M. Körner, “BreizhCrops: A Time Series Dataset for Crop Type Mapping,” 2020, [Online]. Available: http://arxiv.org/abs/1905.11893.\n\n\n[22] \nK. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778, doi: 10.1109/CVPR.2016.90.\n\n\n[23] \nS. Hochreiter, “The vanishing gradient problem during learning recurrent neural nets and problem solutions,” International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 6, no. 2, pp. 107–116, 1998, doi: 10.1142/S0218488598000094.\n\n\n[24] \nH. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Deep learning for time series classification: A review,” Data Mining and Knowledge Discovery, vol. 33, no. 4, pp. 917–963, 2019.\n\n\n[25] \nA. Vaswani et al., “Attention is All you Need,” in Advances in Neural Information Processing Systems, 2017, vol. 30, [Online]. Available: https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\n\n[26] \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” arXiv, 2019, doi: 10.48550/arXiv.1810.04805.\n\n\n[27] \nT. B. Brown et al., “Language Models are Few-Shot Learners.” arXiv, 2020, doi: 10.48550/arXiv.2005.14165.\n\n\n[28] \nV. Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12322–12331, doi: 10.1109/CVPR42600.2020.01234.\n\n\n[29] \nV. S. F. Garnot and L. Landrieu, “Lightweight Temporal Self-attention for Classifying Satellite Images Time Series,” in Advanced Analytics and Learning on Temporal Data, 2020, pp. 171–181."
  },
  {
    "objectID": "cl_tuning.html#introduction",
    "href": "cl_tuning.html#introduction",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.1 Introduction",
    "text": "17.1 Introduction\nModel tuning is the process of selecting the best set of hyperparameters for a specific application. When using deep learning models for image classification, it is a highly recommended step to enable a better fit of the algorithm to the training data. Hyperparameters are parameters of the model that are not learned during training but instead are set prior to training and affect the behavior of the model during training. Examples include the learning rate, batch size, number of epochs, number of hidden layers, number of neurons in each layer, activation functions, regularization parameters, and optimization algorithms.\nDeep learning model tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model on a given task. This is done by training and evaluating the model with different sets of hyperparameters to select the set that gives the best performance.\nDeep learning algorithms try to find the optimal point representing the best value of the prediction function that, given an input \\(X\\) of data points, predicts the result \\(Y\\). In our case, \\(X\\) is a multidimensional time series, and \\(Y\\) is a vector of probabilities for the possible output classes. For complex situations, the best prediction function is time-consuming to estimate. For this reason, deep learning methods rely on gradient descent methods to speed up predictions and converge faster than an exhaustive search [1]. All gradient descent methods use an optimization algorithm adjusted with hyperparameters such as the learning and regularization rates [2]. The learning rate controls the numerical step of the gradient descent function, and the regularization rate controls model overfitting. Adjusting these values to an optimal setting requires using model tuning methods."
  },
  {
    "objectID": "cl_tuning.html#how-sits-performs-model-tuning",
    "href": "cl_tuning.html#how-sits-performs-model-tuning",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.2 How SITS performs model tuning",
    "text": "17.2 How SITS performs model tuning\nTo reduce the learning curve, sits provides default values for all machine learning and deep learning methods, ensuring a reasonable baseline performance. However, refining model hyperparameters might be necessary, especially for more complex models such as sits_lighttae() or sits_tempcnn(). To that end, the package provides the sits_tuning() function.\nThe most straightforward approach to model tuning is to run a grid search; this involves defining a range for each hyperparameter and then testing all possible combinations. This approach leads to a combinatorial explosion and thus is not recommended. Instead, Bergstra and Bengio propose randomly chosen trials [3]. Their paper shows that randomized trials are more efficient than grid search trials, selecting adequate hyperparameters at a fraction of the computational cost. The sits_tuning() function follows Bergstra and Bengio by using a random search on the chosen hyperparameters.\nExperiments with image time series show that other optimizers may have better performance for the specific problem of land classification. For this reason, the authors developed the torchopt R package, which includes several recently proposed optimizers, including Madgrad [4], and Yogi [5]. Using the sits_tuning() function allows testing these and other optimizers available in torch and torch_opt packages.\nThe sits_tuning() function takes the following parameters:\n\n\nsamples: Training dataset to be used by the model.\n\nsamples_validation: Optional dataset containing time series to be used for validation. If missing, the next parameter will be used.\n\nvalidation_split: If samples_validation is not used, this parameter defines the proportion of time series in the training dataset to be used for validation (default is 20%).\n\nml_method(): Deep learning method (either sits_mlp(), sits_tempcnn(), sits_tae() or sits_lighttae()).\n\nparams: Defines the optimizer and its hyperparameters by calling sits_tuning_hparams(), as shown in the example below.\n\ntrials: Number of trials to run the random search.\n\nmulticores: Number of cores to be used for the procedure.\n\nprogress: Show a progress bar?\n\nThe sits_tuning_hparams() function inside sits_tuning() allows defining optimizers and their hyperparameters, including lr (learning rate), eps (controls numerical stability), and weight_decay (controls overfitting). The default values for eps and weight_decay in all sits deep learning functions are 1e-08 and 1e-06, respectively. The default lr for sits_lighttae() and sits_tempcnn() is 0.005.\nUsers have different ways to randomize the hyperparameters, including:\n\n\nchoice() (a list of options);\n\nuniform (a uniform distribution);\n\nrandint (random integers from a uniform distribution);\n\nnormal(mean, sd) (normal distribution);\n\nbeta(shape1, shape2) (beta distribution);\n\nloguniform(max, min) (loguniform distribution).\n\nWe suggest to use the log-uniform distribution to search over a wide range of values that span several orders of magnitude. This is common for hyperparameters like learning rates, which can vary from very small values (e.g., 0.0001) to larger values (e.g., 1.0) in a logarithmic manner. By default, sits_tuning() uses a loguniform distribution between 10^-2 and 10^-4 for the learning rate and the same distribution between 10^-2 and 10^-8 for the weight decay."
  },
  {
    "objectID": "cl_tuning.html#tuning-a-lighttae-model",
    "href": "cl_tuning.html#tuning-a-lighttae-model",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.3 Tuning a LightTAE model",
    "text": "17.3 Tuning a LightTAE model\nOur fist example is tuning a Lightweight Temporal Attention Enconder model [6] on the MOD13Q1 dataset for the state of MatoGrosso. To recall, this data set contains time series samples from the Brazilian Mato Grosso state obtained from the MODIS MOD13Q1 product. It has 1,892 samples and nine classes (Cerrado, Forest, Pasture, Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet) [7] and is available in the R package sitsdata.\n\n\nR\nPython\n\n\n\n\n# Tuning ``sits_lighttae`` model\ntuned_mt &lt;- sits_tuning(\n     samples = samples_matogrosso_mod13q1,\n     ml_method = sits_lighttae(),\n     params = sits_tuning_hparams(\n         optimizer = torch::optim_adamw,\n         opt_hparams = list(\n             lr = loguniform(10^-2, 10^-4),\n             weight_decay = loguniform(10^-2, 10^-8)\n             )\n         ),\n     trials = 40,\n     multicores = 6,\n     progress = FALSE\n)\n\n\n\n\n# Load samples\nsamples_matogrosso_mod13q1 = load_samples_dataset(\n    name = \"samples_matogrosso_mod13q1\", \n    package = \"sitsdata\"\n)\n\n# Tuning ``sits_lighttae`` model\ntuned_mt = sits_tuning(\n     samples = samples_matogrosso_mod13q1,\n     ml_method = sits_lighttae,\n     params = sits_tuning_hparams(\n         optimizer = \"torch::optim_adamw\",\n         opt_hparams = dict(\n             lr = hparam(\"loguniform\", 10**-2, 10**-4),\n             weight_decay = hparam(\"loguniform\", 10**-2, 10**-8)\n             )\n         ),\n     trials = 40,\n     multicores = 6,\n     progress = FALSE\n)\n\n\n\n\nThe result is a tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The best results obtain accuracy values between 0.978 and 0.970, as shown below. The best result is obtained by a learning rate of 0.0013 and a weight decay of 3.73e-07. The worst result has an accuracy of 0.891, which shows the importance of the tuning procedure.\n\n\nR\nPython\n\n\n\n\n# Obtain accuracy, kappa, lr, and weight decay for the 5 best results\n# Hyperparameters are organized as a list\nhparams_5 &lt;- tuned_mt[1:5,]$opt_hparams\n\n# Extract learning rate and weight decay from the list\nlr_5 &lt;- purrr::map_dbl(hparams_5, function(h) h$lr)\nwd_5 &lt;- purrr::map_dbl(hparams_5, function(h) h$weight_decay)\n\n# Create a tibble to display the results\nbest_5 &lt;- tibble::tibble(\n    accuracy = tuned_mt[1:5,]$accuracy,\n    kappa = tuned_mt[1:5,]$kappa,\n    lr    = lr_5,\n    weight_decay = wd_5)\n\n# Print the best five combination of hyperparameters\nbest_5\n\n# A tibble: 5 × 4\n  accuracy kappa       lr weight_decay\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1    0.978 0.974 0.00136  0.000000373 \n2    0.975 0.970 0.00269  0.0000000861\n3    0.973 0.967 0.00162  0.00218     \n4    0.970 0.964 0.000378 0.00000868  \n5    0.970 0.964 0.00198  0.00000275  \n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n# Obtain accuracy, kappa, lr, and weight decay for the 5 best results\n# Hyperparameters are organized as a list\nhparams_5 = tuned_mt.opt_hparams[0:5]\n\n# Extract learning rate and weight decay from the list\nlr_5 = [x for x_ in hparams_5 for x in x_[\"lr\"]]\nwd_5 = [x for x_ in hparams_5 for x in x_[\"weight_decay\"]]\n\n# Create a tibble to display the results\nbest_5 = pd.DataFrame(dict(\n    accuracy = tuned_mt.accuracy[0:5],\n    kappa = tuned_mt.kappa[0:5],\n    lr = lr_5,\n    weight_decay = wd_5\n))\n\n# Print the best five combination of hyperparameters\nbest_5\n\n   accuracy     kappa        lr  weight_decay\n0  0.978202  0.973719  0.001364  3.730247e-07\n1  0.975477  0.970402  0.002686  8.605873e-08\n2  0.972752  0.967170  0.001623  2.179615e-03\n3  0.970027  0.963927  0.000378  8.683021e-06\n4  0.970027  0.963900  0.001981  2.753714e-06"
  },
  {
    "objectID": "cl_tuning.html#tuning-a-tempcnn-model",
    "href": "cl_tuning.html#tuning-a-tempcnn-model",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.4 Tuning a TempCNN model",
    "text": "17.4 Tuning a TempCNN model\nIn the example, we use sits_tuning() to find good hyperparameters to train the sits_tempcnn() algorithm for a dataset for measuring deforestation in Rondonia (samples_deforestation_rondonia) available in package sitsdata. This dataset consists of 6,007 samples collected from Sentinel-2 images covering the state of Rondonia. There are nine classes: Clear_Cut_Bare_Soil, Clear_Cut_Burned_Area, Mountainside_Forest, Forest, Riparian_Forest, Clear_Cut_Vegetation, Water, Wetland, and Seasonally_Flooded. Each time series contains values from Sentinel-2/2A bands B02, B03, B04, B05, B06, B07, B8A, B08, B11 and B12, from 2022-01-05 to 2022-12-23 in 16-day intervals. The samples are intended to detect deforestation events and have been collected by remote sensing experts using visual interpretation.\nThe hyperparameters for the sits_tempcnn() method include the size of the layers, convolution kernels, dropout rates, learning rate, and weight decay. Please refer to the description of the Temporal CNN algorithm in Chapter Machine learning for data cubes.\n\n\nR\nPython\n\n\n\n\n# Tuning ``sits_tempcnn`` model\ntuned_tempcnn &lt;- sits_tuning(\n  samples = samples_deforestation_rondonia,\n  ml_method = sits_tempcnn(),\n  params = sits_tuning_hparams(\n    cnn_layers = choice(c(256, 256, 256), c(128, 128, 128), c(64, 64, 64)),\n    cnn_kernels = choice(c(3, 3, 3), c(5, 5, 5), c(7, 7, 7)),\n    cnn_dropout_rates = choice(c(0.15, 0.15, 0.15), c(0.2, 0.2, 0.2),\n                               c(0.3, 0.3, 0.3), c(0.4, 0.4, 0.4)),\n    optimizer = torch::optim_adamw,\n    opt_hparams = list(\n      lr = loguniform(10^-2, 10^-4),\n      weight_decay = loguniform(10^-2, 10^-8)\n    )\n  ),\n  trials = 50,\n  multicores = 4\n)\n\n\n\n\n# Load samples\nsamples_deforestation_rondonia = load_samples_dataset(\n    name = \"samples_deforestation_rondonia\", \n    package = \"sitsdata\"\n)\n\n# Tuning ``sits_tempcnn`` model\ntuned_tempcnn = sits_tuning(\n  samples = samples_deforestation_rondonia,\n  ml_method = sits_tempcnn,\n  params = sits_tuning_hparams(\n    cnn_layers = hparam(\n        \"choice\", (256, 256, 256), (128, 128, 128), (64, 64, 64)\n    ),\n    cnn_kernels = hparam(\n        \"choice\", (3, 3, 3), (5, 5, 5), (7, 7, 7)\n    ),\n    cnn_dropout_rates = hparam(\n        \"choice\", (0.15, 0.15, 0.15), (0.2, 0.2, 0.2), \n                  (0.3, 0.3, 0.3), (0.4, 0.4, 0.4)\n    ),\n    optimizer = \"torch::optim_adamw\",\n    opt_hparams = dict(\n      lr = hparam(\"loguniform\", 10**-2, 10**-4),\n      weight_decay = hparam(\"loguniform\", 10**-2, 10**-8)\n    )\n  ),\n  trials = 50,\n  multicores = 4\n)\n\n\n\n\nThe result of sits_tuning() is tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The five best results obtain accuracy values between 0.939 and 0.908, as shown below. The best result is obtained by a learning rate of 3.76e-04 and a weight decay of 1.5e-04, and three CNN layers of size 256, kernel size of 5, and dropout rates of 0.2.\n\n\nR\nPython\n\n\n\n\n# Obtain accuracy, kappa, cnn_layers, cnn_kernels, and cnn_dropout_rates the best result\ncnn_params &lt;- tuned_tempcnn[1,c(\"accuracy\", \"kappa\", \"cnn_layers\", \"cnn_kernels\", \"cnn_dropout_rates\"), ]\n\n# Learning rates and weight decay are organized as a list\nhparams_best &lt;- tuned_tempcnn[1,]$opt_hparams[[1]]\n\n# Extract learning rate and weight decay \nlr_wd &lt;- tibble::tibble(lr_best = hparams_best$lr, \n                        wd_best = hparams_best$weight_decay)\n\n# Print the best parameters \ndplyr::bind_cols(cnn_params, lr_wd)\n\n# A tibble: 1 × 7\n  accuracy kappa cnn_layers       cnn_kernels cnn_dropout_rates  lr_best wd_best\n     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n1    0.939 0.929 c(256, 256, 256) c(5, 5, 5)  c(0.2, 0.2, 0.2)  0.000376 1.53e-4\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n# Learning rates and weight decay are organized as a list\nhparams_best = tuned_tempcnn.opt_hparams[0]\n\n# Obtain accuracy, kappa, cnn_layers, cnn_kernels, and cnn_dropout_rates the best result\npd.DataFrame(dict(\n    accuracy = tuned_tempcnn.accuracy[0:1],\n    kappa = tuned_tempcnn.kappa[0:1],\n    cnn_layers = tuned_tempcnn.cnn_layers[0:1],\n    cnn_kernels = tuned_tempcnn.cnn_kernels[0:1],\n    cnn_dropout_rates = tuned_tempcnn.cnn_dropout_rates[0:1],\n    lr_best = hparams_best['lr'],\n    wd_best = hparams_best['weight_decay'],\n))\n\n   accuracy     kappa        cnn_layers cnn_kernels cnn_dropout_rates   lr_best   wd_best\n0  0.939268  0.928595  c(256, 256, 256)  c(5, 5, 5)  c(0.2, 0.2, 0.2)  0.000376  0.000153"
  },
  {
    "objectID": "cl_tuning.html#summary",
    "href": "cl_tuning.html#summary",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.5 Summary",
    "text": "17.5 Summary\nFor large datasets, the tuning process is time-consuming. Despite this cost, it is recommended to achieve the best performance. In general, tuning hyperparameters for models such as sits_tempcnn() and sits_lighttae() will result in a slight performance improvement over the default parameters on overall accuracy. The performance gain will be stronger in the less well represented classes, where significant gains in producer’s and user’s accuracies are possible. When detecting change in less frequent classes, tuning can make a substantial difference in the results."
  },
  {
    "objectID": "cl_tuning.html#references",
    "href": "cl_tuning.html#references",
    "title": "\n17  Deep learning model tuning\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nY. Bengio, “Practical recommendations for gradient-based training of deep architectures,” arXiv:1206.5533 [cs], 2012, [Online]. Available: http://arxiv.org/abs/1206.5533.\n\n\n[2] \nR. M. Schmidt, F. Schneider, and P. Hennig, “Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers,” in Proceedings of the 38th International Conference on Machine Learning, 2021, pp. 9367–9376, [Online]. Available: https://proceedings.mlr.press/v139/schmidt21a.html.\n\n\n[3] \nJ. Bergstra and Y. Bengio, “Random Search for Hyper-Parameter Optimization,” Journal of Machine Learning Research, vol. 13, no. 10, pp. 281–305, 2012, [Online]. Available: http://jmlr.org/papers/v13/bergstra12a.html.\n\n\n[4] \nA. Defazio and S. Jelassi, “Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization.” arXiv, 2021, doi: 10.48550/arXiv.2101.11075.\n\n\n[5] \nM. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar, “Adaptive Methods for Nonconvex Optimization,” in Advances in Neural Information Processing Systems, 2018, vol. 31, [Online]. Available: https://proceedings.neurips.cc/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html.\n\n\n[6] \nV. S. F. Garnot and L. Landrieu, “Lightweight Temporal Self-attention for Classifying Satellite Images Time Series,” in Advanced Analytics and Learning on Temporal Data, 2020, pp. 171–181.\n\n\n[7] \nM. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007."
  },
  {
    "objectID": "cl_rasterclassification.html#data-cube-for-case-study",
    "href": "cl_rasterclassification.html#data-cube-for-case-study",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "\n18.1 Data cube for case study",
    "text": "18.1 Data cube for case study\nThe examples of this chapter use a pre-built data cube of Sentinel-2 images, available in the package sitsdata. These images are from the SENTINEL-2-L2A collection in Microsoft Planetary Computer (MPC). The data consists of bands BO2, B8A, and B11, and indexes NDVI, EVI and NBR in a small area of \\(1200 \\times 1200\\) pixels in the state of Rondonia. As explained in Chapter Data cubes from local files, we need to inform sits how to parse these file names to obtain tile, date, and band information. Image files are named according to the convention “satellite_sensor_tile_band_date” (e.g., SENTINEL-2_MSI_20LKP_BO2_2020_06_04.tif) which is the default format in sits.\n\n\nR\nPython\n\n\n\n\n# Files are available in a local directory \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR/\", \n                        package = \"sitsdata\")\n\n# Read data cube\nrondonia_20LMR &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir\n)\n\n# Plot the cube\nplot(rondonia_20LMR, date = \"2022-07-16\", band = \"NDVI\")\n\n\n\n\n# Files are available in a local directory \ndata_dir = r_package_dir(\"extdata/Rondonia-20LMR/\", \n                        package = \"sitsdata\")\n\n# Read data cube\nrondonia_20LMR = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir\n)\n\n# Plot the cube\nplot(rondonia_20LMR, date = \"2022-07-16\", band = \"NDVI\")\n\n\n\n\n\n\n\n\nFigure 18.1: Color composite image of the cube for date 2022-07-16."
  },
  {
    "objectID": "cl_rasterclassification.html#training-data-for-the-case-study",
    "href": "cl_rasterclassification.html#training-data-for-the-case-study",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "\n18.2 Training data for the case study",
    "text": "18.2 Training data for the case study\nThis case study uses the training dataset samples_deforestation_rondonia, available in package sitsdata. This dataset consists of 6007 samples collected from Sentinel-2 images covering the state of Rondonia. There are nine classes: Clear_Cut_Bare_Soil, Clear_Cut_Burned_Area, Mountainside_Forest, Forest, Riparian_Forest, Clear_Cut_Vegetation, Water, Wetland, and Seasonally_Flooded. Each time series contains values from Sentinel-2/2A bands B02, B03, B04, B05, B06, B07, B8A, B08, B11 and B12, from 2022-01-05 to 2022-12-23 in 16-day intervals. The samples are intended to detect deforestation events and have been collected by remote sensing experts using visual interpretation.\n\n\nR\nPython\n\n\n\n\n# Obtain the samples \ndata(\"samples_deforestation_rondonia\")\n\n# Show the contents of the samples\nsummary(samples_deforestation_rondonia)\n\n# A tibble: 9 × 3\n  label                 count   prop\n  &lt;chr&gt;                 &lt;int&gt;  &lt;dbl&gt;\n1 Clear_Cut_Bare_Soil     944 0.157 \n2 Clear_Cut_Burned_Area   983 0.164 \n3 Clear_Cut_Vegetation    603 0.100 \n4 Forest                  964 0.160 \n5 Mountainside_Forest     211 0.0351\n6 Riparian_Forest        1247 0.208 \n7 Seasonally_Flooded      731 0.122 \n8 Water                   109 0.0181\n9 Wetland                 215 0.0358\n\n\n\n\n\n# Obtain the samples \nsamples_deforestation_rondonia = load_samples_dataset(\n    name = \"samples_deforestation_rondonia\", \n    package = \"sitsdata\"\n)\n\n# Show the contents of the samples\nsummary(samples_deforestation_rondonia)\n\n                   label  count      prop\n1    Clear_Cut_Bare_Soil    944  0.157150\n2  Clear_Cut_Burned_Area    983  0.163642\n3   Clear_Cut_Vegetation    603  0.100383\n4                 Forest    964  0.160479\n5    Mountainside_Forest    211  0.035126\n6        Riparian_Forest   1247  0.207591\n7     Seasonally_Flooded    731  0.121691\n8                  Water    109  0.018145\n9                Wetland    215  0.035792\n\n\n\n\n\nIt is helpful to plot the basic patterns associated with the samples to understand the training set better. The function sits_patterns() uses a generalized additive model (GAM) to predict a smooth, idealized approximation to the time series associated with each class for all bands. Since the data cube used in the classification has 10 bands, we obtain the indexes NDVI, EVI, and NBR before showing the patterns.\n\n\nR\nPython\n\n\n\n\n# Generate indexes \nsamples_deforestation_indices &lt;- samples_deforestation_rondonia |&gt; \n    sits_apply(NDVI = (B08 - B04)/(B08 + B04)) |&gt; \n    sits_apply(NBR = (B08 - B12) / (B08 + B12)) |&gt; \n    sits_apply(EVI = 2.5 * (B08 - B04) / ((B08 + 6.0 * B04 - 7.5 * B02) + 1.0)) \n\n# Generate and plot patterns\nsamples_deforestation_indices |&gt; \n    sits_select(bands = c(\"NDVI\", \"EVI\", \"NBR\")) |&gt; \n    sits_patterns() |&gt; \n    plot()\n\n\n\n\n# Generate indexes \nsamples_deforestation_indices = sits_apply(\n    data = samples_deforestation_rondonia,\n    NDVI = \"(B08 - B04)/(B08 + B04)\",\n    NBR = \"(B08 - B12) / (B08 + B12)\",\n    EVI = \"2.5 * (B08 - B04) / ((B08 + 6.0 * B04 - 7.5 * B02) + 1.0)\"\n)\n\n# Generate patterns\npatterns = sits_patterns(\n    sits_select(\n        data = samples_deforestation_indices, \n        bands = (\"NDVI\", \"EVI\", \"NBR\")\n    )\n)\n\n# Plot patterns\nplot(patterns)\n\n\n\n\n\n\n\n\nFigure 18.2: Time series patterns for deforestation study in Rondonia.\n\n\n\nThe patterns show different temporal responses for the selected classes. They match the typical behavior of deforestation in the Amazon. In most cases, the forest is cut at the start of the dry season (May/June). At the end of the dry season, some clear-cut areas are burned to clean the remains; this action is reflected in the steep fall of the response of B11 values of burned area samples after August. The areas where native trees have been cut but some vegatation remain (“Clear_Cut_Vegetation”) have values in the B8A band that increase during the period."
  },
  {
    "objectID": "cl_rasterclassification.html#training-machine-learning-models",
    "href": "cl_rasterclassification.html#training-machine-learning-models",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "Training machine learning models",
    "text": "Training machine learning models\nThe next step is to train a machine learning model to illustrate CPU-based classification. We build a Random Forest model using sits_train() and then create a plot to find out what are the most important variables for the model.\n\n\nR\nPython\n\n\n\n\n# set the seed to get the same result\nset.seed(03022024)\n\n# Train model using Random Forest algorithm\nrfor_model &lt;- sits_train(\n  samples_deforestation_rondonia,\n  ml_method = sits_rfor()\n)\n\n# plot the model results\nplot(rfor_model)\n\n\n\n\n# set the seed to get the same result\nr_set_seed(03022024)\n\n# Train model using Random Forest algorithm\nrfor_model = sits_train(\n  samples_deforestation_rondonia,\n  ml_method = sits_rfor()\n)\n\n# plot the model results\nplot(rfor_model)\n\n\n\n\n\n\n\n\nFigure 18.3: Most relevant variables of the Random Forest model.\n\n\n\nThe figure shows bands and dates represent relevant inflection points in the image time series."
  },
  {
    "objectID": "cl_rasterclassification.html#classification-of-machine-learning-models-in-cpus",
    "href": "cl_rasterclassification.html#classification-of-machine-learning-models-in-cpus",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "Classification of machine learning models in CPUs",
    "text": "Classification of machine learning models in CPUs\nBy default, all classification algorithms in sits use CPU-based parallel processing, done internally by the package. The algorithms are adaptable; the only requirement for users is to inform the configuration of their machines. To achieve efficiency, sits implements a fault-tolerant multitasking procedure, using a cluster of independent workers linked to a virtual machine. To avoid communication overhead, all large payloads are read and stored independently; direct interaction between the main process and the workers is kept at a minimum. Details of CPU-based parallel processing in sits can be found in the Chapter How parallel processing works in SITS.\nTo classify both data cubes and sets of time series, use sits_classify(), which uses parallel processing to speed up the performance, as described at the end of this Chapter. Its most relevant parameters are: (a) data, either a data cube or a set of time series; (b) ml_model, a trained model using one of the machine learning methods provided; (c) multicores, number of CPU cores that will be used for processing; (d) memsize, memory available for classification; (e) output_dir, directory where results will be stored; (f) version, for version control. To follow the processing steps, turn on the parameters verbose to print information and progress to get a progress bar. The classification result is a data cube with a set of probability layers, one for each output class. Each probability layer contains the model’s assessment of how likely each pixel belongs to the related class. The probability cube can be visualized with plot(). In this example, we show only the probabilities associated to label “Forest”.\n\n\nR\nPython\n\n\n\n\n# Classify data cube to obtain a probability cube\nrondonia_20LMR_probs &lt;- sits_classify(\n    data     = rondonia_20LMR,\n    ml_model = rfor_model,\n    output_dir = tempdir_r,\n    version = \"rf-raster\",\n    multicores = 4,\n    memsize = 16)\n\nplot(rondonia_20LMR_probs, labels = \"Forest\", palette = \"YlGn\")\n\n\n\n\n# Classify data cube to obtain a probability cube\nrondonia_20LMR_probs = sits_classify(\n    data     = rondonia_20LMR,\n    ml_model = rfor_model,\n    output_dir = tempdir_py,\n    version = \"rf-raster\",\n    multicores = 4,\n    memsize = 16)\n\nplot(rondonia_20LMR_probs, labels = \"Forest\", palette = \"YlGn\")\n\n\n\n\n\n\n\n\nFigure 18.4: Probability map for class Forest for Random Forest model.\n\n\n\nThe probability cube provides information on the output values of the algorithm for each class. Most probability maps contain outliers or misclassified pixels. The labeled map generated from the pixel-based time series classification method exhibits several misclassified pixels, which are small patches surrounded by a different class. This occurrence of outliers is a common issue that arises due to the inherent nature of this classification approach. Regardless of their resolution, mixed pixels are prevalent in images, and each class exhibits considerable data variability. As a result, these factors can lead to outliers that are more likely to be misclassified. To overcome this limitation, sits employs post-processing smoothing techniques that leverage the spatial context of the probability cubes to refine the results. These techniques will be discussed in the Chapter Bayesian smoothing for post-processing. In what follows, we will generate the smoothed cube to illustrate the procedure.\n\n\nR\nPython\n\n\n\n\n# Smoothen a probability cube\nrondonia_20LMR_bayes &lt;- sits_smooth(\n    cube    = rondonia_20LMR_probs,\n    output_dir = tempdir_r,\n    version = \"rf-raster\",\n    multicores = 4,\n    memsize = 16)\n\nplot(rondonia_20LMR_bayes, labels = c(\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n# Smoothen a probability cube\nrondonia_20LMR_bayes = sits_smooth(\n    cube    = rondonia_20LMR_probs,\n    output_dir = tempdir_py,\n    version = \"rf-raster\",\n    multicores = 4,\n    memsize = 16)\n\nplot(rondonia_20LMR_bayes, labels = (\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n\n\n\n\nFigure 18.5: Smoothed probability map for class Forest.\n\n\n\nIn general, users should perform a post-processing smoothing after obtaining the probability maps in raster format. After the post-processing operation, we apply sits_label_classification() to obtain a map with the most likely class for each pixel. For each pixel, the sits_label_classification() function takes the label with highest probability and assigns it to the resulting map. The output is a labelled map with classes.\n\n\nR\nPython\n\n\n\n\n# Generate a thematic map\nrondonia_20LMR_class &lt;- sits_label_classification(\n    cube = rondonia_20LMR_bayes,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r,\n    version = \"rf-raster\")\n\n# Plot the thematic map\nplot(rondonia_20LMR_class, legend_text_size = 0.7)\n\n\n\n\n# Generate a thematic map\nrondonia_20LMR_class = sits_label_classification(\n    cube = rondonia_20LMR_bayes,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_py,\n    version = \"rf-raster\")\n\n# Plot the thematic map\nplot(rondonia_20LMR_class, legend_text_size = 0.7)\n\n\n\n\n\n\n\n\nFigure 18.6: Final map of deforestation obtained by Random Forest model."
  },
  {
    "objectID": "cl_rasterclassification.html#training-and-running-deep-learning-models",
    "href": "cl_rasterclassification.html#training-and-running-deep-learning-models",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "Training and running deep learning models",
    "text": "Training and running deep learning models\nThe next examples show how to run deep learning models in sits. The case study uses the Temporal CNN model [1], which is described in Chapter Machine learning for data cubes. Deep learning time series classification methods in sits, which include sits_tempcnn(), sits_mlp(), sits_lightae() and sits_tae(), are written using the torch package, which is an adaptation of pyTorch to the R environment. These algorithms can use a CUDA-compatible NVDIA GPU if one is available and has been properly configured. Please refer to the torch installation guide for details on how to configure torch to use GPUs. If no GPU is available, these algorithms will run on regular CPUs, using the same parallelization methods described in the traditional machine learning methods. Typically, there is a 10-fold performance increase when running torch based methods in GPUs relative to their processing time in GPU.\nWe take the same data cube and training data used in the previous examples and use a Temporal CNN method. The first step is to obtain a deep learning model using the sits_tempcnn() algorithm. We use the tuned parameters obtained in the example of the previous chapter.\n\n\nR\nPython\n\n\n\n\ntcnn_model &lt;- sits_train(\n  samples_deforestation_rondonia,\n  sits_tempcnn(\n    cnn_layers = c(256, 256, 256),\n    cnn_kernels = c(5, 5, 5),\n    cnn_dropout_rates = c(0.2, 0.2, 0.2),\n    opt_hparams = list(\n      lr = 0.0004,\n      weight_decay = 0.00015\n    )\n  )\n)\n\n\n\n\ntcnn_model = sits_train(\n  samples_deforestation_rondonia,\n  sits_tempcnn(\n    cnn_layers = (256, 256, 256),\n    cnn_kernels = (5, 5, 5),\n    cnn_dropout_rates = (0.2, 0.2, 0.2),\n    opt_hparams = dict(\n      lr = 0.0004,\n      weight_decay = 0.00015\n    )\n  )\n)\n\n\n\n\nAfter training the model, we classify the data cube. If a GPU is available, users need to provide the additional parameter gpu_memory to sits_classify(). This information will be used by sits to optimize access to the GPU and speed up processing.\n\n\nR\nPython\n\n\n\n\nrondonia_20LMR_probs_tcnn &lt;- sits_classify(\n    rondonia_20LMR,\n    ml_model = tcnn_model,\n    output_dir = tempdir_r,\n    version = \"tcnn-raster\",\n    gpu_memory = 16,\n    multicores = 6,\n    memsize = 24\n)\n\nplot(rondonia_20LMR_probs_tcnn, labels = c(\"Forest\"),  palette = \"YlGn\")\n\n\n\n\nrondonia_20LMR_probs_tcnn = sits_classify(\n    rondonia_20LMR,\n    ml_model = tcnn_model,\n    output_dir = tempdir_py,\n    version = \"tcnn-raster\",\n    gpu_memory = 16,\n    multicores = 6,\n    memsize = 24\n)\n\nplot(rondonia_20LMR_probs_tcnn, labels = c(\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n\n\n\n\nFigure 18.7: Probability map for class Forest for TempCNN model.\n\n\n\nAfter classification, we can smooth the probability cube. It is useful to compare the smoothed map for class Forest resulting from the TempCNN model with that produced by the Random Forest algorithm. The TempCNN model tends to show more confidence in its predictions than the random forests one. This is a feature of the model more than an intrinsic property of the training data or the data cube.\n\n\nR\nPython\n\n\n\n\n# Smoothen the probability map \nrondonia_20LMR_bayes_tcnn &lt;- sits_smooth(\n    rondonia_20LMR_probs_tcnn,\n    output_dir = tempdir_r,\n    version = \"tcnn-raster\",\n    multicores = 6,\n    memsize = 24\n)\n\nplot(rondonia_20LMR_bayes_tcnn, labels = c(\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n# Smoothen the probability map \nrondonia_20LMR_bayes_tcnn = sits_smooth(\n    rondonia_20LMR_probs_tcnn,\n    output_dir = tempdir_py,\n    version = \"tcnn-raster\",\n    multicores = 6,\n    memsize = 24\n)\n\nplot(rondonia_20LMR_bayes_tcnn, labels = (\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n\n\n\n\nFigure 18.8: Probability map for class Forest for TempCNN model.\n\n\n\nWe then label the resulting smoothed probabilities to obtain a classified map.\n\n\nR\nPython\n\n\n\n\n# Obtain the final labelled map\nrondonia_20LMR_class_tcnn &lt;- sits_label_classification(\n    rondonia_20LMR_bayes_tcnn,\n    output_dir = tempdir_r,\n    version = \"tcnn-raster\",\n    multicores = 6,\n    memsize = 24\n)\n\nplot(rondonia_20LMR_class_tcnn)\n\n\n\n\n# Obtain the final labelled map\nrondonia_20LMR_class_tcnn = sits_label_classification(\n    rondonia_20LMR_bayes_tcnn,\n    output_dir = tempdir_py,\n    version = \"tcnn-raster\",\n    multicores = 6,\n    memsize = 24\n)\n\nplot(rondonia_20LMR_class_tcnn)\n\n\n\n\n\n\n\n\nFigure 18.9: Probability map for class Forest for TempCNN model."
  },
  {
    "objectID": "cl_rasterclassification.html#summary",
    "href": "cl_rasterclassification.html#summary",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "\n18.3 Summary",
    "text": "18.3 Summary\nThis chapter presents a detailed example of how to train models and apply them to raster classification in sits. The procedure is simple and direct, using a workflow that combines sits_train(), sits_classify(), sits_smooth() and sits_label_classification(). For traditional machine learning models, such as Random Forest, the code is optimized for CPU processing and users will see good performance. In case of deep learning model, sits is optimized for GPU processing. The typical time for classifying a 10-band Sentinel-2 tile in a CPU using rRandom Forest is 20 minutes in a moderately sized machine (16 cores, 64 GB RAM). The same performance is expected when running deep learning models in a standard GPU with 16 GB memory."
  },
  {
    "objectID": "cl_rasterclassification.html#references",
    "href": "cl_rasterclassification.html#references",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nC. Pelletier, G. I. Webb, and F. Petitjean, “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,” Remote Sensing, vol. 11, no. 5, 2019."
  },
  {
    "objectID": "cl_smoothing.html#introduction",
    "href": "cl_smoothing.html#introduction",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.1 Introduction",
    "text": "19.1 Introduction\nMachine learning algorithms rely on training samples that are derived from “pure” pixels, hand-picked by users to represent the desired output classes. Given the presence of mixed pixels in images regardless of resolution, and the considerable data variability within each class, these classifiers often produce results with outliers or misclassified pixels. Therefore, post-processing techniques have become crucial to refine the labels of a classified image [1]. Post-processing methods reduce salt-and-pepper and border effects, where single pixels or small groups of pixels are classified differently from their larger surrounding areas; these effects lead to visual discontinuity and inconsistency. By mitigating these errors and minimizing noise, post-processing improves the quality of the initial classification results, bringing a significant gain in the overall accuracy and interpretability of the final output [2].\nThe sits package uses a time-first, space-later approach. Since machine learning classifiers in sits are mostly pixel-based, it is necessary to complement them with spatial smoothing methods. These methods improve the accuracy of land classification by incorporating spatial and contextual information into the classification process. The smoothing method available in sits uses an Empirical Bayes approach, adjusted to the specific properties of land classification. The assumption is that class probabilities at the local level should be similar and provide the baseline for comparison with the pixel values produced by the classifier. Based on these two elements, Bayesian smoothing adjusts the probabilities for the pixels, considering a spatial dependence [3]."
  },
  {
    "objectID": "cl_smoothing.html#the-need-for-post-processing",
    "href": "cl_smoothing.html#the-need-for-post-processing",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.2 The need for post-processing",
    "text": "19.2 The need for post-processing\nThe main idea behind our post-processing method is that a pixel-based classification should take into account its neighborhood pixels. Consider the figure blow which shows a class assignment produced by a Random Forest model. The classified map has been produced by taking, for each pixel, the class of higher probability produced by the algorithm. The resulting map has many noisy areas with a high spatial variability of class assignments. This happens more frequently in two cases: (a) small clusters of pixels of one class inside a larger area of a different class; (b) transition zones between classes. In general, images of heterogeneous landscapes with high spatial variability have many mixed pixels, whose spectral response combines different types of land cover in a single ground resolved cell. For example, many pixels in the border between areas of classes Forest and Clear_Cut_Bare_Soil are wrongly assigned to the Clear_Cut_Vegetation class. This wrong assignment occurs because these pixels have a mixed response. Inside the ground cell captured by the sensor as a single pixel value, there are both trees and bare soil areas. Such results are undesirable and need to be corrected by post-processing.\n\n\n\n\nFigure 19.1: Detail of labelled map produced by Random Forest without smoothing.\n\n\n\nTo maintain consistency and coherence in our class representations, we should minimise small variations or misclassifications. We incorporate spatial coherence as a post-processing step to accomplish this. The probabilities associated with each pixel will change based on statistical inference, which depends on the values for each neighbourhood. Using the recalculated probabilities for each pixel, we get a better version of the final classified map. Consider the figure below, which is the result of Bayesian smoothing on the Random Forest model outcomes. The noisy border pixels between two large areas of the same class have been removed. We have also removed small clusters of pixels belonging to one class inside larger areas of other classes. The outcome is a more uniform map, like the ones created through visual interpretation or object-based analysis. Details like narrow vegetation corridors or small forest roads might be missing in the smoothed image. However, the improved spatial consistency of the final map compensates for such losses, due to the removal of misclassified pixels that have mixed spectral responses.\n\n\n\n\nFigure 19.2: Detail of labelled map produced by pixel-based Random Forest after smoothing."
  },
  {
    "objectID": "cl_smoothing.html#empirical-bayesian-estimation",
    "href": "cl_smoothing.html#empirical-bayesian-estimation",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.5 Empirical Bayesian estimation",
    "text": "19.5 Empirical Bayesian estimation\nWe assume that the class probability \\(p_{i,k}\\) estimated by the pixel-based machine learning method is a noisy version of underlying probability \\(\\pi_{i,k} &gt; 0\\) that we want to learn. If the machine-learning method had perfect accuracy, we should have \\(p_{i,k} = \\pi_{i,k}\\). In practice, they are not equal due to estimation errors inherent in any machine-learning method.\nDefine the corresponding logit for the true and unknown \\(\\pi_{i,k}\\) probabilities: \\(\\mu_{i,k} = \\ln(\\pi_{i,k}/(1-\\pi_{i,k}))\\). We assume that our noisy data version \\(x_{i,k}\\) is randomly distributed around \\(\\mu_{i,k}\\) according to a Gaussian distribution: \\[\n(x_{i,k}|\\mu_{i,k}) \\sim N(\\mu_{i,k}, \\sigma^2_k)\n\\] where \\(\\sigma^2_k\\) is the variance associated with the \\(k\\)-th class. This distribution is called the function. The Bayesian approach assumes a to represent the uncertainty around \\(\\mu_{i,k}\\), also a Gaussian distribution \\[\n\\mu_{i,k} \\sim N(m_{i,k}, s^2_{i,k}) \\: .\n\\] The standard Bayesian updating [4] leads to the posterior distribution which can be expressed as a weighted mean\n\\[\n{E}[\\mu_{i,k} | x_{i,k}] =\n\\Biggl [ \\frac{s^2_{i,k}}{\\sigma^2_{k} +s^2_{i,k}} \\Biggr ] \\times\nx_{i,k} +\n\\Biggl [ \\frac{\\sigma^2_{k}}{\\sigma^2_{k} +s^2_{i,k}} \\Biggr ] \\times m_{i,k},\n\\]\n\n\n\\(x_{i,k}\\) is the logit value for pixel \\(i\\) and class \\(k\\).\n\n\\(m_{i,k}\\) is the average of logit values for pixels of class \\(k\\) in the neighborhood of pixel \\(i\\).\n\n\\(s^2_{i,k}\\) is the variance of logit values for pixels of class \\(k\\) in the neighborhood of pixel \\(i\\).\n\n\\(\\sigma^2_{k}\\) is an user-derived hyperparameter which estimates the variance for class \\(k\\), expressed in logits.\n\nThis posterior distribution represents the updated uncertainty about the actual probability \\(\\pi_{i,k}\\) of the pixel \\(i\\) belonging to class \\(k\\) after we have observed the noisy \\(p_{i,k}\\). The posterior mean is a weighted average between the observed logit value \\(x_{i,k}\\) and the prior logit mean \\(m_{i,k}\\). When the prior variance \\(s^2_{i,k}\\) is high, the algorithm assigns more weight to the observed value \\(x_{i,k}\\). Conversely, as the likelihood variance \\(\\sigma^2_k\\) increases, the update assigns more weight to the prior mean \\(m_{i,k}\\).\nIn the above equation, the value of \\(x_{i,k}\\) is known. To complete the calculation, one needs to work out the values of the likelihood variance \\(\\sigma^2_k\\) and the mean \\(m_{i,k}\\) and variance \\(s^2_{i,k}\\) of the prior distribution. The variance \\(\\sigma^2_{k}\\) will be estimated based on user expertise and taken as a hyperparameter to control the smoothness of the resulting estimate.\nThere is usually no prior information to specify \\(m_{i,k}\\) and \\(s^2_{i,k}\\). Because of that, we adopt an Empirical Bayes (EB) approach to obtain estimates of these prior parameters by considering the pixel neighborhood. However, using a standard symmetrical neighborhood for each pixel, based uniquely on the distance between locations, would not produce reasonable results for border pixels. For this reason, our EB estimates uses non-isotropic neighbourhood, as explained below."
  },
  {
    "objectID": "cl_smoothing.html#using-non-isotropic-neighborhoods",
    "href": "cl_smoothing.html#using-non-isotropic-neighborhoods",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.6 Using non-isotropic neighborhoods",
    "text": "19.6 Using non-isotropic neighborhoods\nThe fundamental idea behind Bayesian smoothing for land classification is that individual pixels area related to those close to it. Each pixel usually has the same class as most of its neighbors. These closeness relations are expressed in similar values of class probability. If we find a pixel assigned to Water surrounded by pixels labeled as Forest, such pixel may have been wrongly labelled. To check if the pixel has been mislabeled, we look at the class probabilities for the pixels and its neighbors. There are possible situations:\n\nThe outlier has a class probability distribution very different from its neighbors. For example, its probability for belonging to the Water class is 80% while that of being a Forest is 20%. If we also consider that Water pixels have a smaller variance, since water areas have a strong signal in multispectral images, our post-processing method will not change the pixel’s label.\nThe outlier has a class probability distribution similar from its neighbors. Consider a case where a pixel has a 47% probability for Water and 43% probability for Forest. This small difference indicates that we need to look at the neighborhood to improve the information produced by the classifier. In these cases, the post-processing estimate may change the pixel’s label.\n\nPixels in the border between two areas of different classes pose a challenge. Only some of their neighbors belong to the same class as the pixel. To address this issue, we employ a non-isotropic definition of a neighborhood to estimate the prior class distribution. For instance, consider a boundary pixel with a neighborhood defined by a 7 x 7 window, located along the border between Forest and Grassland classes. To estimate the prior probability of the pixel being labeled as a Forest, we should only take into account the neighbors on one side of the border that are likely to be correctly classified as Forest. Pixels on the opposite side of the border should be disregarded, since they are unlikely to belong to the same spatial process. In practice, we use only half of the pixels in the 7 x 7 window, opting for those that have a higher probability of being named as Forest. For the prior probability of the Grassland class, we reverse the selection and only consider those on the opposite side of the border.\nAlthough this choice of neighborhood may seem unconventional, it is consistent with the assumption of non-continuity of the spatial processes describing each class. A dense forest patch, for example, will have pixels with strong spatial autocorrelation for values of the Forest class; however, this spatial autocorrelation doesn’t extend across its border with other land classes."
  },
  {
    "objectID": "cl_smoothing.html#effect-of-the-hyperparameter",
    "href": "cl_smoothing.html#effect-of-the-hyperparameter",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.7 Effect of the hyperparameter",
    "text": "19.7 Effect of the hyperparameter\nThe parameter \\(\\sigma^2_k\\) controls the level of smoothness. If \\(\\sigma^2_k\\) is zero, the value \\({E}[\\mu_{i,k} | x_{i,k}]\\) will be equal to the pixel value \\(x_{i,k}\\). The parameter \\(\\sigma^2_k\\) expresses confidence in the inherent variability of the distribution of values of a class \\(k\\). The smaller the parameter \\(\\sigma^2_k\\), the more we trust the estimated probability values produced by the classifier for class \\(k\\). Conversely, higher values of \\(\\sigma^2_k\\) indicate lower confidence in the classifier outputs and improved confidence in the local averages.\nConsider the following two-class example. Take a pixel with probability \\(0.4\\) (logit \\(x_{i,1} = -0.4054\\)) for class A and probability \\(0.6\\) (logit \\(x_{i,2} = 0.4054\\)) for class B. Without post-processing, the pixel will be labeled as class B. Consider that the local average is \\(0.6\\) (logit \\(m_{i,1} = 0.4054\\)) for class A and \\(0.4\\) (logit \\(m_{i,2} = -0.4054\\)) for class B. This is a case of an outlier classified originally as class B in the midst of a set of class A pixels.\nGiven this situation, we apply the proposed method. Suppose the local variance of logits to be \\(s^2_{i,1} = 5\\) for class A and \\(s^2_{i,2} = 10\\) and for class B. This difference is to be expected if the local variability of class A is smaller than that of class B. To complete the estimate, we need to set the parameter \\(\\sigma^2_{k}\\), representing our belief in the variability of the probability values for each class.\nSetting \\(\\sigma^2_{k}\\) will be based on our confidence in the local variability of each class around pixel \\({i}\\). If we considered the local variability to be high, we can take both \\(\\sigma^2_1\\) for class A and \\(\\sigma^2_2\\) for class B to be both 10. In this case, the Bayesian estimated probability for class A is \\(0.52\\) and for class B is \\(0.48\\) and the pixel will be relabeled as being class A.\nBy contrast, if we consider local variability to be high If we set \\(\\sigma^2\\) to be 5 for both classes A and B, the Bayesian probability estimate will be \\(0.48\\) for class A and \\(0.52\\) for class B. In this case, the original class will be kept. Therefore, the result is sensitive to the subjective choice of the hyperparameter. In the example below, we will show how to use the local logit variance to set the appropriate values of \\(\\sigma^2\\)."
  },
  {
    "objectID": "cl_smoothing.html#running-bayesian-smoothing",
    "href": "cl_smoothing.html#running-bayesian-smoothing",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.8 Running Bayesian smoothing",
    "text": "19.8 Running Bayesian smoothing\nWe now show how to run Bayesian smoothing on a data cube covering an area of Sentinel-2 tile “20LLQ” in the period 2020-06-04 to 2021-08-26. The training data has six classes: (a) Forest for natural tropical forest; (b) Water for lakes and rivers; (c) “Wetlands” for areas where water covers the soil in the wet season; (d) Clear_Cut_Burned_Area for areas where fires cleared the land after tree removal; (e) Clear_Cut_Bare_Soil where the forest has been completely removed; (f) Clear_Cut_Vegetation where some vegetation remains after most trees have been removed. To simplify the example, our input is the probability cube generated by a Random Forest model. We recover the probability data cube and then plot the the results of the machine learning method for classes Forest, Clear_Cut_Bare_Soil, Clear_Cut_Vegetation, and Clear_Cut_Burned_Area.\nLoad Sentinel-2 Data Cube\n\n\nR\nPython\n\n\n\n\n# define the classes of the probability cube\nlabels &lt;- c(\"1\" = \"Water\", \n            \"2\" = \"Clear_Cut_Burned_Area\", \n            \"3\" = \"Clear_Cut_Bare_Soil\",\n            \"4\" = \"Clear_Cut_Vegetation\", \n            \"5\" = \"Forest\", \n            \"6\" = \"Wetland\"\n)\n\n# directory where the data is stored \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LLQ/\", package = \"sitsdata\")\n\n# create a probability data cube from a file \nrondonia_20LLQ_probs &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = \"probs\",\n    labels = labels,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \n                   \"start_date\", \"end_date\", \"band\", \"version\"))\n\n\n\n\n# define the classes of the probability cube\nlabels = {\n    \"1\": \"Water\",\n    \"2\": \"Clear_Cut_Burned_Area\",\n    \"3\": \"Clear_Cut_Bare_Soil\",\n    \"4\": \"Clear_Cut_Vegetation\",\n    \"5\": \"Forest\",\n    \"6\": \"Wetland\"\n}\n\n# directory where the data is stored \ndata_dir = r_package_dir(\"extdata/Rondonia-20LLQ/\", package = \"sitsdata\")\n\n# create a probability data cube from a file \nrondonia_20LLQ_probs = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = \"probs\",\n    labels = labels,\n    parse_info = (\"satellite\", \"sensor\", \"tile\", \n                   \"start_date\", \"end_date\", \"band\", \"version\"))\n\n\n\n\nProbabilities for water and forest\n\n\nR\nPython\n\n\n\n\n# plot the probabilities for water and forest\nplot(rondonia_20LLQ_probs, \n     labels = c(\"Forest\", \"Clear_Cut_Bare_Soil\"))\n\n\n\nFigure 19.3: Probability map produced for classes Forest and Clear_Cut_Bare_Soil.\n\n\n\n\n\n\n# plot the probabilities for water and forest\nplot(rondonia_20LLQ_probs, \n     labels = (\"Forest\", \"Clear_Cut_Bare_Soil\"))\n\n\n\n\n\nFigure 19.4: Probability map produced for classes Forest and Clear_Cut_Bare_Soil.\n\n\n\n\n\n\nProbabilities for Clear Cut Vegetation and Burned Area\n\n\nR\nPython\n\n\n\n\nplot(rondonia_20LLQ_probs, \n     labels = c(\"Clear_Cut_Vegetation\", \"Clear_Cut_Burned_Area\"))\n\n\n\nFigure 19.5: Probability map produced for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area.\n\n\n\n\n\n\nplot(rondonia_20LLQ_probs, \n     labels = (\"Clear_Cut_Vegetation\", \"Clear_Cut_Burned_Area\"))\n\n\n\n\n\nFigure 19.6: Probability map produced for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area.\n\n\n\n\n\n\nThe probability map for Forest shows high values associated with compact patches and linear stretches in riparian areas. Class Clear_Cut_Bare_Soil is mostly composed of dense areas of high probability whose geometrical boundaries result from forest cuts. Areas of class Clear_Cut_Vegetation are is less well-defined than the others; this is to be expected since this is a transitional class between a natural forest and areas of bare soil. Patches associated to class Clear_Cut_Burned_Area include both homogeneous areas of high probability and areas of mixed response. Since classes have different behaviours, the post-processing procedure should enable users to control how to handle outliers and border pixels of each class.\nThe next step is to show the labelled map resulting from the raw class probabilites. We produce a classification map by taking the class of higher probability to each pixel, without considering the spatial context. There are many places with the so-called “salt-and-pepper” effect which result from misclassified pixels. The non-smoothed labelled map shows the need for post-processing, since it contains a significant number of outliers and areas with mixed labelling.\n\n\nR\nPython\n\n\n\n\n# Generate the thematic map\nrondonia_20LLQ_class &lt;- sits_label_classification(\n    cube = rondonia_20LLQ_probs,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r,\n    version = \"no_smooth\")\n\n# Plot the result\nplot(rondonia_20LLQ_class,\n     legend_text_size = 0.8, legend_position = \"outside\")\n\n\n\n\n\nFigure 19.7: Classified map without smoothing.\n\n\n\n\n\n\n# Generate the thematic map\nrondonia_20LLQ_class = sits_label_classification(\n    cube = rondonia_20LLQ_probs,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_py,\n    version = \"no_smooth\")\n    \n# Plot the result\nplot(rondonia_20LLQ_class,\n     legend_text_size = 0.8, legend_position = \"outside\")\n\n\n\n\n\nFigure 19.8: Classified map without smoothing."
  },
  {
    "objectID": "cl_smoothing.html#assessing-the-local-logit-variance",
    "href": "cl_smoothing.html#assessing-the-local-logit-variance",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.9 Assessing the local logit variance",
    "text": "19.9 Assessing the local logit variance\nTo determine appropriate settings for the \\(\\sigma^2_{k}\\) hyperparameter for each class to perform Bayesian smoothing, it is useful to calculate the local logit variances for each class. For each pixel, we estimate the local variance \\(s^2_{i,k}\\) by considering the non-isotropic neighborhood. The local logit variances are estimated by sits_variance(); Its main parameters are: (a) cube, a probability data cube; (b) window_size, dimension of the local neighbourhood; (c) neigh_fraction, the percentage of pixels in the neighbourhood used to calculate the variance. The example below uses half of the pixels of a \\(7\\times 7\\) window to estimate the variance. The chosen pixels will be those with the highest probability pixels to be more representative of the actual class distribution. The output values are the logit variances in the vicinity of each pixel.\nThe choice of the \\(7 \\times 7\\) window size is a compromise between having enough values to estimate the parameters of a normal distribution and the need to capture local effects for class patches of small sizes. Classes such as Water tend to be spatially limited; a bigger window size could result in invalid values for their respective normal distributions.\nCalculate variance\n\n\nR\nPython\n\n\n\n\n# calculate variance\nrondonia_20LLQ_var &lt;- sits_variance(\n    cube = rondonia_20LLQ_probs,\n    window_size = 7,\n    neigh_fraction = 0.50,\n    output_dir = tempdir_r, \n    multicores = 4,\n    memsize = 16\n)\n\n\n\n\n# calculate variance\nrondonia_20LLQ_var = sits_variance(\n    cube = rondonia_20LLQ_probs,\n    window_size = 7,\n    neigh_fraction = 0.50,\n    output_dir = tempdir_py, \n    multicores = 4,\n    memsize = 16\n)\n\n\n\n\nVariance map for Forest and Clear Cut Bare Soil\n\n\nR\nPython\n\n\n\n\n# Plot variance map for classes Forest and Clear_Cut_Bare_Soil\nplot(rondonia_20LLQ_var, \n     labels = c(\"Forest\", \"Clear_Cut_Bare_Soil\"), \n     palette = \"Spectral\", \n     rev = TRUE\n)\n\n\n\nFigure 19.9: Variance map for classes Forest and Clear_Cut_Bare_Soil.\n\n\n\n\n\n\n# Plot variance map for classes Forest and Clear_Cut_Bare_Soil\nplot(rondonia_20LLQ_var, \n     labels = (\"Forest\", \"Clear_Cut_Bare_Soil\"), \n     palette = \"Spectral\", \n     rev = True\n)\n\n\n\n\n\nFigure 19.10: Variance map for classes Forest and Clear_Cut_Bare_Soil.\n\n\n\n\n\n\nVariance map for Clear Cut Vegetation and Burned Area\n\n\nR\nPython\n\n\n\n\n# plot variance map for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area\nplot(rondonia_20LLQ_var, \n     labels = c(\"Clear_Cut_Vegetation\", \"Clear_Cut_Burned_Area\"), \n     palette = \"Spectral\", \n     rev = TRUE\n)\n\n\n\nFigure 19.11: Variance map for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area.\n\n\n\n\n\n\n# plot variance map for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area\nplot(rondonia_20LLQ_var, \n     labels = (\"Clear_Cut_Vegetation\", \"Clear_Cut_Burned_Area\"), \n     palette = \"Spectral\", \n     rev = True\n)\n\n\n\n\n\nFigure 19.12: Variance map for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area.\n\n\n\n\n\n\nComparing the variance maps with the probability maps, one sees that areas of high probability of classes Forest and Clear_Cut_Bare_Soil are mostly made of compact patches. Recall these are the two dominant classes in the area, and deforestation is a process that converts forest to bare soil. Many areas of high logit variance for these classes are related to border pixels which have a mixed response. Areas of large patches of high logit variance for these classes are associated to lower class probabilities and will not be relevant to the final result.\nBy contrast, the transitional classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area have a different spatial pattern of their probability and logit variance. The first has a high spatial variability, since pixels of this class arise when the forest has not been completely removed and there is some remaining vegetation after trees are cut. The extent of remaining vegetation after most trees have been removed is not uniform. For this reason, many areas of high local logit variance of class Clear_Cut_Vegetation are located in mixed patches inside pixels of class Forest and on the border between Forest and Clear_Cut_Bare_Soil. This situation is consistent with the earlier observation that transitional classes may appear as artificial effects of mixed pixels in borders between other classes.\nInstances of class ClearCut_Burned_Area arise following a forest fire. Most pixels of this class tend to form mid-sized to large spatial clusters, because of how forest fires start and propagate. It is desirable to preserve the contiguity of the burned areas and remove pixels of other classes inside these clusters. Isolated points of class ClearCut_Burned_Area can be removed without significant information loss.\nThe distinct patterns of these classes are measured quantitatively by the summary() function. For variance cubes, this function provides information on the logit variance values of the higher inter-quartile values.\n\n\nR\nPython\n\n\n\n\n# get the summary of the logit variance\nsummary(rondonia_20LLQ_var)\n\n     Water Clear_Cut_Burned_Area Clear_Cut_Bare_Soil Clear_Cut_Vegetation\n75%   4.22                  0.25              0.3900                 0.53\n80%   4.74                  0.30              0.4800                 0.66\n85%   5.07                  0.37              0.6200                 0.83\n90%   5.35                  0.49              0.9000                 1.11\n95%   5.89                  0.72              1.7405                 1.77\n100% 25.38                  9.17             10.8800                14.02\n     Forest Wetland\n75%    1.13  0.2900\n80%    1.87  0.3400\n85%    2.91  0.4200\n90%    4.22  0.5700\n95%    5.14  1.2905\n100%  19.05 11.3600\n\n\n\n\n\n# get the summary of the logit variance\nsummary(rondonia_20LLQ_var)\n\n      Water  Clear_Cut_Burned_Area  Clear_Cut_Bare_Soil  Clear_Cut_Vegetation  Forest  Wetland\n75%    4.25                   0.25                 0.39                  0.53    1.17     0.28\n80%    4.76                   0.30                 0.48                  0.66    1.82     0.34\n85%    5.07                   0.37                 0.61                  0.87    2.90     0.42\n90%    5.36                   0.48                 0.85                  1.15    4.25     0.57\n95%    5.87                   0.73                 1.65                  1.85    5.11     1.25\n100%  24.40                   7.65                11.71                 13.56   18.99    10.71\n\n\n\n\n\nThe summary statistics show that most local variance values are low, which is an expected result. Areas of low variance correspond to pixel neighborhoods of high logit values for one of the classes and low logit values for the others. High values of the local variances are relevant in areas of confusion between classes."
  },
  {
    "objectID": "cl_smoothing.html#using-the-variance-to-select-values-of-hyperparameters",
    "href": "cl_smoothing.html#using-the-variance-to-select-values-of-hyperparameters",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.10 Using the variance to select values of hyperparameters",
    "text": "19.10 Using the variance to select values of hyperparameters\nWe make the following recommendations for setting the \\(\\sigma^2_{k}\\) parameter, based on the local logit variance:\n\nSet the \\(\\sigma^2_{k}\\) parameter with high values (in the 95%-100% range) to increase the neighborhood influence compared with the probability values for each pixel. Such choice will produce denser spatial clusters and remove “salt-and-pepper” outliers.\nSet the \\(\\sigma^2_{k}\\) parameter with low values (in the 75%-80% range) to reduce the neighborhood influence, for classes that we want to preserve their original spatial shapes.\n\nConsider the case of forest areas and watersheds. If an expert wishes to have compact areas classified as forests without many outliers inside them, she will set the \\(\\sigma^2\\) parameter for the class Forest to be high. For comparison, to avoid that small watersheds with few similar neighbors being relabeled, it is advisable to avoid a strong influence of the neighbors, setting \\(\\sigma^2\\) to be as low as possible. In contrast, transitional classes such as Clear_Cut_Vegetation are likely to be associated with some outliers; use large \\(\\sigma^2_{k}\\) for them.\nTo remove the outliers and classification errors, we run a smoothing procedure with sits_smooth() with parameters: (a) cube, a probability cube produced by sits_classify(); (b) window_size, the local window to compute the neighborhood probabilities; (d) neigh_fraction, fraction of local neighbors used to calculate local statistics; (e) smoothness, a vector with estimates of the prior variance of each class; (f) multicores, number of CPU cores that will be used for processing; (g) memsize, memory available for classification; (h) output_dir, a directory where results will be stored; (i) version, for version control. The resulting cube can be visualized with plot().\nThe parameters window_size and neigh_fraction control how many pixels in a neighborhood the Bayesian estimator will use to calculate the local statistics. For example, setting window size to \\(7\\) and neigh_fraction to \\(0.50\\) (the defaults) ensures that \\(25\\) samples are used to estimate the local statistics. The smoothness values for the classes are set as recommended above.\nCompute Bayesian smoothing\n\n\nR\nPython\n\n\n\n\n# Compute Bayesian smoothing\nrondonia_20LLQ_smooth &lt;- sits_smooth(\n    cube = rondonia_20LLQ_probs,\n    window_size = 7,\n    neigh_fraction = 0.50,\n    smoothness = c(\"Water\" = 5.0, \n                   \"Clear_Cut_Burned_Area\" = 9.5, \n                   \"Clear_Cut_Bare_Soil\" = 0.5, \n                   \"Clear_Cut_Vegetation\" =  15, \n                   \"Forest\" = 2.5, \n                   \"Wetland\" = 0.40),\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r\n)\n\n\n\n\n# Compute Bayesian smoothing\nrondonia_20LLQ_smooth = sits_smooth(\n    cube = rondonia_20LLQ_probs,\n    window_size = 7,\n    neigh_fraction = 0.50,\n    smoothness = dict(Water = 5.0, \n                   Clear_Cut_Burned_Area = 9.5, \n                   Clear_Cut_Bare_Soil = 0.5, \n                   Clear_Cut_Vegetation =  15, \n                   Forest = 2.5, \n                   Wetland = 0.40),\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_py\n)\n\n\n\n\nProbability maps after bayesian smoothing\n\n\nR\nPython\n\n\n\n\n# Plot the result\nplot(rondonia_20LLQ_smooth, labels = c(\"Clear_Cut_Vegetation\", \"Forest\"))\n\n\n\nFigure 19.13: Probability maps after bayesian smoothing.\n\n\n\n\n\n\n# Plot the result\nplot(rondonia_20LLQ_smooth, labels = (\"Clear_Cut_Vegetation\", \"Forest\"))\n\n\n\n\n\nFigure 19.14: Probability maps after bayesian smoothing.\n\n\n\n\n\n\nBayesian smoothing has removed some of the local variability associated with misclassified pixels that differ from their neighbors, specially in the case of transitional classes such as Clear_Cut_Vegetation. The smoothing impact is best appreciated by comparing the labeled map produced without smoothing to the one that follows the procedure, as shown below.\n\n\nR\nPython\n\n\n\n\n# Generate the thematic map\nrondonia_20LLQ_class_v2 &lt;- sits_label_classification(\n    cube = rondonia_20LLQ_smooth,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r,\n    version = \"smooth\")\n\n# plot the thematic map\nplot(rondonia_20LLQ_class_v2, legend_position = \"outside\")\n\n\n\nFigure 19.15: Final classification map after Bayesian smoothing with 7 x 7 window, using high smoothness values.\n\n\n\n\n\n\n# Generate the thematic map\nrondonia_20LLQ_class_v2 = sits_label_classification(\n    cube = rondonia_20LLQ_smooth,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_py,\n    version = \"smooth\")\n    \n# plot the thematic map\nplot(rondonia_20LLQ_class_v2, legend_position = \"outside\")\n\n\n\nFigure 19.16: Final classification map after Bayesian smoothing with 7 x 7 window, using high smoothness values.\n\n\n\n\n\n\nIn the smoothed map, outliers inside forest areas and in the class borders have been removed. The salt-and-pepper effect associated to transitional classes has also been replaced by more coherent estimates. The smoothed map shown much improvements compared with the non-smoothed one."
  },
  {
    "objectID": "cl_smoothing.html#summary",
    "href": "cl_smoothing.html#summary",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.11 Summary",
    "text": "19.11 Summary\nPost-processing is a desirable step in any classification process. Bayesian smoothing improves the borders between the objects created by the classification and removes outliers that result from pixel-based classification. It is a reliable method that should be used in most situations."
  },
  {
    "objectID": "cl_smoothing.html#references",
    "href": "cl_smoothing.html#references",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nX. Huang, Q. Lu, L. Zhang, and A. Plaza, “New postprocessing methods for remote sensing image classification: A systematic study,” IEEE Transactions on Geoscience and Remote Sensing, vol. 52, no. 11, pp. 7140–7159, 2014.\n\n\n[2] \nK. Schindler, “An overview and comparison of smooth labeling methods for land-cover classification,” IEEE transactions on geoscience and remote sensing, vol. 50, no. 11, pp. 4534–4545, 2012.\n\n\n[3] \nG. Camara et al., “Bayesian Inference for Post-Processing of Remote-Sensing Image Classification,” Remote Sensing, vol. 16, no. 23, p. 4572, 2024, doi: 10.3390/rs16234572.\n\n\n[4] \nA. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin, Bayesian Data Analysis, Third Edition. CRC Press, 2014."
  },
  {
    "objectID": "cl_reclassification.html#introduction",
    "href": "cl_reclassification.html#introduction",
    "title": "\n20  Map reclassification\n",
    "section": "\n20.1 Introduction",
    "text": "20.1 Introduction\nReclassification of a remote sensing map refers to changing the classes assigned to different pixels in the image. The purpose of reclassification is to modify the information contained in the image to better suit a specific use case. In sits, reclassification involves assigning new classes to pixels based on additional information from a reference map. Users define rules according to the desired outcome. These rules are then applied to the classified map to produce a new map with updated classes."
  },
  {
    "objectID": "cl_reclassification.html#reclassifying-a-deforestation-map",
    "href": "cl_reclassification.html#reclassifying-a-deforestation-map",
    "title": "\n20  Map reclassification\n",
    "section": "\n20.2 Reclassifying a deforestation map",
    "text": "20.2 Reclassifying a deforestation map\nTo illustrate the reclassification in sits, we take a classified data cube stored in the sitsdata package. As discussed in Chapter Data cubes from local files, sits can create a data cube from a classified image file. Users need to provide the original data source and collection, the directory where data is stored (data_dir), the information on how to retrieve data cube parameters from file names (parse_info), and the labels used in the classification.\n\n\nR\nPython\n\n\n\n\n# Open classification map\ndata_dir &lt;- system.file(\"extdata/Rondonia-Class\", package = \"sitsdata\")\nrondonia_class &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    parse_info = c(\"satellite\", \"sensor\", \n                   \"tile\", \"start_date\", \"end_date\",\n                   \"band\", \"version\"),\n    bands = \"class\",\n    labels = c(\"1\" = \"Water\", \n               \"2\" = \"Clear_Cut_Burned_Area\", \n               \"3\" = \"Clear_Cut_Bare_Soil\",\n               \"4\" = \"Clear_Cut_Vegetation\", \n               \"5\" = \"Forest\", \n               \"6\" = \"Bare_Soil\", \n               \"7\" = \"Wetland\")\n)\n\nplot(rondonia_class, \n     legend_text_size = 0.7)\n\n\n\nFigure 20.1: Original classification map.\n\n\n\n\n\n\n# Open classification map\ndata_dir= r_package_dir(\"extdata/Rondonia-Class\", package = \"sitsdata\")\nrondonia_class = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    parse_info = (\"satellite\", \"sensor\", \n                   \"tile\", \"start_date\", \"end_date\",\n                   \"band\", \"version\"),\n    bands = \"class\",\n    labels = {\n        \"1\": \"Water\",\n        \"2\": \"Clear_Cut_Burned_Area\",\n        \"3\": \"Clear_Cut_Bare_Soil\",\n        \"4\": \"Clear_Cut_Vegetation\",\n        \"5\": \"Forest\",\n        \"6\": \"Bare_Soil\",\n        \"7\": \"Wetland\"\n    },\n    progress = False\n)\n\nplot(rondonia_class, \n     legend_text_size = 0.7)\n\n\n\nFigure 20.2: Original classification map.\n\n\n\n\n\n\nThe above map shows the total extent of deforestation by clear cuts estimated by the sits Random Forest model in an area in Rondonia, Brazil, based on a time series of Sentinel-2 images for the period 2020-06-04 to 2021-08-26. Suppose we want to estimate the deforestation that occurred from June 2020 to August 2021. We need a reference map containing information on forest cuts before 2020.\nIn this example, we use as a reference the PRODES deforestation map of Amazonia created by Brazil’s National Institute for Space Research (INPE). This map is produced by visual interpretation. PRODES measures deforestation every year, starting from August of one year to July of the following year. It contains classes that represent the natural world (Forest, Water, NonForest, and NonForest2) and classes that capture the yearly deforestation increments. These classes are named “dYYYY” and “rYYYY”; the first refers to deforestation in a given year (e.g., “d2008” for deforestation for August 2007 to July 2008); the second to places where the satellite data is not sufficient to determine the land class (e.g., “r2010” for 2010). This map is available on package sitsdata, as shown below.\n\n\nR\nPython\n\n\n\n\n# Set the directory for PRODES data \ndata_dir &lt;- system.file(\"extdata/PRODES\", package = \"sitsdata\")\n\n# Recover the PRODES classified cube\nprodes_2021 &lt;- sits_cube(\n    source = \"USGS\",\n    collection = \"LANDSAT-C2L2-SR\",\n    data_dir = data_dir,\n    parse_info = c(\"product\", \"sensor\", \n                   \"tile\", \"start_date\", \"end_date\",\n                   \"band\", \"version\"),\n    bands = \"class\",\n    version = \"v20220606\",\n    labels = c(\"1\" = \"Forest\", \"2\" = \"Water\", \"3\" = \"NonForest\",\n               \"4\" = \"NonForest2\", \"6\" = \"d2007\", \"7\" = \"d2008\",\n               \"8\" = \"d2009\", \"9\" = \"d2010\", \"10\" = \"d2011\", \n               \"11\" = \"d2012\", \"12\" = \"d2013\", \"13\" = \"d2014\", \n               \"14\" = \"d2015\", \"15\" = \"d2016\", \"16\" = \"d2017\",\n               \"17\" = \"d2018\", \"18\" = \"r2010\", \"19\" = \"r2011\",\n               \"20\" = \"r2012\", \"21\" = \"r2013\", \"22\" = \"r2014\", \n               \"23\" = \"r2015\", \"24\" = \"r2016\", \"25\" = \"r2017\", \n               \"26\" = \"r2018\", \"27\" = \"d2019\", \"28\" = \"r2019\", \n               \"29\" = \"d2020\", \"31\" = \"r2020\", \"32\" = \"Clouds2021\",\n               \"33\" = \"d2021\", \"34\" = \"r2021\")\n    )\n\n\n\n\n# Set the directory for PRODES data \ndata_dir = r_package_dir(\"extdata/PRODES\", package = \"sitsdata\")\n\n# Recover the PRODES classified cube\nprodes_2021 = sits_cube(\n    source = \"USGS\",\n    collection = \"LANDSAT-C2L2-SR\",\n    data_dir = data_dir,\n    parse_info = (\"product\", \"sensor\", \n                   \"tile\", \"start_date\", \"end_date\",\n                   \"band\", \"version\"),\n    bands = \"class\",\n    version = \"v20220606\",\n    labels = {\n        \"1\": \"Forest\", \"2\": \"Water\",\"3\": \"NonForest\",\n        \"4\": \"NonForest2\", \"6\": \"d2007\", \"7\": \"d2008\",\n        \"8\": \"d2009\", \"9\": \"d2010\", \"10\": \"d2011\",\n        \"11\": \"d2012\", \"12\": \"d2013\", \"13\": \"d2014\",\n        \"14\": \"d2015\", \"15\": \"d2016\", \"16\": \"d2017\",\n        \"17\": \"d2018\", \"18\": \"r2010\", \"19\": \"r2011\",\n        \"20\": \"r2012\", \"21\": \"r2013\", \"22\": \"r2014\",\n        \"23\": \"r2015\", \"24\": \"r2016\", \"25\": \"r2017\",\n        \"26\": \"r2018\", \"27\": \"d2019\", \"28\": \"r2019\",\n        \"29\": \"d2020\", \"31\": \"r2020\", \"32\": \"Clouds2021\",\n        \"33\": \"d2021\", \"34\": \"r2021\",\n        },\n    progress = False\n    )\n\n\n\n\nSince the labels of the deforestation map are specialized and are not part of the default sits color table, we define a legend for better visualization of the different deforestation classes.\n\n\nR\nPython\n\n\n\n\n# Use the RColorBrewer palette \"YlOrBr\" for the deforestation years\ncolors &lt;- grDevices::hcl.colors(n = 15, palette = \"YlOrBr\")\n\n# Define the legend for the deforestation map\ndef_legend &lt;- c(\n    \"Forest\" = \"forestgreen\", \"Water\" = \"dodgerblue3\", \n    \"NonForest\" = \"bisque2\", \"NonForest2\" = \"bisque2\",\n    \"d2007\" = colors[1],  \"d2008\" = colors[2],\n    \"d2009\" = colors[3],  \"d2010\" = colors[4], \n    \"d2011\" = colors[5],  \"d2012\" = colors[6],\n    \"d2013\" = colors[7],  \"d2014\" = colors[8],\n    \"d2015\" = colors[9],  \"d2016\" = colors[10],\n    \"d2017\" = colors[11], \"d2018\" = colors[12],\n    \"d2019\" = colors[13], \"d2020\" = colors[14], \n    \"d2021\" = colors[15], \"r2010\" = \"lightcyan\",\n    \"r2011\" = \"lightcyan\", \"r2012\"= \"lightcyan\", \n    \"r2013\" = \"lightcyan\", \"r2014\" = \"lightcyan\", \n    \"r2015\" = \"lightcyan\", \"r2016\" = \"lightcyan\", \n    \"r2017\" = \"lightcyan\", \"r2018\" = \"lightcyan\", \n    \"r2019\" = \"lightcyan\", \"r2020\" = \"lightcyan\",\n    \"r2021\" = \"lightcyan\", \"Clouds2021\" = \"lightblue2\")\n\n\n\n\n# Import matplotlib to handle colors\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\n# Use the palette \"YlOrBr\" for the deforestation years\ncmap = plt.get_cmap('YlOrBr', 15)\ncolors = [mcolors.to_hex(cmap(i)) for i in range(cmap.N)]\n\n# Define the legend for the deforestation map\ndef_legend = {\n    \"Forest\" : \"forestgreen\", \"Water\" : \"dodgerblue3\", \n    \"NonForest\" : \"bisque2\", \"NonForest2\" : \"bisque2\",\n    \"d2007\" : colors[0],  \"d2008\" : colors[1],\n    \"d2009\" : colors[2],  \"d2010\" : colors[3], \n    \"d2011\" : colors[4],  \"d2012\" : colors[5],\n    \"d2013\" : colors[6],  \"d2014\" : colors[7],\n    \"d2015\" : colors[8],  \"d2016\" : colors[9],\n    \"d2017\" : colors[10], \"d2018\" : colors[11],\n    \"d2019\" : colors[12], \"d2020\" : colors[13], \n    \"d2021\" : colors[14], \"r2010\" : \"lightcyan\",\n    \"r2011\" : \"lightcyan\", \"r2012\": \"lightcyan\", \n    \"r2013\" : \"lightcyan\", \"r2014\" : \"lightcyan\", \n    \"r2015\" : \"lightcyan\", \"r2016\" : \"lightcyan\", \n    \"r2017\" : \"lightcyan\", \"r2018\" : \"lightcyan\", \n    \"r2019\" : \"lightcyan\", \"r2020\" : \"lightcyan\",\n    \"r2021\" : \"lightcyan\", \"Clouds2021\" : \"lightblue2\"\n}\n\n\n\n\nUsing this new legend, we can visualize the PRODES deforestation map.\n\n\nR\nPython\n\n\n\n\nsits_view(prodes_2021, legend = def_legend)\n\n\n\n\nsits_view(prodes_2021, legend = def_legend)\n\n\n\n\n\n\n\n\nFigure 20.3: Deforestation map produced by PRODES.\n\n\n\nTaking the PRODES map as our reference, we can include new labels in the classified map produced by sits using sits_reclassify(). The new class “Deforestation_Mask” will be applied to all pixels that PRODES considers that have been deforested before July 2020. We also include a Non_Forest class to include all pixels that PRODES takes as not covered by native vegetation, such as wetlands and rocky areas. The PRODES classes will be used as a mask over the sits deforestation map.\nThe sits_reclassify() operation requires the parameters: (a) cube, the classified data cube whose pixels will be reclassified; (b) mask, the reference data cube used as a mask; (c) rules, a named list. The names of the rules list will be the new label. Each new label is associated with a mask vector that includes the labels of the reference map that will be joined. sits_reclassify() then compares the original and reference map pixel by pixel. For each pixel of the reference map whose labels are in one of the rules, the algorithm relabels the original map. The result will be a reclassified map with the original labels plus the new labels that have been masked using the reference map.\n\n\nR\nPython\n\n\n\n\n# Reclassify cube\nrondonia_def_2021 &lt;- sits_reclassify(\n    cube = rondonia_class,\n    mask = prodes_2021,\n    rules = list(\n        \"Non_Forest\" = mask %in% c(\"NonForest\", \"NonForest2\"),\n        \"Deforestation_Mask\" = mask %in% c(\n            \"d2007\", \"d2008\", \"d2009\",\n            \"d2010\", \"d2011\", \"d2012\",\n            \"d2013\", \"d2014\", \"d2015\",\n            \"d2016\", \"d2017\", \"d2018\",\n            \"d2019\", \"d2020\",\n            \"r2010\", \"r2011\", \"r2012\",\n            \"r2013\", \"r2014\", \"r2015\",\n            \"r2016\", \"r2017\", \"r2018\",\n            \"r2019\", \"r2020\", \"r2021\"),\n        \"Water\" = mask == \"Water\"),\n    memsize = 8,\n    multicores = 2,\n    output_dir = tempdir_r,\n    version = \"reclass\")\n\n# Plot the reclassified map\nplot(rondonia_def_2021,\n     legend_text_size = 0.7)\n\n\n\nFigure 20.4: Deforestation map by sits masked by PRODES map.\n\n\n\n\n\n\n# Reclassify cube\nrondonia_def_2021 = sits_reclassify(\n    cube = rondonia_class,\n    mask = prodes_2021,\n    rules = dict(\n        Non_Forest=MaskValue.in_([\"NonForest\", \"NonForest2\"]),\n        Deforestation_Mask=MaskValue.in_([\n            \"d2007\", \"d2008\", \"d2009\",\n            \"d2010\", \"d2011\", \"d2012\",\n            \"d2013\", \"d2014\", \"d2015\",\n            \"d2016\", \"d2017\", \"d2018\",\n            \"d2019\", \"d2020\",\n            \"r2010\", \"r2011\", \"r2012\",\n            \"r2013\", \"r2014\", \"r2015\",\n            \"r2016\", \"r2017\", \"r2018\",\n            \"r2019\", \"r2020\", \"r2021\"\n        ]),\n        Water=(MaskValue == \"Water\")\n    ),\n    memsize = 8,\n    multicores = 2,\n    output_dir = tempdir_py,\n    version = \"reclass\")\n\n# Plot the reclassified map\nplot(rondonia_def_2021,\n     legend_text_size = 0.7)\n\n\n\nFigure 20.5: Deforestation map by sits masked by PRODES map.\n\n\n\n\n\n\nThe reclassified map has been split into deforestation before mid-2020 (using the PRODES map) and the areas classified by sits that are taken as being deforested from mid-2020 to mid-2021. This allows experts to measure how much deforestation occurred in this period according to sits and compare the result with the PRODES map."
  },
  {
    "objectID": "cl_reclassification.html#summary",
    "href": "cl_reclassification.html#summary",
    "title": "\n20  Map reclassification\n",
    "section": "\n20.3 Summary",
    "text": "20.3 Summary\nIn this chapter, we describe a useful operation that can be applied to classified maps. The sits_reclassify() function is not restricted to comparing deforestation maps. It can be used in any case that requires masking of a result based on a reference map."
  },
  {
    "objectID": "cl_uncertainty.html#introduction",
    "href": "cl_uncertainty.html#introduction",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.1 Introduction",
    "text": "21.1 Introduction\nLand classification tasks have unique characteristics that differ from other machine learning domains, such as image recognition and natural language processing. The main challenge for land classification is to describe the diversity of the planet’s landscapes in a handful of labels. However, the diversity of the world’s ecosystem makes all classification systems to be biased approximations of reality. As stated by Murphy: “The gradation of properties in the world means that our smallish number of categories will never map perfectly onto all objects” [1]. For this reason, sits provides tools to improve classifications using a process called active learning."
  },
  {
    "objectID": "cl_uncertainty.html#active-learning",
    "href": "cl_uncertainty.html#active-learning",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.2 Active learning",
    "text": "21.2 Active learning\nActive learning is an iterative process of sample selection, labeling, and model retraining. The following steps provide a general overview of how to use active learning:\n\nCollect initial training samples: Start by collecting a small set of representative training samples that cover the range of land classes of interest.\nTrain a machine learning model: Use the initial training samples to train a machine learning model to classify remote sensing data.\nClassify the data cube using the model.\nIdentify areas of uncertainty.\nSelect samples for re-labeling: Select a set of unlabeled samples that the model is most uncertain about, i.e., those that the model is least confident in classifying.\nLabel the selected samples: The user labels the selected samples, adding them to the training set.\nRetrain the model: The model is retrained using the newly labeled samples, and the process repeats itself, starting at step 2.\nStop when the classification accuracy is satisfactory: The iterative process continues until the classification accuracy reaches a satisfactory level.\n\nIn traditional classification methods, experts provide a set of training samples and use a machine learning algorithm to produce a map. By contrast, the active learning approach puts the human in the loop [2]. At each iteration, an unlabeled set of samples is presented to the user, which assigns classes to them and includes them in the training set [3]. The process is repeated until the expert is satisfied with the result, as shown in Figure 21.1.\n\n\n\n\nFigure 21.1: Active learning approach (source: [3]).\n\n\n\nActive learning aims to reduce bias and errors in sample selection and, as such, improve the accuracy of the result. At each interaction, experts are asked to review pixels where the machine learning classifier indicates a high uncertainty value. Sources of classification uncertainty include missing classes and or mislabeled samples. In sits, active learning is supported by functions sits_uncertainty() and sits_uncertainty_sampling()."
  },
  {
    "objectID": "cl_uncertainty.html#measuring-uncertainty",
    "href": "cl_uncertainty.html#measuring-uncertainty",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.3 Measuring uncertainty",
    "text": "21.3 Measuring uncertainty\nUncertainty refers to the degree of doubt or ambiguity in the accuracy of the classification results. Several sources of uncertainty can arise during land classification using satellite data, including:\n\nClassification errors: These can occur when the classification algorithm misinterprets the spectral, spatial or temporal characteristics of the input data, leading to the misclassification of land classes.\nAmbiguity in classification schema: The definition of land classes can be ambiguous or subjective, leading to inconsistencies in the classification results.\nVariability in the landscape: Natural and human-induced variations in the landscape can make it difficult to accurately classify land areas.\nLimitations of the data: The quality and quantity of input data can influence the accuracy of the classification results.\n\nQuantifying uncertainty in land classification is important for ensuring that the results are reliable and valid for decision-making. Various methods, such as confusion and error matrices, can be used to estimate and visualize the level of uncertainty in classification results. Additionally, incorporating uncertainty estimates into decision-making processes can help to identify regions where further investigation or data collection is needed.\nThe function sits_uncertainty() calculates the uncertainty cube based on the probabilities produced by the classifier. It takes a probability cube as input. The uncertainty measure is relevant in the context of active learning. It helps to increase the quantity and quality of training samples by providing information about the model’s confidence. The supported types of uncertainty are ‘entropy’, ‘least’, ‘margin’, and ‘ratio’.\nLeast confidence sampling is the difference between no uncertainty (100% confidence) and the probability of the most likely class, normalized by the number of classes. Let \\(P_1(i)\\) be the higher class probability for pixel \\(i\\). Then least confidence sampling is expressed as\n\\[\n\\theta_{LC} = (1 - P_1(i)) * \\frac{n}{n-1}.\n\\]\nMargin of confidence is the difference between the two most confident predictions, expressed from 0% (no uncertainty) to 100% (maximum uncertainty). Let \\(P_1(i)\\) and \\(P_2(i)\\) be the two higher class probabilities for pixel \\(i\\). Then, the margin of confidence is expressed as\n\\[\n\\theta_{MC} = (P_1(i) - P_2(i)).\n\\]\nThe ratio of confidence is the measure of the ratio between the two most confident predictions, expressed in a range from 0% (no uncertainty) to 100% (maximum uncertainty). Let \\(P_1(i)\\) and \\(P_2(i)\\) be the two higher class probabilities for pixel \\(i\\). Then, the ratio of confidence is expressed as \\[\n\\theta_{RC} = \\frac{P_2(i)}{P_1(i)}.\n\\]\nEntropy is a measure of uncertainty used by Claude Shannon in his classic work “A Mathematical Theory of Communication”. It is related to the amount of variability in the probabilities associated with a pixel. The lower the variability, the lower the entropy. Let \\(P_k(i)\\) be the probability of class \\(k\\) for pixel \\(i\\). The entropy is calculated as \\[\n\\theta_{E} = \\frac{-\\sum_{k=1}^K P_k(i) * log_2(P_k(i))}{log_2{n}}.\n\\]\nThe parameters for sits_uncertainty() are: cube, a probability data cube; type, the uncertainty measure (default is least). As with other processing functions, multicores is the number of cores to run the function and memsize is the maximum overall memory (in GB) to run the function, output_dir is the output directory for image files, and version is the result version."
  },
  {
    "objectID": "cl_uncertainty.html#using-uncertainty-measures-for-active-learning",
    "href": "cl_uncertainty.html#using-uncertainty-measures-for-active-learning",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.4 Using uncertainty measures for active learning",
    "text": "21.4 Using uncertainty measures for active learning\nThe following case study shows how uncertainty measures can be used in the context of active learning. The study area is a subset of one Sentinel-2 tile in the state of Rondonia, Brazil. The work aims to detect deforestation in Brazilian Amazonia.\nThe study area is close to the Samuel Hydroelectric Dam, located on the Madeira River in the Brazilian state of Rondônia. Building the dam led to a loss of 56,000 ha of native forest. The dam’s construction caused the displacement of several indigenous communities and traditional populations, leading to social and cultural disruption. Additionally, flooding large forest areas resulted in losing habitats and biodiversity, including several endangered species. The dam has altered the natural flow of the Madeira River, leading to changes in water quality and temperature and affecting the aquatic life that depends on the river. The changes in river flow have also impacted the navigation and transportation activities of the local communities [4].\nThe first step is to produce a regular data cube for the chosen area from 2020-06-01 to 2021-09-01. To reduce processing time and storage, we use only three bands (B02, B8A, and B11) plus the cloud band, and take a small area inside the tile. After obtaining a regular cube, we plot the study area in two dates during the temporal interval of the data cube. The first image is taken at the beginning of the dry season in 2020-07-04, when the inundation area of the dam was covered by shallow water.\nLoad and regularize Sentinel-2 Data Cube\n\n\nR\nPython\n\n\n\n\n# Select a S2 tile\ns2_cube_ro &lt;- sits_cube(\n      source = \"AWS\",\n      collection = \"SENTINEL-S2-L2A-COGS\",\n      tiles = \"20LMR\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"SCL\"),\n      start_date = as.Date(\"2020-06-01\"),\n      end_date = as.Date(\"2021-09-01\"),\n      progress = FALSE\n)\n\n# Select a small area inside the tile\nroi = c(lon_max = -63.25790, lon_min = -63.6078, \n        lat_max = -8.72290, lat_min = -8.95630)\n\n# Regularize the small area cube\ns2_reg_cube_ro &lt;- sits_regularize(\n  cube = s2_cube_ro,\n  output_dir = tempdir_r,\n  res = 20,\n  roi = roi,\n  period = \"P16D\",\n  multicores = 4,\n  progress = FALSE\n)\n\n\n\n\n# Select a S2 tile\ns2_cube_ro = sits_cube(\n      source = \"AWS\",\n      collection = \"SENTINEL-S2-L2A-COGS\",\n      tiles = \"20LMR\",\n      bands = (\"B02\", \"B8A\", \"B11\", \"SCL\"),\n      start_date = \"2020-06-01\",\n      end_date = \"2021-09-01\",\n      progress = False\n)\n\n# Select a small area inside the tile\nroi = dict(lon_max = -63.25790, lon_min = -63.6078, \n        lat_max = -8.72290, lat_min = -8.95630)\n\n# Regularize the small area cube\ns2_reg_cube_ro = sits_regularize(\n  cube = s2_cube_ro,\n  output_dir = tempdir_py,\n  res = 20,\n  roi = roi,\n  period = \"P16D\",\n  multicores = 4,\n  progress = False\n)\n\n\n\n\nPlot Data Cube\n\n\nR\nPython\n\n\n\n\nplot(s2_reg_cube_ro, \n     red = \"B11\", \n     green = \"B8A\", \n     blue = \"B02\",\n     date = \"2020-07-04\")\n\n\n\nFigure 21.2: Area in Rondonia near Samuel dam in July 2020.\n\n\n\n\n\n\nplot(s2_reg_cube_ro, \n     red = \"B11\", \n     green = \"B8A\", \n     blue = \"B02\",\n     date = \"2020-07-04\")\n\n\n\nFigure 21.3: Area in Rondonia near Samuel dam in July 2020.\n\n\n\n\n\n\nThe second image is from 2020-11-09 and shows that most of the inundation area dries during the dry season. In early November 2020, after the end of the dry season, the inundation area is dry and has a response similar to bare soil and burned areas. The Madeira River can be seen running through the dried inundation area.\n\n\nR\nPython\n\n\n\n\nplot(s2_reg_cube_ro, \n     red = \"B11\", \n     green = \"B8A\", \n     blue = \"B02\", \n     date = \"2020-11-09\")\n\n\n\nFigure 21.4: Area in Rondonia near Samuel dam in November 2020.\n\n\n\n\n\n\nplot(s2_reg_cube_ro, \n     red = \"B11\", \n     green = \"B8A\", \n     blue = \"B02\", \n     date = \"2020-11-09\")\n\n\n\nFigure 21.5: Area in Rondonia near Samuel dam in November 2020.\n\n\n\n\n\n\nThe third image is from 2021-08-08. In early August 2021, after the wet season, the inundation area is again covered by a shallow water layer. Several burned and clear-cut areas can also be seen in the August 2021 image compared with the July 2020 one. Given the contrast between the wet and dry seasons, correct land classification of this area is hard.\n\n\nR\nPython\n\n\n\n\nplot(\n    s2_reg_cube_ro, \n    red = \"B11\", \n    green = \"B8A\", \n    blue = \"B02\", \n    date = \"2021-08-08\"\n)\n\n\n\nFigure 21.6: Area in Rondonia near Samuel dam in August 2021.\n\n\n\n\n\n\nplot(\n    s2_reg_cube_ro, \n    red = \"B11\", \n    green = \"B8A\", \n    blue = \"B02\", \n    date = \"2021-08-08\"\n)\n\n\n\nFigure 21.7: Area in Rondonia near Samuel dam in August 2021.\n\n\n\n\n\n\nThe next step is to classify this study area using a training set with 480 times series collected over the state of Rondonia (Brazil) for detecting deforestation. The training set uses 4 classes (Burned_Area, Forest,Highly_Degraded, and Cleared_Area). The cube is classified using a Random Forest model, post-processed by Bayesian smoothing, and then labeled.\n\n\nR\nPython\n\n\n\n\nset.seed(290356)\nlibrary(sitsdata)\n\n# Load the training set\ndata(\"samples_prodes_4classes\")\n\n# Select the same three bands used in the data cube\nsamples_4classes_3bands &lt;- sits_select(\n    data = samples_prodes_4classes, \n    bands = c(\"B02\", \"B8A\", \"B11\"))\n\n# Train a Random Forest model \nrfor_model &lt;- sits_train(\n    samples = samples_4classes_3bands, \n    ml_method = sits_rfor())\n\n# Classify the small area cube\ns2_cube_probs &lt;- sits_classify(\n    data = s2_reg_cube_ro,\n    ml_model = rfor_model,\n    output_dir = tempdir_r,\n    memsize = 15,\n    multicores = 5)\n\n# Post-process the probability cube\ns2_cube_bayes &lt;- sits_smooth(\n    cube = s2_cube_probs,\n    output_dir = tempdir_r,\n    memsize = 16,\n    multicores = 4)\n\n# Label the post-processed  probability cube\ns2_cube_label &lt;- sits_label_classification(\n    cube = s2_cube_bayes,\n    output_dir = tempdir_r,\n    memsize = 16,\n    multicores = 4)\n\nplot(s2_cube_label)\n\n\n\nFigure 21.8: First classified map for area in Rondonia near Samuel dam.\n\n\n\n\n\n\nr_set_seed(290356)\n\n# Load the training set\nsamples_prodes_4classes = load_samples_dataset(\n    name = \"samples_prodes_4classes\",\n    package = \"sitsdata\"\n)\n\n# Select the same three bands used in the data cube\nsamples_4classes_3bands = sits_select(\n    data = samples_prodes_4classes, \n    bands = (\"B02\", \"B8A\", \"B11\"))\n\n# Train a Random Forest model \nrfor_model = sits_train(\n    samples = samples_4classes_3bands, \n    ml_method = sits_rfor())\n\n# Classify the small area cube\ns2_cube_probs = sits_classify(\n    data = s2_reg_cube_ro,\n    ml_model = rfor_model,\n    output_dir = tempdir_py,\n    memsize = 15,\n    multicores = 5)\n\n# Post-process the probability cube\ns2_cube_bayes = sits_smooth(\n    cube = s2_cube_probs,\n    output_dir = tempdir_py,\n    memsize = 16,\n    multicores = 4)\n\n# Label the post-processed  probability cube\ns2_cube_label = sits_label_classification(\n    cube = s2_cube_bayes,\n    output_dir = tempdir_py,\n    memsize = 16,\n    multicores = 4)\n\nplot(s2_cube_label)\n\n\n\nFigure 21.9: First classified map for area in Rondonia near Samuel dam.\n\n\n\n\n\n\nThe resulting map correctly identifies the forest area and the deforestation. However, it misclassifies the area covered by the Samuel hydroelectric dam. The reason is the lack of samples for classes related to surface water and wetlands. To improve the classification, we need to improve our samples. To do that, the first step is to calculate the uncertainty of the classification.\n\n\nR\nPython\n\n\n\n\n# Calculate the uncertainty cube\ns2_cube_uncert &lt;- sits_uncertainty(\n    cube = s2_cube_bayes,\n    type = \"margin\",\n    output_dir = tempdir_r,\n    memsize = 16,\n    multicores = 4,\n    progress = TRUE)\n\nplot(s2_cube_uncert)\n\n\n\nFigure 21.10: Uncertainty map for classification in Rondonia near Samuel dam.\n\n\n\n\n\n\n# Calculate the uncertainty cube\ns2_cube_uncert = sits_uncertainty(\n    cube = s2_cube_bayes,\n    type_ = \"margin\",\n    output_dir = tempdir_py,\n    memsize = 16,\n    multicores = 4)\n\nplot(s2_cube_uncert)\n\n\n\nFigure 21.11: Uncertainty map for classification in Rondonia near Samuel dam.\n\n\n\n\n\n\nAs expected, the places of highest uncertainty are those covered by surface water or associated with wetlands. These places are likely to be misclassified. For this reason, sits provides sits_uncertainty_sampling(), which takes the uncertainty cube as its input and produces a tibble with locations in WGS84 with high uncertainty. The function has three parameters: n, the number of uncertain points to be included; min_uncert, the minimum value of uncertainty for pixels to be included in the list; and sampling_window, which defines a window where only one sample will be selected. The aim of sampling_window is to improve the spatial distribution of the new samples by avoiding points in the same neighborhood to be included. After running the function, we can use sits_view() to visualize the location of the samples.\n\n\nR\nPython\n\n\n\n\n# Find samples with high uncertainty\nnew_samples &lt;- sits_uncertainty_sampling(\n    uncert_cube = s2_cube_uncert,\n    n = 20,\n    min_uncert = 0.5,\n    sampling_window = 10)\n\n\n\n\n# Find samples with high uncertainty\nnew_samples = sits_uncertainty_sampling(\n    uncert_cube = s2_cube_uncert,\n    n = 20,\n    min_uncert = 0.5,\n    sampling_window = 10)\n\n\n\n\nView samples\n\n\nR\nPython\n\n\n\n\nsits_view(new_samples)\n\n\n\n\nsits_view(new_samples)\n\n\n\n\n\n\n\n\nFigure 21.12: Location of uncertain pixels for classification in Rondonia near Samuel dam.\n\n\n\nThe visualization shows that the samples are located in the areas covered by the Samuel data. Thus, we designate these samples as Wetlands. A more detailed evaluation, which is recommended in practice, requires analysing these samples with an exploration software such as QGIS and individually labeling each sample. In our case, we will take a direct approach for illustration purposes.\n\n\nR\nPython\n\n\n\n\n# Label the new samples\nnew_samples$label &lt;- \"Wetland\"\n\n# Obtain the time series from the regularized cube \nnew_samples_ts &lt;- sits_get_data(\n    cube = s2_reg_cube_ro,\n    samples = new_samples)\n\n# Join the new samples with the original ones with 4 classes\nsamples_round_2 &lt;- dplyr::bind_rows(\n    samples_4classes_3bands,\n    new_samples_ts)\n\n# Train a RF model with the new sample set\nrfor_model_v2 &lt;- sits_train(\n    samples = samples_round_2, \n    ml_method = sits_rfor())\n\n# Classify the small area cube\ns2_cube_probs_v2 &lt;- sits_classify(\n    data = s2_reg_cube_ro,\n    ml_model = rfor_model_v2,\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Post-process the probability cube\ns2_cube_bayes_v2 &lt;- sits_smooth(\n    cube = s2_cube_probs_v2,\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Label the post-processed  probability cube\ns2_cube_label_v2 &lt;- sits_label_classification(\n    cube = s2_cube_bayes_v2,\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Plot the second version of the classified cube\nplot(s2_cube_label_v2)\n\n\n\nFigure 21.13: Classified map for area in Rondonia near Samuel dam with added samples.\n\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n# Label the new samples\nnew_samples[\"label\"] = \"Wetland\"\n\n# Obtain the time series from the regularized cube \nnew_samples_ts = sits_get_data(\n    cube = s2_reg_cube_ro,\n    samples = new_samples)\n\n# Join the new samples with the original ones with 4 classes\nsamples_round_2 = pd.concat([\n    samples_4classes_3bands, \n    new_samples_ts\n])\n\n# Train a RF model with the new sample set\nrfor_model_v2 = sits_train(\n    samples = samples_round_2, \n    ml_method = sits_rfor())\n\n# Classify the small area cube\ns2_cube_probs_v2 = sits_classify(\n    data = s2_reg_cube_ro,\n    ml_model = rfor_model_v2,\n    output_dir = tempdir_py,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Post-process the probability cube\ns2_cube_bayes_v2 = sits_smooth(\n    cube = s2_cube_probs_v2,\n    output_dir = tempdir_py,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Label the post-processed  probability cube\ns2_cube_label_v2 = sits_label_classification(\n    cube = s2_cube_bayes_v2,\n    output_dir = tempdir_py,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Plot the second version of the classified cube\nplot(s2_cube_label_v2)\n\n\n\nFigure 21.14: Classified map for area in Rondonia near Samuel dam with added samples.\n\n\n\n\n\n\nThe results show a significant quality gain over the earlier classification. There are still some areas of confusion in the exposed soils inside the inundation area, some of which have been classified as burned areas. It is also useful to show the uncertainty map associated with the second model.\n\n\nR\nPython\n\n\n\n\n# Calculate the uncertainty cube\ns2_cube_uncert_v2 &lt;- sits_uncertainty(\n    cube = s2_cube_bayes_v2,\n    type = \"margin\",\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4,\n    progress = TRUE)\n\nplot(s2_cube_uncert_v2)\n\n\n\nFigure 21.15: Uncertainty map for classification in Rondonia near Samuel dam - improved model.\n\n\n\n\n\n\n# Calculate the uncertainty cube\ns2_cube_uncert_v2 = sits_uncertainty(\n    cube = s2_cube_bayes_v2,\n    type_ = \"margin\",\n    output_dir = tempdir_py,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\nplot(s2_cube_uncert_v2)\n\n\n\nFigure 21.16: Uncertainty map for classification in Rondonia near Samuel dam - improved model.\n\n\n\n\n\n\nAs the new uncertainty map shows, there is a significant improvement in the quality of the classification. The remaining areas of high uncertainty are those affected by the contrast between the wet and dry seasons close to the inundation area. These areas are low-laying places that sometimes are covered by water and sometimes are bare soil areas throughout the year, depending on the intensity of the rainy season. To further improve the classification quality, we could obtain new samples of those uncertain areas, label them, and add them to samples."
  },
  {
    "objectID": "cl_uncertainty.html#summary",
    "href": "cl_uncertainty.html#summary",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.5 Summary",
    "text": "21.5 Summary\nIn general, as this Chapter shows, combining uncertainty measurements with active learning is a recommended practice for improving classification results. This approach offers several important benefits for remote sensing image classification, especially in contexts where obtaining high-quality labeled data is expensive and time-consuming. Uncertainty maps direct expert to places where new samples are required. In this way, combnining uncertainty with active learning minimizes the number of samples that need to be manually labeled by selecting the most informative ones."
  },
  {
    "objectID": "cl_uncertainty.html#references",
    "href": "cl_uncertainty.html#references",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nG. L. Murphy, The Big Book of Concepts. Cambridge, MA, USA: Bradford Books, 2002.\n\n\n[2] \nR. Monarch, Human-in-the-Loop Machine Learning. Shelter Island, NY: Manning Publications, 2021.\n\n\n[3] \nM. M. Crawford, D. Tuia, and H. L. Yang, “Active Learning: Any Value for Classification of Remotely Sensed Data?” Proceedings of the IEEE, vol. 101, no. 3, pp. 593–608, 2013, doi: 10.1109/JPROC.2012.2231951.\n\n\n[4] \nP. M. Fearnside, “Brazil’s Samuel Dam: Lessons for Hydroelectric Development Policy and the Environment in Amazonia,” Environmental Management, vol. 35, no. 1, pp. 1–19, 2005, doi: 10.1007/s00267-004-0100-3."
  },
  {
    "objectID": "cl_ensembleprediction.html#introduction-to-ensemble-prediction",
    "href": "cl_ensembleprediction.html#introduction-to-ensemble-prediction",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.1 Introduction to ensemble prediction",
    "text": "22.1 Introduction to ensemble prediction\nEnsemble prediction is a powerful technique for combining predictions from multiple models to produce more accurate and robust predictions. Errors of individual models cancel out or are reduced when combined with the predictions of other models. As a result, ensemble predictions can lead to better overall accuracy and reduce the risk of overfitting. This can be especially useful when working with complex or uncertain data. By combining the predictions of multiple models, users can identify which features or factors are most important for making accurate predictions. When using ensemble methods, choosing diverse models with different sources of error is essential to ensure that the ensemble predictions are more precise and robust.\nThe sits package provides sits_combine_predictions() to estimate ensemble predictions using probability cubes produced by sits_classify() and optionally post-processed with sits_smooth(). There are two ways to make ensemble predictions from multiple models:\n\nAveraging: In this approach, the predictions of each model are averaged to produce the final prediction. This method works well when the models have similar accuracy and errors.\nUncertainty: Predictions from different models are compared in terms of their uncertainties on a pixel-by-pixel basis; predictions with lower uncertainty are chosen as being more likely to be valid.\n\nIn what follows, we will use the same sample dataset and data cube used in Chapter Image classification in data cubes to illustrate how to produce an ensemble prediction. The dataset samples_deforestation_rondonia consists of 6,007 samples collected from Sentinel-2 images covering the state of Rondonia. Each time series contains values from all Sentinel-2/2A spectral bands for year 2022 in 16-day intervals. The data cube is a subset of the Sentinel-2 tile “20LMR” which contains all spectral bands, plus spectral indices “NVDI”, “EVI” and “NBR” for the year 2022."
  },
  {
    "objectID": "cl_ensembleprediction.html#recovering-the-data-cube",
    "href": "cl_ensembleprediction.html#recovering-the-data-cube",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.2 Recovering the data cube",
    "text": "22.2 Recovering the data cube\nThe first step is to recover the data cube which is available in the sitsdata package, and to select only the spectral bands.\n\n\nR\nPython\n\n\n\n\n# Files are available in a local directory \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR/\", package = \"sitsdata\")\n\n# Read data cube\nro_cube_20LMR &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir\n)\n\n# reduce the number of bands\nro_cube_20LMR &lt;- sits_select(\n  data = ro_cube_20LMR,\n  bands = c(\n      \"B02\", \"B03\", \"B04\", \"B05\", \n      \"B06\", \"B07\", \"B08\", \"B11\", \n      \"B12\", \"B8A\"\n  )\n)\n\n# plot one time step of the cube\nplot(\n    ro_cube_20LMR, \n    blue = \"B02\", \n    green = \"B8A\", \n    red = \"B11\", \n    date = \"2022-08-17\"\n)\n\n\n\n\n# Files are available in a local directory \ndata_dir = r_package_dir(\"extdata/Rondonia-20LMR/\", package = \"sitsdata\")\n\n# Read data cube\nro_cube_20LMR = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir\n)\n\n# reduce the number of bands\nro_cube_20LMR = sits_select(\n  data = ro_cube_20LMR,\n  bands = (\n      \"B02\", \"B03\", \"B04\", \"B05\", \n      \"B06\", \"B07\", \"B08\", \"B11\", \n      \"B12\", \"B8A\"\n  )\n)\n\n# plot one time step of the cube\nplot(\n    ro_cube_20LMR, \n    blue = \"B02\", \n    green = \"B8A\", \n    red = \"B11\", \n    date = \"2022-08-17\"\n)\n\n\n\n\n\n\n\n\nFigure 22.1: Subset of Sentinel-2 tile 20LMR."
  },
  {
    "objectID": "cl_ensembleprediction.html#classification-using-random-forests",
    "href": "cl_ensembleprediction.html#classification-using-random-forests",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.3 Classification using Random Forests",
    "text": "22.3 Classification using Random Forests\nWe will train three models: Random Forests (RF), Light Temporal Attention Encoder (LTAE), and Temporal Convolution Neural Networks (TempCNN), classify the cube with them, and then combine their results. The example uses all spectral bands. We first run the RF classification.\nProbabilities\n\n\nR\nPython\n\n\n\n\n# train a Random Forest model\nrfor_model &lt;- sits_train(samples_deforestation_rondonia, sits_rfor())\n\n# classify the data cube\nro_cube_20LMR_rfor_probs &lt;- sits_classify(\n    ro_cube_20LMR, \n    rfor_model,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\n\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_rfor_probs, labels = \"Forest\")\n\n\n\n\n# load data\nsamples_deforestation_rondonia = load_samples_dataset(\n    name = \"samples_deforestation_rondonia\",\n    package = \"sitsdata\"\n)\n\n# train a Random Forest model\nrfor_model = sits_train(samples_deforestation_rondonia, sits_rfor())\n\n# classify the data cube\nro_cube_20LMR_rfor_probs = sits_classify(\n    ro_cube_20LMR, \n    rfor_model,\n    output_dir = tempdir_py, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\n\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_rfor_probs, labels = \"Forest\")\n\n\n\n\n\n\n\n\nFigure 22.2: Plot of probabilities for class Forest produced by Random Forest algorithm.\n\n\n\nVariance\n\n\nR\nPython\n\n\n\n\nro_cube_20LMR_rfor_variance &lt;- sits_variance(\n    ro_cube_20LMR_rfor_probs, \n    window_size = 9,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\nsummary(ro_cube_20LMR_rfor_variance)\n\n\n\n     Clear_Cut_Bare_Soil Clear_Cut_Burned_Area Clear_Cut_Vegetation  Forest\n75%                 4.62                  4.95                0.420  1.0800\n80%                 5.17                  5.38                0.510  1.4900\n85%                 5.57                  5.70                0.670  2.2115\n90%                 5.93                  6.03                1.261  4.4000\n95%                 6.64                  6.69                5.290  7.1000\n100%               17.31                 13.45               13.950 27.2900\n     Mountainside_Forest Riparian_Forest Seasonally_Flooded   Water Wetland\n75%                0.690          0.9100               0.37  0.5300    4.58\n80%                1.960          1.3800               0.47  0.5300    5.21\n85%                3.830          2.5815               0.61  1.1645    5.60\n90%                5.241          4.3600               0.95  2.3800    5.96\n95%                6.220          6.2000               2.19  7.4215    6.45\n100%              11.900         31.3200              18.27 42.9700   23.91\n\n\n\n\n\nro_cube_20LMR_rfor_variance = sits_variance(\n    ro_cube_20LMR_rfor_probs, \n    window_size = 9,\n    output_dir = tempdir_py, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\nsummary(ro_cube_20LMR_rfor_variance)\n\n\n\n      Clear_Cut_Bare_Soil  Clear_Cut_Burned_Area  Clear_Cut_Vegetation   Forest  Mountainside_Forest  Riparian_Forest  Seasonally_Flooded    Water  Wetland\n75%                  4.62                   4.95                 0.420   1.0800                0.690           0.9100                0.37   0.5300     4.58\n80%                  5.17                   5.38                 0.510   1.4900                1.960           1.3800                0.47   0.5300     5.21\n85%                  5.57                   5.70                 0.670   2.2115                3.830           2.5815                0.61   1.1645     5.60\n90%                  5.93                   6.03                 1.261   4.4000                5.241           4.3600                0.95   2.3800     5.96\n95%                  6.64                   6.69                 5.290   7.1000                6.220           6.2000                2.19   7.4215     6.45\n100%                17.31                  13.45                13.950  27.2900               11.900          31.3200               18.27  42.9700    23.91\n\n\n\n\n\nBased on the variance values, we apply the smoothness hyperparameter according to the recommendations proposed before. We choose values of \\(\\sigma^2_{k}\\) that reflect our prior expectation of the spatial patterns of each class. For classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area, to produce denser spatial clusters and remove “salt-and-pepper” outliers, we take \\(\\sigma^2_{k}\\) values in 95%-100% range. In the case of the most frequent classes Forest and Clear_Cut_Bare_Soil we want to preserve their original spatial shapes as much as possible; the same logic applies to less frequent classes Water and Wetland. For this reason, we set \\(\\sigma^2_{k}\\) values in the 75%-80% range for these classes. The class spatial patterns correspond to our prior expectations.\n\n\nR\nPython\n\n\n\n\nro_cube_20LMR_rfor_bayes &lt;- sits_smooth(\n    ro_cube_20LMR_rfor_probs,\n    output_dir = tempdir_r, \n    smoothness = c(  \n      \"Clear_Cut_Bare_Soil\" = 5.25, \n      \"Clear_Cut_Burned_Area\" = 15.0, \n      \"Clear_Cut_Vegetation\" =  12.0, \n      \"Forest\" = 1.8, \n      \"Mountainside_Forest\" = 6.5,\n      \"Riparian_Forest\" = 6.0, \n      \"Seasonally_Flooded\" = 3.5,\n      \"Water\" = 1.5, \n      \"Wetland\" = 5.5),\n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\n\nro_cube_20LMR_rfor_class &lt;- sits_label_classification(\n    ro_cube_20LMR_rfor_bayes,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\n\nplot(ro_cube_20LMR_rfor_class, \n      legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\nro_cube_20LMR_rfor_bayes = sits_smooth(\n    ro_cube_20LMR_rfor_probs,\n    output_dir = tempdir_py, \n    smoothness = dict(  \n      Clear_Cut_Bare_Soil = 5.25, \n      Clear_Cut_Burned_Area = 15.0, \n      Clear_Cut_Vegetation =  12.0, \n      Forest = 1.8, \n      Mountainside_Forest = 6.5,\n      Riparian_Forest = 6.0, \n      Seasonally_Flooded = 3.5,\n      Water = 1.5, \n      Wetland = 5.5),\n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\n\nro_cube_20LMR_rfor_class = sits_label_classification(\n    ro_cube_20LMR_rfor_bayes,\n    output_dir = tempdir_py, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\n\nplot(ro_cube_20LMR_rfor_class, \n      legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\n\n\n\n\nFigure 22.3: Plot of classified map produced by Random Forests algorithm."
  },
  {
    "objectID": "cl_ensembleprediction.html#classification-using-temporal-convolution-neural-networks-tempcnn",
    "href": "cl_ensembleprediction.html#classification-using-temporal-convolution-neural-networks-tempcnn",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.4 Classification using Temporal Convolution Neural Networks (TempCNN)",
    "text": "22.4 Classification using Temporal Convolution Neural Networks (TempCNN)\nThe next step is to classify the same area using a tempCNN algorithm, as shown below.\nProbabilities\n\n\nR\nPython\n\n\n\n\n# train a tempcnn model\ntcnn_model &lt;- sits_train(samples_deforestation_rondonia, \n                         sits_tempcnn())\n\n# classify the data cube\nro_cube_20LMR_tcnn_probs &lt;- sits_classify(\n    ro_cube_20LMR, \n    tcnn_model,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 8,\n    gpu_memory = 8,\n    version = \"tcnn\"\n)\n\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_tcnn_probs, labels = \"Forest\")\n\n\n\n\n# train a tempcnn model\ntcnn_model = sits_train(samples_deforestation_rondonia, \n                         sits_tempcnn())\n\n# classify the data cube\nro_cube_20LMR_tcnn_probs = sits_classify(\n    ro_cube_20LMR, \n    tcnn_model,\n    output_dir = tempdir_py, \n    multicores = 2,\n    memsize = 8,\n    gpu_memory = 8,\n    version = \"tcnn\"\n)\n\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_tcnn_probs, labels = \"Forest\")\n\n\n\n\n\n\n\n\nFigure 22.4: Plot of probabilities for class Forest produced by TempCNN algorithm.\n\n\n\nVariance\n\n\nR\nPython\n\n\n\n\nro_cube_20LMR_tcnn_variance &lt;- sits_variance(\n    ro_cube_20LMR_tcnn_probs,\n    window_size = 9,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"tcnn\"\n)\n\nsummary(ro_cube_20LMR_tcnn_variance)\n\n\n\n     Clear_Cut_Bare_Soil Clear_Cut_Burned_Area Clear_Cut_Vegetation Forest\n75%               1.1900                 1.150                 2.17  2.390\n80%               1.5200                 1.392                 2.54  3.050\n85%               2.0800                 1.730                 3.00  4.030\n90%               3.3800                 2.370                 3.80  5.821\n95%               6.9225                 3.820                 5.35 10.560\n100%             36.8900                22.630                32.26 44.410\n     Mountainside_Forest Riparian_Forest Seasonally_Flooded  Water Wetland\n75%                 1.40           2.600             2.5225  0.550  1.4400\n80%                 1.63           3.122             3.1000  0.680  1.7800\n85%                 1.90           3.910             4.1115  0.900  2.2600\n90%                 2.32           5.300             5.9500  1.390  3.0400\n95%                 3.16           8.440             9.6105  9.544  4.7205\n100%               11.21          56.980            41.7900 53.920 50.7400\n\n\n\n\n\nro_cube_20LMR_tcnn_variance = sits_variance(\n    ro_cube_20LMR_tcnn_probs,\n    window_size = 9,\n    output_dir = tempdir_py, \n    multicores = 6,\n    memsize = 24,\n    version = \"tcnn\"\n)\n\nsummary(ro_cube_20LMR_tcnn_variance)\n\n\n\n      Clear_Cut_Bare_Soil  Clear_Cut_Burned_Area  Clear_Cut_Vegetation  Forest  Mountainside_Forest  Riparian_Forest  Seasonally_Flooded   Water  Wetland\n75%                1.1900                  1.150                  2.17   2.390                 1.40            2.600              2.5225   0.550   1.4400\n80%                1.5200                  1.392                  2.54   3.050                 1.63            3.122              3.1000   0.680   1.7800\n85%                2.0800                  1.730                  3.00   4.030                 1.90            3.910              4.1115   0.900   2.2600\n90%                3.3800                  2.370                  3.80   5.821                 2.32            5.300              5.9500   1.390   3.0400\n95%                6.9225                  3.820                  5.35  10.560                 3.16            8.440              9.6105   9.544   4.7205\n100%              36.8900                 22.630                 32.26  44.410                11.21           56.980             41.7900  53.920  50.7400\n\n\n\n\n\nSmooth and label classification\n\n\nR\nPython\n\n\n\n\nro_cube_20LMR_tcnn_bayes &lt;- sits_smooth(\n    ro_cube_20LMR_tcnn_probs,\n    output_dir = tempdir_r, \n    window_size = 11,\n    smoothness = c(  \n      \"Clear_Cut_Bare_Soil\" = 1.5, \n      \"Clear_Cut_Burned_Area\" = 20.0, \n      \"Clear_Cut_Vegetation\" =  25.0, \n      \"Forest\" = 4.0, \n      \"Mountainside_Forest\" = 3.0,\n      \"Riparian_Forest\" = 40.0, \n      \"Seasonally_Flooded\" = 30.0,\n      \"Water\" = 1.0, \n      \"Wetland\" = 2.0),\n    multicores = 2,\n    memsize = 6,\n    version = \"tcnn\"\n)\n\nro_cube_20LMR_tcnn_class &lt;- sits_label_classification(\n    ro_cube_20LMR_tcnn_bayes,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 6,\n    version = \"tcnn\"\n)\n\nplot(ro_cube_20LMR_tcnn_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\nro_cube_20LMR_tcnn_bayes = sits_smooth(\n    ro_cube_20LMR_tcnn_probs,\n    output_dir = tempdir_py, \n    window_size = 11,\n    smoothness = dict(  \n      Clear_Cut_Bare_Soil = 1.5, \n      Clear_Cut_Burned_Area = 20.0, \n      Clear_Cut_Vegetation =  25.0, \n      Forest = 4.0, \n      Mountainside_Forest = 3.0,\n      Riparian_Forest = 40.0, \n      Seasonally_Flooded = 30.0,\n      Water = 1.0, \n      Wetland = 2.0),\n    multicores = 2,\n    memsize = 6,\n    version = \"tcnn\"\n)\n\nro_cube_20LMR_tcnn_class = sits_label_classification(\n    ro_cube_20LMR_tcnn_bayes,\n    output_dir = tempdir_py, \n    multicores = 2,\n    memsize = 6,\n    version = \"tcnn\"\n)\n\nplot(ro_cube_20LMR_tcnn_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\n\n\n\n\nFigure 22.5: Land classification in Rondonia using tempCNN."
  },
  {
    "objectID": "cl_ensembleprediction.html#classification-using-lightweight-temporal-attention-encoder-ltae",
    "href": "cl_ensembleprediction.html#classification-using-lightweight-temporal-attention-encoder-ltae",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.5 Classification using Lightweight Temporal Attention Encoder (LTAE)",
    "text": "22.5 Classification using Lightweight Temporal Attention Encoder (LTAE)\nThe third model is the Light Temporal Attention Encoder (LTAE), which has been discussed in the Machine Learning chapter.\nProbabilities\n\n\nR\nPython\n\n\n\n\n# train a tempcnn model\nltae_model &lt;- sits_train(samples_deforestation_rondonia, sits_lighttae())\n\n# classify the data cube\nro_cube_20LMR_ltae_probs &lt;- sits_classify(\n    ro_cube_20LMR, \n    ltae_model,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 8,\n    gpu_memory = 8,\n    version = \"ltae\"\n)\n\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_ltae_probs, labels = \"Forest\")\n\n\n\n\n# train a tempcnn model\nltae_model = sits_train(samples_deforestation_rondonia, sits_lighttae())\n\n# classify the data cube\nro_cube_20LMR_ltae_probs = sits_classify(\n    ro_cube_20LMR, \n    ltae_model,\n    output_dir = tempdir_py, \n    multicores = 2,\n    memsize = 8,\n    gpu_memory = 8,\n    version = \"ltae\"\n)\n\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_ltae_probs, labels = \"Forest\")\n\n\n\n\n\n\n\n\nFigure 22.6: Land classification in Rondonia using LTAE.\n\n\n\nWe then compute the variance of the probability cube produced by the LTAE algorithm.\nVariance\n\n\nR\nPython\n\n\n\n\nro_cube_20LMR_ltae_variance &lt;- sits_variance(\n    ro_cube_20LMR_ltae_probs,\n    window_size = 9,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"ltae\"\n)\n\nsummary(ro_cube_20LMR_ltae_variance)\n\n\n\n     Clear_Cut_Bare_Soil Clear_Cut_Burned_Area Clear_Cut_Vegetation Forest\n75%               0.7300                  0.51               2.2800   1.97\n80%               1.1100                  0.70               2.8320   2.86\n85%               1.8000                  1.00               3.5300   3.84\n90%               3.1710                  1.48               4.4310   5.07\n95%               5.7205                  2.50               5.7605   7.11\n100%             29.6300                 13.77              14.9000  24.17\n     Mountainside_Forest Riparian_Forest Seasonally_Flooded   Water Wetland\n75%                 0.42           1.010             3.6425  0.3000   0.660\n80%                 0.52           1.560             4.3800  0.3400   1.042\n85%                 0.67           2.470             5.2315  0.4000   1.880\n90%                 1.00           3.871             6.5210  0.5400   3.340\n95%                 1.68           6.080             8.7400  8.1905   6.360\n100%               12.74          24.430            31.3100 44.8400  43.080\n\n\n\n\n\nro_cube_20LMR_ltae_variance = sits_variance(\n    ro_cube_20LMR_ltae_probs,\n    window_size = 9,\n    output_dir = tempdir_py, \n    multicores = 6,\n    memsize = 24,\n    version = \"ltae\"\n)\n\nsummary(ro_cube_20LMR_ltae_variance)\n\n\n\n      Clear_Cut_Bare_Soil  Clear_Cut_Burned_Area  Clear_Cut_Vegetation  Forest  Mountainside_Forest  Riparian_Forest  Seasonally_Flooded    Water  Wetland\n75%                0.7300                   0.51                2.2800    1.97                 0.42            1.010              3.6425   0.3000    0.660\n80%                1.1100                   0.70                2.8320    2.86                 0.52            1.560              4.3800   0.3400    1.042\n85%                1.8000                   1.00                3.5300    3.84                 0.67            2.470              5.2315   0.4000    1.880\n90%                3.1710                   1.48                4.4310    5.07                 1.00            3.871              6.5210   0.5400    3.340\n95%                5.7205                   2.50                5.7605    7.11                 1.68            6.080              8.7400   8.1905    6.360\n100%              29.6300                  13.77               14.9000   24.17                12.74           24.430             31.3100  44.8400   43.080\n\n\n\n\n\nWe use the same rationale for selecting the smoothness parameter for the Bayesian smoothing operation as in the cases above.\nSmooth and label classification\n\n\nR\nPython\n\n\n\n\nro_cube_20LMR_ltae_bayes &lt;- sits_smooth(\n    ro_cube_20LMR_tcnn_probs,\n    output_dir = tempdir_r, \n    window_size = 11,\n    smoothness = c(  \n      \"Clear_Cut_Bare_Soil\" = 1.2, \n      \"Clear_Cut_Burned_Area\" = 10.0, \n      \"Clear_Cut_Vegetation\" = 15.0, \n      \"Forest\" = 4.0, \n      \"Mountainside_Forest\" = 8.0,\n      \"Riparian_Forest\" = 25.0, \n      \"Seasonally_Flooded\" = 30.0,\n      \"Water\" = 0.3, \n      \"Wetland\" = 1.8),\n    multicores = 2,\n    memsize = 6,\n    version = \"ltae\"\n)\n\n# generate the classified map\nro_cube_20LMR_ltae_class &lt;- sits_label_classification(\n    ro_cube_20LMR_ltae_bayes,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 6,\n    version = \"ltae\"\n)\n\n# plot the classified map\nplot(ro_cube_20LMR_ltae_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\nro_cube_20LMR_ltae_bayes = sits_smooth(\n    ro_cube_20LMR_tcnn_probs,\n    output_dir = tempdir_py, \n    window_size = 11,\n    smoothness = dict(  \n      Clear_Cut_Bare_Soil = 1.2, \n      Clear_Cut_Burned_Area = 10.0, \n      Clear_Cut_Vegetation = 15.0, \n      Forest = 4.0, \n      Mountainside_Forest = 8.0,\n      Riparian_Forest = 25.0, \n      Seasonally_Flooded = 30.0,\n      Water = 0.3, \n      Wetland = 1.8),\n    multicores = 2,\n    memsize = 6,\n    version = \"ltae\"\n)\n\n# generate the classified map\nro_cube_20LMR_ltae_class = sits_label_classification(\n    ro_cube_20LMR_ltae_bayes,\n    output_dir = tempdir_py, \n    multicores = 2,\n    memsize = 6,\n    version = \"ltae\"\n)\n\n# plot the classified map\nplot(ro_cube_20LMR_ltae_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\n\n\n\n\nFigure 22.7: Land classification in Rondonia using LTAE."
  },
  {
    "objectID": "cl_ensembleprediction.html#differences-between-model-results",
    "href": "cl_ensembleprediction.html#differences-between-model-results",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.6 Differences between model results",
    "text": "22.6 Differences between model results\nTo understand the differences between the results, it is useful to compare the resulting class areas produced by the different algorithms, expressed in \\(km^2\\).\n\n\nR\nPython\n\n\n\n\n# get the summary of the map produced by RF\nsum1 &lt;- summary(ro_cube_20LMR_rfor_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum1) &lt;- c(\"class\", \"rfor\")\n\n# get the summary of the map produced by TCNN\nsum2 &lt;- summary(ro_cube_20LMR_tcnn_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum2) &lt;- c(\"class\", \"tcnn\")\n\n# get the summary of the map produced by LTAE\nsum3 &lt;- summary(ro_cube_20LMR_ltae_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum3) &lt;- c(\"class\", \"ltae\")\n\n# compare class areas of maps produced by the three models\npred_class_areas &lt;- dplyr::inner_join(sum1, sum2, by = \"class\") |&gt; \n  dplyr::inner_join(sum3, by = \"class\")\n\npred_class_areas\n\n\n\n# A tibble: 9 × 4\n  class                     rfor    tcnn    ltae\n  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Clear_Cut_Bare_Soil    81       67      66    \n2 Clear_Cut_Burned_Area   1.6      3.8     3.9  \n3 Clear_Cut_Vegetation   19       22      22    \n4 Forest                280      240     240    \n5 Mountainside_Forest     0.0008   0.064   0.053\n6 Riparian_Forest        46       39      38    \n7 Seasonally_Flooded     72      130     130    \n8 Water                  63       68      67    \n9 Wetland                13       10      11    \n\n\n\n\n\nimport pandas as pd\n\n# Get classification summaries\nsum1 = summary(ro_cube_20LMR_rfor_class)\nsum2 = summary(ro_cube_20LMR_tcnn_class)\nsum3 = summary(ro_cube_20LMR_ltae_class)\n\n# Get only ``class`` and ``area_km2`` columns\nsum1 = sum1[[\"class\", \"area_km2\"]]\nsum2 = sum2[[\"class\", \"area_km2\"]]\nsum3 = sum3[[\"class\", \"area_km2\"]]\n\n# Rename columns\nsum1.columns = [\"class\", \"rfor\"]\nsum2.columns = [\"class\", \"tcnn\"]\nsum3.columns = [\"class\", \"ltae\"]\n\n# Merge columns\npred_class_areas = pd.merge(sum1, sum2, on = \"class\", how = \"inner\")\npred_class_areas = pd.merge(pred_class_areas, sum3, on = \"class\", how = \"inner\")\npred_class_areas\n\n\n\n                   class      rfor     tcnn     ltae\n1    Clear_Cut_Bare_Soil   81.0000   67.000   66.000\n2  Clear_Cut_Burned_Area    1.6000    3.800    3.900\n3   Clear_Cut_Vegetation   19.0000   22.000   22.000\n4                 Forest  280.0000  240.000  240.000\n5    Mountainside_Forest    0.0008    0.064    0.053\n6        Riparian_Forest   46.0000   39.000   38.000\n7     Seasonally_Flooded   72.0000  130.000  130.000\n8                  Water   63.0000   68.000   67.000\n9                Wetland   13.0000   10.000   11.000\n\n\n\n\n\nThe study area presents many challenges for land classification, given the presence of wetlands, riparian forests and seasonally-flooded areas. The results show the algorithms obtain quite different results, since each model has different sensitivities. The RF method is biased towards the most frequent classes, especially for Clear_Cut_Bare_Soil and Forest. The area estimated by RF for class Clear_Cut_Burned_Area is the smallest of the three models. Most pixels assigned by LTAE and TCNN as burned areas are assigned by RF as being areas of bare soil. The RF algorithm tends to be more conservative. The reason is because RF decision-making uses values from single attributes (values of a single band in a given time instance), while LTAE and TCNN consider the relations between instances of the time series. Since the RF model is sensitive to the response of images at the end of the period, it tends to focus on values that indicate the presence of forests and bare soils during the dry season, which peaks in August. The LTAE and TCNN models are more balanced to the overall separation of classes in the entire attribute space, and produces larger estimates of riparian and seasonally flooded forest than the other methods. In contrast, both LTAE and TCNN make more mistakes than RF in including flooded areas in the center-left part of the image on the left side of the rives as Clear_Cut_Vegetation when the right label would be riparian or flooded forests."
  },
  {
    "objectID": "cl_ensembleprediction.html#combining-the-different-predictions",
    "href": "cl_ensembleprediction.html#combining-the-different-predictions",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.7 Combining the different predictions",
    "text": "22.7 Combining the different predictions\nGiven the differences and complementaries between the three predicted outcomes, combining them using sits_combine_predictions() is useful. This function takes the following arguments: (a) cubes, a list of the cubes to be combined. These cubes should be probability cubes generated by which optionally may have been smoothened; (b) type, which indicates how to combine the probability maps. The options are average, which performs a weighted mean of the probabilities, and uncertainty, which uses the uncertainty cubes to combine the predictions; (c) weights, a vector of weights to be used to combine the predictions when average is selected; (d) uncertainty_cubes, a list of uncertainty cubes associated to the predictions; (e) multicores, number of cores to be used; (f) memsize, RAM used in the classification; (g) output_dir, the directory where the classified raster files will be written.\n\n\nR\nPython\n\n\n\n\n# Combine the three predictions by taking the average of the probabilities for each class\nro_cube_20LMR_average_probs &lt;- sits_combine_predictions(\n  cubes = list(ro_cube_20LMR_tcnn_bayes, \n               ro_cube_20LMR_rfor_bayes, \n               ro_cube_20LMR_ltae_bayes),\n  type = \"average\",\n  version = \"average-rfor-tcnn-ltae\",\n  output_dir = tempdir_r,\n  weights = c(0.33, 0.34, 0.33),\n  memsize = 16,\n  multicores = 4\n)\n\n# Label the average probability cube\nro_cube_20LMR_average_class &lt;- sits_label_classification(\n    cube = ro_cube_20LMR_average_probs,\n    output_dir = tempdir_r,\n    version = \"average-rfor-tcnn-ltae\",\n    memsize = 16,\n    multicores = 4\n)\n\n# plot the classified map\nplot(ro_cube_20LMR_average_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\n# Combine the three predictions by taking the average of the probabilities for each class\nro_cube_20LMR_average_probs = sits_combine_predictions(\n    cubes = [\n        ro_cube_20LMR_tcnn_bayes,\n        ro_cube_20LMR_rfor_bayes,\n        ro_cube_20LMR_ltae_bayes\n    ],\n    type = \"average\",\n    version = \"average-rfor-tcnn-ltae\",\n    output_dir = tempdir_py,\n    weights = (0.33, 0.34, 0.33),\n    memsize = 16,\n    multicores = 4\n)\n\n# Label the average probability cube\nro_cube_20LMR_average_class = sits_label_classification(\n    cube = ro_cube_20LMR_average_probs,\n    output_dir = tempdir_py,\n    version = \"average-rfor-tcnn-ltae\",\n    memsize = 16,\n    multicores = 4\n)\n\n# plot the classified map\nplot(ro_cube_20LMR_average_class,\n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n\n\n\n\n\n\nFigure 22.8: Land classification in Rondonia using the average of the probabilities produced by Random Forest, TempCNN and LTAE models.\n\n\n\nWe can also consider the class areas produced by the ensemble combination and compare them to the original estimates, expressed in \\(km^2\\).\n\n\nR\nPython\n\n\n\n\n# get the summary of the map produced by LTAE\nsum4 &lt;- summary(ro_cube_20LMR_average_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum4) &lt;- c(\"class\", \"ave\")\n\n# compare class areas of non-smoothed and smoothed maps\npred_ave_rfor_tcnn_ltae &lt;- dplyr::inner_join(sum1, sum2, by = \"class\") |&gt; \n  dplyr::inner_join(sum3, by = \"class\") |&gt; \n    dplyr::inner_join(sum4, by = \"class\")\n\npred_ave_rfor_tcnn_ltae\n\n\n\n# A tibble: 9 × 5\n  class                     rfor    tcnn    ltae     ave\n  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Clear_Cut_Bare_Soil    81       67      66      70    \n2 Clear_Cut_Burned_Area   1.6      3.8     3.9     3.2  \n3 Clear_Cut_Vegetation   19       22      22      19    \n4 Forest                280      240     240     250    \n5 Mountainside_Forest     0.0008   0.064   0.053   0.038\n6 Riparian_Forest        46       39      38      41    \n7 Seasonally_Flooded     72      130     130     120    \n8 Water                  63       68      67      67    \n9 Wetland                13       10      11      11    \n\n\n\n\n\n# get the summary of the map produced by LTAE\nsum4 = summary(ro_cube_20LMR_average_class)\nsum4 = sum4[[\"class\", \"area_km2\"]]\n\n# Rename columns\nsum4.columns = [\"class\", \"ave\"]\n\n# compare class areas of non-smoothed and smoothed maps\npred_ave_rfor_tcnn_ltae = pd.merge(sum1, sum2, on = \"class\", how = \"inner\")\npred_ave_rfor_tcnn_ltae = pd.merge(pred_ave_rfor_tcnn_ltae, sum3, on = \"class\", how = \"inner\")\npred_ave_rfor_tcnn_ltae = pd.merge(pred_ave_rfor_tcnn_ltae, sum4, on = \"class\", how = \"inner\")\npred_ave_rfor_tcnn_ltae\n\n\n\n                   class      rfor     tcnn     ltae      ave\n1    Clear_Cut_Bare_Soil   81.0000   67.000   66.000   70.000\n2  Clear_Cut_Burned_Area    1.6000    3.800    3.900    3.200\n3   Clear_Cut_Vegetation   19.0000   22.000   22.000   19.000\n4                 Forest  280.0000  240.000  240.000  250.000\n5    Mountainside_Forest    0.0008    0.064    0.053    0.038\n6        Riparian_Forest   46.0000   39.000   38.000   41.000\n7     Seasonally_Flooded   72.0000  130.000  130.000  120.000\n8                  Water   63.0000   68.000   67.000   67.000\n9                Wetland   13.0000   10.000   11.000   11.000\n\n\n\n\n\nAs expected, the ensemble map combines information from the three models. Taking the RF model prediction as a base, there is a reduction in the areas of classes Clear_Cut_Bare_Soil and Forest, confirming the tendency of the RF model to overemphasize the most frequent classes. The LTAE and TempCNN models are more sensitive to class variations and capture time-varying classes such as Riparian_Forest and Clear_Cut_Burned_Area in more detail than the RF model. However, both TempCNN and LTAE tend to confuse the deforestation-related class Clear_Cut_Vegetation and the natural class Riparian_Forest more than the RF model. This effect is evident in the left bank of the Madeira river in the centre-left region of the image. Also, both the LTAE and TempCNN maps are more grainy and have more spatial variability than the RF map.\nThe average map provides a compromise between RF’s strong empahsis on the most frequent classes and the tendency of deep learning methods to produce outliers based on temporal relationship. The average map is less grainy and more spatially consistent than the LTAE and TempCNN maps, while introducing variability which is not present in the RF map."
  },
  {
    "objectID": "cl_ensembleprediction.html#summary",
    "href": "cl_ensembleprediction.html#summary",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.8 Summary",
    "text": "22.8 Summary\nThis chapter shows the possibilities of ensemble prediction. There are many ways to get better results than those presented here. Increasing the number of spectral bands would improve the final accuracy. Also, Bayesian smoothing for deep learning models should not rely on default parameters; rather it needs to rely on variance analysis, increase the spatial window and provide more informed hyperparameters. In general, ensemble prediction should be consider in all situations where one is not satisfied with the results of individual models. Combining model output increses the reliability of the result and thus should be considered in all situations where similar classes are present."
  },
  {
    "objectID": "validation.html#basic-measures-of-accuracy",
    "href": "validation.html#basic-measures-of-accuracy",
    "title": "Validation and accuracy measurement",
    "section": "Basic measures of accuracy",
    "text": "Basic measures of accuracy\nThe metrics of producer’s accuracy, user’s accuracy, F1 score, and overall accuracy are derived from the confusion matrix (also known as the error matrix), which compares the classified map data to reference (ground truth) data. Each of these metrics provides a different perspective on the quality of classification.\nProducer’s Accuracy (PA) measures the probability that a reference (true) sample is correctly classified. It measures omission error — how often real features of a class are missed in the classification. It is measured the proportion of correctly classified instances relative to the total number of reference samples in a given class.\n\\[\n    \\text{PA}_i = \\frac{\\text{Correctly classified samples of class } i}{\\text{Total reference samples of class } i}\n    = \\frac{n_{ii}}{\\sum_j n_{ji}}\n\\]\nUser’s Accuracy (UA) is the probability that a sample classified as a given class actually belongs to that class in the reference data. It measures commission error — how often a class on the map includes misclassified pixels.\n\\[\n    \\text{UA}_i = \\frac{\\text{Correctly classified samples of class } i}{\\text{Total classified samples as class } i}\n    = \\frac{n_{ii}}{\\sum_j n_{ij}}\n\\]\nOverall Accuracy (OA) is the proportion of all samples that are correctly classified. It provides a general measure of classification success across all classes.\n\\[\n    \\text{OA} = \\frac{\\text{Correctly classified samples of class}}{\\text{Total classified samples}} = \\frac{\\sum_i n_{ii}}{N}\n\\]\nThe F1 score is a metric that balances user’s and producer’s accuracy into a single number. It is especially useful as a trade-off between false positives and false negatives. For a given class \\(i\\), the F1 score is the harmonic mean of UA and PA.\n\\[\nF1_i = 2 \\cdot \\frac{UA_i \\cdot PA_i}{UA_i + PA_i}\n\\]\nBy convention, we express the confusion matrix by placing the reference data (assumed to be the ground truth) in the columns and the respective map labels in the lines. In this way the UA and PA values are easily computed from the matrix.\n\n\nTable 1: Confusion matrix example with two classes\n\n\n\nReference class A\nReference class B\nUser Acc.\n\n\n\n\nMap class A\n40\n10\n0.80\n\n\nMap class B\n5\n45\n0.90\n\n\nProd Acc\n0.89\n0.82\n\n\n\nF1 score\n0.84\n0.86\n\n\n\n\n\nIn the data shown in Table 1, the producer’s accuracy (PA) for class A is 0.89 (40/45) and for class B is 0.82 (45/55). The user’s accuracy (UA) for class A is 0.80 (40/50) and for class B is 0.90 (45/50). The F1 score for class A is 0.84 and for class B is 0.87. The overall accuracy (OA) is 0.85 ((40 + 45) / 100)."
  },
  {
    "objectID": "validation.html#cross-validation",
    "href": "validation.html#cross-validation",
    "title": "Validation and accuracy measurement",
    "section": "Cross-validation",
    "text": "Cross-validation\nCross-validation is a widely used statistical technique for assessing the generalization performance of machine learning models. Its primary purpose is to provide an unbiased estimate of a model’s ability to perform on independent, unseen data, thereby helping to prevent overfitting.\nThe first step is to partition the available data into two distinct subsets: one for training and another for validation. The training set is used to develop the classification model, while the validation set is reserved exclusively for assessing the model’s performance. This separation is crucial to avoid biased accuracy estimates. Ideally, the partitioning should ensure that each land cover class is proportionally represented—this can be achieved through stratified random sampling.\nHowever, oerformance estimates obtained via cross-validation may not fully reflect the conditions encountered in real dara, especially when there is a significant distributional shift between the training data and real-world scenarios. Since this is a common situation is land data, measures of cross-validation are not a reliable prediction of map accuracy."
  },
  {
    "objectID": "validation.html#map-accuracy-measures",
    "href": "validation.html#map-accuracy-measures",
    "title": "Validation and accuracy measurement",
    "section": "Map accuracy measures",
    "text": "Map accuracy measures\nOnce the classifier is trained and the image is classified, a reliable set of reference data must be collected or prepared. This reference, or ground truth data, serves as the benchmark for assessing classification quality. It can be derived from field surveys, high-resolution satellite imagery, or expert interpretation, and it must be representative, accurate, and temporally consistent with the classified image.\nThe map accuracy assessment and area estimation procedures in sits follow the best practices outlined by Olofsson et al. [1], which provide a statistically rigorous framework for evaluating land cover and land change maps. These guidelines emphasize the importance of using probability sampling, transparent documentation, and unbiased estimation techniques to ensure that both classification accuracy and area statistics are reliable and reproducible.\nBy adhering to these principles, the assessment of classification results not only quantifies accuracy but also supports statistically sound area estimation, enabling robust and policy-relevant applications of remote sensing data."
  },
  {
    "objectID": "validation.html#references",
    "href": "validation.html#references",
    "title": "Validation and accuracy measurement",
    "section": "References",
    "text": "References\n\n\n\n\n[1] P. Olofsson, G. M. Foody, M. Herold, S. V. Stehman, C. E. Woodcock, and M. A. Wulder, “Good practices for estimating area and assessing accuracy of land change,” Remote Sensing of Environment, vol. 148, pp. 42–57, 2014."
  },
  {
    "objectID": "val_kfold.html#introduction",
    "href": "val_kfold.html#introduction",
    "title": "\n23  Cross-validation of training data\n",
    "section": "\n23.1 Introduction",
    "text": "23.1 Introduction\nCross-validation is a technique to estimate the inherent prediction error of a model [1]. Since cross-validation uses only the training samples, its results are not accuracy measures unless the samples have been carefully collected to represent the diversity of possible occurrences of classes in the study area [2]. In practice, when working in large areas, it is hard to obtain random stratified samples which cover the different variations in land classes associated with the ecosystems of the study area. Thus, cross-validation should be taken as a measure of model performance on the training data and not an estimate of overall map accuracy.\nCross-validation uses part of the available samples to fit the classification model and a different part to test it. The k-fold validation method splits the data into \\(k\\) partitions with approximately the same size and proceeds by fitting the model and testing it \\(k\\) times. At each step, we take one distinct partition for the test and the remaining \\({k-1}\\) for training the model and calculate its prediction error for classifying the test partition. A simple average gives us an estimation of the expected prediction error. The recommended choices of \\(k\\) are \\(5\\) or \\(10\\) [1]."
  },
  {
    "objectID": "val_kfold.html#using-k-fold-validation-in-sits",
    "href": "val_kfold.html#using-k-fold-validation-in-sits",
    "title": "\n23  Cross-validation of training data\n",
    "section": "\n23.2 Using k-fold validation in SITS",
    "text": "23.2 Using k-fold validation in SITS\nsits_kfold_validate() supports k-fold validation in sits. The result is the confusion matrix and the accuracy statistics (overall and by class). In the examples below, we use multiprocessing to speed up the results. The parameters of sits_kfold_validate are:\n\n\nsamples: training samples organized as a time series tibble;\n\nfolds: number of folds, or how many times to split the data (default = 5);\n\nml_method: ML/DL method to be used for the validation (default = Random Forest);\n\nmulticores: number of cores to be used for parallel processing (default = 2).\n\nBelow we show an example of cross-validation on the samples_matogrosso_mod13q1 dataset.\n\n\nR\nPython\n\n\n\n\nrfor_validate_mt &lt;- sits_kfold_validate(\n    samples = samples_matogrosso_mod13q1,\n    folds = 5,\n    ml_method = sits_rfor(),\n    multicores = 5\n)\n\nrfor_validate_mt\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Pasture Soy_Corn Soy_Millet Soy_Cotton Cerrado Forest Soy_Fallow\n  Pasture        340        3          6          0       0      0          0\n  Soy_Corn         1      346          7         17       0      0          0\n  Soy_Millet       0       10        164          0       0      0          2\n  Soy_Cotton       1        5          2        335       0      0          0\n  Cerrado          2        0          0          0     378      2          0\n  Forest           0        0          0          0       1    129          0\n  Soy_Fallow       0        0          1          0       0      0         85\n\nOverall Statistics\n                             \n Accuracy : 0.9673           \n   95% CI : ( 0.9582, 0.975 )\n                             \n    Kappa : 0.9606           \n\nStatistics by Class:\n\n                     Class: Pasture Class: Soy_Corn Class: Soy_Millet\nProd Acc (Recall)            0.9884          0.9505            0.9111\nUser Acc (Precision)         0.9742          0.9326            0.9318\nF1 score                     0.9812          0.9415            0.9213\n                     Class: Soy_Cotton Class: Cerrado Class: Forest\nProd Acc (Recall)               0.9517         0.9974        0.9847\nUser Acc (Precision)            0.9767         0.9895        0.9923\nF1 score                        0.9640         0.9934        0.9885\n                     Class: Soy_Fallow\nProd Acc (Recall)               0.9770\nUser Acc (Precision)            0.9884\nF1 score                        0.9827\n\n\n\n\n\n# Load samples\nsamples_matogrosso_mod13q1 = load_samples_dataset(\n    name = \"samples_matogrosso_mod13q1\", \n    package = \"sitsdata\"\n)\n\nrfor_validate_mt = sits_kfold_validate(\n    samples = samples_matogrosso_mod13q1,\n    folds = 5,\n    ml_method = sits_rfor(),\n    multicores = 5\n)\n\nrfor_validate_mt\n\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Pasture Soy_Corn Soy_Millet Soy_Cotton Cerrado Forest Soy_Fallow\n  Pasture        340        3          6          0       0      0          0\n  Soy_Corn         1      346          7         17       0      0          0\n  Soy_Millet       0       10        164          0       0      0          2\n  Soy_Cotton       1        5          2        335       0      0          0\n  Cerrado          2        0          0          0     378      2          0\n  Forest           0        0          0          0       1    129          0\n  Soy_Fallow       0        0          1          0       0      0         85\n\nOverall Statistics\n                             \n Accuracy : 0.9673           \n   95% CI : ( 0.9582, 0.975 )\n                             \n    Kappa : 0.9606           \n\nStatistics by Class:\n\n                     Class: Pasture Class: Soy_Corn Class: Soy_Millet\nProd Acc (Recall)            0.9884          0.9505            0.9111\nUser Acc (Precision)         0.9742          0.9326            0.9318\nF1 score                     0.9812          0.9415            0.9213\n                     Class: Soy_Cotton Class: Cerrado Class: Forest\nProd Acc (Recall)               0.9517         0.9974        0.9847\nUser Acc (Precision)            0.9767         0.9895        0.9923\nF1 score                        0.9640         0.9934        0.9885\n                     Class: Soy_Fallow\nProd Acc (Recall)               0.9770\nUser Acc (Precision)            0.9884\nF1 score                        0.9827\n\n\n\n\n\nThe results show a good validation, reaching 96% accuracy. However, this accuracy does not guarantee a good classification result. It only shows if the training data is internally consistent. In the next chapters, we present additional methods for measuring classification accuracy."
  },
  {
    "objectID": "val_kfold.html#summary",
    "href": "val_kfold.html#summary",
    "title": "\n23  Cross-validation of training data\n",
    "section": "\n23.3 Summary",
    "text": "23.3 Summary\nCross-validation measures how well the model fits the training data. Using these results to measure classification accuracy is only valid if the training data is a good sample of the entire dataset. Training data is subject to various sources of bias. In land classification, some classes are much more frequent than others, so the training dataset will be imbalanced. Regional differences in soil and climate conditions for large areas will lead the same classes to have different spectral responses. Field analysts may be restricted to places they have access (e.g., along roads) when collecting samples. An additional problem is mixed pixels. Expert interpreters select samples that stand out in fieldwork or reference images. Border pixels are unlikely to be chosen as part of the training data. For all these reasons, cross-validation results do not measure classification accuracy."
  },
  {
    "objectID": "val_kfold.html#references",
    "href": "val_kfold.html#references",
    "title": "\n23  Cross-validation of training data\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nT. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Data Mining, Inference, and Prediction. New York: Springer, 2009.\n\n\n[2] \nA. M. J.-C. Wadoux, G. B. M. Heuvelink, S. de Bruin, and D. J. Brus, “Spatial cross-validation is not the right way to evaluate map accuracy,” Ecological Modelling, vol. 457, p. 109692, 2021, doi: 10.1016/j.ecolmodel.2021.109692."
  },
  {
    "objectID": "val_map.html#introduction",
    "href": "val_map.html#introduction",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.1 Introduction",
    "text": "24.1 Introduction\nStatistically robust and transparent approaches for assessing accuracy are essential parts of the land classification process. The sits package supports the good practice recommendations for designing and implementing an accuracy assessment of a change map and estimating the area based on reference sample data. These recommendations address three components: sampling design, reference data collection, and accuracy estimates [1].\nCrucially, Olofsson et al. argue that area estimation should be based on the reference data, not solely on the classified map. This is because map-derived area estimates are often biased due to classification errors. By weighting the confusion matrix according to the sampling design, one can produce unbiased estimates of the area occupied by each land cover class, along with corresponding standard errors and confidence intervals. This correction is particularly important in studies where accurate area estimation informs policy or decision-making processes."
  },
  {
    "objectID": "val_map.html#example-data-set",
    "href": "val_map.html#example-data-set",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.2 Example data set",
    "text": "24.2 Example data set\nOur study area is the state of Rondonia (RO) in the Brazilian Amazon, which has a total area of . According to official Brazilian government statistics, as of 2021, there are of tropical forests in RO, which corresponds to 53% of the state’s total area. Significant human occupation started in 1970, led by settlement projects promoted by then Brazil’s military government [2]. Small and large-scale cattle ranching occupies most deforested areas. Deforestation in Rondonia is highly fragmented, partly due to the original occupation by small settlers. Such fragmentation poses considerable challenges for automated methods to distinguish between clear-cut and highly degraded areas. While visual interpreters rely upon experience and field knowledge, researchers must carefully train automated methods to achieve the same distinction.\nWe used Sentinel-2 and Sentinel-2A ARD (analysis ready) images from 2022-01-01 to 2022-12-31. Using all 10 spectral bands, we produced a regular data cube with a 16-day interval, with 23 instances per year. The best pixels for each period were selected to obtain as low cloud cover as possible. Persistent cloud cover pixels remaining in each period are then temporally interpolated to obtain estimated values. As a result, each pixel is associated with a valid time series. To fully cover RO, we used 41 MGRS tiles; the final data cube has 1.1 TB.\nThe work considered nine LUCC classes: (a) stable natural land cover, including Forest and Water; (b) events associated with clear-cuts, including Clear_Cut_Vegetation, Clear_Cut_Bare_Soil, and Clear_Cut_Burned_Area; (c) natural areas with seasonal variability, Wetland, Seasonally_Flooded_Forest, and Riparian_Forest; (d) stable forest areas subject to topographic effects, including Mountainside_Forest.\nIn this chapter, we will take the classification map as our starting point for accuracy assessment. This map can be retrieved from the sitsdata package as follows.\nLoad probabilities cube\n\n\nR\nPython\n\n\n\n\n# define the classes of the probability cube\nlabels &lt;- c(\"1\" = \"Clear_Cut_Bare_Soil\",\n            \"2\" = \"Clear_Cut_Burned_Area\", \n            \"3\" = \"Mountainside_Forest\", \n            \"4\" = \"Forest\", \n            \"5\" = \"Riparian_Forest\", \n            \"6\" = \"Clear_Cut_Vegetation\", \n            \"7\" = \"Water\",\n            \"8\" = \"Seasonally_Flooded\",\n            \"9\" = \"Wetland\")\n\n# directory where the data is stored \ndata_dir &lt;- system.file(\"extdata/Rondonia-Class-2022-Mosaic/\", package = \"sitsdata\")\n\n# create a probability data cube from a file \nrondonia_2022_class &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = \"class\",\n    labels = labels,\n    version = \"mosaic\"\n)\n\n\n\n\n# define the classes of the probability cube\nlabels = {\"1\"  : \"Clear_Cut_Bare_Soil\",\n            \"2\" : \"Clear_Cut_Burned_Area\", \n            \"3\" : \"Mountainside_Forest\", \n            \"4\" : \"Forest\", \n            \"5\" : \"Riparian_Forest\", \n            \"6\" : \"Clear_Cut_Vegetation\", \n            \"7\" : \"Water\",\n            \"8\" : \"Seasonally_Flooded\",\n            \"9\" : \"Wetland\"}\n\n# directory where the data is stored \ndata_dir = r_package_dir(\"extdata/Rondonia-Class-2022-Mosaic/\", package = \"sitsdata\")\n\n# create a probability data cube from a file \nrondonia_2022_class = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = \"class\",\n    labels = labels,\n    version = \"mosaic\"\n)\n\n\n\n\nPlot cube\n\n\nR\nPython\n\n\n\n\n# plot the classification map\nplot(rondonia_2022_class)\n\n\n\n\n# plot the classification map\nplot(rondonia_2022_class)\n\n\n\n\n\n\n\n\nFigure 24.1: Classified mosaic for land cover in Rondonia, Brazil for 2022."
  },
  {
    "objectID": "val_map.html#sampling-design",
    "href": "val_map.html#sampling-design",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.3 Sampling design",
    "text": "24.3 Sampling design\nA key recommendation is the use of probability-based sampling designs—such as stratified random sampling—for the selection of reference data. This approach guarantees that each sample unit has a known, non-zero probability of selection, enabling the derivation of statistically valid and unbiased estimates. Moreover, stratified sampling is particularly effective in improving the precision of area estimates, especially when class distributions are imbalanced.\nThe reference data used for validation must be collected independently of the classification process and should be carefully labeled according to a set of clear, mutually exclusive, and exhaustive class definitions. These definitions must be consistently applied across both map labels and reference labels to avoid ambiguities during comparison.\nSampling designs use established statistical methods aimed at providing unbiased estimates. Based on a chosen design, sits supports a selection of random samples per class. These samples should be evaluated accurately using high-quality reference data, ideally collected through field visits or using high-resolution imagery. In this way, we get a reference classification that is more accurate than the map classification being evaluated.\nFollowing the recommended best practices for estimating accuracy of LUCC maps [1], sits uses Cochran’s method for stratified random sampling [3]. The method divides the population into homogeneous subgroups, or strata, and then applying random sampling within each stratum. In the case of LUCC, we take the classification map as the basis for the stratification. The area occupied by each class is considered as an homogeneous subgroup. Cochran’s method for stratified random sampling helps to increase the precision of the estimates by reducing the overall variance, particularly when there is significant variability between strata but relatively less variability within each stratum.\nTo determine the overall number of samples to measure accuracy, we use the following formula [3]:\n\\[\nn = \\left( \\frac{\\sum_{i=1}^L W_i S_i}{S(\\hat{O})} \\right)^2\n\\] where\n\n\n\\(L\\) is the number of classes\n\n\\(S(\\hat{O})\\) is the expected standard error of the accuracy estimate, expressed as a proportion\n\n\\(S_i\\) is the standard deviation of stratum \\(i\\)\n\n\n\\(W_i\\) is is the mapped proportion of area of class \\(i\\)\n\n\nThe standard deviation per class (stratum) is estimated based on the expected user’s accuracy \\(U_i\\) for each class as\n\\[\nS_i = \\sqrt{U_i(1 - U_i)}\n\\]\nTherefore, the total number of samples depends on the assumptions about the user’s accuracies \\(U_i\\) and the expected standard error \\(S(\\hat{O})\\). Once the sample size is estimated, there are several methods for allocating samples per class [1]. One option is proportional allocation, when sample size in each stratum is proportional to the stratum’s size in the population. In land use mapping, some classes often have small areas compared to the more frequent ones. Using proportional allocation, rare classes will have small sample sizes decreasing their accuracy. Another option is equal allocation, where all classes will have the same number of samples; however, equal allocation may fail to capture the natural variation of classes with large areas.\nAs alternatives to proportional and equal allocation, [1] suggests ad-hoc approaches where each class is assigned a minimum number of samples. He proposes three allocations where 50, 75 and 100 sample units are allocated to the less common classes, and proportional allocation is used for more frequent ones. These allocation methods should be considered as suggestions, and users should be flexible to select alternative sampling designs.\nThe allocation methods proposed by [1] are supported by function sits_sampling_design(), which has the following parameters:\n\n\ncube: a classified data cube;\n\nexpected_ua: a named vector with the expected user’s accuracies for each class;\n\nalloc_options: fixed sample allocation for rare classes;\n\nstd_err: expected standard error of the accuracy estimate;\n\nrare_class_prop: proportional area limit to determine which are the rare classes.\n\nIn the case of Rondonia, the following sampling design was adopted.\n\n\nR\nPython\n\n\n\n\nro_sampling_design &lt;- sits_sampling_design(\n    cube = rondonia_2022_class,\n    expected_ua = c(\n        \"Clear_Cut_Bare_Soil\" = 0.75,\n        \"Clear_Cut_Burned_Area\" = 0.70, \n        \"Mountainside_Forest\" = 0.70, \n        \"Forest\" = 0.75,  \n        \"Riparian_Forest\" = 0.70, \n        \"Clear_Cut_Vegetation\" = 0.70,  \n        \"Water\" = 0.70, \n        \"Seasonally_Flooded\" = 0.70, \n         \"Wetland\" = 0.70\n    ),\n    alloc_options = c(120, 100),\n    std_err = 0.01,\n    rare_class_prop = 0.1\n)\n\n# show sampling design\nro_sampling_design\n\n\n\n                      prop        expected_ua std_dev equal alloc_120 alloc_100\nClear_Cut_Bare_Soil   0.3841309   0.75        0.433   210   438       496      \nClear_Cut_Burned_Area 0.004994874 0.7         0.458   210   120       100      \nMountainside_Forest   0.004555433 0.7         0.458   210   120       100      \nForest                0.538726    0.75        0.433   210   614       696      \nRiparian_Forest       0.005482552 0.7         0.458   210   120       100      \nClear_Cut_Vegetation  0.009201698 0.7         0.458   210   120       100      \nWater                 0.007682599 0.7         0.458   210   120       100      \nSeasonally_Flooded    0.007677294 0.7         0.458   210   120       100      \nWetland               0.03754864  0.7         0.458   210   120       100      \n                      alloc_prop\nClear_Cut_Bare_Soil   727       \nClear_Cut_Burned_Area 9         \nMountainside_Forest   9         \nForest                1019      \nRiparian_Forest       10        \nClear_Cut_Vegetation  17        \nWater                 15        \nSeasonally_Flooded    15        \nWetland               71        \n\n\n\n\n\nro_sampling_design = sits_sampling_design(\n    cube = rondonia_2022_class,\n    expected_ua = dict(\n        Clear_Cut_Bare_Soil = 0.75,\n        Clear_Cut_Burned_Area = 0.70, \n        Mountainside_Forest = 0.70, \n        Forest = 0.75,  \n        Riparian_Forest = 0.70, \n        Clear_Cut_Vegetation = 0.70,  \n        Water = 0.70, \n        Seasonally_Flooded = 0.70, \n        Wetland = 0.70\n    ),\n    alloc_options = (120, 100),\n    std_err = 0.01,\n    rare_class_prop = 0.1\n)\n\n# show sampling design\nro_sampling_design\n\n\n\n                      prop        expected_ua std_dev equal alloc_120 alloc_100\nClear_Cut_Bare_Soil   0.3841309   0.75        0.433   210   438       496      \nClear_Cut_Burned_Area 0.004994874 0.7         0.458   210   120       100      \nMountainside_Forest   0.004555433 0.7         0.458   210   120       100      \nForest                0.538726    0.75        0.433   210   614       696      \nRiparian_Forest       0.005482552 0.7         0.458   210   120       100      \nClear_Cut_Vegetation  0.009201698 0.7         0.458   210   120       100      \nWater                 0.007682599 0.7         0.458   210   120       100      \nSeasonally_Flooded    0.007677294 0.7         0.458   210   120       100      \nWetland               0.03754864  0.7         0.458   210   120       100      \n                      alloc_prop\nClear_Cut_Bare_Soil   727       \nClear_Cut_Burned_Area 9         \nMountainside_Forest   9         \nForest                1019      \nRiparian_Forest       10        \nClear_Cut_Vegetation  17        \nWater                 15        \nSeasonally_Flooded    15        \nWetland               71"
  },
  {
    "objectID": "val_map.html#stratified-random-sampling",
    "href": "val_map.html#stratified-random-sampling",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.4 Stratified random sampling",
    "text": "24.4 Stratified random sampling\nThe next step is to chose one of the options for sampling design to generate a set of points for stratified sampling. These points can then be used for accuracy assessment. This is achieved by function sits_stratified_sampling() which takes the following parameters:\n\n\ncube: a classified data cube;\n\nsampling_design: the output of function sits_sampling_design();\n\nalloc: one of the sampling allocation options produced by sits_sampling_design();\n\noverhead: additional proportion of number of samples per class (see below);\n\nmulticores: number of cores to run the function in parallel;\n\nshp_file: name of shapefile to save results for later use (optional);\n\nprogress: show progress bar?\n\nIn the example below, we chose the “alloc_120” option from the sampling design to generate a set of stratified samples. The output of the function is an sf object with points with location (latitude and longitude) and class assigned in the map. We can also generate a SHP file with the sample information. The script below shows how to use sits_stratified_sampling() and also how to convert an sf object to a SHP file.\nGenerate stratified samples\n\n\nR\nPython\n\n\n\n\nro_samples_sf &lt;- sits_stratified_sampling(\n    cube = rondonia_2022_class,\n    sampling_design = ro_sampling_design,\n    alloc = \"alloc_120\",\n    multicores = 4\n)\n\n\n\n\nro_samples_sf = sits_stratified_sampling(\n    cube = rondonia_2022_class,\n    sampling_design = ro_sampling_design,\n    alloc = \"alloc_120\",\n    multicores = 4\n)\n\n\n\n\nSave samples in a shapefile\n\n\nR\nPython\n\n\n\n\n# save sf object as SHP file\nsf::st_write(ro_samples_sf, \n    file.path(tempdir_r, \"ro_samples.shp\"), \n    append = FALSE\n)\n\n\n\n\n# save sf object as SHP file\nro_samples_sf.to_file(tempdir_py /  \"ro_samples.shp\")\n\n\n\n\nUsing the SHP file, users can visualize the points in a standard GIS such as QGIS. For each point, they will indicate what is the correct class. In this way, they will obtain a confusion matrix which will be used for accuracy assessment. The overhead parameter is useful for users to discard border or doubtful pixels where the interpreter cannot be confident of her class assignment. By discarding points whose attribution is uncertain, they will improve the quality of the assessment.\nAfter all sampling points are labelled in QGIS (or similar), users should produce a CSV file, a SHP file, a data frame, or an sf object, with at least three columns: latitude, longitude and label. See the next section for an example on how to use this data set for accuracay assessment."
  },
  {
    "objectID": "val_map.html#accuracy-assessment-of-classified-images",
    "href": "val_map.html#accuracy-assessment-of-classified-images",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.5 Accuracy assessment of classified images",
    "text": "24.5 Accuracy assessment of classified images\nTo measure the accuracy of classified images, sits_accuracy() uses an area-weighted technique, following the best practices proposed by Olofsson et al. [4]. The need for area-weighted estimates arises because the land classes are not evenly distributed in space. In some applications (e.g., deforestation) where the interest lies in assessing how much of the image has changed, the area mapped as deforested is likely to be a small fraction of the total area. If users disregard the relative importance of small areas where change is taking place, the overall accuracy estimate will be inflated and unrealistic. For this reason, Olofsson et al. argue that “mapped areas should be adjusted to eliminate bias attributable to map classification error, and these error-adjusted area estimates should be accompanied by confidence intervals to quantify the sampling variability of the estimated area” [4].\nWith this motivation, when measuring the accuracy of classified images, sits_accuracy() follows the procedure set by Olofsson et al. [4]. Given a classified image and a validation file, the first step calculates the confusion matrix in the traditional way, i.e., by identifying the commission and omission errors. Then it calculates the unbiased estimator of the proportion of area in cell \\(i,j\\) of the error matrix\n\\[\n\\hat{p_{i,j}} = W_i\\frac{n_{i,j}}{n_i},\n\\]\nwhere the total area of the map is \\(A_{tot}\\), the mapping area of class \\(i\\) is \\(A_{m,i}\\) and the proportion of area mapped as class \\(i\\) is \\(W_i = {A_{m,i}}/{A_{tot}}\\).\nAdjusting for area size allows producing an unbiased estimation of the total area of class \\(j\\), defined as a stratified estimator \\[\n\\hat{A_j} = A_{tot}\\sum_{i=1}^KW_i\\frac{n_{i,j}}{n_i}.\n\\]\nThis unbiased area estimator includes the effect of false negatives (omission error) while not considering the effect of false positives (commission error). The area estimates also allow for an unbiased estimate of the user’s and producer’s accuracy for each class. Following Olofsson et al. [4], we provide the 95% confidence interval for \\(\\hat{A_j}\\).\nTo produce the adjusted area estimates for classified maps, sits_accuracy() uses the following parameters:\n\n\ndata: a classified data cube;\n\nvalidation: a CSV file, SHP file, GPKG file, sf object or data frame containing at least three columns: latitude, longitude and label, containing a set of well-selected labeled points obtained from the samples suggested by sits_stratified_sample().\n\nIn the example below, we use a validation set produced by the researchers which produced the Rondonia data set, described above. We selected this data set both to serve as an example of sits_accuracy() and to illustrate the pitfalls of using visual interpretation of results of image time series classification. In this case, the validation team used an image from a single date late in 2022 to assess the results. This choice is not adequate for assessing results of time series classification. In many cases, including the example used in this chapter, the training set includes transitional classes such as Clear_Cut_Burned_Area and Clear_Cut_Vegetation. The associated samples refer to events that occur in specific times of the year. An area may start the year as a Forest land cover, only to be cut and burned during the peak of the dry season and later be completely clean. The classifier will recognize the signs of burned area and will signal that such event occurred. When using only a single date to evaluate the classification results, this correct estimate by the classifier will be missed by the interpreter. For this reason, the results shown below are merely illustrative and do not reflect a correct accuracy assessment.\nThe validation team used QGIS to produce a CSV file with validation data, which is then used to assess the area accuracy using the best practices recommended by [1].\n\n\nR\nPython\n\n\n\n\n# Get ground truth points\nvalid_csv &lt;- system.file(\n    \"extdata/Rondonia-Class-2022-Mosaic/rondonia_samples_validation.csv\", package = \"sitsdata\"\n)\n\n# Calculate accuracy according to Olofsson's method\narea_acc &lt;- sits_accuracy(rondonia_2022_class, \n                          validation = valid_csv,\n                          multicores = 4)\n\n# Print the area estimated accuracy \narea_acc\n\n\n\nArea Weighted Statistics\nOverall Accuracy = 0.84\n\nArea-Weighted Users and Producers Accuracy\n                      User Producer\nClear_Cut_Bare_Soil   0.82     1.00\nClear_Cut_Burned_Area 0.88     0.08\nMountainside_Forest   0.69     0.05\nForest                0.85     1.00\nRiparian_Forest       0.66     0.58\nClear_Cut_Vegetation  0.82     0.24\nWater                 0.97     0.67\nSeasonally_Flooded    0.86     0.68\nWetland               0.87     0.69\n\nMapped Area x Estimated Area (ha)\n                      Mapped Area (ha) Error-Adjusted Area (ha)\nClear_Cut_Bare_Soil          9537617.8                7787913.8\nClear_Cut_Burned_Area         124018.1                1383784.0\nMountainside_Forest           113107.2                1665469.0\nForest                      13376070.4               11377193.6\nRiparian_Forest               136126.7                 155704.6\nClear_Cut_Vegetation          228469.7                 766171.1\nWater                         190751.9                 275599.8\nSeasonally_Flooded            190620.2                 241225.8\nWetland                       932298.3                1176018.6\n                      Conf Interval (ha)\nClear_Cut_Bare_Soil            321996.87\nClear_Cut_Burned_Area          278746.61\nMountainside_Forest            299925.62\nForest                         333181.28\nRiparian_Forest                 60452.25\nClear_Cut_Vegetation           186476.04\nWater                           78786.79\nSeasonally_Flooded              58098.50\nWetland                        163726.86\n\n\n\n\n\n# Get ground truth points\nvalid_csv = r_package_dir(\n    \"extdata/Rondonia-Class-2022-Mosaic/rondonia_samples_validation.csv\", package = \"sitsdata\"\n)\n\n# Calculate accuracy according to Olofsson's method\narea_acc = sits_accuracy(rondonia_2022_class, \n                          validation = valid_csv,\n                          multicores = 4)\n                          \n# Print the area estimated accuracy \narea_acc\n\n\n\nArea Weighted Statistics\nOverall Accuracy = 0.84\n\nArea-Weighted Users and Producers Accuracy\n                      User Producer\nClear_Cut_Bare_Soil   0.82     1.00\nClear_Cut_Burned_Area 0.88     0.08\nMountainside_Forest   0.69     0.05\nForest                0.85     1.00\nRiparian_Forest       0.66     0.58\nClear_Cut_Vegetation  0.82     0.24\nWater                 0.97     0.67\nSeasonally_Flooded    0.86     0.68\nWetland               0.87     0.69\n\nMapped Area x Estimated Area (ha)\n                      Mapped Area (ha) Error-Adjusted Area (ha)\nClear_Cut_Bare_Soil          9537617.8                7787913.8\nClear_Cut_Burned_Area         124018.1                1383784.0\nMountainside_Forest           113107.2                1665469.0\nForest                      13376070.4               11377193.6\nRiparian_Forest               136126.7                 155704.6\nClear_Cut_Vegetation          228469.7                 766171.1\nWater                         190751.9                 275599.8\nSeasonally_Flooded            190620.2                 241225.8\nWetland                       932298.3                1176018.6\n                      Conf Interval (ha)\nClear_Cut_Bare_Soil            321996.87\nClear_Cut_Burned_Area          278746.61\nMountainside_Forest            299925.62\nForest                         333181.28\nRiparian_Forest                 60452.25\nClear_Cut_Vegetation           186476.04\nWater                           78786.79\nSeasonally_Flooded              58098.50\nWetland                        163726.86\n\n\n\n\n\nThe confusion matrix is also available, as follows.\n\n\nR\nPython\n\n\n\n\narea_acc$error_matrix\n\n                       \n                        Clear_Cut_Bare_Soil Clear_Cut_Burned_Area\n  Clear_Cut_Bare_Soil                   415                    65\n  Clear_Cut_Burned_Area                   1                    42\n  Mountainside_Forest                     1                     0\n  Forest                                  0                     0\n  Riparian_Forest                         4                     0\n  Clear_Cut_Vegetation                    1                    17\n  Water                                   0                     0\n  Seasonally_Flooded                      0                     0\n  Wetland                                 0                     2\n                       \n                        Mountainside_Forest Forest Riparian_Forest\n  Clear_Cut_Bare_Soil                     0      0               0\n  Clear_Cut_Burned_Area                   0      0               0\n  Mountainside_Forest                    22      9               0\n  Forest                                 95    680               3\n  Riparian_Forest                         4      5             111\n  Clear_Cut_Vegetation                    0      0               0\n  Water                                   0      0               3\n  Seasonally_Flooded                      0      0               1\n  Wetland                                 0      0               1\n                       \n                        Clear_Cut_Vegetation Water Seasonally_Flooded Wetland\n  Clear_Cut_Bare_Soil                     10     3                  1      15\n  Clear_Cut_Burned_Area                    1     0                  1       3\n  Mountainside_Forest                      0     0                  0       0\n  Forest                                  19     2                  0       3\n  Riparian_Forest                         43     0                  0       0\n  Clear_Cut_Vegetation                    82     0                  0       0\n  Water                                    0   121                  1       0\n  Seasonally_Flooded                       0     1                118      18\n  Wetland                                  4     0                  6      88\n\n\n\n\n\narea_acc.error_matrix\n\n\n\n                       \n                        Clear_Cut_Bare_Soil Clear_Cut_Burned_Area\n  Clear_Cut_Bare_Soil                   415                    65\n  Clear_Cut_Burned_Area                   1                    42\n  Mountainside_Forest                     1                     0\n  Forest                                  0                     0\n  Riparian_Forest                         4                     0\n  Clear_Cut_Vegetation                    1                    17\n  Water                                   0                     0\n  Seasonally_Flooded                      0                     0\n  Wetland                                 0                     2\n                       \n                        Mountainside_Forest Forest Riparian_Forest\n  Clear_Cut_Bare_Soil                     0      0               0\n  Clear_Cut_Burned_Area                   0      0               0\n  Mountainside_Forest                    22      9               0\n  Forest                                 95    680               3\n  Riparian_Forest                         4      5             111\n  Clear_Cut_Vegetation                    0      0               0\n  Water                                   0      0               3\n  Seasonally_Flooded                      0      0               1\n  Wetland                                 0      0               1\n                       \n                        Clear_Cut_Vegetation Water Seasonally_Flooded Wetland\n  Clear_Cut_Bare_Soil                     10     3                  1      15\n  Clear_Cut_Burned_Area                    1     0                  1       3\n  Mountainside_Forest                      0     0                  0       0\n  Forest                                  19     2                  0       3\n  Riparian_Forest                         43     0                  0       0\n  Clear_Cut_Vegetation                    82     0                  0       0\n  Water                                    0   121                  1       0\n  Seasonally_Flooded                       0     1                118      18\n  Wetland                                  4     0                  6      88\n\n\n\n\n\nThese results show the challenges of conducting validation assessments with image time series. While stable classes like Forest and Clear_Cut_Bare_Soil exhibit high user’s accuracy (UA) and producer’s accuracy (PA), the transitional classes (Clear_Cut_Burned_Area and Clear_Cut_Vegetation) have low PA. This discrepancy is not a true reflection of classification accuracy, but rather a result of inadequate visual interpretation practices. As mentioned earlier, the visual interpretation for quality assessment utilised only a single date, a method traditionally used for single images, but ineffective for image time series.\nA detailed examination of the confusion matrix reveals a clear distinction between natural areas (e.g., Forest and Riparian_Forest) and areas associated with deforestation (e.g., Clear_Cut_Bare_Soil and Clear_Cut_Burned_Area). The low producer’s accuracy values for transitional classes Clear_Cut_Burned_Area and Clear_Cut_Vegetation are artefacts of the validation procedure. Validation relied on only one date near the end of the calendar year, causing transitional classes to be overlooked."
  },
  {
    "objectID": "val_map.html#summary",
    "href": "val_map.html#summary",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.6 Summary",
    "text": "24.6 Summary\nThis chapter provides an example of the recommended statistical methods for designing stratified samples for accuracy assessment. However, these sampling methods depend on perfect or near-perfect validation by end-users. Ensuring best practices in accuracy assessment involves a well-designed sample set and a sample interpretation that aligns with the classifier’s training set."
  },
  {
    "objectID": "vector_datacubes.html",
    "href": "vector_datacubes.html",
    "title": "Vector data cubes",
    "section": "",
    "text": "The concept of vector data cubes represents a multidimensional data structure designed to manage, analyze, and query vector-based geospatial data across spatial, temporal, and thematic dimensions. This model extends the well-established framework of raster data cubes—commonly used in remote sensing and Earth observation—to the domain of vector data, which comprises geometries such as points, lines, and polygons that represent discrete spatial features.\nVector data cubes are structured to encapsulate geospatial features that evolve over time and carry associated attributes. Each element of the cube corresponds to a spatiotemporal observation of a vector object, characterized by its geometry, timestamp, and a set of descriptive attributes. Formally, a vector data cube can be described as a collection of tuples\n\\[\nVC = {(g_i, t_j, a_{i_j})}\n\\]\nwhere \\(g_i\\) denotes the spatial geometry, \\(t_j\\) are temporal instances, and \\(a_{i_j})\\) the vector of thematic attributes associated with that geometry at that point in time.\nThe spatial dimension captures the geometric representation of real-world entities (e.g., land parcels, administrative units), while the temporal dimension allows for tracking changes over time, and the attribute dimensions store thematic variables of interest (e.g., land use class, vegetation type). Vector data cubes are particularly advantageous in contexts where object-based spatial representations are more appropriate than gridded data.\nThe sits package supports a restricted version of vector data cubes. In sits, the polygons that make up a vector data cube are calculated using all temporal instances of the data cube. One of the key measures used to build vectors from raster data cube is the local similarity between pixel values. In sits, these similares are computed from all bands and all dates of the data cube.\nThis representation is useful for classification of land parcels whose boundaries have been computed by segmentation algorithms or which are part of a land cadastre. The basic idea is that sits will combine raster and vector information by classifying polygons based on time series defined by their interior pixels. In the following chapters, we will describe how sits can attach a vector data structure (e.g., a polygon shapefile) to an existing raster data cube. We will also show how to extract polygons based on segmentation methods such as SLIC superpixels algorithm."
  },
  {
    "objectID": "vec_obia.html#image-segmentation-in-sits",
    "href": "vec_obia.html#image-segmentation-in-sits",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "\n25.2 Image segmentation in sits",
    "text": "25.2 Image segmentation in sits\nThe first step of the OBIA procedure in sits is to select a data cube to be segmented and function that performs the segmentation. For this purpose, sits provides a generic sits_segment() function, which allows users to select different segmentation algorithms. The sits_segment() function has the following parameters:\n\n\ncube: a regular data cube.\n\nseg_fn: function to apply the segmentation\n\nroi: spatial region of interest in the cube\n\nstart_date: starting date for the space-time segmentation\n\nend_date: final date for the space-time segmentation\n\nmemsize: memory available for processing\n\nmulticores: number of cores available for processing\n\noutput_dir: output directory for the resulting cube\n\nversion: version of the result\n\nprogress: show progress bar?\n\nCurrently, there is only one segmentation function available (sits_slic) which implements the extended version of the Simple Linear Iterative Clustering (SLIC) which is described below. In future versions of sits, we expect to include additional functions that support spatio-temporal segmentation."
  },
  {
    "objectID": "vec_obia.html#simple-linear-iterative-clustering-algorithm",
    "href": "vec_obia.html#simple-linear-iterative-clustering-algorithm",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "\n25.3 Simple linear iterative clustering algorithm",
    "text": "25.3 Simple linear iterative clustering algorithm\nAfter building the multidimensional space, we use the Simple Linear Iterative Clustering (SLIC) algorithm [1] that clusters pixels to efficiently generate compact, nearly uniform superpixels. This algorithm has been adapted by Nowosad and Stepinski [2] to work with multispectral images. SLIC uses spectral similarity and proximity in the image space to segment the image into superpixels. Superpixels are clusters of pixels with similar spectral responses that are close together, which correspond to coherent object parts in the image. Here’s a high-level view of the extended SLIC algorithm:\n\nThe algorithm starts by dividing the image into a grid, where each cell of the grid will become a superpixel.\nFor each cell, the pixel in the center becomes the initial “cluster center” for that superpixel.\nFor each pixel, the algorithm calculates a distance to each of the nearby cluster centers. This distance includes both a spatial component (how far the pixel is from the center of the superpixel in terms of x and y coordinates) and a spectral component (how different the pixel’s spectral values are from the average values of the superpixel). The spectral distance is calculated using all the temporal instances of the bands.\nEach pixel is assigned to the closest cluster. After all pixels have been assigned to clusters, the algorithm recalculates the cluster centers by averaging the spatial coordinates and spectral values of all pixels within each cluster.\nSteps 3-4 are repeated for a set number of iterations, or until the cluster assignments stop changing.\n\nThe outcome of the SLIC algorithm is a set of superpixels which try to capture the to boundaries of objects within the image. The SLIC implementation in sits 1.4.1 uses the supercells R package [2]. The parameters for the sits_slic() function are:\n\n\ndist_fn: metric used to calculate the distance between values. By default, the “euclidean” metric is used. Alternatives include “jsd” (Jensen-Shannon distance), and “dtw” (dynamic time warping) or one of 46 distance and similarity measures implemented in the R package philentropy [3].\n\navg_fn: function to calculate a value of each superpixel. There are two internal functions implemented in C++ - “mean” and “median”. It is also possible to provide a user-defined R function that returns one value based on an R vector.\n\nstep: distance, measured in the number of cells, between initial superpixels’ centers.\n\ncompactness: A value that controls superpixels’ density. Larger values cause clusters to be more compact.\n\nminarea: minimal size of the output superpixels (measured in number of cells)."
  },
  {
    "objectID": "vec_obia.html#example-of-slic-based-segmentation-and-classification",
    "href": "vec_obia.html#example-of-slic-based-segmentation-and-classification",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "\n25.4 Example of SLIC-based segmentation and classification",
    "text": "25.4 Example of SLIC-based segmentation and classification\nTo show an example of SLIC-based segmentation, we first build a data cube, using images available in the sitsdata package.\n\n\nR\nPython\n\n\n\n\n# directory where files are located\ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR\", package = \"sitsdata\")\n\n# builds a cube based on existing files\ncube_20LMR &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = c(\"B02\", \"B8A\", \"B11\")\n)\n\n# Plot\nplot(\n    cube_20LMR, \n    red = \"B11\",\n    green = \"B8A\", \n    blue = \"B02\", \n    date = \"2022-07-16\"\n)\n\n\n\n\n# directory where files are located\ndata_dir = r_package_dir(\"extdata/Rondonia-20LMR\", package = \"sitsdata\")\n\n# Builds a cube based on existing files\ncube_20LMR = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = (\"B02\", \"B8A\", \"B11\")\n)\n\n# Plot\nplot(\n    cube_20LMR, \n    red = \"B11\",\n    green = \"B8A\", \n    blue = \"B02\", \n    date = \"2022-07-16\"\n)\n\n\n\n\n\n\n\n\nFigure 25.1: RGB composite image for part of tile 20LMR in Rondonia, Brasil.\n\n\n\nThe following example produces a segmented image. For the SLIC algorithm, we take the initial separation between cluster centres (step) to be 20 pixels, the compactness to be 1, and the minimum area for each superpixel (min_area) to be 20 pixels.\n\n\nR\nPython\n\n\n\n\n# segment a cube using SLIC\n# Files are available in a local directory \nsegments_20LMR &lt;- sits_segment(\n    cube = cube_20LMR,\n    output_dir = tempdir_r,\n    seg_fn = sits_slic(\n        step = 20,\n        compactness = 1,\n        dist_fun = \"euclidean\",\n        iter = 20,\n        minarea = 20\n    )\n)\n\n# Plot segments\nplot(segments_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", \n          date = \"2022-07-16\")\n\n\n\n\n# segment a cube using SLIC\n# Files are available in a local directory \nsegments_20LMR = sits_segment(\n    cube = cube_20LMR,\n    output_dir = tempdir_py,\n    seg_fn = sits_slic(\n        step = 20,\n        compactness = 1,\n        dist_fun = \"euclidean\",\n        iter = 20,\n        minarea = 20\n    )\n)\n\n# Plot segments\nplot(segments_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", \n          date = \"2022-07-16\")\n\n\n\n\n\n\n\n\nFigure 25.2: Image with segments for part of tile 20LMR in Rondonia, Brasil.\n\n\n\nIt is useful to visualize the segments in a leaflet together with the RGB image using sits_view().\n\n\nR\nPython\n\n\n\n\nsits_view(segments_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", \n          dates = \"2022-07-16\")\n\n\n\n\nsits_view(segments_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", \n          dates = \"2022-07-16\")\n\n\n\n\n\n\n\n\nFigure 25.3: Detail of segementation of image in Amazonia\n\n\n\nAfter obtaining the segments, the next step is to classify them. This is done by first training a classification model. This case study uses the training dataset samples_deforestation_rondonia, available in package sitsdata. This dataset consists of 6007 samples collected from Sentinel-2 images covering the state of Rondonia. There are nine classes: Clear_Cut_Bare_Soil, Clear_Cut_Burned_Area, Mountainside_Forest, Forest, Riparian_Forest, Clear_Cut_Vegetation, Water, Wetland, and Seasonally_Flooded. Each time series contains values from Sentinel-2/2A bands B02, B03, B04, B05, B06, B07, B8A, B08, B11 and B12, from 2022-01-05 to 2022-12-23 in 16-day intervals. The samples are intended to detect deforestation events and have used in earlier examples of the book. For the training, we select the same bands as those of the data cube.\n\n\nR\nPython\n\n\n\n\n# Select bands\nsamples_deforestation &lt;- sits_select(samples_deforestation_rondonia,\n                                     bands = c(\"B02\", \"B8A\", \"B11\"))\n\n# Train TempCNN\ntcnn_model &lt;- sits_train(samples_deforestation, sits_tempcnn())\n\n\n\n\n# Load data\nsamples_deforestation_rondonia = load_samples_dataset(\n    name = \"samples_deforestation_rondonia\",\n    package = \"sitsdata\"\n)\n\n# Select bands\nsamples_deforestation = sits_select(samples_deforestation_rondonia,\n                                     bands = (\"B02\", \"B8A\", \"B11\"))\n\n# Train TempCNN\ntcnn_model = sits_train(samples_deforestation, sits_tempcnn())\n\n\n\n\nThe segment classification procedure applies the model to a number of user-defined samples inside each segment. Each of these samples is then assigned a set of probability values, one for each class. We then obtain the median value of the probabilities for each class and normalize them. The output of the procedure is a vector data cube containing a set of classified segments. The most relevsnt parameters for the sits_classify() function are:\n\n\ndata: vector cube being classified\n\nml_model: machine learning model\n\noutput_dir: directory where results are stored\n\nn_sam_pol: number of samples to choose for each polygon\n\nmemsize: memory available for classification in GB.\n\nmulticores: number of cores to be used for classification\n\ngpu_memory: GPU memory available (if there is a GPU)\n\n\nversion: version name (to control for multiple versions)\n\n\n\nR\nPython\n\n\n\n\nsegments_20LMR_probs &lt;- sits_classify(\n    data = segments_20LMR,\n    ml_model = tcnn_model,\n    output_dir = tempdir_r,\n    n_sam_pol = 10,\n    gpu_memory = 2,\n    memsize = 24,\n    multicores = 1,\n    version = \"tcnn\"\n)\n\n\n\n\nsegments_20LMR_probs = sits_classify(\n    data = segments_20LMR,\n    ml_model = tcnn_model,\n    output_dir = tempdir_py,\n    n_sam_pol = 10,\n    gpu_memory = 2,\n    memsize = 24,\n    multicores = 1,\n    version = \"tcnn\"\n)\n\n\n\n\nAfter computing the probabilities for the segments, we can visualize them.\n\n\nR\nPython\n\n\n\n\nplot(segments_20LMR_probs, labels = \"Forest\")\n\n\n\n\nplot(segments_20LMR_probs, labels = \"Forest\")\n\n\n\n\n\n\n\n\nFigure 25.4: Probability maps for the Forest class inside segments.\n\n\n\nFinally, we can compute the most likely class for each of the segments.\n\n\nR\nPython\n\n\n\n\nsegments_20LMR_class &lt;- sits_label_classification(\n    segments_20LMR_probs,\n    output_dir = tempdir_r,\n    memsize = 24,\n    multicores = 6,\n    version = \"tcnn\"\n)\n\n\n\n\nsegments_20LMR_class = sits_label_classification(\n    segments_20LMR_probs,\n    output_dir = tempdir_py,\n    memsize = 24,\n    multicores = 6,\n    version = \"tcnn\"\n)\n\n\n\n\nTo view the classified segments together with the original image, use plot() or sits_view(), as in the following example.\n\n\nR\nPython\n\n\n\n\n# view the classified segments\nsits_view(\n    segments_20LMR_class, \n    red = \"B11\", \n    green = \"B8A\", \n    blue = \"B02\", \n    dates = \"2022-07-16\",\n)\n\n# put the original segments on top\nsits_view(segments_20LMR, add = TRUE)\n\n\n\n\n# view the classified segments\nsits_view(\n    segments_20LMR_class, \n    red = \"B11\", \n    green = \"B8A\", \n    blue = \"B02\", \n    dates = \"2022-07-16\",\n)\n\n# put the original segments on top\nsits_view(segments_20LMR, add = True)\n\n\n\n\n\n\n\n\nFigure 25.5: Detail of labeled segments for image in Rondonia, Brazil."
  },
  {
    "objectID": "vec_obia.html#summary",
    "href": "vec_obia.html#summary",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "\n25.5 Summary",
    "text": "25.5 Summary\nOBIA analysis applied to image time series is a worthy and efficient technique for land classification, combining the desirable sharp object boundary properties required by land use and cover maps with the analytical power of image time series."
  },
  {
    "objectID": "vec_obia.html#references",
    "href": "vec_obia.html#references",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nR. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk, “SLIC Superpixels Compared to State-of-the-Art Superpixel Methods,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 11, pp. 2274–2282, 2012, doi: 10.1109/TPAMI.2012.120.\n\n\n[2] \nJ. Nowosad and T. F. Stepinski, “Extended SLIC superpixels algorithm for applications to non-imagery geospatial rasters,” International Journal of Applied Earth Observation and Geoinformation, vol. 112, p. 102935, 2022, doi: 10.1016/j.jag.2022.102935.\n\n\n[3] \nH.-G. Drost, “Philentropy: Information Theory and Distance Quantification with R,” Journal of Open Source Software, vol. 3, no. 26, p. 765, 2018, doi: 10.21105/joss.00765."
  },
  {
    "objectID": "vec_creating.html#introduction",
    "href": "vec_creating.html#introduction",
    "title": "\n26  Creating vector data cube from local files\n",
    "section": "\n26.1 Introduction",
    "text": "26.1 Introduction\nIn many situations, user may want to create vector data cubes from local files. The most common case is when the want to recover a vector file has been previously created. An alternative is when users want to join a vector data file to a raster data cube. Recalll that sits uses a restricted notion of vector data cubes, which are single polygon sets that cover an entire period of time. One example are farm boundaries in case of agricultural statistics.\nTo be used for classification, vector cubes are associated to a raster cube. In this way, classification is done on a polygon-by-polygon basis. Given a polygon which matches part of a raster cube, sits uses pixels inside the polygon to obtain a set of time series that are used for classification, as shown in the example below.\nIn what follows, we illustrate the procedure of linking vector and raster files by creating segments from a raster data cube and then recovering them later."
  },
  {
    "objectID": "vec_creating.html#producing-a-vector-data-cube",
    "href": "vec_creating.html#producing-a-vector-data-cube",
    "title": "\n26  Creating vector data cube from local files\n",
    "section": "\n26.2 Producing a vector data cube",
    "text": "26.2 Producing a vector data cube\nWe take the MODIS data set for a small area in Mato Grosso, Brazil available in the sits package. For details on how to retrieve raster data cubes from local files, please see chapter “Data cubes from local files”.\n\n\nR\nPython\n\n\n\n\n# Retrieve a local cube based with MODIS data\n# data directory\ndata_dir &lt;- system.file(\"extdata/raster/mod13q1\", package = \"sits\")\n\n# local cube\nmodis_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\")\n)\n\n\n\n\n# Retrieve a local cube based with MODIS data\n# data directory\ndata_dir = r_package_dir(\"extdata/raster/mod13q1\", package = \"sits\")\n\n# local cube\nmodis_cube = sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir,\n    parse_info = (\"satellite\", \"sensor\", \"tile\", \"band\", \"date\")\n)\n\n\n\n\nThen, we produce a vector data cube associated to the raster data cube using sits_segment, as described in the previous chapter.\n\n\nR\nPython\n\n\n\n\n# segment the vector cube\nsegs_cube &lt;- sits_segment(\n    cube = modis_cube,\n    output_dir = tempdir_r \n)\n\n# plot the segmented cube\nplot(segs_cube)\n\n\n\n\n\n\n\n# segment the vector cube\nsegs_cube = sits_segment(\n    cube = modis_cube,\n    output_dir = tempdir_py\n)\n\n# plot the segmented cube\nplot(segs_cube)\n\n\n\n\n\n\n\nThe resulting vector file is stored in the tempdir directory, following a convention set by sits that vector files need to be stored in the geopackage format, and should contained closed polygons which are geometrically consistent. Listing the resulting file, one can better understand the convention.\n\n\nR\nPython\n\n\n\n\nlist.files(tempdir_r, pattern = \"*segments*\")\n\n[1] \"TERRA_MODIS_012010_2013-09-14_2014-08-29_segments_v1.gpkg\"\n\n\n\n\n\nlist(\n    tempdir_py.glob(pattern = \"*segments*\")\n)\n\n[PosixPath('/Users/gilbertocamara/sitsbook/tempdir/Python/vc_create/TERRA_MODIS_012010_2013-09-14_2014-08-29_segments_v1.gpkg')]\n\n\n\n\n\nThe segments file is a geopackage whose title contains optional information on satellite and sensor and mandatory information on tile, start_date, end_date, band (in this case segments) and version. Any geopackage polygon file that contains these required items can be imported and linked to a corresponding raster file, as shown below."
  },
  {
    "objectID": "vec_creating.html#linking-raster-and-vector-data",
    "href": "vec_creating.html#linking-raster-and-vector-data",
    "title": "\n26  Creating vector data cube from local files\n",
    "section": "\n26.3 Linking raster and vector data",
    "text": "26.3 Linking raster and vector data\nNow consider the inverse situation, where a set of a segments is already stored locally and users want to recover them and associate to a raster data cube. In the same way as we\nTo do so, they need to use sits_cube(), which the following minimum parameters:\n\n\nsource: data source of the raster data (in this case, the Brazil Data Cube).\n\ncollection: ARD collection associated to the images (in the example, `MOD13Q1-6.1”).\n\nraster_cube: raster data cube to associate with the polygons.\n\nvector_dir: directory where the vectors are stored.\n\nvector_band: kind of vector cube. Either segments for polygons, probs for class probabilities associated to each polygon, or class for the classified areas.\n\nparse_info: where to find metadata on the file title. The minimum required elements are tile, start_date, end_date, band and version. The start and end dates must match those of the raster cube.\n\ndelim: delimiter character for breaking down the file into parts (“_” by default.)\n\nIn the example below, we recover the file we produced before. We could replace it by any other polygon file in geopackage format that matches the parse_info information listed above. As satellite and sensor are optional, we use place holders from them.\n\n\nR\nPython\n\n\n\n\n# recover the local segmented cube\nlocal_segs_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir_r,\n    vector_band = \"segments\",\n    parse_info =  c(\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\n\n# plot the recover model and compare with previous plot\nplot(local_segs_cube)\n\n\n\n\n# recover the local segmented cube\nlocal_segs_cube = sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir_py,\n    vector_band = \"segments\",\n    parse_info = (\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\n\n# plot the recover model and compare with previous plot\nplot(local_segs_cube)\n\n\n\n\nThe resulting plot should be the same as the original one. We now train a model with the Random Forest algorithm, and use the result to classify the segments. Then we show how to recover the classified data from the local files which was produced by the classifier.\n\n\nR\nPython\n\n\n\n\n# classify the segments\n# create a Random Forest model\nrfor_model &lt;- sits_train(samples_modis_ndvi, sits_rfor())\nprobs_vector_cube &lt;- sits_classify(\n    data = segs_cube,\n    ml_model = rfor_model,\n    output_dir = tempdir_r,\n    n_sam_pol = 10\n)\n\n# plot cube\nplot(probs_vector_cube, labels = \"Forest\")\n\n\n\n\n\n\n\n# classify the segments\n# create a Random Forest model\nrfor_model = sits_train(samples_modis_ndvi, sits_rfor())\nprobs_vector_cube = sits_classify(\n    data = segs_cube,\n    ml_model = rfor_model,\n    output_dir = tempdir_py,\n    n_sam_pol = 10\n)\n\n# plot cube\nplot(probs_vector_cube, labels = \"Forest\")\n\n\n\n\n\n\n\nIn the same way as before, we can recover the probability cube from local files using sits_cube().\n\n\nR\nPython\n\n\n\n\n# recover vector cube\nlocal_probs_vector_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir_r,\n    vector_band = \"probs\",\n    parse_info =  c(\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\n\n# plot\nplot(local_probs_vector_cube, labels = \"Forest\")\n\n\n\n\n# recover vector cube\nlocal_probs_vector_cube = sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir_py,\n    vector_band = \"probs\",\n    parse_info =  (\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\n\n# plot\nplot(local_probs_vector_cube, labels = \"Forest\")\n\n\n\n\nFinally, we produce a classified map.\n\n\nR\nPython\n\n\n\n\n# label the segments\nclass_vector_cube &lt;- sits_label_classification(\n    cube = probs_vector_cube,\n    output_dir = tempdir(),\n)\nplot(class_vector_cube)\n\n\n\n\n\n\n\n# Import tempfile module\nimport tempfile\n\n# Define tempdir\ntempdir = tempfile.mkdtemp()\n\n# label the segments\nclass_vector_cube = sits_label_classification(\n    cube = probs_vector_cube,\n    output_dir = tempdir,\n)\n\n# plot\nplot(class_vector_cube)\n\n\n\n\n\n\n\nWe then recover the map from a local file.\n\n\nR\nPython\n\n\n\n\n# recover vector cube\nlocal_class_vector_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir(),\n    vector_band = \"class\",\n    parse_info =  c(\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\n\n# plot\nplot(local_class_vector_cube)\n\n\n\n\n\n\n\n# recover vector cube\nlocal_class_vector_cube = sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir,\n    vector_band = \"class\",\n    parse_info =  (\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\n\n# plot\nplot(local_class_vector_cube)"
  },
  {
    "objectID": "vec_creating.html#summary",
    "href": "vec_creating.html#summary",
    "title": "\n26  Creating vector data cube from local files\n",
    "section": "\n26.4 Summary",
    "text": "26.4 Summary\nIn this chapter, we show how mix vector and raster data cubes. For the operation to succeed, the vector cube must be a geopackage file whose polygons and contained inside the raster data cube and files need to provide adequate metadata. The polygon file can come from outside sources, provided it spatio-temporal coordinates match those of the raster data cube to which it is being merged. This allows much flexibility to users to combine external polygon files to sits."
  },
  {
    "objectID": "annex.html",
    "href": "annex.html",
    "title": "Advanced Topics",
    "section": "",
    "text": "The next chapters contains technical details on sits and are intended mainly for developers. It is intended to support those that want to understand how the package works and also want to contribute to its development. It includes general principles of sits software development, describes how parallel processing works. It shows how to support new STAC-based cloud collections and how to include new methods for machine learning in sits. It discusses how to export data to other packages and provides a comparison between SITS and Google Earth Engine."
  },
  {
    "objectID": "annex_export.html#r-formats",
    "href": "annex_export.html#r-formats",
    "title": "\n27  Exporting data to other packages\n",
    "section": "\n27.1 R formats",
    "text": "27.1 R formats\nThis section demonstrates how to export sits data to commonly used data structures in R, facilitating integration with other R packages.\n\n27.1.1 Exporting time series data to sf\nThe sf package is the backbone of geospatial vector processing in R [1]. To export time series tibbles from sits to sf, the function sits_as_sf() creates an sf POINT object with the locations of each sample and includes the time_series column as a list. Each row in the sf object contains the time series associated to the sample.\n\n# Export a sits tibble to sf\nsf_obj &lt;- sits_as_sf(samples_modis_ndvi)\n# Display the sf object\nsf_obj\n\nSimple feature collection with 1218 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -60.4875 ymin: -17.4373 xmax: -51.565 ymax: -9.3126\nGeodetic CRS:  WGS 84\n# A tibble: 1,218 × 9\n   crs                  geometry longitude latitude start_date end_date  \n * &lt;chr&gt;             &lt;POINT [°]&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;    \n 1 EPSG:4326 (-55.1852 -10.8378)     -55.2   -10.8  2013-09-14 2014-08-29\n 2 EPSG:4326   (-57.794 -9.7573)     -57.8    -9.76 2006-09-14 2007-08-29\n 3 EPSG:4326 (-51.9412 -13.4198)     -51.9   -13.4  2014-09-14 2015-08-29\n 4 EPSG:4326 (-55.9643 -10.0621)     -56.0   -10.1  2005-09-14 2006-08-29\n 5 EPSG:4326  (-54.554 -10.3749)     -54.6   -10.4  2013-09-14 2014-08-29\n 6 EPSG:4326 (-52.4572 -10.9512)     -52.5   -11.0  2013-09-14 2014-08-29\n 7 EPSG:4326 (-52.1443 -13.9981)     -52.1   -14.0  2013-09-14 2014-08-29\n 8 EPSG:4326 (-57.6907 -13.3382)     -57.7   -13.3  2015-09-14 2016-08-28\n 9 EPSG:4326 (-54.7034 -16.4265)     -54.7   -16.4  2015-09-14 2016-08-28\n10 EPSG:4326 (-56.7898 -11.4209)     -56.8   -11.4  2011-09-14 2012-08-28\n# ℹ 1,208 more rows\n# ℹ 3 more variables: label &lt;chr&gt;, cube &lt;chr&gt;, time_series &lt;list&gt;\n\n\n\n27.1.2 Exporting data cubes to stars\nThe stars R package handles spatiotemporal arrays. It provides a general framework for working with raster and vector data. Data cubes in sits can be converted to stars objects using sits_as_stars(). By default, the stars object will be loaded in memory. This can result in heavy memory usage. To produce a stars.proxy object, users have to select a single date, since stars does not allow proxy objects to be created with two dimensions.\n\n# create a data cube from a local set of TIFF files\n# this is a cube with 23 instances and one band (\"NDVI\")\ndata_dir &lt;- system.file(\"extdata/raster/mod13q1\", package = \"sits\")\ncube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir\n)\nstars_object &lt;- sits_as_stars(cube)\n# plot the first date of the stars object\nplot(stars_object[,,,,1])\n\n\n\n\n\n27.1.3 Exporting data cubes to terra\nThe terra package in R is a high-performance framework for spatial raster and vector data analysis. It was developed as the successor to the older raster package, offering a faster, more memory-efficient, and flexible API for working with geographic data. To export data cubes to terra, sits uses sits_as_terra() function which takes information about files, bands and dates in a data cube to produce an object of class SpatRaster in terra. Because terra does not understand multi-tiles and multi-temporal cubes, users have to select a tile and a date from the data cube. By default, all bands are included in the terra object, but users can also select which bands to export.\n\n# create a data cube from a local set of TIFF files\n# this is a cube with 23 instances and one band (\"NDVI\")\ndata_dir &lt;- system.file(\"extdata/raster/mod13q1\", package = \"sits\")\ncube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir\n)\nterra_object &lt;- sits_as_terra(cube, date = \"2013-09-14\")\n# plot the first date of the stars object\nterra_object\n\nclass       : SpatRaster \ndimensions  : 147, 255, 1  (nrow, ncol, nlyr)\nresolution  : 231.6564, 231.6564  (x, y)\nextent      : -6073798, -6014726, -1312333, -1278280  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \nsource      : TERRA_MODIS_012010_NDVI_2013-09-14.jp2 \nname        : TERRA_MODIS_012010_NDVI_2013-09-14"
  },
  {
    "objectID": "annex_export.html#python-formats",
    "href": "annex_export.html#python-formats",
    "title": "\n27  Exporting data to other packages\n",
    "section": "\n27.2 Python formats",
    "text": "27.2 Python formats\nThis section presents the available options for Python users to export sits data to various formats for further analysis and interoperability\n\n27.2.1 Exporting time series and data cubes to xarray\nThe xarray library is a core tool for labeled multi-dimensional arrays in Python and it is widely used for handling various types of data, including time-series and Earth Observation Data Cubes. This section presents how to convert sits objects to xarray.\nExporting data cubes\nData cubes in sits can be converted to xarray objects using sits_as_xarray(). By default, the xarray object will be loaded in memory. This can result in heavy memory usage.\n\nfrom pysits import sits_cube, sits_as_xarray\nfrom pysits import r_package_dir\n\n# Create a data cube from a local set of TIFF files\n# this is a cube with 23 instances and one band (\"NDVI\")\ndata_dir = r_package_dir(\"extdata/raster/mod13q1\", package = \"sits\")\n\n# Load cube\ncube = sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir\n)\n\n# Transform to xarray\nxcube = sits_as_xarray(cube)\nxcube\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:      (time: 12, y: 148, x: 255)\nCoordinates:\n  * time         (time) datetime64[ns] 96B 2013-09-14 2013-10-16 ... 2014-08-29\n  * y            (y) float64 1kB -1.278e+06 -1.279e+06 ... -1.312e+06 -1.312e+06\n  * x            (x) float64 2kB -6.074e+06 -6.073e+06 ... -6.015e+06 -6.015e+06\n    spatial_ref  int64 8B 0\nData variables:\n    NDVI         (time, y, x) float32 2MB dask.array&lt;chunksize=(1, 148, 255), meta=np.ndarray&gt;\n\nxarray.Dataset\n\n\nDimensions:\n\ntime: 12\n\ny: 148\n\nx: 255\n\n\n\n\nCoordinates: (4)\n\n\ntime\n(time)\ndatetime64[ns]\n2013-09-14 ... 2014-08-29\n\narray(['2013-09-14T00:00:00.000000000', '2013-10-16T00:00:00.000000000',\n       '2013-11-17T00:00:00.000000000', '2013-12-19T00:00:00.000000000',\n       '2014-01-17T00:00:00.000000000', '2014-02-18T00:00:00.000000000',\n       '2014-03-22T00:00:00.000000000', '2014-04-23T00:00:00.000000000',\n       '2014-05-25T00:00:00.000000000', '2014-06-26T00:00:00.000000000',\n       '2014-07-28T00:00:00.000000000', '2014-08-29T00:00:00.000000000'],\n      dtype='datetime64[ns]')\n\n\ny\n(y)\nfloat64\n-1.278e+06 ... -1.312e+06\n\narray([-1278395.61308 , -1278627.269438, -1278858.925796, -1279090.582154,\n       -1279322.238513, -1279553.894871, -1279785.551229, -1280017.207587,\n       -1280248.863946, -1280480.520304, -1280712.176662, -1280943.83302 ,\n       -1281175.489379, -1281407.145737, -1281638.802095, -1281870.458454,\n       -1282102.114812, -1282333.77117 , -1282565.427528, -1282797.083887,\n       -1283028.740245, -1283260.396603, -1283492.052961, -1283723.70932 ,\n       -1283955.365678, -1284187.022036, -1284418.678394, -1284650.334753,\n       -1284881.991111, -1285113.647469, -1285345.303827, -1285576.960186,\n       -1285808.616544, -1286040.272902, -1286271.929261, -1286503.585619,\n       -1286735.241977, -1286966.898335, -1287198.554694, -1287430.211052,\n       -1287661.86741 , -1287893.523768, -1288125.180127, -1288356.836485,\n       -1288588.492843, -1288820.149201, -1289051.80556 , -1289283.461918,\n       -1289515.118276, -1289746.774635, -1289978.430993, -1290210.087351,\n       -1290441.743709, -1290673.400068, -1290905.056426, -1291136.712784,\n       -1291368.369142, -1291600.025501, -1291831.681859, -1292063.338217,\n       -1292294.994575, -1292526.650934, -1292758.307292, -1292989.96365 ,\n       -1293221.620008, -1293453.276367, -1293684.932725, -1293916.589083,\n       -1294148.245442, -1294379.9018  , -1294611.558158, -1294843.214516,\n       -1295074.870875, -1295306.527233, -1295538.183591, -1295769.839949,\n       -1296001.496308, -1296233.152666, -1296464.809024, -1296696.465382,\n       -1296928.121741, -1297159.778099, -1297391.434457, -1297623.090815,\n       -1297854.747174, -1298086.403532, -1298318.05989 , -1298549.716249,\n       -1298781.372607, -1299013.028965, -1299244.685323, -1299476.341682,\n       -1299707.99804 , -1299939.654398, -1300171.310756, -1300402.967115,\n       -1300634.623473, -1300866.279831, -1301097.936189, -1301329.592548,\n       -1301561.248906, -1301792.905264, -1302024.561622, -1302256.217981,\n       -1302487.874339, -1302719.530697, -1302951.187056, -1303182.843414,\n       -1303414.499772, -1303646.15613 , -1303877.812489, -1304109.468847,\n       -1304341.125205, -1304572.781563, -1304804.437922, -1305036.09428 ,\n       -1305267.750638, -1305499.406996, -1305731.063355, -1305962.719713,\n       -1306194.376071, -1306426.03243 , -1306657.688788, -1306889.345146,\n       -1307121.001504, -1307352.657863, -1307584.314221, -1307815.970579,\n       -1308047.626937, -1308279.283296, -1308510.939654, -1308742.596012,\n       -1308974.25237 , -1309205.908729, -1309437.565087, -1309669.221445,\n       -1309900.877803, -1310132.534162, -1310364.19052 , -1310595.846878,\n       -1310827.503237, -1311059.159595, -1311290.815953, -1311522.472311,\n       -1311754.12867 , -1311985.785028, -1312217.441386, -1312449.097744])\n\n\nx\n(x)\nfloat64\n-6.074e+06 ... -6.015e+06\n\narray([-6073682.229142, -6073450.572784, -6073218.916425, ..., -6015304.826859,\n       -6015073.170501, -6014841.514143], shape=(255,))\n\n\nspatial_ref\n()\nint64\n0\n\ncrs_wkt :\nPROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not_specified_based_on_custom_spheroid\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\nsemi_major_axis :\n6371007.181\nsemi_minor_axis :\n6371007.181\ninverse_flattening :\n0.0\nreference_ellipsoid_name :\nCustom spheroid\nlongitude_of_prime_meridian :\n0.0\nprime_meridian_name :\nGreenwich\ngeographic_crs_name :\nUnknown datum based upon the custom spheroid\nhorizontal_datum_name :\nNot_specified_based_on_custom_spheroid\nprojected_crs_name :\nunnamed\ngrid_mapping_name :\nsinusoidal\nlongitude_of_projection_origin :\n0.0\nfalse_easting :\n0.0\nfalse_northing :\n0.0\nspatial_ref :\nPROJCS[\"unnamed\",GEOGCS[\"Unknown datum based upon the custom spheroid\",DATUM[\"Not_specified_based_on_custom_spheroid\",SPHEROID[\"Custom spheroid\",6371007.181,0]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Sinusoidal\"],PARAMETER[\"longitude_of_center\",0],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n\narray(0)\n\n\n\n\nData variables: (1)\n\nNDVI\n(time, y, x)\nfloat32\ndask.array&lt;chunksize=(1, 148, 255), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\nBytes\n1.73 MiB\n147.42 kiB\n\n\nShape\n(12, 148, 255)\n(1, 148, 255)\n\n\nDask graph\n12 chunks in 13 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\n\nIndexes: (3)\n\n\ntime\nPandasIndex\nPandasIndex(DatetimeIndex(['2013-09-14', '2013-10-16', '2013-11-17', '2013-12-19', '2014-01-17', '2014-02-18', '2014-03-22', '2014-04-23', '2014-05-25', '2014-06-26', '2014-07-28', '2014-08-29'], dtype='datetime64[ns]', name='time', freq=None))\n\n\ny\nPandasIndex\nPandasIndex(Index([-1278395.6130795793, -1278627.2694378432,  -1278858.925796107, -1279090.5821543708, -1279322.2385126348, -1279553.8948708987, -1279785.5512291624, -1280017.2075874263, -1280248.8639456902,  -1280480.520303954,\n       ...\n        -1310364.190519991,  -1310595.846878255,  -1310827.503236519, -1311059.1595947826, -1311290.8159530465, -1311522.4723113105, -1311754.1286695744,  -1311985.785027838,  -1312217.441386102,  -1312449.097744366], dtype='float64', name='y', length=148))\n\n\nx\nPandasIndex\nPandasIndex(Index([  -6073682.22914186,  -6073450.572783597,  -6073218.916425332,  -6072987.260067069,  -6072755.603708805,  -6072523.947350541,  -6072292.290992277,  -6072060.634634013,   -6071828.97827575,  -6071597.321917485,\n       ...\n        -6016926.421367216, -6016694.7650089525,  -6016463.108650688,  -6016231.452292425,   -6015999.79593416,  -6015768.139575897,  -6015536.483217633,  -6015304.826859369, -6015073.1705011055,  -6014841.514142841], dtype='float64', name='x', length=255))\n\n\n\n\nAttributes: (0)\n\n\n\n\n\n\n\nAs an example, to plot the new xarray variable, you can use:\n\nimport matplotlib.pyplot as plt\n\n# Select first time\nxcube_slice = xcube.NDVI.sel(time=xcube.time[0])\n\n# Extract timestamp\ntimestamp = str(xcube_slice.time.values)[:10]\n\n# Plot\nplt.figure(figsize=(10, 6))\nxcube_slice.plot(cmap='YlGn')\nplt.title(f\"NDVI - {timestamp}\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n\n\n\n\nExporting time series\nFollowing the same strategy used for data cubes, you can convert sits time series to xarray using the sits_as_xarray() function. The example below demonstrates the process\n\nfrom pysits import sits_as_xarray\nfrom pysits import samples_modis_ndvi\n\nxts = sits_as_xarray(samples_modis_ndvi)\nxts\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 166kB\nDimensions:    (sample: 1218, time: 12)\nCoordinates:\n  * sample     (sample) int64 10kB 0 1 2 3 4 5 ... 1212 1213 1214 1215 1216 1217\n  * time       (time) object 96B 2013-09-14 2013-10-16 ... 2014-07-28 2014-08-29\n    longitude  (sample) float64 10kB -55.19 -57.79 -51.94 ... -55.94 -55.92\n    latitude   (sample) float64 10kB -10.84 -9.757 -13.42 ... -12.04 -12.04\n    label      (sample) object 10kB 'Pasture' 'Pasture' ... 'Forest' 'Forest'\n    cube       (sample) object 10kB 'MOD13Q1' 'MOD13Q1' ... 'MOD13Q1' 'MOD13Q1'\nData variables:\n    NDVI       (sample, time) float64 117kB 0.388 0.5273 ... 0.8186 0.4642\n\nxarray.Dataset\n\n\nDimensions:\n\nsample: 1218\n\ntime: 12\n\n\n\n\nCoordinates: (6)\n\n\nsample\n(sample)\nint64\n0 1 2 3 4 ... 1214 1215 1216 1217\n\narray([   0,    1,    2, ..., 1215, 1216, 1217], shape=(1218,))\n\n\ntime\n(time)\nobject\n2013-09-14 ... 2014-08-29\n\narray([datetime.date(2013, 9, 14), datetime.date(2013, 10, 16),\n       datetime.date(2013, 11, 17), datetime.date(2013, 12, 19),\n       datetime.date(2014, 1, 17), datetime.date(2014, 2, 18),\n       datetime.date(2014, 3, 22), datetime.date(2014, 4, 23),\n       datetime.date(2014, 5, 25), datetime.date(2014, 6, 26),\n       datetime.date(2014, 7, 28), datetime.date(2014, 8, 29)], dtype=object)\n\n\nlongitude\n(sample)\nfloat64\n-55.19 -57.79 ... -55.94 -55.92\n\narray([-55.1852, -57.794 , -51.9412, ..., -55.9212, -55.9366, -55.92  ],\n      shape=(1218,))\n\n\nlatitude\n(sample)\nfloat64\n-10.84 -9.757 ... -12.04 -12.04\n\narray([-10.8378,  -9.7573, -13.4198, ..., -12.0385, -12.0407, -12.036 ],\n      shape=(1218,))\n\n\nlabel\n(sample)\nobject\n'Pasture' 'Pasture' ... 'Forest'\n\narray(['Pasture', 'Pasture', 'Pasture', ..., 'Forest', 'Forest', 'Forest'],\n      shape=(1218,), dtype=object)\n\n\ncube\n(sample)\nobject\n'MOD13Q1' 'MOD13Q1' ... 'MOD13Q1'\n\narray(['MOD13Q1', 'MOD13Q1', 'MOD13Q1', ..., 'MOD13Q1', 'MOD13Q1',\n       'MOD13Q1'], shape=(1218,), dtype=object)\n\n\n\n\nData variables: (1)\n\nNDVI\n(sample, time)\nfloat64\n0.388 0.5273 ... 0.8186 0.4642\n\narray([[0.388     , 0.5273    , 0.6772    , ..., 0.4937    , 0.4166    ,\n        0.4422    ],\n       [0.4995    , 0.7161    , 0.5911    , ..., 0.5018    , 0.4645    ,\n        0.3101    ],\n       [0.3504    , 0.3636    , 0.5162    , ..., 0.4813    , 0.37      ,\n        0.3262    ],\n       ...,\n       [0.7443    , 0.62915888, 0.7754    , ..., 0.8345    , 0.8321    ,\n        0.8147    ],\n       [0.8442    , 0.8507    , 0.8522    , ..., 0.8236    , 0.8195    ,\n        0.4441    ],\n       [0.8487    , 0.8692    , 0.7984    , ..., 0.842     , 0.8186    ,\n        0.4642    ]], shape=(1218, 12))\n\n\n\nIndexes: (2)\n\n\nsample\nPandasIndex\nPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217], dtype='int64', name='sample', length=1218))\n\n\ntime\nPandasIndex\nPandasIndex(Index([2013-09-14, 2013-10-16, 2013-11-17, 2013-12-19, 2014-01-17, 2014-02-18, 2014-03-22, 2014-04-23, 2014-05-25, 2014-06-26, 2014-07-28, 2014-08-29], dtype='object', name='time'))\n\n\n\n\nAttributes: (0)\n\n\n\n\n\n\n\nAs an example, to plot the new xarray variable, you can use:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Extract time and NDVI values\ntime = xts.time.values\nlabels = xts.label.values\nndvi_values = xts.NDVI.values\n\n# Assign a color to each unique label\nunique_labels = pd.unique(labels)\nlabel_colors = {label: color for label, color in zip(unique_labels, plt.cm.tab10.colors)}\n\n# Plot all time series\nplt.figure(figsize=(12, 6))\n\nfor i in range(ndvi_values.shape[0]):\n    plt.plot(time, ndvi_values[i], alpha=0.2, color=label_colors[ labels[i] ])\n\n# Add legend\nfor label in unique_labels:\n    plt.plot([], [], color=label_colors[label], label=label)\n\n# Add labels\nplt.xlabel(\"Time\")\nplt.ylabel(\"NDVI\")\nplt.title(\"NDVI Time Series\")\nplt.legend(title=\"Label\")\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "annex_gee.html#introduction",
    "href": "annex_gee.html#introduction",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.1 Introduction",
    "text": "28.1 Introduction\nThis section presents a side-by-side exploration of the sits and Google Earth Engine (gee) APIs, focusing on their respective capabilities in handling satellite data. The exploration is structured around three key examples: (1) creating a mosaic, (2) calculating the Normalized Difference Vegetation Index (NDVI), and (3) performing a Land Use and Land Cover (LULC) classification. Each example demonstrates how these tasks are executed using sits and gee, offering a clear view of their methodologies and highlighting the similarities and the unique approaches each API employs."
  },
  {
    "objectID": "annex_gee.html#creating-a-mosaic",
    "href": "annex_gee.html#creating-a-mosaic",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.2 Creating a Mosaic",
    "text": "28.2 Creating a Mosaic\nA common application among scientists and developers in the field of Remote Sensing is the creation of satellite image mosaics. These mosaics are formed by combining two or more images, typically used for visualization in various applications. In this example, we will demonstrate how to create an image mosaic using sits and gee APIs.\nIn this example, a Region of Interest (ROI) is defined using a bounding box with longitude and latitude coordinates. Below are the code snippets for specifying this ROI in both sits and gee environments.\n\n\nSITS\nGEE\n\n\n\n\nroi &lt;- c(\"lon_min\" = -63.410, \"lat_min\" = -9.783,\n         \"lon_max\" = -62.614, \"lat_max\" = -9.331)\n\n\n\n\nroi = ee.Geometry.Rectangle([-63.410,-9.783,-62.614,-9.331]);\n\n\n\n\nNext, we will load the satellite imagery. For this example, we used data from Sentinel-2. In sits, several providers offer Sentinel-2 ARD images. In this example, we will use images provided by the Microsoft Planetary Computer (MPC).\n\n\nSITS\nGEE\n\n\n\n\ndata &lt;- sits_cube(\n  source     = \"MPC\",\n  collection = \"SENTINEL-2-L2A\",\n  bands      = c(\"B02\", \"B03\", \"B04\"),\n  tiles      = c(\"20LNQ\", \"20LMQ\"),\n  start_date = \"2024-08-01\",\n  end_date   = \"2024-08-03\"\n)\n\n\n\n\ndata = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n  .filterDate('2024-08-01', '2024-08-03')\n  .filter(ee.Filter.inList('MGRS_TILE', ['20LNQ', '20LMQ']))\n  .select(['B4', 'B3', 'B2']);\n\n\n\n\nsits provides search filters for a collection as parameters in the sits_cube() function, whereas gee offers these filters as methods of an ImageCollection object.\nIn sits, we will use the sits_mosaic() function to create mosaics of our images. In gee, we will take the mosaic() method. In sits, sits_mosaic() crops the mosaic based on the roi parameter. In gee, cropping is performed using the clip() method. We will use the same roi that was used to filter the images to perform the cropping on the mosaic. See the following code:\n\n\nSITS\nGEE\n\n\n\n\nmosaic &lt;- sits_mosaic(\n  cube       = data,\n  roi        = roi,\n  multicores = 4,\n  output_dir = tempdir()\n)\n\n\n\n\nmosaic = data.mosaic().clip(roi);\n\n\n\n\nFinally, the results can be visualized in an interactive map.\nsits\n\n# Visualization in SITS\nsits_view(\n  x     = mosaic,\n  red   = \"B04\",\n  green = \"B03\",\n  blue  = \"B02\"\n)\n\n\n\n\n\n\ngee\n\n# Visualization in GEE\n# Define view region\n#\nMap.centerObject(roi, 10);\n\n# Add mosaic Image\nMap.addLayer(mosaic, {\n      min: 0, \n      max: 3000\n}, 'Mosaic');"
  },
  {
    "objectID": "annex_gee.html#calculating-ndvi",
    "href": "annex_gee.html#calculating-ndvi",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.3 Calculating NDVI",
    "text": "28.3 Calculating NDVI\nThis example demonstrates how to generate time-series of Normalized Difference Vegetation Index (NDVI) using both the sits and gee APIs. In this example, a Region of Interest (ROI) is defined using the sinop_roi.shp file. Below are the code snippets for specifying this file in both sits and gee environments.\nTo reproduce the example, you can download the shapefile using this link. In sits, you can just use it. In gee, it would be required to upload the file in your user space.\n\n\nSITS\nGEE\n\n\n\n\nroi_data &lt;- \"sinop_roi.shp\"\n\n\n\n\nroi_data = ee.FeatureCollection(\"/path/to/sinop_roi\");\n\n\n\n\nNext, we load the satellite imagery. For this example, we use data from Landsat-8. In sits, this data is retrieved from the Brazil Data Cube, although other sources are available. For gee, the data provided by the platform is used. In sits, when the data is loaded, all necessary transformations to make the data ready for use (e.g., factor, offset, cloud masking) are applied automatically. In gee, users are responsible for performing these transformations themselves.\n\n\nSITS\nGEE\n\n\n\n\ndata &lt;- sits_cube(\n    source      = \"BDC\",\n    collection  = \"LANDSAT-OLI-16D\",\n    bands       = c(\"RED\", \"NIR08\", \"CLOUD\"),\n    roi         = roi_data,\n    start_date  = \"2019-05-01\",\n    end_date    = \"2019-07-01\"\n)\n\n\n\n\ndata = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n  .filterBounds(roi_data)\n  .filterDate(\"2019-05-01\", \"2019-07-01\")\n  .select([\"SR_B4\", \"SR_B5\", \"QA_PIXEL\"]);\n\n# factor and offset\ndata = data.map(function(image) {\n  opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  return image.addBands(opticalBands, null, true);\n});\n\ndata = data.map(function(image) {\n  # Select the pixel_qa band\n  qa = image.select('QA_PIXEL');\n\n  // Create a mask to identify cloud and cloud shadow\n  cloudMask = qa.bitwiseAnd(1 &lt;&lt; 5).eq(0) # Clouds\n           .and(qa.bitwiseAnd(1 &lt;&lt; 3).eq(0)); # Cloud shadows\n\n  # Apply the cloud mask to the image\n  return image.updateMask(cloudMask);\n});\n\n\n\n\nAfter loading the satellite imagery, the NDVI can be generated. In sits, a function allows users to specify the formula used to create a new attribute, in this case, NDVI. In gee, a callback function is used, where the NDVI is calculated for each image.\n\n\nSITS\nGEE\n\n\n\n\ndata_ndvi &lt;- sits_apply(\n  data        = data,\n  NDVI        = (NIR08 - RED) / (NIR08 + RED),\n  output_dir  = tempdir(),\n  multicores  = 4,\n  progress    = TRUE\n)\n\n\n\n\nvar data_ndvi = data.map(function(image) {\n  var ndvi = image.normalizedDifference([\"SR_B5\", \"SR_B4\"]).rename('NDVI');\n\n  return image.addBands(ndvi);\n});\n\ndata_ndvi = data_ndvi.select(\"NDVI\");\n\n\n\n\nThe results are clipped to the ROI defined at the beginning of the example to facilitate visualization. In both APIs, you can define a ROI before performing the operation to optimize resource usage. However, in this example, the data is cropped after the calculation.\n\n\nSITS\nGEE\n\n\n\n\ndata_ndvi &lt;- sits_mosaic(\n  cube       = data_ndvi,\n  roi        = roi_data,\n  output_dir = tempdir(),\n  multicores = 4\n)\n\n\n\n\ndata_ndvi = data_ndvi.map(function(image) {\n  return image.clip(roi_data);\n});\n\n\n\n\nFinally, the results can be visualized in an interactive map.\nsits\n\nsits_view(data_ndvi, band = \"NDVI\", date = \"2019-05-25\", opacity = 1)\n\n\n\n\n\n\ngee\n\n# Define view region\nMap.centerObject(roi_data, 10);\n# Add classification map (colors from sits)\nMap.addLayer(data_ndvi, {\n      min: 0, \n      max: 1, \n      palette: [\"red\", 'white', 'green']\n}, \"NDVI Image\");"
  },
  {
    "objectID": "annex_gee.html#land-use-and-land-cover-lulc-classification",
    "href": "annex_gee.html#land-use-and-land-cover-lulc-classification",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "Land Use and Land Cover (LULC) Classification",
    "text": "Land Use and Land Cover (LULC) Classification\nThis example demonstrates how to perform Land Use and Land Cover (LULC) classification using satellite image time series and machine-learning models in both sits and gee.\nThis example defines the region of interest (ROI) using a shapefile named sinop_roi.shp. Below are the code snippets for specifying this file in both sits and gee environments. To reproduce the example, you can download the shapefile using this link. In sits, you can just use it. In gee, it would be required to upload the file in your user space.\n\n\nSITS\nGEE\n\n\n\n\nroi_data &lt;- \"sinop_roi.shp\"\n\n\n\n\nroi_data = ee.FeatureCollection(\"/path/to/sinop_roi\");\n\n\n\n\nTo train a classifier, sample data with labels representing the behavior of each class to be identified is necessary. In this example, we use a small set with 18 samples. The following code snippets show how these samples are defined in each environment.\nIn sits, labels can be of type string, whereas gee requires labels to be integers. To accommodate this difference, two versions of the same sample set were created: (1) one with string labels for use with sits, and (2) another with integer labels for use with gee.\nTo download these samples, you can use the following links: samples_sinop_crop for sits or samples_sinop_crop for gee\n\n\nSITS\nGEE\n\n\n\n\nsamples &lt;- \"samples_sinop_crop.shp\"\n\n\n\n\nsamples = ee.FeatureCollection(\"samples_sinop_crop_gee\");\n\n\n\n\nNext, we load the satellite imagery. For this example, we use data from MOD13Q1 . In sits, this data is retrieved from the Brazil Data Cube, but other sources are also available. In gee, the platform directly provides this data. In sits, all necessary data transformations for classification tasks are handled automatically. In contrast, gee requires users to manually transform the data into the correct format.\nIn the gee code, transforming all images into bands mimics the approach used by sits for non-temporal classifiers. However, this method is not inherently scalable in gee and may need adjustments for larger datasets or more bands. Additionally, for temporal classifiers like TempCNN, other transformations are necessary and must be manually implemented ingee . In contrast, sits provides a consistent API experience, regardless of the data size or machine learning algorithm.\n\n\nSITS\nGEE\n\n\n\n\ndata &lt;- sits_cube(\n    source      = \"BDC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands       = c(\"NDVI\"),\n    roi         = roi_data,\n    start_date  = \"2013-09-01\",\n    end_date    = \"2014-08-29\"\n)\n\n\n\n\ndata = ee.ImageCollection(\"MODIS/061/MOD13Q1\")\n  .filterBounds(roi_data)\n  .filterDate(\"2013-09-01\", \"2014-09-01\")\n  .select([\"NDVI\"]);\n\n# Transform all images to bands\ndata = data.toBands();\n\n\n\n\nIn this example, we’ll use a Random Forest classifier to create a LULC map. To train the classifier, we need sample data linked to time-series. This step shows how to extract and associate time-series with samples.\n\n\nSITS\nGEE\n\n\n\n\nsamples_ts &lt;- sits_get_data(\n    cube       = data,\n    samples    = samples,\n    multicores = 4\n)\n\n\n\n\nsamples_ts = data.sampleRegions({\n  collection: samples,\n  properties: [\"label\"]\n});\n\n\n\n\nWith the time-series data extracted for each sample, we can now train the Random Forest classifier.\n\n\nSITS\nGEE\n\n\n\n\nclassifier &lt;- sits_train(\n    samples_ts, sits_rfor(num_trees = 100)\n)\n\n\n\n\nvar classifier = ee.Classifier.smileRandomForest(100).train({\n  features: samples_ts,\n  classProperty: \"label\",\n  inputProperties: data.bandNames()\n});\n\n\n\n\nNow, it is possible to generate the classification map using the trained model. In sits, the classification process starts with a probability map. This map provides the probability of each class for every pixel, offering insights into the classifier’s performance. It also allows for refining the results using methods like Bayesian probability smoothing. After generating the probability map, it is possible to produce the class map, where each pixel is assigned to the class with the highest probability.\nIn gee, while it is possible to generate probabilities, it is not strictly required to produce the classification map. Yet, as of the date of this document, there is no out-of-the-box solution available for using these probabilities to enhance classification results.\n\n\nSITS\nGEE\n\n\n\n\nprobabilities &lt;- sits_classify(\n    data       = data,\n    ml_model   = classifier,\n    multicores = 4,\n    roi        = roi_data,\n    output_dir = tempdir()\n)\n\nclass_map &lt;- sits_label_classification(\n    cube       = probabilities,\n    output_dir = tempdir(),\n    multicores = 4\n)\n\n\n\n\n#  Get the probabilities maps\nprobs_map = data.classify(classifier.setOutputMode(\"MULTIPROBABILITY\"));\n#  Get the classified map\nclass_map = data.classify(classifier);\n\n\n\n\nThe results are clipped to the ROI defined at the beginning of the example to facilitate visualization. In both APIs, it’s possible to define an ROI before processing. However, this was not done in this example.\n\n\nSITS\nGEE\n\n\n\n\nclass_map &lt;- sits_mosaic(\n    cube       = class_map,\n    roi        = roi_data,\n    output_dir = tempdir(),\n    multicores = 4\n)\n\n\n\n\nclass_map = class_map.clip(roi_data);\n\n\n\n\nFinally, the results can be visualized on an interactive map.\nsits\n\nsits_view(class_map, opacity = 1)\n\n\n\n\n\n\ngee\n\n# Define view region\nMap.centerObject(roi_data, 10);\n# Add classification map (colors from sits)\nMap.addLayer(class_map, {\n      min: 1,\n      max: 4,\n      palette: [\"#FAD12D\", \"#1E8449\", \"#D68910\", \"#a2d43f\"]\n}, \"Classification map\");"
  },
  {
    "objectID": "annex_gee.html#summary",
    "href": "annex_gee.html#summary",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.4 Summary",
    "text": "28.4 Summary\nThis chapter provides an overview of the main differences between the APIs of sits and gee. The sits authors recognize their debt to gee, which was the first application with a coherent API to deal with big EO data. By making a coherent API, gee showed the way forward to big EO application developers. To a large extent, the improvements of sits in relation to gee stem from a careful analysis of Google’s API, which allowed an appraisal of its strengths and drawbacks."
  },
  {
    "objectID": "annex_api.html#general-principles",
    "href": "annex_api.html#general-principles",
    "title": "29  Developing new functions in SITS",
    "section": "29.1 General principles",
    "text": "29.1 General principles\nNew functions that build on the sits API should follow the general principles below.\n\nThe target audience for sits is the community of remote sensing experts with Earth Sciences background who want to use state-of-the-art data analysis methods with minimal investment in programming skills. The design of the sits API considers the typical workflow for land classification using satellite image time series and thus provides a clear and direct set of functions, which are easy to learn and master.\nFor this reason, we welcome contributors that provide useful additions to the existing API, such as new ML/DL classification algorithms. In case of a new API function, before making a pull request please raise an issue stating your rationale for a new function.\nMost functions in sits use the S3 programming model with a strong emphasis on generic methods wich are specialized depending on the input data type. See for example the implementation of the sits_bands() function.\nPlease do not include contributed code using the S4 programming model. Doing so would break the structure and the logic of existing code. Convert your code from S4 to S3.\nUse generic functions as much as possible, as they improve modularity and maintenance. If your code has decision points using if-else clauses, such as if A, do X; else do Y consider using generic functions.\nFunctions that use the torch package use the R6 model to be compatible with that package. See for example, the code in sits_tempcnn.R and api_torch.R. To convert pyTorch code to R and include it is straightforward. Please see the Technical Annex of the sits on-line book.\nThe sits code relies on the packages of the tidyverse to work with tables and list. We use dplyr and tidyr for data selection and wrangling, purrr and slider for loops on lists and table, lubridate to handle dates and times."
  },
  {
    "objectID": "annex_api.html#adherence-to-the-sits-data-types",
    "href": "annex_api.html#adherence-to-the-sits-data-types",
    "title": "29  Developing new functions in SITS",
    "section": "29.2 Adherence to the sits data types",
    "text": "29.2 Adherence to the sits data types\nThe sits package in built on top of three data types: time series tibble, data cubes and models. Most sits functions have one or more of these types as inputs and one of them as return values. The time series tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. All time series tibbles are objects of class sits.\nThe cube data type is designed to store metadata about image files. In principle, images which are part of a data cube share the same geographical region, have the same bands, and have been regularized to fit into a pre-defined temporal interval. Data cubes in sits are organized by tiles. A tile is an element of a satellite’s mission reference system, for example MGRS for Sentinel-2 and WRS2 for Landsat. A cube is a tibble where each row contains information about data covering one tile. Each row of the cube tibble contains a column named file_info; this column contains a list that stores a tibble\nThe cube data type is specialised in raster_cube (ARD images), vector_cube (ARD cube with segmentation vectors). probs_cube (probabilities produced by classification algorithms on raster data), probs_vector_cube(probabilites generated by vector classification of segments), uncertainty_cube (cubes with uncertainty information), and class_cube (labelled maps). See the code in sits_plot.R as an example of specialisation of plot to handle different classes of raster data.\nAll ML/DL models in sits which are the result of sits_train belong to the ml_model class. In addition, models are assigned a second class, which is unique to ML models (e.g, rfor_model, svm_model) and generic for all DL torch based models (torch_model). The class information is used for plotting models and for establishing if a model can run on GPUs."
  },
  {
    "objectID": "annex_api.html#literal-values-error-messages-and-testing",
    "href": "annex_api.html#literal-values-error-messages-and-testing",
    "title": "29  Developing new functions in SITS",
    "section": "29.3 Literal values, error messages, and testing",
    "text": "29.3 Literal values, error messages, and testing\nThe internal sits code has no literal values, which are all stored in the YAML configuration files ./inst/extdata/config.yml and ./inst/extdata/config_internals.yml. The first file contains configuration parameters that are relevant to users, related to visualisation and plotting; the second contains parameters that are relevant only for developers. These values are accessible using the .conf function. For example, the value of the default size for ploting COG files is accessed using the command .conf[\"plot\", \"max_size\"].\nError messages are also stored outside of the code in the YAML configuration file ./inst/extdata/config_messages.yml. These values are accessible using the .conf function. For example, the error associated to an invalid NA value for an input parameter is accessible using th function .conf(\"messages\", \".check_na_parameter\").\nWe strive for high code coverage (&gt; 90%). Every parameter of all sits function (including internal ones) is checked for consistency. Please see api_check.R."
  },
  {
    "objectID": "annex_ml.html#introduction",
    "href": "annex_ml.html#introduction",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.1 Introduction",
    "text": "30.1 Introduction\nThis section provides guidance for experts that want to include new methods for machine learning that work in connection with sits. The discussion below assumes familiarity with the R language. Developers should consult Hadley Wickham’s excellent book Advanced R, especially Chapter 10 on “Function Factories”."
  },
  {
    "objectID": "annex_ml.html#common-features-for-all-classification-algorithms",
    "href": "annex_ml.html#common-features-for-all-classification-algorithms",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.2 Common features for all classification algorithms",
    "text": "30.2 Common features for all classification algorithms\nAll machine learning and deep learning algorithm in sits follow the same logic; all models are created by sits_train(). This function has two parameters: (a) samples, a set of time series with the training samples; (b) ml_method, a function that fits the model to the input data. The result is a function that is passed on to sits_classify() to classify time series or data cubes. The structure of sits_train() is simple, as shown below.\n\nsits_train &lt;- function(samples, ml_method){\n    # train a ml classifier with the given data\n    result &lt;- ml_method(samples)\n    # return a valid machine learning method\n    return(result)\n}\n\nIn R terms, sits_train() is a function factory, i.e., a function that makes functions. Such behavior is possible because functions are first-class objects in R. In other words, they can be bound to a name in the same way that variables are. A second propriety of R is that functions capture (enclose) the environment in which they are created. In other words, when a function is returned as a result of another function, the internal variables used to create it are available inside its environment. In programming language, this technique is called “closure”.\nThe following definition from Wikipedia captures the purpose of clousures: “Operationally, a closure is a record storing a function together with an environment. The environment is a mapping associating each free variable of the function with the value or reference to which the name was bound when the closure was created. A closure allows the function to access those captured variables through the closure’s copies of their values or references, even when the function is invoked outside their scope.”\nIn sits, the properties of closures are used as a basis for making training and classification independent. The return of sits_train() is a model that contains information on how to classify input values, as well as information on the samples used to train the model."
  },
  {
    "objectID": "annex_ml.html#transforming-samples-into-predictors-for-ml-models",
    "href": "annex_ml.html#transforming-samples-into-predictors-for-ml-models",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.3 Transforming samples into predictors for ML models",
    "text": "30.3 Transforming samples into predictors for ML models\nTo ensure all models work in the same fashion, machine learning functions in sits also share the same data structure for prediction. This data structure is created by sits_predictors(), which transforms the time series tibble into a set of values suitable for using as training data, as shown in the following example.\n\ndata(\"samples_matogrosso_mod13q1\", package = \"sitsdata\")\npred &lt;- sits_predictors(samples_matogrosso_mod13q1)\npred\n\nThe predictors tibble is organized as a combination of the “X” and “Y” values used by machine learning algorithms. The first two columns are sample_id and label. The other columns contain the data values, organized by band and time. For machine learning methods that are not time-sensitive, such as Random Forest, this organization is sufficient for training. In the case of time-sensitive methods such as tempCNN, further arrangements are necessary to ensure the tensors have the right dimensions. Please refer to the sits_tempcnn() source code for an example of how to adapt the prediction table to appropriate torch tensor."
  },
  {
    "objectID": "annex_ml.html#data-normalization",
    "href": "annex_ml.html#data-normalization",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.4 Data normalization",
    "text": "30.4 Data normalization\nSome ML algorithms, such as svm, require data normalization. Therefore, the sits_predictors() code is usually combined with methods that extract statistical information and then normalize the data, as in the example below.\n\n # Data normalization\nml_stats &lt;- sits_stats(samples)\n# extract the training samples\ntrain_samples  &lt;- sits_predictors(samples)\n# normalize the training samples\ntrain_samples  &lt;- sits_pred_normalize(pred = train_samples, stats = ml_stats)"
  },
  {
    "objectID": "annex_ml.html#implementing-the-lightgbm-algorithm",
    "href": "annex_ml.html#implementing-the-lightgbm-algorithm",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.5 Implementing the LightGBM algorithm",
    "text": "30.5 Implementing the LightGBM algorithm\nThe following example shows the implementation of the LightGBM algorithm, designed to efficiently handle large-scale datasets and perform fast training and inference [1]. Gradient boosting is a machine learning technique that builds an ensemble of weak prediction models, typically decision trees, to create a stronger model. LightGBM specifically focuses on optimizing the training and prediction speed, making it particularly suitable for large datasets. The example builds a model using the lightgbm package. This model will then be applied later to obtain a classification.\nSince LightGBM is a gradient boosting model, it uses part of the data as testing data to improve the model’s performance. The split between the training and test samples is controlled by a parameter, as shown in the following code extract.\n\n# split the data into training and validation datasets\n# create partitions different splits of the input data\ntest_samples &lt;- sits_pred_sample(train_samples,\n                                 frac = validation_split\n)\n# Remove the lines used for validation\nsel &lt;- !(train_samples$sample_id %in% test_samples$sample_id)\ntrain_samples &lt;- train_samples[sel, ]\n\nTo include the lightgbm package as part of sits, we need to create a new training function which is compatible with the other machine learning methods of the package and will be called by sits_train(). For compatibility, this new function will be called sits_lightgbm(). Its implementation uses two functions from the lightgbm: (a) lgb.Dataset(), which transforms training and test samples into internal structures; (b) lgb.train(), which trains the model.\nThe parameters of lightgbm::lgb.train() are: (a) boosting_type, boosting algorithm; (b) objective, classification objective (c) num_iterations, number of runs; (d) max_depth, maximum tree depth; (d) min_samples_leaf, minimum size of data in one leaf (to avoid overfitting); (f) learning_rate, learning rate of the algorithm; (g) n_iter_no_change, number of successive iterations to stop training when validation metrics do not improve; (h) validation_split, fraction of training data to be used as validation data.\n\n# install \"lightgbm\" package if not available \nif (!require(\"lightgbm\")) install.packages(\"lightgbm\")\n# create a function in sits style for LightGBM algorithm\nsits_lightgbm &lt;- function(samples = NULL,\n                          boosting_type = \"gbdt\",\n                          objective = \"multiclass\",\n                          min_samples_leaf = 10,\n                          max_depth = 6,\n                          learning_rate = 0.1,\n                          num_iterations = 100,\n                          n_iter_no_change = 10,\n                          validation_split = 0.2, ...){\n\n    # function that returns a model based on training data\n    train_fun &lt;- function(samples) {\n        # Extract the predictors\n        train_samples &lt;- sits_predictors(samples)\n        \n        # find number of labels\n        labels &lt;- sits_labels(samples)\n        n_labels &lt;- length(labels)\n        # lightGBM uses numerical labels starting from 0\n        int_labels &lt;- c(1:n_labels) - 1\n        # create a named vector with integers match the class labels\n        names(int_labels) &lt;- labels\n        \n        # add number of classes to lightGBM params\n        # split the data into training and validation datasets\n        # create partitions different splits of the input data\n        test_samples &lt;- sits_pred_sample(train_samples,\n                                         frac = validation_split\n        )\n        \n        # Remove the lines used for validation\n        sel &lt;- !(train_samples$sample_id %in% test_samples$sample_id)\n        train_samples &lt;- train_samples[sel, ]\n        \n        # transform the training data to LGBM dataset\n        lgbm_train_samples &lt;- lightgbm::lgb.Dataset(\n            data = as.matrix(train_samples[, -2:0]),\n            label = unname(int_labels[train_samples[[2]]])\n        )\n        # transform the test data to LGBM dataset\n        lgbm_test_samples &lt;- lightgbm::lgb.Dataset(\n            data = as.matrix(test_samples[, -2:0]),\n            label = unname(int_labels[test_samples[[2]]])\n        )\n        # set the parameters for the lightGBM training\n        lgb_params &lt;- list(\n            boosting_type = boosting_type,\n            objective = objective,\n            min_samples_leaf = min_samples_leaf,\n            max_depth = max_depth,\n            learning_rate = learning_rate,\n            num_iterations = num_iterations,\n            n_iter_no_change = n_iter_no_change,\n            num_class = n_labels\n        )\n        # call method and return the trained model\n        lgbm_model &lt;- lightgbm::lgb.train(\n            data    = lgbm_train_samples,\n            valids  = list(test_data = lgbm_test_samples),\n            params  = lgb_params,\n            verbose = -1,\n            ...\n        )\n        # serialize the model for parallel processing\n        lgbm_model_string &lt;- lgbm_model$save_model_to_string(NULL)\n        # construct model predict closure function and returns\n        predict_fun &lt;- function(values) {\n            # reload the model (unserialize)\n            lgbm_model &lt;- lightgbm::lgb.load(model_str = lgbm_model_string)\n            # predict probabilities\n            prediction &lt;- stats::predict(lgbm_model,\n                               newdata = as.matrix(values),\n                               type = \"response\"\n            )\n            # adjust the names of the columns of the probs\n            colnames(prediction) &lt;- labels\n            # retrieve the prediction results\n            return(prediction)\n        }\n        # Set model class\n        # This tells sits that the resulting function\n        # will be handled as a prediction model\n        class(predict_fun) &lt;- c(\"lightgbm_model\", \n                                \"sits_model\", \n                                class(predict_fun))\n        return(predict_fun)\n    }\n    result &lt;- sits_factory_function(samples, train_fun)\n    return(result)\n}"
  },
  {
    "objectID": "annex_ml.html#understanding-how-models-are-organized-in-sits",
    "href": "annex_ml.html#understanding-how-models-are-organized-in-sits",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.6 Understanding how models are organized in SITS",
    "text": "30.6 Understanding how models are organized in SITS\nThe above code has two nested functions: train_fun() and predict_fun(). When sits_lightgbm() is called, train_fun() transforms the input samples into predictors and uses them to train the algorithm, creating a model (lgbm_model). This model is included as part of the function’s closure and becomes available at classification time. Inside train_fun(), we include predict_fun(), which applies the lgbm_model object to classify to the input values. The train_fun object is then returned as a closure, using the sits_factory_function constructor. This function allows the model to be called either as part of sits_train() or to be called independently, with the same result.\n\nsits_factory_function &lt;- function(data, fun) {\n    # if no data is given, we prepare a\n    # function to be called as a parameter of other functions\n    if (purrr::is_null(data)) {\n        result &lt;- fun\n    } else {\n        # ...otherwise compute the result on the input data\n        result &lt;- fun(data)\n    }\n    return(result)\n}\n\nAs a result, the following calls are equivalent.\n\n# building a model using sits_train\nlgbm_model &lt;- sits_train(samples, sits_lightgbm())\n# building a model directly\nlgbm_model &lt;- sits_lightgbm(samples)"
  },
  {
    "objectID": "annex_ml.html#model-serialization-for-parallel-processing",
    "href": "annex_ml.html#model-serialization-for-parallel-processing",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.7 Model serialization for parallel processing",
    "text": "30.7 Model serialization for parallel processing\nThere is one additional requirement for the algorithm to be compatible with sits. Data cube processing algorithms in sits run in parallel. For this reason, once the classification model is trained, it is serialized, as shown in the following line. The serialized version of the model is exported to the function closure, so it can be used at classification time.\n\n# serialize the model for parallel processing\nlgbm_model_string &lt;- lgbm_model$save_model_to_string(NULL)\n\nDuring classification, predict_fun() is called in parallel by each CPU. At this moment, the serialized string is transformed back into a model, which is then run to obtain the classification, as shown in the code.\n\n# unserialize the model\nlgbm_model &lt;- lightgbm::lgb.load(model_str = lgbm_model_string)"
  },
  {
    "objectID": "annex_ml.html#case-study",
    "href": "annex_ml.html#case-study",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.8 Case study",
    "text": "30.8 Case study\nTo illustrate this separation between training and classification, the new algorithm developed in the chapter using lightgbm will be used to classify a data cube. Note that no change is required for the sits_classify() function. Therefore, the training model can be developed as an additional model, without requiring adjustments in the rest of sits functions.\n\ndata(\"samples_matogrosso_mod13q1\", package = \"sitsdata\")\n# Create a data cube using local files\nsinop &lt;- sits_cube(\n  source = \"BDC\", \n  collection  = \"MOD13Q1-6.1\",\n  data_dir = system.file(\"extdata/sinop\", package = \"sitsdata\"),  \n  parse_info = c(\"X1\", \"X2\", \"tile\", \"band\", \"date\")\n)\n# The data cube has only \"NDVI\" and \"EVI\" bands \n# Select the bands NDVI and EVI\nsamples_2bands &lt;- sits_select(\n    data = samples_matogrosso_mod13q1, \n    bands = c(\"NDVI\", \"EVI\")\n)\n# train lightGBM model\nlgb_model &lt;- sits_train(samples_2bands, sits_lightgbm())\n\n# Classify the data cube\nsinop_probs &lt;- sits_classify(\n    data = sinop, \n    ml_model = lgb_model,\n    multicores = 1,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n# Perform spatial smoothing\nsinop_bayes &lt;- sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n# Label the smoothed file \nsinop_map &lt;- sits_label_classification(\n    cube = sinop_bayes, \n    output_dir = tempdir_r\n)\n# plot the result\nplot(sinop_map, title = \"Sinop Classification Map\")\n\n\n\n\n\nFigure 30.1: Classification map for Sinop using LightGBM"
  },
  {
    "objectID": "annex_ml.html#summary",
    "href": "annex_ml.html#summary",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.9 Summary",
    "text": "30.9 Summary\nUsing function factories that produce closures, sits keeps the classification function independent of the machine learning or deep learning algorithm. This policy allows independent proposal, testing, and development of new classification methods. It also enables improvements on parallel processing methods without affecting the existing classification methods."
  },
  {
    "objectID": "annex_stac.html#introduction",
    "href": "annex_stac.html#introduction",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.1 Introduction",
    "text": "31.1 Introduction\nA useful facility in sits is its capability to access different ARD collections in many cloud services. Moreover, sits is able to make the data ready for use without user intervention. Each ARD collection has different conventions from image attributes such band names, maximum and minimum value, encoding, scale factor and cloud masking. In sits, when the data is loaded, all necessary transformations are applied automatically.\nTo make data transformation transparent to users, sits relies on YAML configuration files and a flexible interface to the rstac package. In what follows, we show how configuration files are defined and how the sits code uses them."
  },
  {
    "objectID": "annex_stac.html#ard-collections-configuration-files",
    "href": "annex_stac.html#ard-collections-configuration-files",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.2 ARD collections configuration files",
    "text": "31.2 ARD collections configuration files\nAll STAC-based catalogues supported by sits are associated to YAML description files, which are available in the directory exdata/sources in the sits package. The example below shows how to accessthe YAML file config_source_mpc.yml describes the contents of the MPC collections supported by sits.\n\nlibrary(yaml)\n# location of YAML configuration file for MPC\nmpc_yaml_file &lt;- system.file(\"extdata/sources/config_source_mpc.yml\", package = \"sits\")\n# convert to R list object\nmpc_yaml &lt;- yaml::read_yaml(mpc_yaml_file)\n\nAccess to other configuration files is done in a similar way, replacing mpc in the above code by one of aws, bdc, cdse, deafrica, deaustralia, hls30, mpc, planet, or terrascope."
  },
  {
    "objectID": "annex_stac.html#explaining-the-contents-of-an-yaml-file",
    "href": "annex_stac.html#explaining-the-contents-of-an-yaml-file",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.3 Explaining the contents of an YAML file",
    "text": "31.3 Explaining the contents of an YAML file\nTo describe in detail the contents of an YAML configuration file for ARD catalogues, we take the example of the SENTINEL-2-L2A catalogue in AWS, shown below.\nsources:\n    AWS                     :\n        s3_class            : [\"aws_cube\", \"stac_cube\", \"eo_cube\",\n                               \"raster_cube\"]\n        service             : \"STAC\"\n        url                 : \"https://earth-search.aws.element84.com/v1/\"\n        collections         :\n            SENTINEL-2-L2A  :\n                bands       :\n                    B01     : &aws_msi_60m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  60\n                        band_name    : \"coastal\"\n                        data_type     : \"INT2S\"\n                    B02     : &aws_msi_10m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  10\n                        band_name    : \"blue\"\n                        data_type     : \"INT2S\"\n                    B03     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"green\"\n                    B04     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"red\"\n                    B05     : &aws_msi_20m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  20\n                        band_name    : \"rededge1\"\n                        data_type     : \"INT2S\"\n                    B06     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"rededge2\"\n                    B07     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"rededge3\"\n                    B08     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"nir\"\n                    B8A     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"nir08\"\n                    B09     :\n                        &lt;&lt;: *aws_msi_60m\n                        band_name    : \"nir09\"\n                    B11     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"swir16\"\n                    B12     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"swir22\"\n                    CLOUD   :\n                        bit_mask     : false\n                        band_name    : \"scl\"\n                        values       :\n                            0        : \"missing_data\"\n                            1        : \"defective pixel\"\n                            2        : \"shadows\"\n                            3        : \"cloud shadows\"\n                            4        : \"vegetation\"\n                            5        : \"non-vegetated\"\n                            6        : \"water\"\n                            7        : \"unclassified\"\n                            8        : \"cloud medium\"\n                            9        : \"cloud high\"\n                            10       : \"thin cirrus\"\n                            11       : \"snow or ice\"\n                        interp_values: [0, 1, 2, 3, 8, 9, 10]\n                        resolution  : 20\n                        data_type     : \"INT1U\"\n                satellite   : \"SENTINEL-2\"\n                sensor      : \"MSI\"\n                platforms   :\n                    SENTINEL-2A: \"sentinel-2a\"\n                    SENTINEL-2B: \"sentinel-2b\"\n                collection_name: \"sentinel-2-l2a\"\n                access_vars :\n                   AWS_DEFAULT_REGION   : \"us-west-2\"\n                   AWS_S3_ENDPOINT      : \"s3.amazonaws.com\"\n                   AWS_NO_SIGN_REQUEST  : true\n                open_data       : true\n                open_data_token : false\n                metadata_search : \"tile\"\n                ext_tolerance   : 0\n                grid_system     : \"MGRS\"\n                dates           : \"2015 to now\"\nThe main keyword is sources, which tells sits that comes below is an ARD config file. The next level contains the source name (AWS) by which the cloud provided is referred to in sits. At the next level we find the following keywords:\n\ns3_class: this is the internal S3 class used by sits to deal with data from this provider. In this case, the value aws_cube is specific to the provider and is used by sits to support modularity in STAC access. For MPC, for example, sits uses mpc_cube. The value stac_cube indicates that this catalogue is accessible via STAC. The values eo_cube and raster_cube indicate that this collection consists of raster data.\nservice: for STAC-based collections, use “STAC”.\nurl: STAC endpoint for the collection.\ncollections: keyword for collections associated to the service and described at the next level.\n\nIn the next level, we find the list of collections supported by the cloud service. In this level, we shown one collection (SENTINEL-2-L2A). Below this level, the keywords are:\n\nbands: describes the bands associated with the satellite. The value for each band is the one used by sits(e.g. “B01”) while band_name is the name of the band user by AWS. This correspondence ensures that the same band in different providers has a single name in sits. For each band, we need to provide missing_value, minimum_value, maximum_value, scale_factor, offset_value, resolution, band_name and data_type.\nsatellite: name of the satellite\nsensor: sensor designation\nplatform: in case of multiple platforms associated to the satellite constellation, provide their names.\ncollection_name: name used by AWS to designate the collection.\naccess_vars: environmental variable used to access the collection.\nopen_data: is the collection available as open data?\nopen_data_token: is a token required to access the collection?\nmetadata_search: does the collection allow searching by tiles or only by ROI?\ngrid_system: tiling system used (e.g, “MGRS” or “WRS2”)\n\nCustom YAML files can be placed in the user’s configuration file, which is a YAML file located at the place indicated by the SITS_CONFIG_USER_FILE environment variable."
  },
  {
    "objectID": "annex_stac.html#acessing-stac-collections",
    "href": "annex_stac.html#acessing-stac-collections",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.4 Acessing STAC collections",
    "text": "31.4 Acessing STAC collections\nAfter writing the YAML file, you need to consider how to access and query the new catalogue. The entry point for access to all catalogues is the sits_cube.stac_cube() function, which in turn calls a sequence of functions which are described in the generic interface api_source.R. Most calls of this API are handled by the functions of api_source_stac.R which provides an interface to the rstac package and handles STAC queries.\nThe STAC specification is flexible aand allows providers to implement their data descriptions with specific information. Thus, STAC specifications may differ among the cloud providers. For example, many providers do not allow queries by tiles. For this reason, the generic API described in api_source.R needs to be specialized for each provider. Whenever a provider needs specific implementations of parts of the STAC protocol, we include them in separate files. For example, api_source_mpc.R implements specific quirks of the MPC platform. Similarly, specific support for CDSE (Copernicus Data Space Environment) is available in api_source_cdse.R. We suggest developers interested in including a new cloud service to follow one current implementation for a cloud provider step-by-step. With a little bit of effort, it will become clear to developers how to achieve their needs."
  },
  {
    "objectID": "annex_parallel.html",
    "href": "annex_parallel.html",
    "title": "\n32  How parallel processing works in SITS\n",
    "section": "",
    "text": "This section provides an overview of how sits_classify(), sits_smooth(), and sits_label_classification() process images in parallel. To achieve efficiency, sits implements a fault-tolerant multitasking procedure for big Earth observation data classification. The learning curve is shortened as there is no need to learn how to do multiprocessing. Image classification in sits is done by a cluster of independent workers linked to a virtual machine. To avoid communication overhead, all large payloads are read and stored independently; direct interaction between the main process and the workers is kept at a minimum.\nThe classification procedure benefits from the fact that most images available in cloud collections are stored as COGs (cloud-optimized GeoTIFF). COGs are regular GeoTIFF files organized in regular square blocks to improve visualization and access for large datasets. Thus, data requests can be optimized to access only portions of the images. All cloud services supported by sits use COG files. The classification algorithm in sits uses COGs to ensure optimal data access, reducing I/O demand as much as possible.\nThe approach for parallel processing in sits, depicted in Figure @ref(fig:par), has the following steps:\n\nBased on the block size of individual COG files, calculate the size of each chunk that must be loaded in memory, considering the number of bands and the timeline’s length. Chunk access is optimized for the efficient transfer of data blocks.\nDivide the total memory available by the chunk size to determine how many processes can run in parallel.\nEach core processes a chunk and produces a subset of the result.\nRepeat the process until all chunks in the cube have been processed.\nCheck that subimages have been produced correctly. If there is a problem with one or more subimages, run a failure recovery procedure to ensure all data is processed.\nAfter generating all subimages, join them to obtain the result.\n\n\n\n\n\nFigure 32.1: Parallel processing in sits\n\n\n\nThis approach has many advantages. It has no dependencies on proprietary software and runs in any virtual machine that supports R. Processing is done in a concurrent and independent way, with no communication between workers. Failure of one worker does not cause the failure of big data processing. The software is prepared to resume classification processing from the last processed chunk, preventing failures such as memory exhaustion, power supply interruption, or network breakdown.\nTo reduce processing time, it is necessary to adjust sits_classify(), sits_smooth(), and sits_label_classification() according to the capabilities of the host environment. The memsize parameter controls the size of the main memory (in GBytes) to be used for classification. A practical approach is to set memsize to the maximum memory available in the virtual machine for classification and to choose multicores as the largest number of cores available. Based on the memory available and the size of blocks in COG files, sits will access the images in an optimized way. In this way, sits tries to ensure the best possible use of the available resources."
  },
  {
    "objectID": "index.html#greetings",
    "href": "index.html#greetings",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Greetings",
    "text": "Greetings\n\n\n\n\n\n\n\n\nWelcome to the age of big Earth observation data! With free access to massive data sets, we need new methods to measure change on our planet. This book will help you to use state-of-the-art tools to work with image time series. Combined with Earth observation data cubes, time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. time series analysis are a new and exciting paradigm. This book offers a comprehensive appraisal of this emerging discipline."
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "What is this book about?",
    "text": "What is this book about?\nThis book introduces sits, an open-source R package for big Earth observation data analysis using satellite image time series. Users build regular data cubes from cloud services such as Amazon Web Services, Microsoft Planetary Computer, Copernicus Data Space Ecosystem, NASA Harmonized Landsat-Sentinel, Brazil Data Cube, Swiss Data Cube, Digital Earth Australia, and Digital Earth Africa. The sits API includes training sample quality measures, machine learning and deep learning classification algorithms, and Bayesian post-processing methods for smoothing and uncertainty assessment. To evaluate results, sits supports best-practice accuracy assessments. The authors also provide a Python API that interfaces with the R API, and thus allows Python users to directly run sits and convert its data structures to Python data.frames and xarrays."
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Target audience",
    "text": "Target audience\nThe sits package is designed for remote sensing experts in the Earth Sciences field who want to use advanced data analysis techniques with only basic programming knowledge. The package provides a clear and direct set of functions that are easy to learn and master."
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "About the authors",
    "text": "About the authors\n\nGilberto Camara is a Senior Research Fellow at Brazil’s National Institute for Space Reseach (INPE).\nRolf Simoes is a Research Engineer at Open Geo Hub (Netherlands).\nFelipe Souza and Pedro Brito are PhD students at INPE.\nFelipe Carlos is a Software Engineer at the Group on Earth Observations (GEO).\nPedro Andrade and Karine Ferreira are Senior Researchers at INPE.\nLorena Santos is a Researcher at CTrees.org.\nAlexandre Assunção is a software consultant.\nCharlotte Pelletier is an Associate Professor at Université Bretagne-Sud in France."
  },
  {
    "objectID": "index.html#how-much-r-knowledge-is-required",
    "href": "index.html#how-much-r-knowledge-is-required",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "How much R knowledge is required?",
    "text": "How much R knowledge is required?\nTo quickly master what is needed to run sits in R, please read Parts 1 and 2 of Garrett Grolemund’s book, Hands-On Programming with R. Although not needed to run sits, your R skills will benefit from the book by Hadley Wickham and Gareth Grolemund, R for Data Science (2nd edition). Important concepts of spatial analysis are presented by Edzer Pebesma and Roger Bivand in their book Spatial Data Science."
  },
  {
    "objectID": "index.html#how-does-one-run-sits-in-python",
    "href": "index.html#how-does-one-run-sits-in-python",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "How does one run SITS in Python?",
    "text": "How does one run SITS in Python?\nFrom version 1.5.3 onwards, users can run sits in Python. Follow the instructions in the “Setup” chapter on how to set your Python environment to interface with R. Then follow the book examples provided for using sits in Python. The book provides code in both R and Python. Therefore, after correctly setting up their working environment, Python experts can run sits functions in their favorite tools, such as Jupyter Notebooks."
  },
  {
    "objectID": "index.html#software-version-described-in-this-book",
    "href": "index.html#software-version-described-in-this-book",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Software version described in this book",
    "text": "Software version described in this book\nThe version of the sits package described in this book is 1.5.3."
  },
  {
    "objectID": "index.html#main-reference-for-sits",
    "href": "index.html#main-reference-for-sits",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Main reference for sits\n",
    "text": "Main reference for sits\n\nIf you use sits in your work, please cite the following paper:\nRolf Simoes, Gilberto Camara, Gilberto Queiroz, Felipe Souza, Pedro R. Andrade, Lorena Santos, Alexandre Carvalho, and Karine Ferreira. Satellite Image Time Series Analysis for Big Earth Observation Data. Remote Sensing, 13, p. 2428, 2021."
  },
  {
    "objectID": "index.html#intellectual-property-rights",
    "href": "index.html#intellectual-property-rights",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Intellectual property rights",
    "text": "Intellectual property rights\nThis book is licensed as Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) by Creative Commons. The sits package is licensed under the GNU General Public License, version 3.0."
  },
  {
    "objectID": "dc_ardcollections.html#introduction",
    "href": "dc_ardcollections.html#introduction",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nARD (analysis-ready) image collections are organized into spatial partitions. Sentinel-2/2A images follow the Military Grid Reference System (MGRS) tiling system, which divides the world into 60 UTM zones of 8 degrees of longitude. Each zone contains blocks of 6 degrees of latitude. Blocks are split into tiles of 110 \\(\\times\\) 110 km\\(^2\\) with a 10 km overlap. Figure 4.1 shows the MGRS tiling system for a part of the northeastern coast of Brazil, contained in UTM zone 24, block M.\n\n\n\n\nFigure 4.1: MGRS tiling system used by Sentinel-2 images (source: US Army).\n\n\n\nThe Landsat-4/5/7/8/9 satellites use the Worldwide Reference System (WRS-2), which divides the coverage of Landsat satellites into images identified by path and row (Figure 4.2). The path is the descending orbit of the satellite; the WRS-2 system has 233 paths per orbit, and each path has 119 rows, where each row refers to a latitudinal center line of a frame of imagery. Images in WRS-2 are geometrically corrected to the UTM projection.\n\n\n\n\nFigure 4.2: MGRS tiling system used by Sentinel-2 images (source: US Army)."
  },
  {
    "objectID": "dc_ardcollections.html#image-collections-handled-by-sits",
    "href": "dc_ardcollections.html#image-collections-handled-by-sits",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.2 Image collections handled by sits\n",
    "text": "4.2 Image collections handled by sits\n\nIn version 1.5.3,sits supports access to the following ARD image cloud providers:\n\nAmazon Web Services (AWS): Open data Sentinel-2/2A Level-2A collections for the Earth’s land surface.\nBrazil Data Cube (BDC): Open data collections of Sentinel-2/2A, Landsat-8, CBERS-4/4A, and MOD13Q1 products for Brazil. These collections are organized as regular data cubes.\nCopernicus Data Space Ecosystem (CDSE): Open data collections of Sentinel-1 RTC and Sentinel-2/2A images.\nDigital Earth Africa (DEAFRICA): Open data collections of Sentinel-1 RTC, Sentinel-2/2A, Landsat-5/7/8/9 for Africa. Additional products include ALOS_PALSAR mosaics, DEM_COP_30, NDVI_ANOMALY based on Landsat data, and monthly and daily rainfall data from CHIRPS.\nDigital Earth Australia (DEAUSTRALIA): Open data ARD collections of Sentinel-2A/2B and Landsat-5/7/8/9 images, yearly geomedians of Landsat 5/7/8 images; yearly fractional land cover from 1986 to 2024.\nHarmonized Landsat-Sentinel (HLS): HLS, provided by NASA, is an open data collection that processes Landsat 8 and Sentinel-2 imagery to a common standard.\nMicrosoft Planetary Computer (MPC): Open data collections of Sentinel-1 GRD, Sentinel-1 RTC, Sentinel-2/2A, Landsat-4/5/7/8/9 images for the Earth’s land areas. Also supported are the Copernicus DEM-30 and MOD13Q1, MOD10A1, MOD09A1 products, and the Harmonized Landsat-Sentinel collections (HLSL30 and HLSS30).\nSwiss Data Cube (SDC): Collection of Sentinel-2/2A and Landsat-8 images for Switzerland.\nTerrascope: Cloud service with EO products, which includes the ESA World Cover map.\nUSGS: Landsat-4/5/7/8/9 collections available in AWS, which require access payment.\n\nIn addition, sits supports the use of Planet monthly mosaics stored as local files. For a detailed description of the providers and collections supported by sits, please run sits_list_collections()."
  },
  {
    "objectID": "dc_ardcollections.html#accessing-ard-image-collections-in-cloud-providers",
    "href": "dc_ardcollections.html#accessing-ard-image-collections-in-cloud-providers",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.3 Accessing ARD image collections in cloud providers",
    "text": "4.3 Accessing ARD image collections in cloud providers\n\nTo obtain information on ARD image collections from cloud providers, sits uses the SpatioTemporal Asset Catalogue (STAC) protocol, a specification of geospatial information that many large image collection providers have adopted. A ‘spatiotemporal asset’ is any file that represents information about the Earth captured at a specific space and time. To access STAC endpoints, sits uses the rstac R package.\nThe function sits_cube() supports access to image collections from cloud services; it has the following parameters:\n\n\nsource: Name of the provider.\n\ncollection: A collection available in the provider and supported by sits. To find out which collections are supported by sits, see sits_list_collections().\n\nplatform: Optional parameter specifying the platform in collections with multiple satellites.\n\ntiles: Set of tiles of image collection reference system. Either tiles or roi should be specified.\n\nroi: A region of interest. Either: (a) a named vector (lon_min, lon_max, lat_min, lat_max) in WGS 84 coordinates; (b) an sf object; (c) a path to a shapefile polygon; or (d) A named vector (xmin, xmax, ymin, ymax) with XY coordinates. All images intersecting the convex hull of the roi are selected.\n\nbands: Optional parameter with the bands to be used. If missing, all bands from the collection are used.\n\norbit: Optional parameter required only for Sentinel-1 images (default = “descending”).\n\nstart_date: The initial date for the temporal interval containing the time series of images.\n\nend_date: The final date for the temporal interval containing the time series of images.\n\nThe result of sits_cube() is a tibble with a description of the selected images required for further processing. It does not contain the actual data, but only pointers to the images. The attributes of individual image files can be accessed by listing the file_info column of the tibble."
  },
  {
    "objectID": "dc_ardcollections.html#amazon-web-services",
    "href": "dc_ardcollections.html#amazon-web-services",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.4 Amazon Web Services",
    "text": "4.4 Amazon Web Services\nAmazon Web Services (AWS) holds two kinds of collections: open-data and requester-pays. Open-data collections can be accessed without cost. Requester-pays collections require payment from an AWS account. Currently, sits supports the SENTINEL-2-L2A collection, which is open data. The bands at 10 m resolution are B02, B03, B04, and B08. The 20 m bands are B05, B06, B07, B8A, B11, and B12. Bands B01 and B09 are available at 60 m resolution. A CLOUD band is also available. The example below shows how to access one tile of the open-data SENTINEL-2-L2A collection. The tiles parameter allows selecting the desired area according to the MGRS reference system.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\n\n# Plot\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = (\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\n\n# Plot\nplot(s2_23MMU_cube, \n    red = \"B11\", \n    blue = \"B02\", \n    green = \"B8A\", \n    date = \"2018-10-05\"\n)\n\n\n\n\n\n\n\n\nFigure 4.3: Sentinel-2 image in an area of the Northeastern coast of Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#microsoft-planetary-computer",
    "href": "dc_ardcollections.html#microsoft-planetary-computer",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.5 Microsoft Planetary Computer",
    "text": "4.5 Microsoft Planetary Computer\nThe sits package supports access to open-data collection from Microsoft’s Planetary Computer (MPC), including SENTINEL-1-GRD, SENTINEL-1-RTC, SENTINEL-2-L2A, LANDSAT-C2-L2, COP-DEM-GLO-30 (Copernicus Global DEM at 30-meter resolution), MOD13Q1-6.1, MOD09A1-6.1, MOD10A1-6.1, (version 6.1 of these MODIS products), HLSS30 and HLSL30 (Harmonized Landsat Sentinel images).\n\n4.5.1 SENTINEL-2/2A images in MPC\nThe SENTINEL-2/2A ARD images available in MPC have the same bands and resolutions as those available in AWS (see above). The example below shows how to access the SENTINEL-2-L2A collection.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC &lt;- sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC = sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = (\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n\n\n\n\nFigure 4.4: Sentinel-2 image in an area of the state of Rondonia, Brazil.\n\n\n\n\n4.5.2 LANDSAT-C2-L2 images in MPC\nThe LANDSAT-C2-L2 collection provides access to data from the Landsat-5/7/8/9 satellites. Images from these satellites have been intercalibrated to ensure data consistency. For compatibility between the different Landsat sensors, the band names are BLUE, GREEN, RED, NIR08, SWIR16, and SWIR22. All images have 30 m resolution. For this collection, tile search is not supported; the roi parameter should be used. The example below shows how to retrieve data from a region of interest covering the city of Brasília in Brazil.\n\n\nR\nPython\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi &lt;- c(lon_min = -43.5526, lat_min = -2.9644, \n         lon_max = -42.5124, lat_max = -2.1671)\n\n# Select the cube\ns2_L8_cube_MPC &lt;- sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = c(\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi = dict(lon_min = -43.5526, lat_min = -2.9644, \n         lon_max = -42.5124, lat_max = -2.1671)\n\n# Select the cube\ns2_L8_cube_MPC = sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = (\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n\n\n\n\nFigure 4.5: Landsat-8 image in an area in Northeast Brazil.\n\n\n\n\n4.5.3 SENTINEL-1-GRD images in MPC\nSentinel-1 GRD products consist of focused SAR data that has been detected, multi-looked, and projected to ground range using the WGS84 Earth ellipsoid model. GRD images are subject to variations in the radar signal’s intensity due to topographic effects, antenna pattern, range spreading loss, and other radiometric distortions. The most common types of distortions include foreshortening, layover, and shadowing.\nForeshortening occurs when the radar signal strikes a steep terrain slope facing the radar, causing the slope to appear compressed in the image. Features like mountains can appear much steeper than they are, and their true heights can be difficult to interpret. Layover happens when the radar signal reaches the top of a tall feature (like a mountain or building) before it reaches the base. As a result, the top of the feature is displaced towards the radar and appears in front of its base. This results in a reversal of the order of features along the radar line of sight, making the image interpretation challenging. Shadowing occurs when a radar signal is obstructed by a tall object, casting a shadow on the area behind it that the radar cannot illuminate. The shadowed areas appear dark in SAR images, and no information is available from these regions, similar to optical shadows.\nAccess to Sentinel-1 GRD images can be done either by MGRS tiles (tiles) or by region of interest (roi). We recommend using the MGRS tiling system for specifying the area of interest, since when these images are regularized, they will be reprojected into MGRS tiles. By default, only images in descending orbit are selected.\nThe following example shows how to create a data cube of S1 GRD images over a region in Mato Grosso, Brazil, which is a deforested area of the Amazon forest. The resulting cube will not follow any specific projection and its coordinates will be stated as EPSG 4326 (latitude/longitude). Its geometry is derived from the SAR slant-range perspective; thus, it will appear skewed in relation to the Earth’s longitude.\n\n\nR\nPython\n\n\n\n\ncube_s1_grd &lt;- sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = c(\"VV\"),\n  orbit = \"descending\",\n  tiles = c(\"21LUJ\",\"21LVJ\"),\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\n\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_grd = sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = (\"VV\"),\n  orbit = \"descending\",\n  tiles = (\"21LUJ\",\"21LVJ\"),\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\n\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.6: Sentinel-1 image in an area in Mato Grosso, Brazil.\n\n\n\nAs explained earlier in this chapter, in areas with large elevation differences, Sentinel-1 GRD images will have geometric distortions. For this reason, whenever possible, we recommend the use of RTC (radiometrically terrain-corrected) images as described in the next section.\n\n4.5.4 SENTINEL-1-RTC images in MPC\nAn RTC SAR image has undergone corrections for both geometric and radiometric distortions caused by the terrain. The purpose of RTC processing is to enhance the interpretability and usability of SAR images for various applications by providing a more accurate representation of the Earth’s surface. The radar backscatter values are normalized to account for these variations, ensuring that the image accurately represents the reflectivity of surface features.\nThe terrain correction addresses geometric distortions caused by the side-looking geometry of SAR imaging, such as foreshortening, layover, and shadowing. It uses a Digital Elevation Model (DEM) to model the terrain and reproject the SAR image from the slant range (radar line of sight) to the ground range (true geographic coordinates). This process aligns the SAR image with the actual topography, providing a more accurate spatial representation.\n\n\nR\nPython\n\n\n\n\ncube_s1_rtc &lt;- sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = c(\"VV\", \"VH\"),\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\n\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_rtc = sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = (\"VV\", \"VH\"),\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\n\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.7: Sentinel-1-RTC image of an area in Colombia.\n\n\n\nThe above image is from the central region of Colombia, a country with large variations in altitude due to the Andes Mountains. Users are invited to compare this image with the one from the SENTINEL-1-GRD collection and observe the significant geometrical distortions of the GRD image compared with the RTC one.\n\n4.5.5 Copernicus DEM 30 meter images in MPC\nThe Copernicus Digital Elevation Model 30-meter global dataset (COP-DEM-GLO-30) is a high-resolution topographic data product provided by the European Space Agency (ESA) under the Copernicus Program. The vertical accuracy of the Copernicus DEM 30-meter dataset is typically within a few meters, but this can vary depending on the region and the original data sources. The primary data source for the Copernicus DEM is data from the TanDEM-X mission, developed by the German Aerospace Center (DLR). TanDEM-X provides high-resolution radar data through interferometric synthetic aperture radar (InSAR) techniques.\nThe Copernicus DEM 30-meter is organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. In sits, access to COP-DEM-GLO-30 images can be done either by MGRS tiles (tiles) or by region of interest (roi). In both cases, the cube is retrieved based on the parts of the grid that intersect the region of interest or the chosen tiles.\n\n\nR\nPython\n\n\n\n\ncube_dem_30 &lt;- sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\n\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\ncube_dem_30 = sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\n\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = True)\n\n\n\n\n\n\n\n\nFigure 4.8: Copernicus 30-meter DEM of an area in Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#brazil-data-cube",
    "href": "dc_ardcollections.html#brazil-data-cube",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.6 Brazil Data Cube",
    "text": "4.6 Brazil Data Cube\nThe Brazil Data Cube (BDC) is built by Brazil’s National Institute for Space Research (INPE) to provide regular EO data cubes from CBERS, LANDSAT, SENTINEL-2, and TERRA/MODIS satellites for environmental applications. The collections available in the BDC are: LANDSAT-OLI-16D (Landsat-8 OLI, 30 m resolution, 16-day intervals), SENTINEL-2-16D (Sentinel-2A and 2B MSI images at 10 m resolution, 16-day intervals), CBERS-WFI-16D (CBERS-4 WFI, 64 m resolution, 16-day intervals), CBERS-WFI-8D (CBERS-4 and 4A WFI images, 64 m resolution, 8-day intervals), and MOD13Q1-6.1 (MODIS MOD13SQ1 product, collection 6, 250 m resolution, 16-day intervals). For more details, use sits_list_collections(source = \"BDC\").\nThe BDC uses three hierarchical grids based on the Albers Equal Area projection and SIRGAS 2000 datum. The large grid has tiles of 422.4 \\(\\times4\\) 422.4 km2 and is used for CBERS-4 AWFI collections at 64 m resolution; each CBERS-4 AWFI tile contains images with 6600 \\(\\times\\) 6600 pixels. The medium grid is used for Landsat-8 OLI collections at 30 m resolution; tiles have an extent of 211.2 \\(\\times\\) 211.2 km2, and each image has 7040 \\(\\times\\) 7040 pixels. The small grid covers 105.6 \\(\\times\\) 105.6 km2 and is used for Sentinel-2 MSI collections at 10 m resolutions; each image has 10560 \\(\\times\\) 10560 pixels. The data cubes in the BDC are regularly spaced in time and cloud-corrected [1].\n\n\n\n\nFigure 4.9: Hierarchical BDC tiling system showing (a) large BDC grid overlayed on Brazilian biomes, (b) one learge tile from the grid used for CBERS-4 AWFI data, (c) four medium tiles from the grid used for LANDSAT data; and (d) sixteen small tiles from the grid used for SENTINEL-2 data. Tiles in (b), (c), and (d) are nested.\n\n\n\nTo access the BDC, users must provide their credentials using environment variables, as shown below. Obtaining a BDC access key is free. Users must register at the BDC site to obtain a key. Please include your BDC access key in your .Rprofile.\n\n\nR\nPython\n\n\n\n\nSys.setenv(BDC_ACCESS_KEY = \"&lt;your_bdc_access_key&gt;\")\n\n\n\n\nimport os\n\nos.environ[\"BDC_ACCESS_KEY\"] = \"&lt;your_bdc_access_key&gt;\"\n\n\n\n\nIn the example below, the data cube is defined as one tile (“005004”) of CBERS-WFI-16D collection, which contains CBERS AWFI images at 16-day resolution.\n\n\nR\nPython\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = c(\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"),\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile = sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = (\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"),\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n\n\n\n\nFigure 4.10: CBERS-4 WFI image in a Cerrado area in Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#copernicus-data-space-ecosystem-cdse",
    "href": "dc_ardcollections.html#copernicus-data-space-ecosystem-cdse",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.7 Copernicus Data Space Ecosystem (CDSE)",
    "text": "4.7 Copernicus Data Space Ecosystem (CDSE)\nThe Copernicus Data Space Ecosystem (CDSE) is a cloud service designed to support access to Earth observation data from the Copernicus Sentinel missions and other sources. It is designed and maintained by the European Space Agency (ESA) with support from the European Commission.\nConfiguring user access to CDSE involves several steps to ensure proper registration, access to data, and utilization of the platform’s tools and services. Visit the Copernicus Data Space Ecosystem registration page. Complete the registration form with your details, including name, email address, organization, and sector. Confirm your email address through the verification link sent to your inbox.\nAfter registration, you will need to obtain access credentials to the S3 service implemented by CDSE, which can be obtained using the CDSE S3 credentials site. The site will request that you add a new credential. You will receive two keys: an S3 access key and a secret access key. Take note of both and include the following lines in your .Rprofile.\n\nSys.setenv(\n    AWS_ACCESS_KEY_ID = \"your access key\",\n    AWS_SECRET_ACCESS_KEY = \"your secret access key\",\n    AWS_S3_ENDPOINT = \"eodata.dataspace.copernicus.eu\",\n    AWS_VIRTUAL_HOSTING = \"FALSE\"\n)\n\nIn Python, you can set environment variables either by using the .Rprofile file or by executing the following command:\n\nimport os\n\nos.environ.update(dict(\n    AWS_ACCESS_KEY_ID = \"your access key\",\n    AWS_SECRET_ACCESS_KEY = \"your secret access key\",\n    AWS_S3_ENDPOINT = \"eodata.dataspace.copernicus.eu\",\n    AWS_VIRTUAL_HOSTING = \"FALSE\"\n))\n\nAfter completing the configuration, restart your environment (R or Python) for the changes to take effect. By following these steps, you will gain access to the Copernicus Data Space Ecosystem.\n\n4.7.1 SENTINEL-2/2A images in CDSE\nCDSE hosts a global collection of Sentinel-2 Level-2A images, which are processed according to the CEOS Analysis-Ready Data specifications. One example is provided below, where we present a Sentinel-2 image of the Lena River Delta in Siberia during summertime.\n\n\nR\nPython\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = c(\"52XDF\")\n)\n\n# plot an image from summertime\nplot(\n    lena_cube, \n    date = \"2023-07-06\", \n    red = \"B12\", \n    green = \"B8A\", \n    blue = \"B04\"\n)\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = (\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = \"52XDF\"\n)\n\n# plot an image from summertime\nplot(\n    lena_cube, \n    date = \"2023-07-06\", \n    red = \"B12\", \n    green = \"B8A\", \n    blue = \"B04\"\n)\n\n\n\n\n\n\n\n\nFigure 4.11: Sentinel-2 image of the Lena river delta in summertime.\n\n\n\n\n4.7.2 SENTINEL-1-RTC images in CDSE\nAn important product under development at CDSE is the radiometric terrain corrected (RTC) Sentinel-1 images. In CDSE, this product is referred to as normalized terrain backscatter (NRB). The S1-NRB product contains radiometrically terrain corrected (RTC) gamma nought backscatter (γ⁰) processed from Single Look Complex (SLC) Level-1A data. Each acquired polarization is stored in an individual binary image file.\nAll images are projected and gridded into the United States Military Grid Reference System (US-MGRS). The use of the US-MGRS tile grid ensures a very high level of interoperability with Sentinel-2 Level-2A ARD products making it easy to also set up complex analysis systems that exploit both SAR and optical data. While speckle is inherent in SAR acquisitions, speckle filtering is not applied to the S1-NRB product in order to preserve spatial resolution. Some applications (or processing methods) may require spatial or temporal filtering for stationary backscatter estimates.\nFor more details, please refer to the S1-NRB product website. Global coverage is expected to grow as ESA expands the S1-RTC archive. The following example shows an S1-RTC image for the Rift Valley in Ethiopia.\n\n\nR\nPython\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube &lt;- sits_cube(\n    source = \"CDSE-OS\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = c(\"37NCH\")\n)\n\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube = sits_cube(\n    source = \"CDSE-OS\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = (\"VV\", \"VH\"),\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = \"37NCH\"\n)\n\nplot(s1_cube, band = \"VV\", date = (\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.12: Sentinel-1-RTC image of the Rift Valley in Ethiopia."
  },
  {
    "objectID": "dc_ardcollections.html#digital-earth-africa",
    "href": "dc_ardcollections.html#digital-earth-africa",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.8 Digital Earth Africa",
    "text": "4.8 Digital Earth Africa\nDigital Earth Africa (DEAFRICA) is a cloud service that provides open-access Earth observation data for the African continent. The ARD image collections in sits are:\n\nSentinel-2 level-2A (SENTINEL-2-L2A), organized as MGRS tiles.\nSentinel-1 radiometrically terrain corrected (SENTINEL-1-RTC)\nLandsat-5 (LS5-SR), Landsat-7 (LS7-SR), Landsat-8 (LS8-SR) and Landsat-9 (LS9-SR). All Landsat collections are ARD data and are organized as WRS-2 tiles.\nSAR L-band images produced by PALSAR sensor onboard the Japanese ALOS satellite(ALOS-PALSAR-MOSAIC). Data is organized in a 5\\(^\\circ\\) by 5\\(^\\circ\\) grid with a spatial resolution of 25 meters. Images are available annually from 2007 to 2010 (ALOS/PALSAR) and from 2015 to 2022 (ALOS-2/PALSAR-2).\nEstimates of vegetation condition using NDVI anomalies (NDVI-ANOMALY) compared with the long-term baseline condition. The available measurements are “NDVI_MEAN” (mean NDVI for a month) and “NDVI-STD-ANOMALY” (standardized NDVI anomaly for a month).\nRainfall information provided by Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) from University of California, Santa Barbara. There are monthly (RAINFALL-CHIRPS-MONTHLY) and daily (RAINFALL-CHIRPS-DAILY) products over Africa.\nDigital elevation model provided by the EC Copernicus program (COP-DEM-30) in 30-meter resolution organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid.\nAnnual geomedian images for Landsat 8 and Landsat 9 (GM-LS8-LS9-ANNUAL (LANDSAT/OLI)`) in the WRS-2 grid system.\nAnnual geomedian images for Sentinel-2 (GM-S2-ANNUAL) in MGRS grid.\nRolling three-month geomedian images for Sentinel-2 (GM-S2-ROLLING) in MGRS grid.\nSemestral geomedian images for Sentinel-2 (GM-S2-SEMIANNUAL) in MGRS grid.\n\nAccess to DEAFRICA Sentinel-2 images can be done using the tiles or roi parameter. In this example, the requested roi produces a cube that contains one MGRS tile (“35LPH”) covering an area of Madagascar that includes the Betsiboka Estuary.\n\n\nR\nPython\n\n\n\n\ndea_s2_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = c(\n        lon_min = 46.1, lat_min = -15.6,\n        lon_max = 46.6, lat_max = -16.1\n    ),\n    bands = c(\"B02\", \"B04\", \"B08\"),\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\n\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\ndea_s2_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = dict(\n        lon_min = 46.1, lat_min = -15.6,\n        lon_max = 46.6, lat_max = -16.1\n    ),\n    bands = (\"B02\", \"B04\", \"B08\"),\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\n\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\n\n\n\n\nFigure 4.13: Sentinel-2 image in an area over Madagascar.\n\n\n\nThe next example retrieves a set of ARD Landsat-9 data covering the Serengeti Plain in Tanzania.\n\n\nR\nPython\n\n\n\n\ndea_l9_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = c(\n        lon_min = 33.0, lat_min = -3.60, \n        lon_max = 33.6, lat_max = -3.00\n    ),\n    bands = c(\"B04\", \"B05\", \"B06\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\n\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\ndea_l9_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = dict(\n        lon_min = 33.0, lat_min = -3.60, \n        lon_max = 33.6, lat_max = -3.00\n    ),\n    bands = (\"B04\", \"B05\", \"B06\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\n\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 4.14: Landsat-9 image in an area over the Serengeti in Tanzania.\n\n\n\nThe following example shows how to retrieve a subset of the ALOS-PALSAR mosaic for the year 2020. The area is near the Congo-Rwanda border.\n\n\nR\nPython\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = c(\n        lon_min = 28.69, lat_min = -2.35, \n        lon_max = 29.35, lat_max = -1.56\n    ),\n    bands = c(\"HH\", \"HV\"),\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\n\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = dict(\n        lon_min = 28.69, lat_min = -2.35, \n        lon_max = 29.35, lat_max = -1.56\n    ),\n    bands = (\"HH\", \"HV\"),\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\n\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\n\n\n\n\nFigure 4.15: ALOS-PALSAR mosaic in the Congo forest area."
  },
  {
    "objectID": "dc_ardcollections.html#digital-earth-australia",
    "href": "dc_ardcollections.html#digital-earth-australia",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.9 Digital Earth Australia",
    "text": "4.9 Digital Earth Australia\nDigital Earth Australia (DEAUSTRALIA) is an initiative by Geoscience Australia that uses satellite data to monitor and analyze environmental changes and resources across the Australian continent. It provides many datasets offering detailed information on droughts, agriculture, water availability, floods, coastal erosion, and urban development. The DEAUSTRALIA image collections in sits are:\n\nGA_LS5T_ARD_3: ARD images from the Landsat-5 satellite, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, and “CLOUD”.\nGA_LS7E_ARD_3: ARD images from the Landsat-7 satellite, with the same bands as Landsat-5.\nGA_LS8C_ARD_3: ARD images from the Landsat-8 satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, “PANCHROMATIC”, and “CLOUD”.\nGA_LS9C_ARD_3: ARD images from the Landsat-9 satellite, with the same bands as Landsat-8.\nGA_S2AM_ARD_3: ARD images from the Sentinel-2A satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “RED-EDGE-1”, “RED-EDGE-2”, “RED-EDGE-3”, “NIR-1”, “NIR-2”, “SWIR-2”, “SWIR-3”, and “CLOUD”.\nGA_S2BM_ARD_3: ARD images from the Sentinel-2B satellite, with the same bands as Sentinel-2A.\nGA_LS5T_GM_CYEAR_3: Landsat-5 geomedian images, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR1”, “SWIR2”, “EDEV”, “SDEV”, “BCDEV”.\nGA_LS7E_GM_CYEAR_3: Landsat-7 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS8CLS9C_GM_CYEAR_3: Landsat-8/9 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS_FC_3: Landsat fractional land cover, with bands “BS”, “PV”, “NPV”.\nGA_S2LS_INTERTIDAL_CYEAR_3: Landsat/Sentinel intertidal data, with bands “ELEVATION”, “ELEVATION-UNCERTAINTY”, “EXPOSURE”, “TA-HAT”, “TA-HOT”, “TA-LOT”, “TA-LAT”, “TA-OFFSET-HIGH”, “TA-OFFSET-LOW”, “TA-SPREAD”, “QA-NDWI-CORR”, and “QA-NDWI-FREQ”.\n\nThe following code retrieves an image from Sentinel-2A.\n\n\nR\nPython\n\n\n\n\n# retrieve images for the chosen roi\ns2_56KKV &lt;- sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"),\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n# retrieve images for the chosen roi\ns2_56KKV = sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = (\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"),\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n\n\n\n\nFigure 4.16: Sentinel-2A image from the DEAUSTRALIA collection showing MGRS tile 56KKV."
  },
  {
    "objectID": "dc_ardcollections.html#harmonized-landsat-sentinel",
    "href": "dc_ardcollections.html#harmonized-landsat-sentinel",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.10 Harmonized Landsat-Sentinel",
    "text": "4.10 Harmonized Landsat-Sentinel\nHarmonized Landsat Sentinel (HLS) is a NASA initiative that processes and harmonizes Landsat 8 and Sentinel-2 imagery to a common standard, including atmospheric correction, alignment, resampling, and corrections for BRDF (bidirectional reflectance distribution function). The purpose of the HLS project is to create a unified and consistent dataset that integrates the advantages of both systems, making it easier to work with the data.\nThe NASA Harmonized Landsat and Sentinel (HLS) service provides two image collections:\n\nLandsat 8 OLI Surface Reflectance HLS (HLSL30) – The HLSL30 product includes atmospherically corrected surface reflectance from the Landsat 8 OLI sensors at 30 m resolution. The dataset includes 11 spectral bands.\nSentinel-2 MultiSpectral Instrument Surface Reflectance HLS (HLSS30) – The HLSS30 product includes atmospherically corrected surface reflectance from the Sentinel-2 MSI sensors at 30 m resolution. The dataset includes 12 spectral bands.\n\nThe HLS and Sentinel-2 tiling systems are identical (MGRS). Each tile is 109.8 km wide, with 4,900 m of overlap on each side.\nTo access NASA HLS, users need to register at NASA EarthData, and save their login and password in a ~/.netrc plain text file on Unix (or %HOME%_netrc in Windows). The file must contain the following fields:\n\nmachine urs.earthdata.nasa.gov\nlogin &lt;username&gt;\npassword &lt;password&gt;\n\nWe recommend using the earthdatalogin package to create a .netrc file with earthdatalogin::edl_netrc(). This function creates a properly configured .netrc file in the user’s home directory and sets the environment variable GDAL_HTTP_NETRC_FILE, as shown in the example. As an alternative, we recommend using the HLS collections available in Microsoft Planetary Computer. They are copies of the NASA collections and are faster to access.\n\n\nR\nPython\n\n\n\n\nlibrary(earthdatalogin)\n\nearthdatalogin::edl_netrc( \n    username = \"&lt;your user name&gt;\", \n    password = \"&lt;your password&gt;\" \n) \n\n\n\n\nfrom pysits.extras import earthdatalogin_edl_netrc\n\nearthdatalogin_edl_netrc( \n    username = \"&lt;your user name&gt;\", \n    password = \"&lt;your password&gt;\" \n)\n\n\n\n\nImages in NASA HLS can be accessed by region of interest or by tiles. The following example shows an HLS Sentinel-2 image over the Brazilian coast.\n\n\nR\nPython\n\n\n\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n# define a region of interest\nroi = dict(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 = sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = (\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = \"2020-06-01\",\n  end_date = \"2020-09-01\",\n  progress = False\n)\n\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n\n\n\n\nFigure 4.17: Sentinel-2 image from NASA HLSS30 collection showing the island of Ilhabela in the coast of Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#eo-products-from-terrascope",
    "href": "dc_ardcollections.html#eo-products-from-terrascope",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.11 EO products from TERRASCOPE",
    "text": "4.11 EO products from TERRASCOPE\nTerrascope is an online platform for accessing open-source satellite images. This service, operated by VITO, offers a range of free Earth observation data and processing services. Currently, sits supports the World Cover 2021 maps, produced by VITO with support from the European Commission and ESA. The following code shows how to access the World Cover 2021 covering tile “22LBL”. First, we use sits_mgrs_to_roi() to get the region of interest expressed as a bounding box; this box is then entered as the roi parameter in the sits_cube() function. Since the World Cover data is available as a 3\\(^\\circ\\) by 3\\(^\\circ\\) grid, it is necessary to use sits_cube_copy() to extract the exact MGRS tile.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL &lt;- sits_mgrs_to_roi(\"22LBL\")\n\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 &lt;- sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL &lt;- sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_r\n)\n\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL = sits_mgrs_to_roi(\"22LBL\")\n\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 = sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL = sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_py\n)\n\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\n\n\n\nFigure 4.18: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "dc_ardcollections.html#references",
    "href": "dc_ardcollections.html#references",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nK. R. Ferreira et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products,” Remote Sensing, vol. 12, no. 24, p. 4033, 2020, doi: 10.3390/rs12244033."
  },
  {
    "objectID": "datacubes.html#analysis-ready-data-collections",
    "href": "datacubes.html#analysis-ready-data-collections",
    "title": "Earth observation data cubes",
    "section": "Analysis-ready data collections",
    "text": "Analysis-ready data collections\nAnalysis Ready Data (CEOS-ARD) are satellite data that have been processed to meet the ARD standards defined by the Committee on Earth Observation Satellites (CEOS). ARD data simplify and accelerate the analysis of Earth observation data by providing consistent and high-quality images that are standardized across different sensors and platforms. ARD image processing includes geometric, radiometric, and atmospheric corrections. Images are georeferenced, meaning they are accurately aligned with a coordinate system. Optical ARD images include cloud and shadow masking information. These masks indicate which pixels are affected by clouds or cloud shadows. For optical sensors, CEOS-ARD images are converted to surface reflectance values, which represent the fraction of light that is reflected by the surface. This makes the data more comparable across different times and locations. For SAR images, CEOS-ARD specification require images to undergo Radiometric Terrain Correction (RTC) and be provided in the gamma nought (\\(\\gamma_0\\)) backscatter values. This value which mitigates the variations from diverse observation geometries and is recommended for most land applications.\nARD images are available from various satellite platforms, including Landsat, Sentinel, and commercial satellites. This provides a wide range of spatial, spectral, and temporal resolutions to suit different applications. They are organized as a collection of files, where each pixel contains a single value for each spectral band for a given date. These collections are available in cloud services such as Brazil Data Cube, Digital Earth Africa, and Microsoft’s Planetary Computer. In general, the timelines of the images in an ARD collection are different. Images may still contain cloudy or missing pixels, and bands for the images in the collection may have different resolutions. Figure 1 shows an example of the Landsat ARD image collection.\n\n\n\n\nFigure 1: ARD image collection (source: USGS)."
  },
  {
    "objectID": "datacubes.html#regular-earth-observation-data-cubes",
    "href": "datacubes.html#regular-earth-observation-data-cubes",
    "title": "Earth observation data cubes",
    "section": "Regular Earth observation data cubes",
    "text": "Regular Earth observation data cubes\nMachine learning and deep learning (ML/DL) classification algorithms require the input data to be consistent. The dimensionality of the data used for training the model has to be the same as that of the data to be classified. There should be no gaps and no missing values. Thus, to use ML/DL algorithms for remote sensing data, ARD image collections should be converted to regular data cubes.\nA regular EO data cube is a partition of the Earth’s surface which covers the area in periodic intervals. Regular data cubes are derived by ARD images by filling coverage gaps, accounting for cloud coverage, and reprocessing irregular temporal coverage to regular periods. Also, regular data cubes can cover more than a single coordinate projection zone, so as to be defined in large areas.\nIn sits, regular data cubes have the following properties:\n\nA regular data cube is a set of three-dimensional arrays with two spatial dimensions and one temporal dimension. Each array contains cells which have the same spatial resolution, the same temporal duration, and the same set of attributes (spectral bands and/or indices).\nThe number of dimensions is fixed (2D + time), while the number of attributes is not limited.\nThe spatial dimensions are associated to a coordinate projection system, such as the MGRS grid (Military Grid Reference System). A tile of the grid corresponds to a unique zone of the coordinate system. A data cube may span various tiles and projection zones.\nThe temporal dimension is a set of continuous and equally spaced intervals.\nFor each position in space, the cube should provide a multi-attribute time series. For each time interval, the cube should provide a multi-attribute 2D image (see Figure 2).\n\n\n\n\n\nFigure 2: Conceptual view of a data cube.\n\n\n\nThe definition of data cubes in sits differs from an earlier proposal by Appel and Pebesma [1] which has been used in the R package stars and in the openEO system. Appel and Pebesma consider that a data cube will only cover a single tile of a coordinate system (e.g., MGRS). In sits, this restriction has been removed and data cubes many span multiple tiles.\nCurrently, the only cloud service that provides regular data cubes by default is the Brazil Data Cube (BDC). ARD collections available in other cloud services are not regular in space and time. Bands may have different resolutions, images may not cover the entire timeline, and time intervals may be irregular. For this reason, subsets of these collections need to be converted into regular data cubes before further processing. To produce data cubes for machine learning data analysis, this part of the book describes the steps involved in producing and using regular data cubes:\n\nObtaining data from ARD image collections\nProducing regular data cubes from single- and multi-source data\nRecovering data cubes from local files\nPerforming operations on data cubes"
  },
  {
    "objectID": "datacubes.html#references",
    "href": "datacubes.html#references",
    "title": "Earth observation data cubes",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Appel and E. Pebesma, “On-Demand Processing of Data Cubes from Satellite Image Collections with the gdalcubes Library,” Data, vol. 4, no. 3, 2019, doi: 10.3390/data4030092."
  },
  {
    "objectID": "setup.html#configuring-the-base-environment-for-r-and-python",
    "href": "setup.html#configuring-the-base-environment-for-r-and-python",
    "title": "Setup",
    "section": "Configuring the base environment for R and Python",
    "text": "Configuring the base environment for R and Python\n\n\n\n\n\n\nThe R environment is the foundation for sits and must be installed regardless of whether you plan to use R or Python interfaces.\n\n\n\nTo use this book, you need an R environment configured with the following tools: including:\n\nR Programming Language\nSystem Dependencies\nThe sits R package\nThe sitsdata data package (for examples)\n\nThe sections below outline how you can install these tools.\nInstalling R\nDownload and install the latest version of R for your operating system:\n\n\nWindows\nmacOS\nLinux\n\n\n\nDownload the installer from CRAN Windows\n\n\nDownload from CRAN macOS\n\n\nUse your distribution’s package manager or download from CRAN Linux\n\n\n\nInstall RStudio for R users\nRStudio provides an integrated development environment (IDE) for R, available for Linux, macOS and Windows, offering a code editor, plot preview, debugging tools, package management, and more. You can download RStudio from the Posit website.\nInstalling RStudio is optional, as you can use R in any other IDE or code editor. However, it is recommended for R users as it offers a very easy-to-use environment and amazing integration with the R Package ecossystem.\nInstalling SITS in R\nThe sits package is available through the Comprehensive R Archive Network (CRAN), which is the official distribution channel for R packages.\n\n\nmacOS and Windows\nUbuntu\nFedora\nOther Linux distributions\n\n\n\nmacOS and Windows users should install binary packages from CRAN. The installation process requires specific dependencies to be installed in the correct order.\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\n\n\n\n\n\n\nPackage compilation\n\n\n\nSome dependencies from the sits package may cause errors during installation, as they require compilation. If you encounter this issue, there are additional tools that can help. For source compilation on Windows, install Rtools. For macOS, follow the instructions at mac.r-project.org/tools/.\n\n\n\n\nLinux installations require system-level dependencies to be installed first.\nStep 1: Install system dependencies\n\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt-get update\nsudo apt-get install libudunits2-dev \\\n                        libgdal-dev \\\n                        libgeos-dev \\\n                        libproj-dev \\\n                        gdal-bin \\\n                        proj-bin\n\nIf you encounter PPA repository errors, install the required package first:\n\nsudo apt-get install software-properties-common\n\nStep 2: Install sits\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\n\n\nStep 1: Install system dependencies\n\nsudo dnf install gdal \\\n                    gdal-devel \\\n                    proj-devel \\\n                    geos-devel \\\n                    sqlite-devel \\\n                    udunits2-devel\n\nStep 2: Install sits\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\n\n\nFor other Linux distributions, we recommend using Docker containers or following the distribution-specific GDAL installation guides.\n\n\n\nInstalling the R sitsdata package\nExamples in this book use open data available in the sitsdata package. This package is freely available on GitHub, and can be installed in Linux, macOS and Windows, as follows:\n1. Increase the network timeout time\n\noptions(timeout = 300) # Set network timeout to 5 minutes\n\n2. Install sitsdata\n\ndevtools::install_github(\"e-sensing/sitsdata\")"
  },
  {
    "objectID": "setup.html#configuring-the-python-environment",
    "href": "setup.html#configuring-the-python-environment",
    "title": "Setup",
    "section": "Configuring the Python environment",
    "text": "Configuring the Python environment\n\n\n\n\n\n\nThe Python API uses the R implementation, so R must be installed before setting up the Python environment.\n\n\n\nThe Python interface for sits is as a wrapper around the R implementation. It gives Python users access to the all features of the sits package. This section outlines the available options for installing it.\nPrerequisites\nTo use sits in Python, you must fill the following prerequisites:\n\n\nR environment, including dependencies.\nArrow package in R\nPython 3.10 or higher\nInstall Python\nDownload and install the latest version of Python for your operating system:\n\n\nWindows\nmacOS\nLinux\n\n\n\nDownload the installer from the Python website\n\n\nDownload the installer from the Python website\n\n\nUse your distribution’s package manager or download from the Python website\n\n\n\nInstalling arrow in R\nTo transfer objects between R and Python, the Apache Arrow is used. Therefore, to take advantage of its features, it must be installed in your R environment. You can install it using the following command:\ninstall.packages('arrow')\nInstalling SITS in Python\nThe sits package is available in the PyPi, and can be installing using the following tools:\n\n\npip\npip (with venv)\n\n\n\nIn your terminal, run:\n\npip install pysits\n\n\n\nIn your terminal, run:\n1. Create virtual environment\n\npython -m venv venv\n\n2. Activate environment\n\nsource venv/bin/activate # or .venv\\Scripts\\activate on Windows\n\n3. Install pysits\n\npip install pysits\n\n\n\n\n\n\n\nVirtual environments\n\n\n\nIf you’d like to learn more about Python virtual environments, please refer to the official Python documentation on the topic.\n\n\n\n\n\nDevelopment versions\nApart from the official versions of sits available on CRAN and PyPI, as described above, users who want to preview features planned for future releases have the option to install the development version.\n\n\n\n\n\n\nAdvanced users\n\n\n\nUsing the development version is recommended for advanced users or those comfortable with troubleshooting within the sits ecosystem.\n\n\nYou can install development versions in R and Python, using the following commands:\n\n\nR\nPython\n\n\n\nIn your terminal, run:\n\ndevtools::install_github(\"e-sensing/sits@dev\", dependencies = TRUE)\n\n\n\nIn your terminal, run:\n\npip install git+https://github.com/e-sensing/pysits.git@dev\n\n\n\n\nGPU Support for Deep Learning\nFor users working with deep learning models, sits supports GPU acceleration through the torch package, which is already included as a dependency when you install sits. This provides significant performance improvements for deep learning operations.\nAutomatic GPU detection:\nThe torch package automatically detects available GPUs and utilizes them when possible, requiring no additional configuration in most cases.\nSystem requirements:\n\nNVIDIA CUDA toolkit\nCompatible NVIDIA GPU\nAppropriate GPU drivers\n\nGPU configuration:\nTo use GPU in sits with torch you must have an environment with proper configuration of the NVIDIA toolkit, including:\n\n\nCUDA installation: Ensure CUDA toolkit is properly installed and accessible\n\nDriver compatibility: Verify NVIDIA drivers are compatible with your CUDA version\n\nFor more details on GPU setup instructions and troubleshooting, refer to the torch documentation."
  },
  {
    "objectID": "intro_quicktour.html#overview",
    "href": "intro_quicktour.html#overview",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.1 Overview",
    "text": "1.1 Overview\nIn this chapter, we present a simple example of using sits for agricultural classification. We start by taking a set of samples with land use and land cover types for an area in the Cerrado biome in Brazil, near the city of Luis Eduardo Magalhães in the state of Bahia. These samples were collected by a team of INPE researchers [1]."
  },
  {
    "objectID": "intro_quicktour.html#training-samples",
    "href": "intro_quicktour.html#training-samples",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.2 Training samples",
    "text": "1.2 Training samples\nIn this example, we take the data set for agriculture in the municipality of Luis Eduardo Magalhaes (henceforth LEM) as our starting point. This is a common situation in land classification. To be able to obtain time series information, sits requires that the training samples provide, for each sample, information on location, temporal range, and associated label. In this example, we use a data frame with five columns: longitude, latitude, start_date, end_date and label. The training samples are available in the R sitsdata package. Alternative ways of defining data samples include CSV files and shapefiles. Please refer to chapter “Working with time series”.\n\n\nR\nPython\n\n\n\n\n# Load the samples for LEM from the \"sitsdata\" package\n# select the directory for the samples \nsamples_dir &lt;- system.file(\"data\", package = \"sitsdata\")\n\n# retrieve a data.frame with the samples\ndf_samples_cerrado_lem &lt;- readRDS(file.path(samples_dir, \"df_samples_cerrado_lem.rds\"))\ndf_samples_cerrado_lem\n\n# A tibble: 2,302 × 5\n   longitude latitude start_date end_date   label  \n       &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;  \n 1     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 2     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 3     -46.4    -12.3 2019-09-30 2020-09-29 Pasture\n 4     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 5     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 6     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 7     -46.6    -12.4 2019-09-30 2020-09-29 Pasture\n 8     -46.6    -12.3 2019-09-30 2020-09-29 Pasture\n 9     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n10     -46.6    -12.4 2019-09-30 2020-09-29 Pasture\n# ℹ 2,292 more rows\n\n\n\n\n\n# Load the samples for Mato Grosso from the \"sitsdata\" package\n# select the directory for the samples \nsamples_file = r_package_dir(\"data/df_samples_cerrado_lem.rds\", package = \"sitsdata\")\n\n# retrieve a data.frame with the samples\ndf_samples_cerrado_lem = read_rds(samples_file)\ndf_samples_cerrado_lem\n\n      longitude   latitude  start_date    end_date              label\n0    -46.536000 -12.306000  2019-09-30  2020-09-29            Pasture\n1    -46.545000 -12.310000  2019-09-30  2020-09-29            Pasture\n2    -46.420000 -12.338000  2019-09-30  2020-09-29            Pasture\n3    -46.513000 -12.322000  2019-09-30  2020-09-29            Pasture\n4    -46.506000 -12.328000  2019-09-30  2020-09-29            Pasture\n...         ...        ...         ...         ...                ...\n2297 -45.871588 -12.401668  2019-09-30  2020-09-29            Cerrado\n2298 -45.871573 -12.426070  2019-09-30  2020-09-29  Cropland_2_cycles\n2299 -45.869899 -12.429560  2019-09-30  2020-09-29  Cropland_2_cycles\n2300 -45.869663 -12.430767  2019-09-30  2020-09-29  Cropland_2_cycles\n2301 -45.868158 -12.430211  2019-09-30  2020-09-29  Cropland_2_cycles\n\n[2302 rows x 5 columns]\n\n\n\n\n\nThe data set contains 2,302 samples divided into four classes: (a) “Cerrado,” which corresponds to the natural vegetation associated with the Brazilian savanna; (b) “Cropland_1_cycle,” temporary agriculture (mostly soybeans) planted in a single cycle from October to March; (c) “Cropland_2_cycles,” temporary agriculture planted in two cycles, the first from October to March and the second from April to July; (d) “Pasture,” areas for cattle raising. For convenience, we present a high-resolution image of the area with the location of the samples.\n\n\n\n\nFigure 1.1: High resolution image of the LEM area with samples. Cerrado samples are shown in green, Pasture ones in yellow, Cropland_1_cycle in light brown and Cropland_2_cycles in dark brown."
  },
  {
    "objectID": "intro_quicktour.html#creating-a-data-cube-based-on-the-ground-truth-samples",
    "href": "intro_quicktour.html#creating-a-data-cube-based-on-the-ground-truth-samples",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.3 Creating a data cube based on the ground truth samples",
    "text": "1.3 Creating a data cube based on the ground truth samples\nThere are two kinds of data cubes in sits: (a) non-regular data cubes generated by selecting ARD image collections on cloud providers such as AWS and Planetary Computer; (b) regular data cubes with images fully covering a chosen area, where each image has the same spectral bands and spatial resolution, and images follow a set of adjacent and regular time intervals. Machine learning applications need regular data cubes. Please refer to Chapter Building regular data cubes for further details.\nThe first steps in using sits are: (a) select an analysis-ready data image collection available from a cloud provider or stored locally using sits_cube(); (b) if the collection is not regular, use sits_regularize() to build a regular data cube.\nThis example builds a data cube from local images already organized as a regular data cube available in the Brazil Data Cube (“BDC”). The data cube is composed of CBERS-4 and CBERS-4A images for the LEM data set, covering the training samples. The images are taken with the WFI (wide field imager) sensor with 64-meter resolution. All images have indices NDVI and EVI covering a one-year period from 2019-09-30 to 2020-09-29 (we use “year-month-day” for dates). There are 24 time instances, each covering a 16-day period. We first define the region of interest using the bounding box of the LEM data set, and then define a data cube in the BDC repository based on this region. This data cube will be composed of the CBERS images that intersect with the region of interest.\n\n\nR\nPython\n\n\n\n\n# Find the the bounding box of the data\nlat_max &lt;- max(df_samples_cerrado_lem[[\"latitude\"]])\nlat_min &lt;- min(df_samples_cerrado_lem[[\"latitude\"]])\nlon_max &lt;- max(df_samples_cerrado_lem[[\"longitude\"]])\nlon_min &lt;- min(df_samples_cerrado_lem[[\"longitude\"]])\n\n# Define the roi for the LEM dataset\nroi_lem &lt;- c(\n    \"lat_max\" = lat_max,\n    \"lat_min\" = lat_min,\n    \"lon_max\" = lon_max,\n    \"lon_min\" = lon_min)\n\n# Define a data cube in the BDC repository based on the LEM ROI\nbdc_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection  = \"CBERS-WFI-16D\",\n    bands = c(\"NDVI\", \"EVI\"),\n    roi = roi_lem,\n    start_date = \"2019-09-30\",\n    end_date = \"2020-09-29\"\n)\n\n\n\n\n# Find the the bounding box of the data\nlat_max = max(df_samples_cerrado_lem[\"latitude\"])\nlat_min = min(df_samples_cerrado_lem[\"latitude\"])\nlon_max = max(df_samples_cerrado_lem[\"longitude\"])\nlon_min = min(df_samples_cerrado_lem[\"longitude\"])\n\n# Define the roi for the LEM dataset\nroi_lem = dict(\n    lat_max = lat_max,\n    lat_min = lat_min,\n    lon_max = lon_max,\n    lon_min = lon_min\n)\n\n# Define a data cube in the BDC repository based on the LEM ROI\nbdc_cube = sits_cube(\n    source = \"BDC\", \n    collection  = \"CBERS-WFI-16D\",\n    bands = (\"NDVI\", \"EVI\"),\n    roi = roi_lem,\n    start_date = \"2019-09-30\",\n    end_date = \"2020-09-29\"\n)\n\n\n\n\nThe next step is to copy the data cube to a local directory for further processing. When using a region of interest to select a part of an ARD collection, sits intersects the region with the tiles of that collection. Thus, when one wants to get a subset of a tile, it is better to copy this subset to the local computer. After downloading the data, we use plot() to view it. The plot() function, by default, selects the image with the least cloud cover.\n\n\nR\nPython\n\n\n\n\n# Copy the region of interest to a local directory\nlem_cube &lt;- sits_cube_copy(\n    cube = bdc_cube,\n    roi = roi_lem,\n    output_dir = tempdir_r\n)\n\n# Plot the cube \nplot(lem_cube, palette = \"RdYlGn\")\n\n\n\n\n# Copy the region of interest to a local directory\nlem_cube = sits_cube_copy(\n    cube = bdc_cube,\n    roi = roi_lem,\n    output_dir = tempdir_py\n)\n\n# Plot the cube\nplot(lem_cube, palette = \"RdYlGn\")\n\n\n\n\n\n\n\n\nFigure 1.2: False color CBERS image for NDVI band in 2013-09-30.\n\n\n\nThe object returned by sits_cube() and by sits_cube_copy() contains the metadata describing the contents of the data cube. It includes the data source and collection, satellite, sensor, tile in the collection, bounding box, projection, and list of files. Each file refers to one band of an image at one of the temporal instances of the cube. Since data cubes obtained from the BDC are already regularized, there is no need to run sits_regularize(). Please refer to chapter “Building regular EO data cubes” for information on dealing with non-regular ARD collections.\n\n\nR\nPython\n\n\n\n\n# Show the description of the data cube\nlem_cube\n\n# A tibble: 1 × 12\n  source collection    satellite sensor tile     xmin   xmax   ymin   ymax crs  \n  &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 BDC    CBERS-WFI-16D CBERS-4   WFI    007004 5.79e6 5.95e6 9.88e6 9.96e6 \"PRO…\n# ℹ 2 more variables: labels &lt;list&gt;, file_info &lt;list&gt;\n\n\n\n\n\n# Show the description of the data cube\nlem_cube\n\n  source     collection satellite sensor  ...       ymin       ymax                                                crs                                          file_info\n0    BDC  CBERS-WFI-16D   CBERS-4    WFI  ...  9876544.0  9955136.0  PROJCRS[\"unknown\",\\n    BASEGEOGCRS[\"unknown\",...                             fid  band        da...\n\n[1 rows x 11 columns]\n\n\n\n\n\nThe list of image files that make up the data cube is stored as a data frame in the column file_info. For each file, sits stores information about the spectral band, reference date, size, spatial resolution, coordinate reference system, bounding box, path to the file location, and cloud cover information (when available).\n\n\nR\nPython\n\n\n\n\n# Show information on the images files which are part of a data cube\nlem_cube$file_info[[1]]\n\n# A tibble: 48 × 13\n   fid      band  date       ncols nrows  xres  yres   xmin   xmax   ymin   ymax\n   &lt;chr&gt;    &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 CB4-16D… EVI   2019-09-30  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 2 CB4-16D… NDVI  2019-09-30  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 3 CB4-16D… EVI   2019-10-16  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 4 CB4-16D… NDVI  2019-10-16  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 5 CB4-16D… EVI   2019-11-01  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 6 CB4-16D… NDVI  2019-11-01  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 7 CB4-16D… EVI   2019-11-17  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 8 CB4-16D… NDVI  2019-11-17  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 9 CB4-16D… EVI   2019-12-03  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n10 CB4-16D… NDVI  2019-12-03  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n# ℹ 38 more rows\n# ℹ 2 more variables: crs &lt;chr&gt;, path &lt;chr&gt;\n\n\n\n\n\n# Show information on the images files which are part of a data cube\nlem_cube[\"file_info\"][0].head(5)\n\n                          fid  band        date   ncols  ...       ymin       ymax                                                crs                                               path\n0  CB4-16D_V2_007004_20190930   EVI  2019-09-30  2539.0  ...  9876544.0  9955136.0  PROJCRS[\"unknown\",\\n    BASEGEOGCRS[\"unknown\",...  /Users/gilbertocamara/sitsbook/tempdir/R/intro...\n1  CB4-16D_V2_007004_20190930  NDVI  2019-09-30  2539.0  ...  9876544.0  9955136.0  PROJCRS[\"unknown\",\\n    BASEGEOGCRS[\"unknown\",...  /Users/gilbertocamara/sitsbook/tempdir/R/intro...\n2  CB4-16D_V2_007004_20191016   EVI  2019-10-16  2539.0  ...  9876544.0  9955136.0  PROJCRS[\"unknown\",\\n    BASEGEOGCRS[\"unknown\",...  /Users/gilbertocamara/sitsbook/tempdir/R/intro...\n3  CB4-16D_V2_007004_20191016  NDVI  2019-10-16  2539.0  ...  9876544.0  9955136.0  PROJCRS[\"unknown\",\\n    BASEGEOGCRS[\"unknown\",...  /Users/gilbertocamara/sitsbook/tempdir/R/intro...\n4  CB4-16D_V2_007004_20191101   EVI  2019-11-01  2539.0  ...  9876544.0  9955136.0  PROJCRS[\"unknown\",\\n    BASEGEOGCRS[\"unknown\",...  /Users/gilbertocamara/sitsbook/tempdir/R/intro...\n\n[5 rows x 13 columns]\n\n\n\n\n\nA key attribute of a data cube is its timeline, as shown below. The command sits_timeline() lists the temporal references associated to sits objects, including samples, data cubes and models.\n\n\nR\nPython\n\n\n\n\n# Show the R object that describes the data cube\nsits_timeline(lem_cube)\n\n [1] \"2019-09-30\" \"2019-10-16\" \"2019-11-01\" \"2019-11-17\" \"2019-12-03\"\n [6] \"2019-12-19\" \"2020-01-01\" \"2020-01-17\" \"2020-02-02\" \"2020-02-18\"\n[11] \"2020-03-05\" \"2020-03-21\" \"2020-04-06\" \"2020-04-22\" \"2020-05-08\"\n[16] \"2020-05-24\" \"2020-06-09\" \"2020-06-25\" \"2020-07-11\" \"2020-07-27\"\n[21] \"2020-08-12\" \"2020-08-28\" \"2020-09-13\" \"2020-09-29\"\n\n\n\n\n\n# Show the R object that describes the data cube\nsits_timeline(lem_cube)\n\n['2019-09-30', '2019-10-16', '2019-11-01', '2019-11-17', '2019-12-03', '2019-12-19', '2020-01-01', '2020-01-17', '2020-02-02', '2020-02-18', '2020-03-05', '2020-03-21', '2020-04-06', '2020-04-22', '2020-05-08', '2020-05-24', '2020-06-09', '2020-06-25', '2020-07-11', '2020-07-27', '2020-08-12', '2020-08-28', '2020-09-13', '2020-09-29']\n\n\n\n\n\nThe timeline of lem_cube has 24 intervals with a temporal difference of 16 days. The chosen dates capture the agricultural calendar in the west of Bahia in Brazil. The agricultural year starts in September-October with the sowing of the summer crop (usually soybeans), which is harvested in February-March. Then the winter crop (mostly Corn, Cotton, or Millet) is planted in March and harvested in June-July. For LULC classification, the training samples and the data cube should share a timeline with the same number of intervals and similar start and end dates."
  },
  {
    "objectID": "intro_quicktour.html#the-time-series-tibble",
    "href": "intro_quicktour.html#the-time-series-tibble",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.4 The time series tibble",
    "text": "1.4 The time series tibble\nTo handle time series information, sits uses a tibble. Tibbles are extensions of the data.frame tabular data structures provided by the tidyverse set of packages. In this chapter, we will use a training set with 1,101 time series obtained from MODIS MOD13Q1 images. Each series has two indexes (NDVI and EVI). To obtain the time series, we need two inputs:\n\nA CSV file, shapefile or a data.frame containing information on the location of the sample and date of validity. The following information is required: longitude, latitude, start_date, end_date, class. It can either be provided as columns of a data.frame or a CSV file, or as attributes of a shapefile. Please refer to Chapter “Working with time series” for more information.\nA regular data cube which covers the dates indicated in the sample file. Each sample will be located in the data cube based on its longitude and latitude, and the time series is extracted based on the start and end dates and on the available bands of the cube.\n\nIn what follows, we use the data frame with the LEM samples. The data.frame contains spatial and temporal information and the label assigned to the sample. Based on this information, we will retrieve the time series from the BDC cube using sits_get_data(). In general, it is necessary to regularize the data cube so that all time series have the same dates. In our case, we use a regular data cube provided by the BDC repository.\n\n\nR\nPython\n\n\n\n\n# Retrieve the time series for each samples based on a data.frame\nsamples_lem_time_series &lt;- sits_get_data(\n    cube = lem_cube,\n    samples = df_samples_cerrado_lem\n)\n\n\n\n\n# Retrieve the time series for each samples based on a data.frame\nsamples_lem_time_series = sits_get_data(\n    cube = lem_cube,\n    samples = df_samples_cerrado_lem\n)\n\n\n\n\nThe time series tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band. The time series can be displayed by showing the time_series nested column.\n\n\nR\nPython\n\n\n\n\n# Load the time series for the first MODIS sample for Mato Grosso\nsamples_lem_time_series[1,]$time_series[[1]]\n\n# A tibble: 24 × 3\n   Index        EVI  NDVI\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2019-09-30 0.203 0.240\n 2 2019-10-16 0.254 0.352\n 3 2019-11-01 0.297 0.390\n 4 2019-11-17 0.344 0.542\n 5 2019-12-03 0.174 0.206\n 6 2019-12-19 0.353 0.498\n 7 2020-01-01 0.433 0.659\n 8 2020-01-17 0.441 0.595\n 9 2020-02-02 0.431 0.674\n10 2020-02-18 0.565 0.672\n# ℹ 14 more rows\n\n\n\n\n\n# Load the time series for the first MODIS sample for Mato Grosso\nsamples_lem_time_series[\"time_series\"][0]\n\n         Index     EVI    NDVI\n0   2019-09-30  0.2031  0.2403\n1   2019-10-16  0.2537  0.3517\n2   2019-11-01  0.2973  0.3899\n3   2019-11-17  0.3443  0.5415\n4   2019-12-03  0.1740  0.2059\n..         ...     ...     ...\n19  2020-07-27  0.2504  0.3612\n20  2020-08-12  0.2272  0.3378\n21  2020-08-28  0.2080  0.2962\n22  2020-09-13  0.2077  0.2755\n23  2020-09-29  0.2018  0.2866\n\n[24 rows x 3 columns]\n\n\n\n\n\nThe distribution of samples per class can be obtained using the summary() command. The classification schema uses four labels, two associated with crops (Cropland_2_cycles, Cropland_1_cycle), one with natural vegetation (Cerrado), and one with Pasture.\n\n\nR\nPython\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_lem_time_series)\n\n# A tibble: 4 × 3\n  label             count   prop\n  &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado             136 0.0592\n2 Cropland_1_cycle   1250 0.544 \n3 Cropland_2_cycles   790 0.344 \n4 Pasture             123 0.0535\n\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_lem_time_series)\n\n               label  count      prop\n1            Cerrado    136  0.059156\n2   Cropland_1_cycle   1250  0.543715\n3  Cropland_2_cycles    790  0.343628\n4            Pasture    123  0.053502\n\n\n\n\n\nThe sample dataset is highly imbalanced. To improve performance, we will produce a balanced set after obtaining the time series, as seen below. The sits_reduce_imbalance() function will reduce the maximum number of samples per class to 256 and increase the minimum number to 100. This procedure is recommended to improve the performance of the classification model. The parameter n_samples_over selects the minimum number of samples of the less frequent classes, and the parameter n_samples_under indicates the maximum number of samples for the more frequent ones.\n\n\nR\nPython\n\n\n\n\n# Reduce imbalance between the classes\nsamples_lem &lt;- sits_reduce_imbalance(\n    samples_lem_time_series,\n    n_samples_over = 150,\n    n_samples_under = 300\n)\n\n\n\n\n# Reduce imbalance between the classes\nsamples_lem = sits_reduce_imbalance(\n    samples_lem_time_series,\n    n_samples_over = 150,\n    n_samples_under = 300\n)\n\n\n\n\nThe new distribution of samples per class can be obtained using the summary() command. It should show a more balanced data set.\n\n\nR\nPython\n\n\n\n\n# Show the summary of the balanced time series sample data\nsummary(samples_lem)\n\n# A tibble: 4 × 3\n  label             count  prop\n  &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n1 Cerrado             150 0.159\n2 Cropland_1_cycle    320 0.339\n3 Cropland_2_cycles   324 0.343\n4 Pasture             150 0.159\n\n\n\n\n\n# Show the summary of the balanced time series sample data\nsummary(samples_lem)\n\n               label  count      prop\n1            Cerrado    150  0.158898\n2   Cropland_1_cycle    320  0.338983\n3  Cropland_2_cycles    324  0.343220\n4            Pasture    150  0.158898\n\n\n\n\n\nIt is helpful to plot the dispersion of the time series. In what follows, for brevity, we will filter only one label (Cropland_2_cycles) and select one index (NDVI).\n\n\nR\nPython\n\n\n\n\n# Select all samples with label \"Cropland_2_cycles\" using `dplyr::filter`\n# since the label attribute is a column of the samples data.frame\nsamples_cropland_2cycles &lt;- dplyr::filter(\n    samples_lem, \n    label == \"Cropland_2_cycles\"\n)\n\n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsamples_crops_2cycles_ndvi &lt;- sits_select(\n    samples_cropland_2cycles,\n    bands = \"NDVI\"\n) \n\n# plot the samples for label Cropland_2_cycles and band NDVI\nplot(samples_crops_2cycles_ndvi)\n\n\n\nFigure 1.3: Joint plot of all samples in band NDVI for label Cropland_2_cycles.\n\n\n\n\n\n\n# Select all samples with label \"Cropland_2_cycles\" using `query` \n# since the label attribute is a column of the samples data.frame\nsamples_crops_2cycles = samples_lem.query('label == \"Cropland_2_cycles\"')\n\n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsamples_crops_2cycles_ndvi = sits_select(samples_crops_2cycles, bands = \"NDVI\")\n\n# plot the samples for label Cropland_2_cycles and band NDVI\nplot(samples_crops_2cycles_ndvi)\n\n\n\nFigure 1.4: Joint plot of all samples in band NDVI for label Forest.\n\n\n\n\n\n\nThe figure above shows all the time series associated with the label Cropland_2_cycles and the band NDVI (in light blue), highlighting the median (shown in dark red) and the first and third quartiles (shown in brown). The two-crop-cycles pattern is clearly visible. The spikes at the end of the year are due to high cloud cover."
  },
  {
    "objectID": "intro_quicktour.html#training-a-machine-learning-model",
    "href": "intro_quicktour.html#training-a-machine-learning-model",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.5 Training a machine learning model",
    "text": "1.5 Training a machine learning model\nThe next step is to train a machine learning (ML) model using sits_train(). It takes two inputs, samples (a time series tibble) and ml_method (a function that implements a machine learning algorithm). The result is a model that is used for classification. Each ML algorithm requires specific parameters that are user-controllable. For novice users, sits provides default parameters that produce good results. Please see Chapter Machine learning algorithms for image time series for more details.\nTo build the classification model, we use a Random Forest model called by sits_rfor(). Results from the Random Forest model can vary between different runs, due to the stochastic nature of the algorithm. In the code fragment below, we set the seed of R’s pseudo-random number generator explicitly to ensure results don’t change when running multiple times. This is done for documentation purposes.\n\n\nR\nPython\n\n\n\n\nset.seed(03022024)\n\n# Train a random forest model\nrf_model &lt;- sits_train(\n    samples = samples_lem, \n    ml_method = sits_rfor()\n)\n\n# Plot the most important variables of the model\nplot(rf_model)\n\n\n\nFigure 1.5: Most relevant variables of trained Random Forest model.\n\n\n\n\n\n\nr_set_seed(3022024)\n\n# Train a Random Forest model\nrf_model = sits_train(\n    samples = samples_lem, \n    ml_method = sits_rfor()\n)\n\n# Plot the most important variables of the model\nplot(rf_model)\n\n\n\nFigure 1.6: Most relevant variables of trained Random Forest model."
  },
  {
    "objectID": "intro_quicktour.html#data-cube-classification",
    "href": "intro_quicktour.html#data-cube-classification",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.6 Data cube classification",
    "text": "1.6 Data cube classification\nAfter training the machine learning model, the next step is to classify the data cube using sits_classify(). This function produces a set of raster probability maps, one for each class. For each of these maps, the value of a pixel is proportional to the probability that it belongs to the class. This function has two mandatory parameters: data, the data cube or time series tibble to be classified; and ml_model, the trained ML model. Optional parameters include: (a) multicores, number of cores to be used; (b) memsize, RAM used in the classification; (c) output_dir, the directory where the classified raster files will be written. Details of the classification process are available in Chapter Classification of raster data cubes.\n\n\nR\nPython\n\n\n\n\n# Classify the data cube to produce a map of probabilities\nlem_probs &lt;- sits_classify(\n    data = lem_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n\n# Plot the probability cube for class Forest\nplot(lem_probs, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n# Classify the data cube to produce a map of probabilities\nlem_probs = sits_classify(\n    data = lem_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\n\n# Plot the probability cube for class Forest\nplot(lem_probs, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n\n\n\n\nFigure 1.7: Probability map for class Cropland_2_cycles.\n\n\n\nAfter completing the classification, we plot the probability maps for the class Cropland_2_cycles. Probability maps help visualize the degree of confidence the classifier assigns to the labels for each pixel. They can be used to produce uncertainty information and support active learning, as described in Chapter Uncertainty and active learning."
  },
  {
    "objectID": "intro_quicktour.html#spatial-smoothing",
    "href": "intro_quicktour.html#spatial-smoothing",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.7 Spatial smoothing",
    "text": "1.7 Spatial smoothing\nWhen working with big Earth observation data, there is much variability within each class. As a result, some pixels will be misclassified. These errors are more likely to occur in transition areas between classes. To address these problems, sits_smooth() takes a probability cube as input and uses the class probabilities of each pixel’s neighborhood to reduce labeling uncertainty. Plotting the smoothed probability map for the class Forest shows that most outliers have been removed.\n\n\nR\nPython\n\n\n\n\n# Perform spatial smoothing\nlem_smooth &lt;- sits_smooth(\n    cube = lem_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n\nplot(lem_smooth, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n# Perform spatial smoothing\nlem_smooth = sits_smooth(\n    cube = lem_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\n\nplot(lem_smooth, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n\n\n\n\nFigure 1.8: Probability map for class Cropland_2_cycles."
  },
  {
    "objectID": "intro_quicktour.html#labeling-a-probability-data-cube",
    "href": "intro_quicktour.html#labeling-a-probability-data-cube",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.8 Labeling a probability data cube",
    "text": "1.8 Labeling a probability data cube\nAfter removing outliers using local smoothing, the final classification map can be obtained using sits_label_classification(). This function assigns each pixel to the class with the highest probability.\n\n\nR\nPython\n\n\n\n\n# Label the probability file \nlem_map &lt;- sits_label_classification(\n    cube = lem_smooth, \n    output_dir = tempdir_r\n)\n\nplot(lem_map)\n\n\n\n\n# Label the probability file \nlem_map = sits_label_classification(\n    cube = lem_smooth, \n    output_dir = tempdir_py\n)\n\nplot(lem_map)\n\n\n\n\n\n\n\n\nFigure 1.9: Probability map for class Cropland_2_cycles."
  },
  {
    "objectID": "intro_quicktour.html#summary",
    "href": "intro_quicktour.html#summary",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.9 Summary",
    "text": "1.9 Summary\nThis chapter provides an introduction to the main workflow of the sits package, starting from the definition of a data cube from an ARD image collection. Then, we show how to create a regular data cube and how to extract time series using ground truth data. Next, we present a simple way to define machine learning models and how to use them for classifying data cubes. All functions are simple and direct. Once you learn the basic workflow, it becomes easier to understand more detailed examples of sits in the next chapter."
  },
  {
    "objectID": "intro_quicktour.html#references",
    "href": "intro_quicktour.html#references",
    "title": "\n1  A quick tour of SITS\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nL. V. Oldoni, I. D. Sanches, M. C. A. Picoli, R. M. Covre, and J. G. Fronza, “LEM+ dataset: For agricultural remote sensing applications,” Data in Brief, vol. 33, p. 106553, 2020, doi: 10.1016/j.dib.2020.106553."
  },
  {
    "objectID": "annex_export.html#references",
    "href": "annex_export.html#references",
    "title": "\n27  Exporting data to other packages\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nE. Pebesma, “Simple Features for R: Standardized Support for Spatial Vector Data,” The R Journal, vol. 10, no. 1, p. 439, 2018, doi: 10.32614/RJ-2018-009."
  },
  {
    "objectID": "cl_machinelearning.html#residual-1d-cnn-networks-resnet",
    "href": "cl_machinelearning.html#residual-1d-cnn-networks-resnet",
    "title": "\n16  Machine learning algorithms for image time series\n",
    "section": "\n16.8 Residual 1D CNN networks (ResNet)",
    "text": "16.8 Residual 1D CNN networks (ResNet)\nA residual 1D CNN network, also known as ResNet, is an extension of the standard 1D CNN architecture, adding residual connections between the layers. Residual connections allow the network to learn residual mappings, which are the difference between the input and output of a layer. By adding these residual connections, the network can learn to bypass specific layers and still capture essential features in the data.\nThe Residual Network (ResNet) for time series classification was proposed by Wang et al. [17], based on the idea of deep residual networks for 2D image recognition [22]. The ResNet architecture comprises 11 layers, with three blocks of three 1D CNN layers each (see Figure 16.13). Each block corresponds to a 1D CNN architecture. The output of each block is combined with a shortcut that links its output to its input, called a skip connection. The purpose of combining the input layer of each block with its output layer (after the convolutions) is to avoid the so-called “vanishing gradient problem”. This issue occurs in deep networks as the neural network’s weights are updated based on the partial derivative of the error function. If the gradient is too small, the weights will not be updated, stopping the training [23]. Skip connections aim to avoid vanishing gradients from occurring, allowing deep networks to be trained.\n\n\n\n\nFigure 16.13: Structure of ResNet architecture (source: [17]).\n\n\n\nIn sits, the Residual Network is implemented using sits_resnet(). The default parameters are those proposed by Wang et al. [17], as implemented by Fawaz et al. [24]. The first parameter is blocks, which controls the number of blocks and the size of filters in each block. By default, the model implements three blocks, the first with 64 filters and the others with 128. The parameter kernels controls the size of the kernels of the three layers inside each block. It is useful to experiment a bit with these kernel sizes in the case of satellite image time series. The default activation is “relu”, which is recommended in the literature to reduce the problem of vanishing gradients. The default optimizer is optim_adamw, available in package torchopt.\n\n# Train using ResNet\nresnet_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_resnet(\n                          blocks               = c(64, 128, 128),\n                          kernels              = c(7, 5, 3),\n                          epochs               = 100,\n                          batch_size           = 64,\n                          validation_split     = 0.2,\n                          verbose              = FALSE))\n# Show training evolution\nplot(resnet_model)\n\n\n\n\n\nFigure 16.14: Training evolution of ResNet model.\n\n\n\nUsing the TempCNN model, we classify a 16-year time series.\n\n# Classify using Resnet model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(resnet_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.15: Classification of time series using TempCNN\n\n\n\nIn this case, the result of the ResNet model is quite similar to that of the TempCNN model. In the same way as TempCNN, the ResNet algorithm tends to be better at detecting agricultural or natural forest classes. When it comes to situation where transitions happen during the classification period, as in the case of the transition from forest to pasture in 2004, random forest models tend to be more efficient than TempCNN or ResNet."
  },
  {
    "objectID": "intro_visualisation.html#plotting",
    "href": "intro_visualisation.html#plotting",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.1 Plotting",
    "text": "3.1 Plotting\nThe plot() function produces a graphical display of data cubes, time series, models, and SOM maps. For each type of data, there is a dedicated version of the plot() function. See ?plot.sits for details. The plotting of time series, models, and SOM outputs uses the ggplot2 package; maps are plotted using the tmap package. When plotting images and classified maps, users can control the output, with appropriate parameters for each type of image. In this chapter, we provide examples of the options available for plotting different types of maps.\nPlotting and visualisation function in sits use COG overviews if available. COG overviews are reduced-resolution versions of the main image, stored within the same file. Overviews allow for quick rendering at lower zoom levels, improving performance when dealing with large images. Usually, a single GeoTIFF will have many overviews to match different zoom levels.\nIn the case of maps and images, the parameters discussed in the following sections are optional. Simply using plot() with a data cube or classified map as the first parameter works. The function will search for the date with the least cloud cover and will select an RGB product based on the available bands. By default, plot() will try to produce a standard color combination; otherwise, it produces a false-color plot.\n\n3.1.1 Plotting false color maps\nWe refer to false color maps as images that are plotted on a color scale. Usually, these are single bands, indexes such as NDVI or DEMs. For these datasets, the parameters for plot() are:\n\n\nx: data cube containing data to be visualised;\n\nband: band or index to be plotted;\n\npallete: color scheme to be used for false color maps, which should be one of the RColorBrewer palettes. These palletes were designed to be effective for map display by Prof. Cynthia Brewer as described at the Brewer website. By default, optical images use the RdYlGn scheme, SAR images use Greys, and DEM cubes use Spectral.\n\nrev: whether the color pallete should be reversed; TRUE for DEM cubes, and FALSE otherwise.\n\nscale: global scale parameter used by tmap. All font sizes, symbol sizes, border widths, and line widths are controlled by this value. Default is 0.75; users should vary this parameter and see the results.\n\nfirst_quantile: 1st quantile for stretching images (default = 0.05).\n\nlast_quantile: last quantile for stretching images (default = 0.95).\n\nmax_cog_size: for cloud-oriented GeoTIFF files (COG), sets the maximum number of lines or columns of the COG overview to be used for plotting.\n\nThe following optional parameters are available to allow for detailed control over the plot output:\n\n\ngraticules_labels_size: size of coordinate labels (default = 0.8).\n\nlegend_title_size: relative size of legend title (default = 1.0).\n\nlegend_text_size: relative size of legend text (default = 1.0).\n\nlegend_bg_color: color of the legend background (default = “white”).\n\nlegend_bg_alpha: legend opacity (default = 0.5).\n\nlegend_position: where to place the legend (options = “inside” or “outside”, with “inside” except for probability cubes).\n\nThe following example shows a plot of an NDVI index of a data cube. This data cube covers part of MGRS tile 20LMR and contains bands “B02”, “B03”, “B04”, “B05”, “B06”, “B07”, “B08”, “B11”, “B12”, “B8A”, “EVI”, “NBR”, and “NDVI” for the period 2022-01-05 to 2022-12-23. We will use parameters other than their defaults.\n\n\nR\nPython\n\n\n\n\n# set the directory where the data is \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR\", package = \"sitsdata\")\n\n# read the data cube\nro_20LMR &lt;- sits_cube(\n  source = \"MPC\", \n  collection = \"SENTINEL-2-L2A\",\n  data_dir = data_dir\n)\n\n# plot the NDVI for date 2022-08-01\nplot(ro_20LMR, \n     band = \"NDVI\", \n     date = \"2022-08-01\", \n     palette = \"Greens\",\n     legend_position = \"outside\",\n     scale = 1.0)\n\n\n\n\n# set the directory where the data is \ndata_dir = r_package_dir(\"extdata/Rondonia-20LMR\", package = \"sitsdata\")\n\n# read the data cube\nro_20LMR = sits_cube(\n  source = \"MPC\", \n  collection = \"SENTINEL-2-L2A\",\n  data_dir = data_dir\n)\n\n# plot the NDVI for date 2022-08-01\nplot(ro_20LMR, \n     band = \"NDVI\", \n     date = \"2022-08-01\", \n     palette = \"Greens\",\n     legend_position = \"outside\",\n     scale = 1.0)\n\n\n\n\n\n\n\n\nFigure 3.1: Sentinel-2 NDVI index covering tile 20LMR.\n\n\n\n\n3.1.2 Plotting RGB color composite maps\nFor RGB color composite maps, the parameters for the plot function are:\n\n\nx: data cube containing data to be visualized;\n\nband: band or index to be plotted;\n\ndate: date to be plotted (must be part of the cube timeline);\n\nred: band or index associated with the red color;\n\ngreen: band or index associated to the green color;\n\nblue: band or index associated to the blue color;\n\nscale: global scale parameter used by tmap. All font sizes, symbol sizes, border widths, and line widths are controlled by this value. Default is 0.75; users should vary this parameter and see the results.\n\nfirst_quantile: 1st quantile for stretching images (default = 0.05).\n\nlast_quantile: last quantile for stretching images (default = 0.95).\n\nmax_cog_size: for cloud-optimized GeoTIFF files (COG), sets the maximum number of lines or columns of the COG overview to be used for plotting.\n\nThe optional parameters listed in the previous section are also available. An example follows:\n\n\nR\nPython\n\n\n\n\n# plot a color composite for date 2022-08-01\nplot(ro_20LMR, \n     red = \"B11\", \n     green = \"B8A\",\n     blue = \"B02\",\n     date = \"2022-08-01\", \n     scale = 1.0)\n\n\n\n\n# plot a color composite for date 2022-08-01\nplot(ro_20LMR, \n     red = \"B11\", \n     green = \"B8A\",\n     blue = \"B02\",\n     date = \"2022-08-01\", \n     scale = 1.0)\n\n\n\n\n\n\n\n\nFigure 3.2: Sentinel-2 color composite covering tile 20LMR\n\n\n\nPlotting classified maps\nClassified maps pose an additional challenge for plotting because of the association between labels and colors. In this case, sits allows three alternatives:\n\nPredefined color scheme: sits includes some well-established color schemes such as IBGP, UMD, ESA_CCI_LC, and WORLDCOVER. There is a predefined color table that associates labels commonly used in LUCC classification to colors. Users can also create their color schemes. Please see section “How Colors Work on sits” in this chapter.\nLegend: in this case, users provide a named vector with labels and colors, as shown in the example below.\nPalette: an RColorBrewer categorical palette, which is assigned to labels that are not in the color table.\n\nThe parameters for plot() applied to a classified data cube are:\n\n\nx: data cube containing a classified map;\n\nlegend: legend that associates colors to the classes; NULL by default.\n\npalette: color palette used for undefined colors; Spectral by default.\n\nscale: global scale parameter used by tmap.\n\nThe optional parameters listed in the previous section are also available. For an example of plotting a classified data cube with default color scheme, please see the section “Reading classified images as local data cube” in the “Earth observation data cubes” chapter. In what follows, we show a similar case using a legend.\n\n\nR\nPython\n\n\n\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LLP\", \n                        package = \"sitsdata\")\n\n# Read the classified cube\nrondonia_class_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    bands = \"class\",\n    labels = c(\"1\" = \"Burned\", \"2\" = \"Cleared\", \n               \"3\" = \"Degraded\", \"4\" =  \"Natural_Forest\"),\n    data_dir = data_dir\n)\n\n# Plot the classified cube\nplot(rondonia_class_cube,\n  legend = c(\"Burned\" = \"#a93226\",\n             \"Cleared\" = \"#f9e79f\",\n             \"Degraded\" = \"#d4efdf\",\n             \"Natural_Forest\" = \"#1e8449\"\n             ),\n  scale = 1.0,\n  legend_position = \"outside\"\n)\n\n\n\nFigure 3.3: Sentinel-2 color composite covering tile 20LMR\n\n\n\n\n\n\n# Create a cube based on a classified image \ndata_dir = r_package_dir(\"extdata/Rondonia-20LLP\", \n                package = \"sitsdata\")\n\n# Read the classified cube\nrondonia_class_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    bands = \"class\",\n    labels = {\n        \"1\": \"Burned\",\n        \"2\": \"Cleared\",\n        \"3\": \"Degraded\",\n        \"4\": \"Natural_Forest\"\n    },\n    data_dir = data_dir\n)\n\n# Plot the classified cube\nplot(\n    rondonia_class_cube,\n    legend = {\n        \"Burned\" : \"#a93226\",\n        \"Cleared\": \"#f9e79f\",\n        \"Degraded\": \"#d4efdf\",\n        \"Natural_Forest\": \"#1e8449\"\n    },\n    scale = 1.0,\n    legend_position = \"outside\"\n)\n\n\n\nFigure 3.4: Sentinel-2 color composite covering tile 20LMR"
  },
  {
    "objectID": "intro_visualisation.html#visualization-of-data-cubes-in-interactive-maps",
    "href": "intro_visualisation.html#visualization-of-data-cubes-in-interactive-maps",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.2 Visualization of data cubes in interactive maps",
    "text": "3.2 Visualization of data cubes in interactive maps\nData cubes and samples can also be shown as interactive maps using sits_view(). This function creates tiled overlays of different kinds of data cubes, allowing comparison between the original, intermediate, and final results. It also includes background maps. The following example creates an interactive map combining the original data cube with the classified map.\n\n\nR\nPython\n\n\n\n\nsits_view(rondonia_class_cube,\n            legend = c(\"Burned\" = \"#a93226\",\n             \"Cleared\" = \"#f9e79f\",\n             \"Degraded\" = \"#d4efdf\",\n             \"Natural_Forest\" = \"#1e8449\"\n             )\n)\n\n\n\n\nsits_view(rondonia_class_cube,\n            legend = dict(\n                 Burned = \"#a93226\",\n                 Cleared = \"#f9e79f\",\n                 Degraded = \"#d4efdf\",\n                 Natural_Forest = \"#1e8449\"\n             )\n)\n\n\n\n\n\n\n\n\nFigure 3.5: Leaflet visualization of classification of an area in Rondonia, Brasil"
  },
  {
    "objectID": "intro_visualisation.html#how-colors-work-in-sits",
    "href": "intro_visualisation.html#how-colors-work-in-sits",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.3 How colors work in sits",
    "text": "3.3 How colors work in sits\nIn the examples provided in the book, the color legend is taken from a predefined color palette provided by sits. The default color definition file used by sits includes 220 class names, which can be shown using sits_colors().\n\n\nR\nPython\n\n\n\n\n# Point default `sits` colors\nsits_colors()\n\n# A tibble: 239 × 2\n   name                             color  \n   &lt;chr&gt;                            &lt;chr&gt;  \n 1 Evergreen_Broadleaf_Forest       #1E8449\n 2 Evergreen_Broadleaf_Forests      #1E8449\n 3 Tree_Cover_Broadleaved_Evergreen #1E8449\n 4 Forest                           #1E8449\n 5 Forests                          #1E8449\n 6 Closed_Forest                    #1E8449\n 7 Closed_Forests                   #1E8449\n 8 Mountainside_Forest              #229C59\n 9 Mountainside_Forests             #229C59\n10 Open_Forest                      #53A145\n# ℹ 229 more rows\n\n\n\n\n\n# Point default `sits` colors\nsits_colors()\n\n                                 name    color\n1          Evergreen_Broadleaf_Forest  #1E8449\n2         Evergreen_Broadleaf_Forests  #1E8449\n3    Tree_Cover_Broadleaved_Evergreen  #1E8449\n4                              Forest  #1E8449\n5                             Forests  #1E8449\n..                                ...      ...\n235                 Rice, Germination  #F6DDCC\n236                   Rice, Tillering  #EDBB99\n237                 Rice, Stem elong.  #E59866\n238                     Rice, Booting  #DC7633\n239                   Rice, Leaf dev.  #BA4A00\n\n[239 rows x 2 columns]\n\n\n\n\n\nThese colors are grouped by typical legends used by the Earth observation community, which include “IGBP”, “UMD”, “ESA_CCI_LC”, “WORLDCOVER”, “PRODES”, “PRODES_VISUAL”, “TERRA_CLASS”, and “TERRA_CLASS_PT”. The following commands show the colors associated with the IGBP legend [1].\n\n\n\n\nFigure 3.6: Colors used in the sits package to represeny IGBP legend.\n\n\n\nThe default color table can be extended using sits_colors_set(). As an example of a user-defined color table, consider a definition that covers level 1 of the Anderson Classification System used in the U.S. National Land Cover Data, obtained by defining a set of colors associated with a new legend. The colors should be defined by HEX values, and the color names should consist of a single string; multiple names need to be connected with an underscore(“_“).\n\n\nR\nPython\n\n\n\n\n# Define a color table based on the Anderson Land Classification System\nus_nlcd &lt;- tibble::tibble(name = character(), color = character())\nus_nlcd &lt;- us_nlcd |&gt;  \n  tibble::add_row(name = \"Urban_Built_Up\", color =  \"#85929E\") |&gt; \n  tibble::add_row(name = \"Agricultural_Land\", color = \"#F0B27A\") |&gt;  \n  tibble::add_row(name = \"Rangeland\", color = \"#F1C40F\") |&gt; \n  tibble::add_row(name = \"Forest_Land\", color = \"#27AE60\") |&gt;  \n  tibble::add_row(name = \"Water\", color = \"#2980B9\") |&gt;  \n  tibble::add_row(name = \"Wetland\", color = \"#D4E6F1\") |&gt; \n  tibble::add_row(name = \"Barren_Land\", color = \"#FDEBD0\") |&gt; \n  tibble::add_row(name = \"Tundra\", color = \"#EBDEF0\") |&gt; \n  tibble::add_row(name = \"Snow_and_Ice\", color = \"#F7F9F9\")\n\n# Load the color table into `sits`\nsits_colors_set(colors = us_nlcd, legend = \"US_NLCD\")\n\n# Show the new legend\nsits_colors_show(legend = \"US_NLCD\")\n\n\n\nFigure 3.7: Example of defining colors for the Anderson Land Classification Scheme.\n\n\n\n\n\n\n# Import pandas\nimport pandas as pd\n\n# Define a color table based on the Anderson Land Classification System\nus_nlcd = pd.DataFrame([\n    {\"name\": \"Urban_Built_Up\", \"color\": \"#85929E\"},\n    {\"name\": \"Agricultural_Land\", \"color\": \"#F0B27A\"},\n    {\"name\": \"Rangeland\", \"color\": \"#F1C40F\"},\n    {\"name\": \"Forest_Land\", \"color\": \"#27AE60\"},\n    {\"name\": \"Water\", \"color\": \"#2980B9\"},\n    {\"name\": \"Wetland\", \"color\": \"#D4E6F1\"},\n    {\"name\": \"Barren_Land\", \"color\": \"#FDEBD0\"},\n    {\"name\": \"Tundra\", \"color\": \"#EBDEF0\"},\n    {\"name\": \"Snow_and_Ice\", \"color\": \"#F7F9F9\"}\n])\n\n# Load the color table into `sits`\n_ = sits_colors_set(colors = us_nlcd, legend = \"US_NLCD\")\n\n# Show the new legend\nsits_colors_show(legend = \"US_NLCD\")\n\n\n\nFigure 3.8: Example of defining colors for the Anderson Land Classification Scheme.\n\n\n\n\n\n\nThe original default sits color table can be restored using sits_colors_reset().\n\n\nR\nPython\n\n\n\n\n# Reset the color table\nsits_colors_reset()\n\n\n\n\n# Reset the color table\nsits_colors_reset()"
  },
  {
    "objectID": "intro_visualisation.html#exporting-colors-to-qgis",
    "href": "intro_visualisation.html#exporting-colors-to-qgis",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.4 Exporting colors to QGIS",
    "text": "3.4 Exporting colors to QGIS\nTo simplify the process of importing your data into QGIS, the color palette used to display classified maps in sits can be exported as a QGIS style using sits_colors_qgis. The function takes two parameters: (a) cube, a classified data cube; and (b) file, the file where the QGIS style in XML will be written. In this case study, we first retrieve and plot a classified data cube, and then export the colors to a QGIS XML style.\n\n\nR\nPython\n\n\n\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-Class-2022-Mosaic\", \n                        package = \"sitsdata\")\n\n# labels of the classified image\nlabels &lt;- c(\"1\" = \"Clear_Cut_Bare_Soil\",\n            \"2\" = \"Clear_Cut_Burned_Area\",\n            \"3\" = \"Clear_Cut_Vegetation\",\n            \"4\" = \"Forest\",\n            \"5\" = \"Mountainside_Forest\",\n            \"6\" = \"Riparian_Forest\",\n            \"7\" = \"Seasonally_Flooded\",\n            \"8\" = \"Water\",\n            \"9\" = \"Wetland\" \n)\n\n# read classified data cube\nro_class &lt;- sits_cube(\n    source = \"MPC\", \n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir, \n    bands = \"class\",\n    labels = labels,\n    version = \"mosaic\"\n)\n\n# Plot the classified cube\nplot(ro_class, scale = 1.0)\n\n\n\n\n# Create a cube based on a classified image \ndata_dir = r_package_dir(\"extdata/Rondonia-Class-2022-Mosaic\", \n                        package = \"sitsdata\")\n\n# labels of the classified image\nlabels = {\n    \"1\": \"Clear_Cut_Bare_Soil\",\n    \"2\": \"Clear_Cut_Burned_Area\",\n    \"3\": \"Clear_Cut_Vegetation\",\n    \"4\": \"Forest\",\n    \"5\": \"Mountainside_Forest\",\n    \"6\": \"Riparian_Forest\",\n    \"7\": \"Seasonally_Flooded\",\n    \"8\": \"Water\",\n    \"9\": \"Wetland\"\n}\n\n# read classified data cube\nro_class = sits_cube(\n    source = \"MPC\", \n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir, \n    bands = \"class\",\n    labels = labels,\n    version = \"mosaic\"\n)\n\n# Plot the classified cube\nplot(ro_class, scale = 1.0)\n\n\n\n\n\n\n\n\nFigure 3.9: Classified mosaic for land cover in Rondonia, Brazil for 2022.\n\n\n\nThe file to be read by QGIS is a TIFF file whose location is specified by the data cube, as follows.\n\n\nR\nPython\n\n\n\n\n# Show the location of the classified map\nro_class[[\"file_info\"]][[1]]$path\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sitsdata/extdata/Rondonia-Class-2022-Mosaic/SENTINEL-2_MSI_MOSAIC_2022-01-05_2022-12-23_class_mosaic.tif\"\n\n\n\n\n\n# Show the location of the classified map\nro_class[\"file_info\"][0][\"path\"]\n\n0    /Library/Frameworks/R.framework/Versions/4.4-a...\nName: path, dtype: object\n\n\n\n\n\nThe color schema can be exported to QGIS as follows.\n\n\nR\nPython\n\n\n\n\n# Export the color schema to QGIS\nsits_colors_qgis(ro_class, file = file.path(tempdir_r, \"qgis_style.xml\"))\n\n\n\n\n# Export the color schema to QGIS\nsits_colors_qgis(ro_class, file = tempdir_py / \"qgis_style.xml\")"
  },
  {
    "objectID": "intro_visualisation.html#references",
    "href": "intro_visualisation.html#references",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Herold, R. Hubald, and A. Di Gregorio, “Translating and evaluating land cover legends using the UN Land Cover Classification System (LCCS),” GOFC-GOLD Florence, Italy, 2009."
  },
  {
    "objectID": "intro_examples.html#summary",
    "href": "intro_examples.html#summary",
    "title": "\n2  How to use SITS with real examples\n",
    "section": "\n2.3 Summary",
    "text": "2.3 Summary\nThese examples demonstrate the complete workflow for generating a classified land use and land cover map using the SITS package. The process includes data cube creation, sample extraction, model training with machine and deep learning approaches, classification, and post-processing steps such as smoothing and labeling. By following these steps, users can apply the SITS package to produce accurate and spatially coherent maps from satellite image time series, supporting environmental monitoring and land change analysis.\n\n\n\n\n\n[1] \nD. C. Nepstad et al., “Large-scale impoverishment of Amazonian forests by logging and fire,” Nature, vol. 398, no. 6727, pp. 505–508, 1999, doi: 10.1038/19066.\n\n\n[2] \nJ. J. Gerwing, “Degradation of forests through logging and fire in the eastern Brazilian Amazon,” Forest Ecology and Management, vol. 157, no. 1, pp. 131–141, 2002, doi: 10.1016/S0378-1127(00)00644-7.\n\n\n[3] \nC. Pelletier, S. Valero, J. Inglada, N. Champion, and G. Dedieu, “Assessing the robustness of Random Forests to map land cover with high resolution satellite image time series over large areas,” Remote Sensing of Environment, vol. 187, pp. 156–168, 2016, doi: 10.1016/j.rse.2016.10.010.\n\n\n[4] \nC. A. Klink and R. B. Machado, “Conservation of the Brazilian cerrado,” Conservation Biology, vol. 19, no. 3, pp. 707–713, 2005.\n\n\n[5] \nR. Goodland, “A Physiognomic Analysis of the ‘Cerrado’ Vegetation of Central Brasil,” The Journal of Ecology, vol. 59, no. 2, p. 411, 1971, doi: 10.2307/2258321.\n\n\n[6] \nK. Del-Claro and H. M. Torezan-Silingardi, “The study of biotic interactions in the Brazilian Cerrado as a path to the conservation of biodiversity,” Anais da Academia Brasileira de Ciências, vol. 91, no. suppl 3, p. e20180768, 2019, doi: 10.1590/0001-3765201920180768.\n\n\n[7] \nB. M. T. Walter, “Fitofisionomias do bioma Cerrado: síntese terminológica e relações florísticas.” PhD thesis, Universidade de Brasilia, 2006.\n\n\n[8] \nR. Simoes et al., “Satellite Image Time Series Analysis for Big Earth Observation Data,” Remote Sensing, vol. 13, no. 13, p. 2428, 2021, doi: 10.3390/rs13132428.\n\n\n[9] \nL. Parente, V. Mesquita, F. Miziara, L. Baumann, and L. Ferreira, “Assessing the pasturelands and livestock dynamics in Brazil, from 1985 to 2017: A novel approach based on high spatial resolution imagery and Google Earth Engine cloud computing,” Remote Sensing of Environment, vol. 232, p. 111301, 2019, doi: 10.1016/j.rse.2019.111301.\n\n\n[10] \nC. M. Souza Jr et al., “Reconstructing three decades of land use and land cover changes in brazilian biomes with landsat archive and earth engine,” Remote Sensing, vol. 12, no. 17, p. 2735, 2020.\n\n\n[11] \nIBGE, “Monitoramento da cobertura e uso da terra do Brasil: 2016–2018,” Brazilian Institute of Geography and Statistics, Rio de Janeiro, {Book} 101703, 2020. [Online]. Available: https://biblioteca.ibge.gov.br/index.php/biblioteca-catalogo?view=detalhes&id=2101703.\n\n\n[12] \nC. Pelletier, G. I. Webb, and F. Petitjean, “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,” Remote Sensing, vol. 11, no. 5, 2019.\n\n\n[13] \nP. Olofsson, G. M. Foody, S. V. Stehman, and C. E. Woodcock, “Making better use of accuracy data in land change studies: Estimating accuracy and area and quantifying uncertainty using stratified estimation,” Remote Sensing of Environment, vol. 129, pp. 122–131, 2013, doi: 10.1016/j.rse.2012.10.031."
  },
  {
    "objectID": "dc_regularize.html#the-need-for-regular-eo-data-cubes",
    "href": "dc_regularize.html#the-need-for-regular-eo-data-cubes",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.1 The need for regular EO data cubes",
    "text": "5.1 The need for regular EO data cubes\nAnalysis Ready Data (ARD) collections are often irregular in space and time. Bands may have different resolutions, images may not cover entire tiles, and time intervals are inconsistent. Clouds and sensor artifacts introduce “holes” in the data, corrupting the time series. If time steps differ or values are missing, batch training breaks and the model learns spurious correlations. Additionally, most machine learning and deep learning libraries expect tensors of identical shape (e.g., k samples × m features × n temporal intervals). Regular data cubes guarantee fixed-length feature vectors and CPU and GPU-friendly batches. Regularization turns heterogeneous image archives into clean, structured data ready for machine learning models.\nData from ARD collections can be converted into regular data cubes with sits_regularize(), which uses the gdalcubes package [1]. This function has two components:\n\nSpatial harmonization: reproject and resample everything onto the same tiling system and spatial resolution. For example, when Sentinel-1 and Sentinel-2 images are merged in sits, they are projected onto MGRS grid tiles.\nTemporal harmonization: creates equispaced intervals (e.g., 16-day, monthly, or seasonal composites), filling gaps introduced by cloud cover and sensor errors. sits stacks every image within a chosen interval to combine them. It sorts images in increasing order of cloud cover percentage. The least cloud-filled image is taken as a reference, and the others are used to try to fill its gaps. Pixels with persistent cloud cover are marked as NA and are temporally interpolated during computation."
  },
  {
    "objectID": "dc_regularize.html#regularizing-sentinel-2-images",
    "href": "dc_regularize.html#regularizing-sentinel-2-images",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.2 Regularizing Sentinel-2 images",
    "text": "5.2 Regularizing Sentinel-2 images\nIn the following example, we create a non-regular data cube from the Sentinel-2 collection available in Microsoft’s Planetary Computer (MPC). The area lies within the state of Rondônia, Brazil, and is defined by the MGRS tiles 20LKP and 20LLP. We use sits_cube() to retrieve the collection.\n\n\nR\nPython\n\n\n\n\n# Retrieving a non-regular ARD collection from AWS\ns2_cube_rondonia &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = c(\"20LLP\", \"20LKP\"),\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = as.Date(\"2018-06-30\"),\n    end_date = as.Date(\"2018-08-31\")\n)\n# Show the different timelines of the cube tiles\nsits_timeline(s2_cube_rondonia)\n\n\n\n\n# Retrieving a non-regular ARD collection from AWS\ns2_cube_rondonia = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = (\"20LLP\", \"20LKP\"),\n    bands = (\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-06-30\",\n    end_date = \"2018-08-31\"\n)\n# Show the different timelines of the cube tiles\nsits_timeline(s2_cube_rondonia)\n\n\n\n\n\n\n$`20LKP`\n [1] \"2018-07-03\" \"2018-07-08\" \"2018-07-13\" \"2018-07-18\" \"2018-07-23\"\n [6] \"2018-07-28\" \"2018-08-02\" \"2018-08-07\" \"2018-08-12\" \"2018-08-17\"\n[11] \"2018-08-22\" \"2018-08-27\"\n\n$`20LLP`\n [1] \"2018-06-30\" \"2018-07-03\" \"2018-07-05\" \"2018-07-08\" \"2018-07-10\"\n [6] \"2018-07-13\" \"2018-07-15\" \"2018-07-18\" \"2018-07-20\" \"2018-07-23\"\n[11] \"2018-07-25\" \"2018-07-28\" \"2018-07-30\" \"2018-08-02\" \"2018-08-04\"\n[16] \"2018-08-07\" \"2018-08-09\" \"2018-08-12\" \"2018-08-14\" \"2018-08-17\"\n[21] \"2018-08-19\" \"2018-08-22\" \"2018-08-24\" \"2018-08-27\" \"2018-08-29\"\n\n\n\n\nR\nPython\n\n\n\n\n# plot the least cloudy image of the cube\ns2_cube_rondonia |&gt;  \n    dplyr::filter(tile == \"20LLP\") |&gt;  \n    plot()\n\n\n\n\n# plot the least cloudy image of the cube\nplot(\n    s2_cube_rondonia.query(\"tile == '20LLP'\")\n)\n\n\n\n\n\n\n\n\nFigure 5.1: Instance of non-regularized Sentinel-2 image covering only part of tile tile 20LLP.\n\n\n\nDifferent satellites—even those within the same mission, such as Sentinel-2A and Sentinel-2B—follow slightly different orbits and acquire data at different times. Due to factors such as the Earth’s rotation and the lack of perfect alignment between Earth’s orbit and the satellites’ paths, some regions are not observed by both satellites during each orbital cycle. As a result, image acquisition timelines can differ between tiles.\nIn our example, tile 20LKP has twelve images within the selected time period, while tile 20LLP has twenty-four. To harmonize these differences, we use the sits_regularize() function, which builds a data cube with a regular timeline and estimates the best available pixel value for each time interval.\nLet’s now examine some technical aspects of sits_regularize(). The period parameter defines the time interval between observations, using the ISO 8601 time period format. This format specifies intervals as P[n]Y[n]M[n]D, where “Y” stands for years, “M” for months, and “D” for days. For example, P1M denotes a one-month interval, and P15D denotes a fifteen-day interval. For each time step, sits_regularize() identifies all available images within the defined window. Then, for each pixel, it sorts these candidate values by increasing cloud cover and selects the first cloud-free value. In this way, the function builds a regular time series for each pixel, even when observations come from different dates or satellites.\nIn this example, we set the regular cube’s spatial resolution to forty meters to speed up processing. For real-world applications, however, we recommend using a resolution of ten meters. Except for Microsoft Planetary Computer (which has a fast data access system), we recommend copying the ARD data to a local directory using sits_cube_copy() before applying regularization. This separates the process of creating a regular data cube into two distinct steps: (a) downloading data from ARD collections and (b) building the data cube from local files. This approach can significantly speed up processing. After sits builds the regular cube, the ARD images can be deleted to save space. Finally, keep in mind that depending on the speed of your Internet connection, sits_cube_copy() may take some time to complete.\n\n\nR\nPython\n\n\n\n\n# set output dir for ARD data if it does not exist \ntempdir_r_s2 &lt;- \"~/sitsbook/tempdir/R/dc_regularize/s2\"\ndir.create(tempdir_r_s2, showWarnings = FALSE, recursive = TRUE)\n\ns2_cube_local &lt;- sits_cube_copy(\n    cube = s2_cube_rondonia,\n    output_dir = tempdir_r_s2\n)\n\n# set output dir fir regular cube if it does not exist \ntempdir_r_s2_reg &lt;- \"~/sitsbook/tempdir/R/dc_regularize/s2_reg\"\ndir.create(tempdir_r_s2_reg, showWarnings = FALSE, recursive = TRUE)\n\n# Regularize the cube to 16-day intervals\nreg_cube_rondonia &lt;- sits_regularize(\n          cube       = s2_cube_local,\n          output_dir = tempdir_r_s2_reg,\n          res        = 40,\n          period     = \"P16D\",\n          multicores = 6)\n\n# Plot tile 20LLP of the regularized cube with the least cloud cover\n# The pixels of the regular data cube cover the full MGRS tile\nplot(reg_cube_rondonia, tile = \"20LLP\")\n\n\n\n\n# set output dir for ARD data if it does not exist \ntempdir_py_s2 = tempdir_py / \"s2\"\ntempdir_py_s2.mkdir(parents = True, exist_ok = True)\n\ns2_cube_local = sits_cube_copy(\n    cube = s2_cube_rondonia,\n    output_dir = tempdir_py_s2\n)\n\n# set output dir fir regular cube if it does not exist \ntempdir_py_s2_reg = tempdir_py / \"s2_reg\"\ntempdir_py_s2_reg.mkdir(parents = True, exist_ok = True)\n\n# Regularize the cube to 16-day intervals\nreg_cube_rondonia = sits_regularize(\n          cube       = s2_cube_local,\n          output_dir = tempdir_py_s2_reg,\n          res        = 40,\n          period     = \"P16D\",\n          multicores = 6)\n\n# Plot tile 20LLP of the regularized cube with the least cloud cover\n# The pixels of the regular data cube cover the full MGRS tile\nplot(reg_cube_rondonia, tile = \"20LLP\")\n\n\n\n\n\n\n\n\nFigure 5.2: Regularized image for tile Sentinel-2 tile 20LLP."
  },
  {
    "objectID": "dc_regularize.html#regularizing-sentinel-1-images",
    "href": "dc_regularize.html#regularizing-sentinel-1-images",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.3 Regularizing Sentinel-1 images",
    "text": "5.3 Regularizing Sentinel-1 images\nWe have already discussed how different acquisition orbits can result in mismatched timelines. But that is not the only irregularity we need to address. Different satellites may also have different acquisition modes—that is, the way their sensors capture data, including direction, resolution, swath width, and polarization.\nIn the case of SAR (Synthetic Aperture Radar) satellites like Sentinel-1, the acquisition mode determines:\n\nViewing geometry (how the radar observes the ground),\nIncidence angle (the angle between the radar beam and the vertical to the Earth),\nSpatial resolution and coverage area,\nWhether it collects single or dual polarization (e.g., VV, VH).\n\nSAR images are usually captured at an oblique angle (not straight down), resulting in a slanted geometry known as slant range. As a result, raw SAR images do not align well with optical imagery like Sentinel-2, which uses a nadir (straight-down) viewing geometry. To facilitate the integration of Sentinel-1 and Sentinel-2 data, sits_regularize() reprojects SAR images to the MGRS grid. Internally, it uses the gdalwarp() function (via the gdalcubes backend or an equivalent tool), which supports several interpolation methods:\n\nNearest: Assigns the value of the nearest input pixel (fastest, preserves discrete classes).\nBilinear: Performs linear interpolation from 4 nearest input pixels. Smooths intensity values.\nCubic: Uses 16 surrounding pixels. Smoother, more complex but can introduce artifacts.\n\nBy default, sits applies nearest-neighbor interpolation for categorical or discrete bands (e.g., land cover or polarization labels), and bilinear interpolation for continuous-valued bands (e.g., backscatter intensity). However, the exact interpolation method may vary depending on how the raster cube is created and the backend in use. Advanced users can customize reprojection parameters through sits_config() or environment variables that modify GDAL behavior.\nWe illustrate the spatial harmonization feature of sits_regularize() in the following example, which uses the \"SENTINEL-1-RTC\" collection from the Microsoft Planetary Computer (MPC).\n\n\nR\nPython\n\n\n\n\n# create an RTC cube from MPC collection for a region in Mato Grosso, Brazil.\ncube_s1_rtc &lt;-  sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    tiles = c(\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-10-01\"\n)\n\nplot(cube_s1_rtc, band = \"VH\", palette = \"Greys\", scale = 0.7)\n\n\n\n\n# create an RTC cube from MPC collection for a region in Mato Grosso, Brazil.\ncube_s1_rtc = sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = (\"VV\", \"VH\"),\n    orbit = \"descending\",\n    tiles = (\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-10-01\"\n)\n\nplot(cube_s1_rtc, band = \"VH\", palette = \"Greys\", scale = 0.7)\n\n\n\n\n\n\n\n\nFigure 5.3: “Original Sentinel-1 image covering tile 22LBL.\n\n\n\nAfter retrieving a non-regular ARD collection from the Microsoft Planetary Computer (MPC), we use sits_regularize() to produce a SAR data cube aligned with MGRS tile “22LBL”. To visualize the SAR data, we generate a multi-date plot of the “VH” polarization band. In this plot, the first date is displayed in red, the second in green, and the third in blue—producing an RGB composite that visually highlights changes over time.\n\n\nR\nPython\n\n\n\n\n# define the output directory\ntempdir_r_sar &lt;- \"~/sitsbook/tempdir/R/dc_regularize/sar\"\n\n# set output dir if it does not exist \ndir.create(tempdir_r_sar, showWarnings = FALSE)\n\n# create a regular RTC cube from MPC collection for a tile 22LBL.\ncube_s1_reg &lt;- sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P16D\",\n    res = 40,\n    tiles = c(\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_r_sar\n)\n\nplot(cube_s1_reg, band = \"VH\", palette = \"Greys\", scale = 0.7, \n     dates = c(\"2021-06-06\", \"2021-07-24\", \"2021-09-26\"))\n\n\n\n\n# define the output directory\ntempdir_py_sar = tempdir_py / \"sar\"\n\n# set output dir if it does not exist \ntempdir_py_sar.mkdir(parents = True, exist_ok = True)\n\n# create a regular RTC cube from MPC collection for a tile 22LBL.\ncube_s1_reg = sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P16D\",\n    res = 40,\n    tiles = (\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_py_sar\n)\n\nplot(cube_s1_reg, band = \"VH\", palette = \"Greys\", scale = 0.7, \n     dates = (\"2021-06-06\", \"2021-07-24\", \"2021-09-26\"))\n\n\n\n\n\n\n\n\nFigure 5.4: Regularized Sentinel-1 image covering tile 22LBL."
  },
  {
    "objectID": "dc_regularize.html#summary",
    "href": "dc_regularize.html#summary",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.4 Summary",
    "text": "5.4 Summary\nIn this chapter, we learned how to produce regular Earth observation (EO) data cubes from non-regular subsets of ARD collections. Regularization is a key operation when working with time series, as it enables the use of machine learning models on temporally aligned data. In the next chapter, we will discuss how to merge sensors from different data sources and, when necessary, how to combine these datasets with regularization operations."
  },
  {
    "objectID": "dc_regularize.html#references",
    "href": "dc_regularize.html#references",
    "title": "\n5  Building regular data cubes\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Appel and E. Pebesma, “On-Demand Processing of Data Cubes from Satellite Image Collections with the gdalcubes Library,” Data, vol. 4, no. 3, 2019, doi: 10.3390/data4030092."
  },
  {
    "objectID": "annex_ml.html#references",
    "href": "annex_ml.html#references",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nG. Ke et al., “Lightgbm: A highly efficient gradient boosting decision tree,” in Advances in neural information processing systems, 2017, pp. 3146–3154."
  },
  {
    "objectID": "vec_obia.html#introduction",
    "href": "vec_obia.html#introduction",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "\n25.1 Introduction",
    "text": "25.1 Introduction\nObject-Based Image Analysis (OBIA) is an approach to remote sensing image analysis that partitions an image into closed segments which are then classified and analyzed. For high-resolution images (1 meter or smaller) the aim of OBIA is to create objects that represent meaningful features in the real world, like buildings, roads, fields, forests, and water bodies. In case of medium resolution images (such as Sentinel-2 or Landsat) the segments represent groups of the image with similar spectral responses which in general do not correspond directly to individual objects in the ground. These groups of pixels are called super-pixels. In both situations, the aim of OBIA is to obtain a spatial partition of the image which can then be assigned to a single class. When applicable, OBIA reduces processing time and produces labeled maps with greater spatial consistency.\nThe general sequence of the processes involved in OBIA in sits is:\n\nSegmentation: The first step is to group together pixels that are similar based a distance metric that considers the values of all bands in all time instances. We build a multitemporal attribute space where each time/band combination is taken as an independent dimension. Thus, distance metrics for segmentation in a data cube with 10 bands and 24 time steps use a 240-dimension space.\nProbability Estimation: After the image has been partitioned into distinct objects, the next step is to classify each segment. For satellite image time series, a subset of time series inside each segment is classified.\nLabeling: Once the set of probabilities have been obtained for each time series inside a segment, they can be used for labeling. This is done by considering the median value of the probabilities for the time series inside the segment that have been classified. For each class, we take the median of the probability values. Then, median values for the classes are normalised, and the most likely value is assigned as the class for the segment."
  },
  {
    "objectID": "val_map.html#references",
    "href": "val_map.html#references",
    "title": "\n24  Map accuracy assessment\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nP. Olofsson, G. M. Foody, M. Herold, S. V. Stehman, C. E. Woodcock, and M. A. Wulder, “Good practices for estimating area and assessing accuracy of land change,” Remote Sensing of Environment, vol. 148, pp. 42–57, 2014.\n\n\n[2] \nD. S. Alves, M. I. S. Escada, J. L. G. Pereira, and C. de Albuquerque Linhares, “Land use intensification and abandonment in Rondônia, Brazilian Amazônia,” International Journal of Remote Sensing, vol. 24, no. 4, pp. 899–903, 2003, doi: 10.1080/0143116021000015807.\n\n\n[3] \nW. G. Cochran, Sampling techniques. john wiley & sons, 1977.\n\n\n[4] \nP. Olofsson, G. M. Foody, S. V. Stehman, and C. E. Woodcock, “Making better use of accuracy data in land change studies: Estimating accuracy and area and quantifying uncertainty using stratified estimation,” Remote Sensing of Environment, vol. 129, pp. 122–131, 2013, doi: 10.1016/j.rse.2012.10.031."
  },
  {
    "objectID": "cl_smoothing.html#understanding-the-outputs-of-machine-learning-algorithms",
    "href": "cl_smoothing.html#understanding-the-outputs-of-machine-learning-algorithms",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.3 Understanding the outputs of machine learning algorithms",
    "text": "19.3 Understanding the outputs of machine learning algorithms\nA machine-learning algorithm outputs a \\(m\\)-dimensional vector \\(\\mathbf{p}_i = (p_{i,1}, \\ldots, p_{i,m})\\) for each pixel with \\(0 \\leq p_{i,k} \\leq 1\\) and \\(\\sum_k p_{i,k} = 1 \\forall i = 1, \\ldots, n\\). The value \\(p_{i,k}\\) is the empirically estimated probability that the \\(i\\)-th pixel belongs to the \\(k\\)-th class according to the machine-learning algorithm. The probability \\(p_{i,k}\\) is the classifier’s output, subject to noise, outliers, and classification errors. The highest probability among the available options determines the class assigned to the pixel."
  },
  {
    "objectID": "cl_smoothing.html#a-bayesian-approach-to-post-processing",
    "href": "cl_smoothing.html#a-bayesian-approach-to-post-processing",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.3 A Bayesian approach to post-processing",
    "text": "19.3 A Bayesian approach to post-processing\nOur post-processing method is based on the Bayesian approach to data analysis, which interprets probability as a measure of belief in an event. This approach begins with an initial set of beliefs, known as the , derived from previous experiments or expert knowledge. Bayesian statistics then combines this prior knowledge with new data to update and refine these beliefs. This is done using Bayes’ theorem, a mathematical framework for revising probabilities as new evidence becomes available.\nUnlike classical (frequentist) methods, which treat parameters as fixed but unknown values, the Bayesian approach considers parameters as random variables with associated probability distributions. By integrating prior distributions with observed data through a likelihood function, the Bayesian method generates a posterior distribution. This posterior distribution represents the updated beliefs about the parameters, offering a comprehensive framework for inference and decision-making."
  },
  {
    "objectID": "cl_smoothing.html#converting-probabilities-to-logits",
    "href": "cl_smoothing.html#converting-probabilities-to-logits",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.4 Converting probabilities to logits",
    "text": "19.4 Converting probabilities to logits\nA machine-learning algorithm outputs a \\(m\\)-dimensional vector \\(\\mathbf{p}_i = (p_{i,1}, \\ldots, p_{i,m})\\) for each pixel with \\(0 \\leq p_{i,k} \\leq 1\\) and \\(\\sum_k p_{i,k} = 1 \\forall i = 1, \\ldots, n\\). The value \\(p_{i,k}\\) is the empirically estimated probability that the \\(i\\)-th pixel belongs to the \\(k\\)-th class according to the machine-learning algorithm. The probability \\(p_{i,k}\\) is the classifier’s output, subject to noise, outliers, and classification errors. The highest probability among the available options determines the class assigned to the pixel.\nThe class probability values \\(p_{i,k}\\), which are the outputs of the machine learning classifier, are converted to log-odds values to make the inference more tractable. The logit function converts probability values between \\(0\\) to \\(1\\) to values from \\(-\\infty\\) to \\(\\infty\\). The conversion from probabilities to logit values is expressed by: \\[\n    x_{i,k} = \\ln \\left(\\frac{p_{i,k}}{1 - p_{i,k}}\\right)\n\\]\nThe conversion from probabilities to logit values helps support our assumption of a normal distribution for our data. Confidence in pixel classification increases with logit. However, there are situations, such as border or mixed pixels, where the logit of different classes is similar in magnitude. These are cases of low confidence in the classification result. The post-classification smoothing method borrows strength from the neighbours to assess and correct these cases."
  },
  {
    "objectID": "acknowledgements.html#funding-sources",
    "href": "acknowledgements.html#funding-sources",
    "title": "Acknowledgements",
    "section": "Funding Sources",
    "text": "Funding Sources\nThe authors acknowledge the funders that supported the development of sits:\n\nAmazon Fund, established by Brazil with financial contribution from Norway, through contract 17.2.0536.1. between the Brazilian Development Bank (BNDES) and the Foundation for Science, Technology, and Space Applications (FUNCATE), for the establishment of the Brazil Data Cube.\nCoordenação de Aperfeiçoamento de Pessoal de Nível Superior-Brasil (CAPES) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) for grants 312151/2014-4, 140684/2016-6, and 303589/2023-0.\nSao Paulo Research Foundation (FAPESP) under eScience Program grant 2014/08398-6, for providing MSc, PhD, and post-doc scholarships, equipment, and travel support.\nInternational Climate Initiative of the Germany Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (IKI) under grant 17-III-084-Global-A-RESTORE+ (“RESTORE+: Addressing Landscape Restoration on Degraded Land in Indonesia and Brazil”).\nMicrosoft Planetary Computer initiative under the GEO-Microsoft Cloud Computer Grants Programme.\nInstituto Clima e Sociedade, under the project grant “Modernization of PRODES and DETER Amazon monitoring systems”.\nOpen-Earth-Monitor Cyberinfrastructure project, which has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No. 101059548.\nFAO-EOSTAT initiative, which uses next generation Earth observation tools to produce land cover and land use statistics."
  },
  {
    "objectID": "acknowledgements.html#community-contributions",
    "href": "acknowledgements.html#community-contributions",
    "title": "Acknowledgements",
    "section": "Community Contributions",
    "text": "Community Contributions\nThe authors thank the R-spatial community for their foundational work, including Marius Appel, Tim Appelhans, Robert Hijmans, Jakub Nowosad, Edzer Pebesma, and Martijn Tennekes for their R packages gdalcubes, leafem, terra, supercells, sf/stars, and tmap. We are grateful for the work of Dirk Eddelbuettel on Rcpp and RcppArmadillo and Ron Wehrens in package kohonen. We are much indebted to Hadley Wickham for the tidyverse, Daniel Falbel for the torch and luz packages, and the RStudio team for package leaflet. The multiple authors of machine learning packages randomForest, e1071, and xgboost provided robust algorithms. We would like to thank Python developers who shared their deep learning algorithms for image time series classification: Vivien Sainte Fare Garnot, Zhiguang Wang, Maja Schneider, and Marc Rußwurm. The first author also thanks Roger Bivand for his benign influence in all things related to R."
  },
  {
    "objectID": "acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "href": "acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "title": "Acknowledgements",
    "section": "Reproducible papers and books used in building sits",
    "text": "Reproducible papers and books used in building sits\nWe thank the authors of the following papers for making their code and papers open and reusable. Their contribution has been essential to build sits.\n\nEdzer Pebesma, Simple Features for R: Standardized Support for Spatial Vector Data. R Journal, 10(1), 2018.\nMartin Tennekes, tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39, 2018.\nRon Wehrens and Johannes Kruisselbrink, Flexible Self-Organising Maps in kohonen 3.0. Journal of Statistical Software, 87, 7, 2018.\nHassan Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller, Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4): 917–963, 2019.\nCharlotte Pelletier, Geoffrey Webb, and Francois Petitjean. Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series. Remote Sensing 11 (5), 2019.\nMarc Rußwurm, Charlotte Pelletier, Maximilian Zollner, Sèbastien Lefèvre, and Marco Körner, Breizhcrops: a Time Series Dataset for Crop Type Mapping. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences ISPRS, 2020.\nMarius Appel and Edzer Pebesma, On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library. Data 4 (3): 1–16, 2020.\nVivien Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata, Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention, Conference on Computer Vision and Pattern Recognition, 2020.\nVivien Garnot and Loic Landrieu, Lightweight Temporal Self-Attention for Classifying Satellite Images Time Series, 2020.\nMaja Schneider, Marco Körner, Re: Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention ReScience C 7 (2), 2021.\nRolf Simoes, Felipe Souza, Mateus Zaglia, Gilberto Queiroz, Rafael dos Santos and Karine Ferreira, Rstac: An R Package to Access Spatiotemporal Asset Catalog Satellite Imagery. IGARSS, 2021, pp. 7674-7677.\nJakub Nowosad, Tomasz Stepinksi, Extended SLIC superpixels algorithm for applications to non-imagery geospatial rasters. International Journal of Applied Earth Observations and Geoinformation, 2022.\nSigrid Keydana, Deep Learning and Scientific Computing with R torch, Chapman and Hall/CRC, London, 2023.\nRobin Lovelace, Jakub Nowosad, Jannes Münchow, Geocomputation with R. Chapman and Hall/CRC, London, 2023.\nEdzer Pebesma, Roger Bivand, Spatial Data Science: With applications in R. Chapman and Hall/CRC, London, 2023."
  },
  {
    "objectID": "acknowledgements.html#publications-using-sits",
    "href": "acknowledgements.html#publications-using-sits",
    "title": "Acknowledgements",
    "section": "Publications using SITS",
    "text": "Publications using SITS\nThis section includes publications that have used sits to generate their results.\n2025\n\nZahra Mobarakeh, Saeid Pourmanafi, Mohsen Ahmadi, Employing Sentinel-2 time-series and noisy data quality control enhance crop classification in arid environments: A comparison of machine learning and deep learning methods. International Journal of Applied Earth Observation and Geoinformation, 2025.\nSimone, Lorenzo de, Estefanía Pizarro, Jonathan Paredes, Alberto Jopia, Gilberto Camara, Pierre Defourny, and Sophie Bontemps. Quality Control of Training Samples for Agricultural Statistics Using Earth Observation. Statistical Journal of the IAOS, 2025.\nXiaofang Sun, Meng Wang, Junbang Wang, Guicai Li, Xuehui Hou, Deep learning classification of winter wheat from Sentinel optical-radar image time series in smallholder farming areas. Advances in Space Research, 2025.\n\n2024\n\nGiuliani, Gregory. Time-First Approach for Land Cover Mapping Using Big Earth Observation Data Time-Series in a Data Cube – a Case Study from the Lake Geneva Region (Switzerland). Big Earth Data, 2024.\nWerner, João, Mariana Belgiu et al., Mapping Integrated Crop–Livestock Systems Using Fused Sentinel-2 and PlanetScope Time Series and Deep Learning. Remote Sensing 16, no. 8 (January 2024): 1421.\n\n2023\n\nHadi, Firman, Laode Muhammad Sabri, Yudo Prasetyo, and Bambang Sudarsono. Leveraging Time-Series Imageries and Open Source Tools for Enhanced Land Cover Classification. In IOP Conference Series: Earth and Environmental Science, 1276:012035. IOP Publishing, 2023.\nBruno Adorno, Thales Körting, and Silvana Amaral, Contribution of time-series data cubes to classify urban vegetation types by remote sensing. Urban Forest & Urban Greening, 79, 127817, 2023.\n\n2021\n\nLorena Santos, Karine R. Ferreira, Gilberto Camara, Michelle Picoli, and Rolf Simoes, Quality control and class noise reduction of satellite image time series. ISPRS Journal of Photogrammetry and Remote Sensing, 177, 75–88, 2021.\nLorena Santos, Karine Ferreira, Michelle Picoli, Gilberto Camara, Raul Zurita-Milla and Ellen-Wien Augustijn, Identifying Spatiotemporal Patterns in Land Use and Cover Samples from Satellite Image Time Series. Remote Sensing, 13(5), 974, 2021.\n\n2020\n\nRolf Simoes, Michelle Picoli, Gilberto Camara, Adeline Maciel, Lorena Santos, Pedro Andrade, Alber Sánchez, Karine Ferreira, and Alexandre Carvalho, Land use and cover maps for Mato Grosso State in Brazil from 2001 to 2017. Nature Scientific Data, 7, article 34, 2020.\nMichelle Picoli, Ana Rorato, Pedro Leitão, Gilberto Camara, Adeline Maciel, Patrick Hostert, and Ieda Sanches, Impacts of Public and Private Sector Policies on Soybean and Pasture Expansion in Mato Grosso—Brazil from 2001 to 2017. Land, 9(1), 2020.\nKarine Ferreira, Gilberto Queiroz et al., Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products. Remote Sensing, 12, 4033, 2020.\nAdeline Maciel, Lubia Vinhas, Michelle Picoli, and Gilberto Camara, Identifying Land Use Change Trajectories in Brazil’s Agricultural Frontier. Land, 9, 506, 2020.\n\n2018\n\nMichelle Picoli, Gilberto Camara, et al., Big Earth Observation Time Series Analysis for Monitoring Brazilian Agriculture. ISPRS Journal of Photogrammetry and Remote Sensing, 145, 328–339, 2018."
  },
  {
    "objectID": "acknowledgements.html#ai-support-in-preparing-the-book",
    "href": "acknowledgements.html#ai-support-in-preparing-the-book",
    "title": "Acknowledgements",
    "section": "AI support in preparing the book",
    "text": "AI support in preparing the book\nThe authors have use Generative AI tools (Chat-GPT, Grammarly and ProWritingAid) to improve readability and language of the work. The core technical and scientific content of the book has been prepared exclusively by the authors. Assistance from Generative AI has been limited to improving definitions and making the text easier to follow."
  }
]