[
  {
    "objectID": "index.html#greetings",
    "href": "index.html#greetings",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Greetings",
    "text": "Greetings\n\n\n\n\n\n\n\n\nWelcome to the age of big Earth observation data! With free access to massive data sets, we need new methods to measure change on our planet. This book will help you to use state-of-the-art tools to work with image time series. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Combined with Earth observation data cube, time series analysis are a new and exciting paradigm. This book offers a comprehensive appraisal of this emerging discipline."
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "What is this book about?",
    "text": "What is this book about?\nThis book introduces sits, an open-source R package for big Earth observation data analysis using satellite image time series. Users build regular data cubes from cloud services such as Amazon Web Services, Microsoft Planetary Computer, Copernicus Data Space Ecosystem, NASA Harmonized Landsat-Sentinel, Brazil Data Cube, Swiss Data Cube, Digital Earth Australia, and Digital Earth Africa. The sits API includes training sample quality measures, machine learning and deep learning classification algorithms, and Bayesian post-processing methods for smoothing and uncertainty assessment. To evaluate results, sits supports best-practice accuracy assessments. The authors also provide a Python API that interfaces with the R API, and thus allows Python users to directly run sits and convert its data structures to Python data.frames and xarrays."
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "About the authors",
    "text": "About the authors\n\nGilberto Camara is a Senior Research Fellow at Brazil’s National Institute for Space Reseach (INPE).\nRolf Simoes is a Research Engineer at Open Geo Hub (Netherlands).\nFelipe Souza and Pedro Brito are PhD students at INPE.\nFelipe Carlos is a Software Engineer at the Group on Earth Observations (GEO).\nPedro Andrade and Karine Ferreira are Senior Researchers at INPE.\nLorena Santos is a Researcher at CTrees.org.\nAlexandre Assunção is a software consultant.\nCharlotte Pelletier is an Associate Professor at Université Bretagne-Sud in France."
  },
  {
    "objectID": "index.html#how-much-r-knowledge-is-required",
    "href": "index.html#how-much-r-knowledge-is-required",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "How much R knowledge is required?",
    "text": "How much R knowledge is required?\nThe sits package is designed for remote sensing experts in the Earth Sciences field who want to use advanced data analysis techniques with only basic programming knowledge. The package provides a clear and direct set of functions that are easy to learn and master. To quickly master what is needed to run sits, please read Parts 1 and 2 of Garrett Grolemund’s book, Hands-On Programming with R. Although not needed to run sits, your R skills will benefit from the book by Hadley Wickham and Gareth Grolemund, R for Data Science (2nd edition). Important concepts of spatial analysis are presented by Edzer Pebesma and Roger Bivand in their book Spatial Data Science."
  },
  {
    "objectID": "index.html#how-does-one-run-sits-in-python",
    "href": "index.html#how-does-one-run-sits-in-python",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "How does one run SITS in Python?",
    "text": "How does one run SITS in Python?\nFrom version 1.5.3 onwards, users can run sits in Python. Follow the instructions in the “Setup” chapter on how to set your Python environment to interface with R. Then follow the book examples provided for using sits in Python. The book provides code in both R and Python. Therefore, after correctly setting up their working environment, Python experts can run sits functions in their favorite tools, such as Jupyter Notebooks."
  },
  {
    "objectID": "index.html#software-version-described-in-this-book",
    "href": "index.html#software-version-described-in-this-book",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Software version described in this book",
    "text": "Software version described in this book\nThe version of the sits package described in this book is 1.5.3."
  },
  {
    "objectID": "index.html#main-reference-for-sits",
    "href": "index.html#main-reference-for-sits",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Main reference for sits\n",
    "text": "Main reference for sits\n\nIf you use sits in your work, please cite the following paper:\nRolf Simoes, Gilberto Camara, Gilberto Queiroz, Felipe Souza, Pedro R. Andrade, Lorena Santos, Alexandre Carvalho, and Karine Ferreira. Satellite Image Time Series Analysis for Big Earth Observation Data. Remote Sensing, 13, p. 2428, 2021."
  },
  {
    "objectID": "index.html#intellectual-property-rights",
    "href": "index.html#intellectual-property-rights",
    "title": "Satellite Image Time Series Analysis on Earth Observation Data Cubes",
    "section": "Intellectual property rights",
    "text": "Intellectual property rights\nThis book is licensed as Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) by Creative Commons. The sits package is licensed under the GNU General Public License, version 3.0."
  },
  {
    "objectID": "setup.html#how-to-use-this-on-line-book",
    "href": "setup.html#how-to-use-this-on-line-book",
    "title": "Setup",
    "section": "How to use this on-line book",
    "text": "How to use this on-line book\nThis book contains reproducible code that can be run in an R environment. There are three options to set up your working environment: Install R and RStudio, and the packages required by sits, with specific procedures for each type of operating system.\n\nInstall R and RStudio, and the packages required by sits, with specific procedures for each type of operating system.\nUse a Docker image provided by the Brazil Data Cube.\nInstall sits and all its dependencies using conda."
  },
  {
    "objectID": "setup.html#how-to-install-sits-using-r-and-rstudio",
    "href": "setup.html#how-to-install-sits-using-r-and-rstudio",
    "title": "Setup",
    "section": "How to install sits using R and RStudio",
    "text": "How to install sits using R and RStudio\nWe suggest a staged installation, as follows:\n\nGet and install base R from CRAN.\nInstall RStudio from the Posit website.\n\nInstalling sits from CRAN\nThe Comprehensive R Archive Network (CRAN) is a network of servers (also known as mirrors) from around the world that store up-to-date versions of basic code and packages for R. In what follows, we describe how to use CRAN to install sits on Windows, Linux and macOS.\nInstalling in Microsoft Windows and macOS environments\nWindows and macOS users are strongly encouraged to install binary packages from CRAN. The sits package relies on the sf and terra packages, which require the GDAL and PROJ libraries. Run RStudio and install the binary packages sf and terra, in this order:\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\n\nAfter installing the binaries for sf and terra, install sits as follows;\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\nTo run the examples in the book, please also install sitsdata package, which is available from GitHub. It is necessary to use package devtools to install sitsdata.\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sitsdata\")\n\nTo install sits from source, please install Rtools for Windows to have access to the compiling environment. For Mac, please follow the instructions available here.\nInstalling in Ubuntu environments\nFor Ubuntu, the first step should be to install the latest version of the GDAL, GEOS, and PROJ4 libraries and binaries. To do so, use the repository ubuntugis-unstable, which should be done as follows:\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt-get update\nsudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev \nsudo apt-get install gdal-bin\nsudo apt-get install proj-bin\nGetting an error while adding this PPA repository could be due to the absence of the package software-properties-common. After installing GDAL, GEOS, and PROJ4, please install packages sf and terra:\n\ninstall.packages(\"sf\")\ninstall.packages(\"terra\")\n\nThen please proceed to install sits, which can be installed as a regular R package.\n\ninstall.packages(\"sits\", dependencies = TRUE)\n\nInstalling in Debian environments\nFor Debian, use the rocker geospatial dockerfiles.\nInstalling in Fedora environments\nIn the case of Fedora, the following command installs all required dependencies:\n\nsudo dnf install gdal-devel proj-devel geos-devel sqlite-devel udunits2-devel"
  },
  {
    "objectID": "setup.html#using-docker-images",
    "href": "setup.html#using-docker-images",
    "title": "Setup",
    "section": "Using Docker images",
    "text": "Using Docker images\nIf you are familiar with Docker, there are images for sits available with RStudio or Jupyter Notebook. Such images are provided by the Brazil Data Cube team:\n\n\nVersion for R and RStudio.\n\nVersion for Jupyter Notebooks.\n\nOn a Windows or macOS platform, install Docker and then obtain one of the two images listed above from the Brazil Data Cube. Both images contain the full sits running environment. When GDAL is running in docker containers, please add the security flag --security-opt seccomp=unconfined on start."
  },
  {
    "objectID": "setup.html#install-sits-from-conda",
    "href": "setup.html#install-sits-from-conda",
    "title": "Setup",
    "section": "Install sits from CONDA",
    "text": "Install sits from CONDA\nConda is an open-source, cross-platform package manager. It is a convenient way to install Python and R packages. To use conda, first download the software from the CONDA website. After installation, use conda to install sits from the terminal as follows:\n\n# add conda-forge to the download channels \nconda config --add channels conda-forge\nconda config --set channel_priority strict\n# install sits using conda\nconda install conda-forge::r-sits\n\nThe conda installer will download all packages and libraries required to run sits. This is the easiest way to install sits on Windows."
  },
  {
    "objectID": "setup.html#accessing-the-development-version",
    "href": "setup.html#accessing-the-development-version",
    "title": "Setup",
    "section": "Accessing the development version",
    "text": "Accessing the development version\nThe source code repository of sits is on GitHub. There are two versions available on GitHub: master and dev. The master contains the current stable version, which is either the same code available on CRAN or a minor update with bug fixes. To install the master version, install devtools (if not already available) and do as follows:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sits\", dependencies = TRUE)\n\nTo install the dev (development) version, which contains the latest updates but might be unstable, install devtools (if not already available), and then install sits as follows:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"e-sensing/sits@dev\", dependencies = TRUE)"
  },
  {
    "objectID": "setup.html#additional-requirements",
    "href": "setup.html#additional-requirements",
    "title": "Setup",
    "section": "Additional requirements",
    "text": "Additional requirements\nTo run the examples in the book, please also install the sitsdata package. We recommend installing it using wget. See instructions in the GNU Wget site.\n\noptions(download.file.method = \"wget\")\ndevtools::install_github(\"e-sensing/sitsdata\")"
  },
  {
    "objectID": "setup.html#using-gpus-with-sits",
    "href": "setup.html#using-gpus-with-sits",
    "title": "Setup",
    "section": "Using GPUs with sits\n",
    "text": "Using GPUs with sits\n\nThe torch package automatically recognizes if a GPU is available on the machine and uses it for training and classification. There is a significant performance gain when GPUs are used instead of CPUs for deep learning models. There is no need for specific adjustments to torch scripts. To use GPUs, torch requires version 11.6 of the CUDA library, which is available for Ubuntu 18.04 and 20.04. Please follow the detailed instructions for setting up torch available here.\n\ninstall.packages(\"torch\")"
  },
  {
    "objectID": "acknowledgements.html#funding-sources",
    "href": "acknowledgements.html#funding-sources",
    "title": "Acknowledgements",
    "section": "Funding Sources",
    "text": "Funding Sources\nThe authors acknowledge the funders that supported the development of sits:\n\nAmazon Fund, established by Brazil with financial contribution from Norway, through contract 17.2.0536.1. between the Brazilian Development Bank (BNDES) and the Foundation for Science, Technology, and Space Applications (FUNCATE), for the establishment of the Brazil Data Cube.\nCoordenação de Aperfeiçoamento de Pessoal de Nível Superior-Brasil (CAPES) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq) for grants 312151/2014-4 and 140684/2016-6.\nSao Paulo Research Foundation (FAPESP) under eScience Program grant 2014/08398-6, for providing MSc, PhD, and post-doc scholarships, equipment, and travel support.\nInternational Climate Initiative of the Germany Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (IKI) under grant 17-III-084-Global-A-RESTORE+ (“RESTORE+: Addressing Landscape Restoration on Degraded Land in Indonesia and Brazil”).\nMicrosoft Planetary Computer initiative under the GEO-Microsoft Cloud Computer Grants Programme.\nInstituto Clima e Sociedade, under the project grant “Modernization of PRODES and DETER Amazon monitoring systems”.\nOpen-Earth-Monitor Cyberinfrastructure project, which has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No. 101059548.\nFAO-EOSTAT initiative, which uses next generation Earth observation tools to produce land cover and land use statistics."
  },
  {
    "objectID": "acknowledgements.html#community-contributions",
    "href": "acknowledgements.html#community-contributions",
    "title": "Acknowledgements",
    "section": "Community Contributions",
    "text": "Community Contributions\nThe authors thank the R-spatial community for their foundational work, including Marius Appel, Tim Appelhans, Robert Hijmans, Jakub Nowosad, Edzer Pebesma, and Martijn Tennekes for their R packages gdalcubes, leafem, terra, supercells, sf/stars, and tmap. We are grateful for the work of Dirk Eddelbuettel on Rcpp and RcppArmadillo and Ron Wehrens in package kohonen. We are much indebted to Hadley Wickham for the tidyverse, Daniel Falbel for the torch and luz packages, and the RStudio team for package leaflet. The multiple authors of machine learning packages randomForest, e1071, and xgboost provided robust algorithms. We would like to thank Python developers who shared their deep learning algorithms for image time series classification: Vivien Sainte Fare Garnot, Zhiguang Wang, Maja Schneider, and Marc Rußwurm. The first author also thanks Roger Bivand for his benign influence in all things related to R."
  },
  {
    "objectID": "acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "href": "acknowledgements.html#reproducible-papers-and-books-used-in-building-sits",
    "title": "Acknowledgements",
    "section": "Reproducible papers and books used in building sits",
    "text": "Reproducible papers and books used in building sits\nWe thank the authors of the following papers for making their code and papers open and reusable. Their contribution has been essential to build sits.\n\nEdzer Pebesma, Simple Features for R: Standardized Support for Spatial Vector Data. R Journal, 10(1), 2018.\nMartin Tennekes, tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39, 2018.\nRon Wehrens and Johannes Kruisselbrink, Flexible Self-Organising Maps in kohonen 3.0. Journal of Statistical Software, 87, 7, 2018.\nHassan Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller, Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4): 917–963, 2019.\nCharlotte Pelletier, Geoffrey Webb, and Francois Petitjean. Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series. Remote Sensing 11 (5), 2019.\nMarc Rußwurm, Charlotte Pelletier, Maximilian Zollner, Sèbastien Lefèvre, and Marco Körner, Breizhcrops: a Time Series Dataset for Crop Type Mapping. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences ISPRS, 2020.\nMarius Appel and Edzer Pebesma, On-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library. Data 4 (3): 1–16, 2020.\nVivien Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata, Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention, Conference on Computer Vision and Pattern Recognition, 2020.\nVivien Garnot and Loic Landrieu, Lightweight Temporal Self-Attention for Classifying Satellite Images Time Series, 2020.\nMaja Schneider, Marco Körner, Re: Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention ReScience C 7 (2), 2021.\nRolf Simoes, Felipe Souza, Mateus Zaglia, Gilberto Queiroz, Rafael dos Santos and Karine Ferreira, Rstac: An R Package to Access Spatiotemporal Asset Catalog Satellite Imagery. IGARSS, 2021, pp. 7674-7677.\nJakub Nowosad, Tomasz Stepinksi, Extended SLIC superpixels algorithm for applications to non-imagery geospatial rasters. International Journal of Applied Earth Observations and Geoinformation, 2022.\nSigrid Keydana, Deep Learning and Scientific Computing with R torch, Chapman and Hall/CRC, London, 2023.\nRobin Lovelace, Jakub Nowosad, Jannes Münchow, Geocomputation with R. Chapman and Hall/CRC, London, 2023.\nEdzer Pebesma, Roger Bivand, Spatial Data Science: With applications in R. Chapman and Hall/CRC, London, 2023."
  },
  {
    "objectID": "acknowledgements.html#publications-using-sits",
    "href": "acknowledgements.html#publications-using-sits",
    "title": "Acknowledgements",
    "section": "Publications using sits",
    "text": "Publications using sits\nThis section gathers the publications that have used sits to generate their results.\n2024\n\nGiuliani, Gregory. Time-First Approach for Land Cover Mapping Using Big Earth Observation Data Time-Series in a Data Cube – a Case Study from the Lake Geneva Region (Switzerland). Big Earth Data, 2024.\nWerner, João, Mariana Belgiu et al., Mapping Integrated Crop–Livestock Systems Using Fused Sentinel-2 and PlanetScope Time Series and Deep Learning. Remote Sensing 16, no. 8 (January 2024): 1421.\n\n2023\n\nHadi, Firman, Laode Muhammad Sabri, Yudo Prasetyo, and Bambang Sudarsono. Leveraging Time-Series Imageries and Open Source Tools for Enhanced Land Cover Classification. In IOP Conference Series: Earth and Environmental Science, 1276:012035. IOP Publishing, 2023.\nBruno Adorno, Thales Körting, and Silvana Amaral, Contribution of time-series data cubes to classify urban vegetation types by remote sensing. Urban Forest & Urban Greening, 79, 127817, 2023.\n\n2021\n\nLorena Santos, Karine R. Ferreira, Gilberto Camara, Michelle Picoli, and Rolf Simoes, Quality control and class noise reduction of satellite image time series. ISPRS Journal of Photogrammetry and Remote Sensing, 177, 75–88, 2021.\nLorena Santos, Karine Ferreira, Michelle Picoli, Gilberto Camara, Raul Zurita-Milla and Ellen-Wien Augustijn, Identifying Spatiotemporal Patterns in Land Use and Cover Samples from Satellite Image Time Series. Remote Sensing, 13(5), 974, 2021.\n\n2020\n\nRolf Simoes, Michelle Picoli, Gilberto Camara, Adeline Maciel, Lorena Santos, Pedro Andrade, Alber Sánchez, Karine Ferreira, and Alexandre Carvalho, Land use and cover maps for Mato Grosso State in Brazil from 2001 to 2017. Nature Scientific Data, 7, article 34, 2020.\nMichelle Picoli, Ana Rorato, Pedro Leitão, Gilberto Camara, Adeline Maciel, Patrick Hostert, and Ieda Sanches, Impacts of Public and Private Sector Policies on Soybean and Pasture Expansion in Mato Grosso—Brazil from 2001 to 2017. Land, 9(1), 2020.\nKarine Ferreira, Gilberto Queiroz et al., Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products. Remote Sensing, 12, 4033, 2020.\nAdeline Maciel, Lubia Vinhas, Michelle Picoli, and Gilberto Camara, Identifying Land Use Change Trajectories in Brazil’s Agricultural Frontier. Land, 9, 506, 2020.\n\n2018\n\nMichelle Picoli, Gilberto Camara, et al., Big Earth Observation Time Series Analysis for Monitoring Brazilian Agriculture. ISPRS Journal of Photogrammetry and Remote Sensing, 145, 328–339, 2018."
  },
  {
    "objectID": "acknowledgements.html#ai-support-in-preparing-the-book",
    "href": "acknowledgements.html#ai-support-in-preparing-the-book",
    "title": "Acknowledgements",
    "section": "AI support in preparing the book",
    "text": "AI support in preparing the book\nThe authors have use Generative AI tools (Chat-GPT, Grammarly and ProWritingAid) to improve readability and language of the work. The core technical and scientific content of the book has been prepared exclusively by the authors. Assistance from Generative AI has been limited to improving definitions and making the text easier to follow."
  },
  {
    "objectID": "introduction.html#who-is-this-book-for",
    "href": "introduction.html#who-is-this-book-for",
    "title": "The Basics of SITS",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book, tailored for land use change experts and researchers, is a practical guide that enables them to analyze big Earth observation data sets. It provides readers with the means to produce high-quality maps of land use and land cover, guiding them through all the steps to achieve good results. Given the natural world’s complexity and the huge variations in human-nature interactions, only local experts who know their countries and ecosystems can extract full information from big EO data.\nOne group of readers that we are keen to engage with is the national authorities on forestry, agriculture, and statistics in developing countries. We aim to foster a collaborative environment where they can use EO data to enhance their national land use and cover estimates, supporting sustainable development policies. To achieve this goal, sits has strong backing from the FAO Expert Group on the Use of Earth Observation data (FAO-EOSTAT). FAO-EOSTAT is at the forefront of using advanced EO data analysis methods for agricultural statistics in developing countries [1], [2]."
  },
  {
    "objectID": "introduction.html#why-work-with-satellite-image-time-series",
    "href": "introduction.html#why-work-with-satellite-image-time-series",
    "title": "The Basics of SITS",
    "section": "Why work with satellite image time series?",
    "text": "Why work with satellite image time series?\nSatellite imagery provides the most extensive data on our environment. By encompassing vast areas of the Earth’s surface, satellite images enable researchers to analyze local and worldwide transformations. By observing the same location multiple times, satellites provide data on environmental changes and survey areas that are difficult to observe from the ground. Given its unique features, images offer essential information for many applications, including deforestation, crop production, food security, urban footprints, water scarcity, and land degradation. Using time series, experts improve their understanding of ecological patterns and processes. Instead of selecting individual images from specific dates and comparing them, researchers track change continuously [3]."
  },
  {
    "objectID": "introduction.html#time-first-space-later",
    "href": "introduction.html#time-first-space-later",
    "title": "The Basics of SITS",
    "section": "Time-first, space-later",
    "text": "Time-first, space-later\n“Time-first, space-later” is a concept in satellite image classification that takes time series analysis as the first step for analyzing remote sensing data, with spatial information being considered after all time series are classified. The time-first part brings a better understanding of changes in landscapes. Detecting and tracking seasonal and long-term trends becomes feasible, as well as identifying anomalous events or patterns in the data, such as wildfires, floods, or droughts. Each pixel in a data cube is treated as a time series, using information available in the temporal instances of the case. Time series classification is pixel-based, producing a set of labeled pixels. This result is then used as input for the space-later part of the method. In this phase, a smoothing algorithm improves the results of the time-first classification by considering the spatial neighborhood of each pixel. The resulting map thus combines both spatial and temporal information.\n\n\n\n\nFigure 1: Satellite image time series classification (source: [4])."
  },
  {
    "objectID": "introduction.html#land-use-and-land-cover",
    "href": "introduction.html#land-use-and-land-cover",
    "title": "The Basics of SITS",
    "section": "Land use and land cover",
    "text": "Land use and land cover\nThe UN Food and Agriculture Organization defines land cover as “the observed biophysical cover on the Earth’s surface” [5]. Land cover can be observed and mapped directly through remote sensing images. In FAO’s guidelines and reports, land use is described as “the human activities or purposes for which land is managed or exploited.” Although land cover and land use denote different approaches for describing the Earth’s landscape, in practice there is considerable overlap between these concepts [6]. When classifying remote sensing images, natural areas are classified using land cover types (e.g., forest), while human-modified areas are described with land use classes (e.g., pasture).\nOne of the advantages of using image time series for land classification is its capacity to measure changes in the landscape related to agricultural practices. For example, the time series of a vegetation index in an area of crop production will show a pattern of minima (planting and sowing stages) and maxima (flowering stage). Thus, classification schemas based on image time series data can be richer and more detailed than those associated only with land cover. In what follows, we use the term “land classification” to refer to image classification representing both land cover and land use classes."
  },
  {
    "objectID": "introduction.html#how-sits-works",
    "href": "introduction.html#how-sits-works",
    "title": "The Basics of SITS",
    "section": "How SITS works",
    "text": "How SITS works\nThe sits package uses satellite image time series for land classification, using a time-first, space-later approach. In the data preparation part, collections of big Earth observation images are organized as data cubes. Each spatial location of a data cube is associated with a time series. Locations with known labels are used to train a machine learning algorithm, which classifies all time series of a data cube, as shown in Figure 2.\n\n\n\n\nFigure 2: General view of sits.\n\n\n\nThe sits API is a set of functions that can be chained to create a workflow for land classification. At its heart, the sits package has eight functions, as shown in Figure 3:\n\nExtract data from an analysis-ready data (ARD) collection using sits_cube(), producing a non-regular data cube object.\nFrom a non-regular data_cube create a regular one, using sits_regularize(). Regular data cubes are required to train machine learning algorithms.\nObtain new bands and indices with operations on regular data cubes with sits_apply().\nGiven a set of ground truth values in formats such as CSV or SHP and a regular data cube, use sits_get_data() to obtain training samples containing time series for selected locations in the training area.\nSelect a machine learning algorithm and use sits_train() to produce a classification model.\nGiven a classification model and a regular data cube, use sits_classify() to get a probability data cube, which contains the probabilities for class allocation for each pixel.\nRemove outliers in a probability data cube using sits_smooth().\nUse sits_label_classification() to produce a thematic map from a smoothed probability cube.\n\n\n\n\n\nFigure 3: Main functions of the sits API (source: authors).\n\n\n\nEach workflow step corresponds to a function of the sits API, as shown in the table below. These functions have convenient default parameters and behaviors. A single function builds machine learning (ML) models. The classification function processes big data cubes with efficient parallel processing. Since the sits API is simple to learn, achieving good results does not require in-depth knowledge about machine learning and parallel processing.\n\n\n\nThe sits API workflow for land classification.\n\nAPI_function\nInputs\nOutput\n\n\n\nsits_cube()\nARD image collection\nNon-regular data cube\n\n\nsits_regularize()\nNon-regular data cube\nRegular data cube\n\n\nsits_apply()\nRegular data cube\nRegular data cube with new bands and indices\n\n\nsits_get_data()\nRegular data cube and sample locations\nTime series samples\n\n\nsits_train()\nTime series and ML method\nML classification model\n\n\nsits_classify()\nML classification model and regular data cube\nProbability cube\n\n\nsits_smooth()\nProbability cube\nSmoothed probability cube\n\n\nsits_label_classification()\nSmoothed probability cube\nClassified map"
  },
  {
    "objectID": "introduction.html#additional-functions-in-sits",
    "href": "introduction.html#additional-functions-in-sits",
    "title": "The Basics of SITS",
    "section": "Additional functions in SITS",
    "text": "Additional functions in SITS\nIn addition to the eight basic functions of its API, sits supports additional tools for improving training data quality and evaluating classification results. They include:\n\nPerforming quality control and filtering on the time series samples.\nMerging multi-source data to capture responses from different sensors.\nMeasuring classification uncertainty to support active learning.\nSupporting vector data cubes and object-based time series image analysis.\nEvaluating the accuracy of the classification using best practices.\n\nThese functions are also described in this book."
  },
  {
    "objectID": "introduction.html#references",
    "href": "introduction.html#references",
    "title": "The Basics of SITS",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nL. De Simone and P. Gennari, “Earth observations for official crop statistics in the context of scarcity of in-situ data,” Statistical Journal of the IAOS, vol. 38, no. 3, pp. 1009–1019, 2022, doi: 10.3233/SJI-220054.\n\n\n[2] \nL. De Simone, W. Ouellette, and P. Gennari, “Operational Use of EO Data for National Land Cover Official Statistics in Lesotho,” Remote Sensing, vol. 14, no. 14, p. 3294, 2022, doi: 10.3390/rs14143294.\n\n\n[3] \nC. E. Woodcock, T. R. Loveland, M. Herold, and M. E. Bauer, “Transitioning from change detection to monitoring with remote sensing: A paradigm shift,” Remote Sensing of Environment, vol. 238, p. 111558, 2020, doi: 10.1016/j.rse.2019.111558.\n\n\n[4] \nC. W. Tan, G. I. Webb, and F. Petitjean, “Indexing and classifying gigabytes of time series under time warping,” in Proceedings of the 2017 SIAM International Conference on Data Mining (SDM), Society for Industrial and Applied Mathematics, 2017, pp. 282–290.\n\n\n[5] \nA. Di Gregorio, “Land Cover Classification System - Classification concepts Software version 3,” FAO, 2016.\n\n\n[6] \nA. J. Comber, R. A. Wadsworth, and P. F. Fisher, “Using semantics to clarify the conceptual confusion between land cover and land use: The example of forest,” Journal of Land Use Science, vol. 3, no. 2–3, pp. 185–198, 2008."
  },
  {
    "objectID": "intro_quicktour.html#overview",
    "href": "intro_quicktour.html#overview",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.1 Overview",
    "text": "1.1 Overview\nIn this chapter, we present a simple example of using sits for agricultural classification. We start by taking a set of samples with land use and land cover types for an area in the Cerrado biome in Brazil, near the city of Luis Eduardo Magalhães in the state of Bahia. These samples were collected by a team of INPE researchers [1]."
  },
  {
    "objectID": "intro_quicktour.html#training-samples",
    "href": "intro_quicktour.html#training-samples",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.2 Training samples",
    "text": "1.2 Training samples\nIn this example, we take the data set for agriculture in the municipality of Luis Eduardo Magalhaes (henceforth LEM) as our starting point. This is a common situation in land classification. To be able to obtain time series information, sits requires that the training samples provide, for each sample, information on location, temporal range, and associated label. In this example, we use a data frame with five columns: longitude, latitude, start_date, end_date and label. The training samples are available in the R sitsdata package. Alternative ways of defining data samples include CSV files and shapefiles. Please refer to chapter “Working with time series”.\n\n\nR\nPython\n\n\n\n\n# Load the samples for LEM from the \"sitsdata\" package\n# select the directory for the samples \nsamples_dir &lt;- system.file(\"data\", package = \"sitsdata\")\n# retrieve a data.frame with the samples\ndf_samples_cerrado_lem &lt;- readRDS(file.path(samples_dir, \"df_samples_cerrado_lem.rds\"))\ndf_samples_cerrado_lem\n\n# A tibble: 2,302 × 5\n   longitude latitude start_date end_date   label  \n       &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;  \n 1     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 2     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 3     -46.4    -12.3 2019-09-30 2020-09-29 Pasture\n 4     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 5     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 6     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n 7     -46.6    -12.4 2019-09-30 2020-09-29 Pasture\n 8     -46.6    -12.3 2019-09-30 2020-09-29 Pasture\n 9     -46.5    -12.3 2019-09-30 2020-09-29 Pasture\n10     -46.6    -12.4 2019-09-30 2020-09-29 Pasture\n# ℹ 2,292 more rows\n\n\n\n\n\n# Load the samples for LEM from the \"sitsdata\" package\nsamples_file = sitsdata_dir + \"/data/df_samples_cerrado_lem.rds\"\ndf_samples_cerrado_lem = read_sits_rds(samples_file)\ndf_samples_cerrado_lem\n\n\n\n\nThe data set contains 2,302 samples divided into four classes: (a) “Cerrado,” which corresponds to the natural vegetation associated with the Brazilian savanna; (b) “Cropland_1_cycle,” temporary agriculture (mostly soybeans) planted in a single cycle from October to March; (c) “Cropland_2_cycles,” temporary agriculture planted in two cycles, the first from October to March and the second from April to July; (d) “Pasture,” areas for cattle raising. For convenience, we present a high-resolution image of the area with the location of the samples.\n\n\n\n\nFigure 1.1: High resolution image of the LEM area with samples. Cerrado samples are shown in green, Pasture ones in yellow, Cropland_1_cycle in light brown and Cropland_2_cycles in dark brown."
  },
  {
    "objectID": "intro_quicktour.html#creating-a-data-cube-based-on-the-ground-truth-samples",
    "href": "intro_quicktour.html#creating-a-data-cube-based-on-the-ground-truth-samples",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.3 Creating a data cube based on the ground truth samples",
    "text": "1.3 Creating a data cube based on the ground truth samples\nThere are two kinds of data cubes in sits: (a) non-regular data cubes generated by selecting ARD image collections on cloud providers such as AWS and Planetary Computer; (b) regular data cubes with images fully covering a chosen area, where each image has the same spectral bands and spatial resolution, and images follow a set of adjacent and regular time intervals. Machine learning applications need regular data cubes. Please refer to Chapter Earth observation data cubes for further details.\nThe first steps in using sits are: (a) select an analysis-ready data image collection available from a cloud provider or stored locally using sits_cube(); (b) if the collection is not regular, use sits_regularize() to build a regular data cube.\nThis example builds a data cube from local images already organized as a regular data cube available in the Brazil Data Cube (“BDC”). The data cube is composed of CBERS-4 and CBERS-4A images for the LEM data set, covering the training samples. The images are taken with the WFI (wide field imager) sensor with 64-meter resolution. All images have indices NDVI and EVI covering a one-year period from 2019-09-30 to 2020-09-29 (we use “year-month-day” for dates). There are 24 time instances, each covering a 16-day period. We first define the region of interest using the bounding box of the LEM data set, and then define a data cube in the BDC repository based on this region. This data cube will be composed of the CBERS images that intersect with the region of interest.\n\n\nR\nPython\n\n\n\n\n# Find the the bounding box of the data\nlat_max &lt;- max(df_samples_cerrado_lem[[\"latitude\"]])\nlat_min &lt;- min(df_samples_cerrado_lem[[\"latitude\"]])\nlon_max &lt;- max(df_samples_cerrado_lem[[\"longitude\"]])\nlon_min &lt;- min(df_samples_cerrado_lem[[\"longitude\"]])\n# Define the roi for the LEM dataset\nroi_lem &lt;- c(\n    \"lat_max\" = lat_max,\n    \"lat_min\" = lat_min,\n    \"lon_max\" = lon_max,\n    \"lon_min\" = lon_min)\n# Define a data cube in the BDC repository based on the LEM ROI\nbdc_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection  = \"CBERS-WFI-16D\",\n    bands = c(\"NDVI\", \"EVI\"),\n    roi = roi_lem,\n    start_date = \"2019-09-30\",\n    end_date = \"2020-09-29\"\n)\n\n\n\n\n# Define a region of interest based on the LEM data set\nlat_max = max(df_samples_cerrado_lem[[\"latitude\"]])\nlat_min = min(df_samples_cerrado_lem[[\"latitude\"]])\nlon_max = max(df_samples_cerrado_lem[[\"longitude\"]])\nlon_min = min(df_samples_cerrado_lem[[\"longitude\"]])\n\n# Define the roi for the LEM dataset\nroi_lem = { \n    \"lat_max\" : lat_max,\n    \"lat_min\" : lat_min,\n    \"lon_max\" : lon_max,\n    \"lon_min\" : lon_min}\n# Define a data cube in the BDC repository based on the ROI\nbdc_cube = sits_cube(\n    source = \"BDC\", \n    collection  = \"CBERS-WFI-16D\",\n    bands = [\"NDVI\", \"EVI\"],\n    roi = roi_lem,\n    start_date = \"2019-09-30\",\n    end_date = \"2020-09-29\"\n)\n\n\n\n\nThe next step is to copy the data cube to a local directory for further processing. When using a region of interest to select a part of an ARD collection, sits intersects the region with the tiles of that collection. Thus, when one wants to get a subset of a tile, it is better to copy this subset to the local computer. After downloading the data, we use plot() to view it. The plot() function, by default, selects the image with the least cloud cover.\n\n\nR\nPython\n\n\n\n\n# Copy the region of interest to a local directory\nlem_cube &lt;- sits_cube_copy(\n    cube = bdc_cube,\n    roi = roi_lem,\n    output_dir = tempdir_r\n)\n# Plot the cube \nplot(lem_cube, palette = \"RdYlGn\")\n\n\n\n\n# Copy the region of interest to a local directory\nlem_cube = sits_cube_copy(\n    cube = bdc_cube,\n    roi = roi_lem,\n    output_dir = tempdir_py\n)\n# Plot the NDVI for the first date (2013-09-14)\nplot(lem_cube, palette = \"RdYlGn\")\n\n\n\n\n\n\nno bands provided - using a best guess color composite\n\n\nno date provided - using date with least cloud cover\n\n\n\n\n\n\n\n\n\nFigure 1.2: False color CBERS image for NDVI band in 2013-09-30.\n\n\n\nThe object returned by sits_cube() and by sits_cube_copy() contains the metadata describing the contents of the data cube. It includes the data source and collection, satellite, sensor, tile in the collection, bounding box, projection, and list of files. Each file refers to one band of an image at one of the temporal instances of the cube. Since data cubes obtained from the BDC are already regularized, there is no need to run sits_regularize(). Please refer to chapter “Building regular EO data cubes” for information on dealing with non-regular ARD collections.\n\n\nR\nPython\n\n\n\n\n# Show the description of the data cube\nlem_cube\n\n# A tibble: 1 × 12\n  source collection    satellite sensor tile     xmin   xmax   ymin   ymax crs  \n  &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n1 BDC    CBERS-WFI-16D CBERS-4   WFI    007004 5.79e6 5.95e6 9.88e6 9.96e6 \"PRO…\n# ℹ 2 more variables: labels &lt;list&gt;, file_info &lt;list&gt;\n\n\n\n\n\n# Show the description of the data cube\nlem_cube\n\n\n\n\nThe list of image files that make up the data cube is stored as a data frame in the column file_info. For each file, sits stores information about the spectral band, reference date, size, spatial resolution, coordinate reference system, bounding box, path to the file location, and cloud cover information (when available).\n\n\nR\nPython\n\n\n\n\n# Show information on the images files which are part of a data cube\nlem_cube$file_info[[1]]\n\n# A tibble: 48 × 13\n   fid      band  date       ncols nrows  xres  yres   xmin   xmax   ymin   ymax\n   &lt;chr&gt;    &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 CB4-16D… EVI   2019-09-30  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 2 CB4-16D… NDVI  2019-09-30  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 3 CB4-16D… EVI   2019-10-16  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 4 CB4-16D… NDVI  2019-10-16  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 5 CB4-16D… EVI   2019-11-01  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 6 CB4-16D… NDVI  2019-11-01  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 7 CB4-16D… EVI   2019-11-17  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 8 CB4-16D… NDVI  2019-11-17  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n 9 CB4-16D… EVI   2019-12-03  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n10 CB4-16D… NDVI  2019-12-03  2539  1228    64    64 5.79e6 5.95e6 9.88e6 9.96e6\n# ℹ 38 more rows\n# ℹ 2 more variables: crs &lt;chr&gt;, path &lt;chr&gt;\n\n\n\n\n\n# Show information on the images files which are part of a data cube\nlem_cube_cube[\"file_info\"][0]\n\n\n\n\nA key attribute of a data cube is its timeline, as shown below. The command sits_timeline() lists the temporal references associated to sits objects, including samples, data cubes and models.\n\n\nR\nPython\n\n\n\n\n# Show the R object that describes the data cube\nsits_timeline(lem_cube)\n\n [1] \"2019-09-30\" \"2019-10-16\" \"2019-11-01\" \"2019-11-17\" \"2019-12-03\"\n [6] \"2019-12-19\" \"2020-01-01\" \"2020-01-17\" \"2020-02-02\" \"2020-02-18\"\n[11] \"2020-03-05\" \"2020-03-21\" \"2020-04-06\" \"2020-04-22\" \"2020-05-08\"\n[16] \"2020-05-24\" \"2020-06-09\" \"2020-06-25\" \"2020-07-11\" \"2020-07-27\"\n[21] \"2020-08-12\" \"2020-08-28\" \"2020-09-13\" \"2020-09-29\"\n\n\n\n\n\n# Show the R object that describes the data cube\nsits_timeline(lem_cube)\n\n\n\n\nThe timeline of lem_cube has 24 intervals with a temporal difference of 16 days. The chosen dates capture the agricultural calendar in the west of Bahia in Brazil. The agricultural year starts in September-October with the sowing of the summer crop (usually soybeans), which is harvested in February-March. Then the winter crop (mostly Corn, Cotton, or Millet) is planted in March and harvested in June-July. For LULC classification, the training samples and the data cube should share a timeline with the same number of intervals and similar start and end dates."
  },
  {
    "objectID": "intro_quicktour.html#the-time-series-tibble",
    "href": "intro_quicktour.html#the-time-series-tibble",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.4 The time series tibble",
    "text": "1.4 The time series tibble\nTo handle time series information, sits uses a tibble. Tibbles are extensions of the data.frame tabular data structures provided by the tidyverse set of packages. In this chapter, we will use a training set with 1,101 time series obtained from MODIS MOD13Q1 images. Each series has two indexes (NDVI and EVI). To obtain the time series, we need two inputs:\n\nA CSV file, shapefile or a data.frame containing information on the location of the sample and date of validity. The following information is required: longitude, latitude, start_date, end_date, class. It can either be provided as columns of a data.frame or a CSV file, or as attributes of a shapefile. Please refer to Chapter “Working with time series” for more information.\nA regular data cube which covers the dates indicated in the sample file. Each sample will be located in the data cube based on its longitude and latitude, and the time series is extracted based on the start and end dates and on the available bands of the cube.\n\nIn what follows, we use the data frame with the LEM samples. The data.frame contains spatial and temporal information and the label assigned to the sample. Based on this information, we will retrieve the time series from the BDC cube using sits_get_data(). In general, it is necessary to regularize the data cube so that all time series have the same dates. In our case, we use a regular data cube provided by the BDC repository.\n\n\nR\nPython\n\n\n\n\n# Retrieve the time series for each samples based on a data.frame\nsamples_lem_time_series &lt;- sits_get_data(\n    cube = lem_cube,\n    samples = df_samples_cerrado_lem\n)\n\n\n\n\n# Retrieve the time series for each samples based on a data.frame\nsamples_lem_time_series = sits_get_data(\n    cube = lem_cube,\n    samples = df_samples_cerrado_lem\n)\n\n\n\n\nThe time series tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band. The time series can be displayed by showing the time_series nested column.\n\n\nR\nPython\n\n\n\n\n# Load the time series for the first MODIS sample for Mato Grosso\nsamples_lem_time_series[1,]$time_series[[1]]\n\n# A tibble: 24 × 3\n   Index        EVI  NDVI\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 2019-09-30 0.203 0.240\n 2 2019-10-16 0.254 0.352\n 3 2019-11-01 0.297 0.390\n 4 2019-11-17 0.344 0.542\n 5 2019-12-03 0.174 0.206\n 6 2019-12-19 0.353 0.498\n 7 2020-01-01 0.433 0.659\n 8 2020-01-17 0.441 0.595\n 9 2020-02-02 0.431 0.674\n10 2020-02-18 0.565 0.672\n# ℹ 14 more rows\n\n\n\n\n\n# Load the time series for the first MODIS sample for Mato Grosso\nsamples_lem_time_series[\"time_series\"][1]\n\n\n\n\nThe distribution of samples per class can be obtained using the summary() command. The classification schema uses four labels, two associated with crops (Cropland_2_cycles, Cropland_1_cycle), one with natural vegetation (Cerrado), and one with Pasture.\n\n\nR\nPython\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_lem_time_series)\n\n# A tibble: 4 × 3\n  label             count   prop\n  &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado             136 0.0592\n2 Cropland_1_cycle   1250 0.544 \n3 Cropland_2_cycles   790 0.344 \n4 Pasture             123 0.0535\n\n\n\n\n\n# Show the summary of the time series sample data\nsummary(samples_lem_cbers)\n\n\n\n\nThe sample dataset is highly imbalanced. To improve performance, we will produce a balanced set after obtaining the time series, as seen below. The sits_reduce_imbalance() function will reduce the maximum number of samples per class to 256 and increase the minimum number to 100. This procedure is recommended to improve the performance of the classification model. The parameter n_samples_over selects the minimum number of samples of the less frequent classes, and the parameter n_samples_under indicates the maximum number of samples for the more frequent ones.\n\n\nR\nPython\n\n\n\n\n# Reduce imbalance between the classes\nsamples_lem &lt;- sits_reduce_imbalance(\n    samples_lem_time_series,\n    n_samples_over = 150,\n    n_samples_under = 300\n)\n\n\n\n\n# Reduce imbalance between the classes\nsamples_lem = sits_reduce_imbalance(\n    samples_lem_cbers,\n    n_samples_over = 150,\n    n_samples_under = 300\n)\n\n\n\n\nThe new distribution of samples per class can be obtained using the summary() command. It should show a more balanced data set.\n\n\nR\nPython\n\n\n\n\n# Show the summary of the balanced time series sample data\nsummary(samples_lem)\n\n# A tibble: 4 × 3\n  label             count  prop\n  &lt;chr&gt;             &lt;int&gt; &lt;dbl&gt;\n1 Cerrado             150 0.159\n2 Cropland_1_cycle    320 0.339\n3 Cropland_2_cycles   324 0.343\n4 Pasture             150 0.159\n\n\n\n\n\n# Show the summary of the balanced time series sample data\nsummary(samples_lem)\n\n\n\n\nIt is helpful to plot the dispersion of the time series. In what follows, for brevity, we will filter only one label (Cropland_2_cycles) and select one index (NDVI).\n\n\nR\nPython\n\n\n\n\n# Select all samples with label \"Cropland_2_cycles\" using `dplyr::filter`\n# since the label attribute is a column of the samples data.frame\nsamples_cropland_2cycles &lt;- dplyr::filter(\n    samples_lem, \n    label == \"Cropland_2_cycles\"\n) \n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsamples_crops_2cycles_ndvi &lt;- sits_select(\n    samples_cropland_2cycles,\n    bands = \"NDVI\"\n) \nplot(samples_crops_2cycles_ndvi)\n\n\n\nFigure 1.3: Joint plot of all samples in band NDVI for label Cropland_2_cycles.\n\n\n\n\n\n\n# Select all samples with label \"Cropland_2_cycles\" using `query` \n# since the label attribute is a column of the samples data.frame\nsamples_crops_2cycles = samples_lem.query('label == \"Cropland_2_cycles\"')\n# Select the NDVI band values using sits_select\n# because band values are in a nested data.frame\nsamples_crops_2cycles_ndvi = sits_select(samples_crops_2cycles, bands = \"NDVI\")\n# plot the samples for label Cropland_2_cycles and band NDVI\nplot(samples_crops_2cycles_ndvi)\n\n\n\n\nThe figure above shows all the time series associated with the label Cropland_2_cycles and the band NDVI (in light blue), highlighting the median (shown in dark red) and the first and third quartiles (shown in brown). The two-crop-cycles pattern is clearly visible. The spikes at the end of the year are due to high cloud cover."
  },
  {
    "objectID": "intro_quicktour.html#training-a-machine-learning-model",
    "href": "intro_quicktour.html#training-a-machine-learning-model",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.5 Training a machine learning model",
    "text": "1.5 Training a machine learning model\nThe next step is to train a machine learning (ML) model using sits_train(). It takes two inputs, samples (a time series tibble) and ml_method (a function that implements a machine learning algorithm). The result is a model that is used for classification. Each ML algorithm requires specific parameters that are user-controllable. For novice users, sits provides default parameters that produce good results. Please see Chapter Machine learning for data cubes for more details.\nTo build the classification model, we use a random forest model called by sits_rfor(). Results from the random forest model can vary between different runs, due to the stochastic nature of the algorithm. In the code fragment below, we set the seed of R’s pseudo-random number generator explicitly to ensure results don’t change when running multiple times. This is done for documentation purposes.\n\n\nR\nPython\n\n\n\n\nset.seed(03022024)\n# Train a random forest model\nrf_model &lt;- sits_train(\n    samples = samples_lem, \n    ml_method = sits_rfor()\n)\n# Plot the most important variables of the model\nplot(rf_model)\n\n\n\nFigure 1.4: Most relevant variables of trained random forest model.\n\n\n\n\n\n\n# Train a random forest model\nrf_model = sits_train(\n    samples = samples_lem, \n    ml_method = sits_rfor()\n)\n# Plot the most important variables of the model\nplot(rf_model)"
  },
  {
    "objectID": "intro_quicktour.html#data-cube-classification",
    "href": "intro_quicktour.html#data-cube-classification",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.6 Data cube classification",
    "text": "1.6 Data cube classification\nAfter training the machine learning model, the next step is to classify the data cube using sits_classify(). This function produces a set of raster probability maps, one for each class. For each of these maps, the value of a pixel is proportional to the probability that it belongs to the class. This function has two mandatory parameters: data, the data cube or time series tibble to be classified; and ml_model, the trained ML model. Optional parameters include: (a) multicores, number of cores to be used; (b) memsize, RAM used in the classification; (c) output_dir, the directory where the classified raster files will be written. Details of the classification process are available in Chapter Image classification in data cubes.\n\n\nR\nPython\n\n\n\n\n# Classify the data cube to produce a map of probabilities\nlem_probs &lt;- sits_classify(\n    data = lem_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n# Plot the probability cube for class Forest\nplot(lem_probs, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n# Classify the data cube\nlem_probs = sits_classify(\n    data = lem_cube, \n    ml_model = rf_model,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\n# Plot the probability cube for class Forest\nplot(sinop_probs, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n\n\n\n\nFigure 1.5: Probability map for class Cropland_2_cycles.\n\n\n\nAfter completing the classification, we plot the probability maps for the class Cropland_2_cycles. Probability maps help visualize the degree of confidence the classifier assigns to the labels for each pixel. They can be used to produce uncertainty information and support active learning, as described in Chapter Uncertainty and active learning."
  },
  {
    "objectID": "intro_quicktour.html#spatial-smoothing",
    "href": "intro_quicktour.html#spatial-smoothing",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.7 Spatial smoothing",
    "text": "1.7 Spatial smoothing\nWhen working with big Earth observation data, there is much variability within each class. As a result, some pixels will be misclassified. These errors are more likely to occur in transition areas between classes. To address these problems, sits_smooth() takes a probability cube as input and uses the class probabilities of each pixel’s neighborhood to reduce labeling uncertainty. Plotting the smoothed probability map for the class Forest shows that most outliers have been removed.\n\n\nR\nPython\n\n\n\n\n# Perform spatial smoothing\nlem_smooth &lt;- sits_smooth(\n    cube = lem_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\nplot(lem_smooth, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n# Perform spatial smoothing\nlem_smooth = sits_smooth(\n    cube = lem_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_py\n)\nplot(lem_smooth, labels = \"Cropland_2_cycles\", palette = \"YlOrBr\")\n\n\n\n\n\n\n\n\nFigure 1.6: Probability map for class Cropland_2_cycles."
  },
  {
    "objectID": "intro_quicktour.html#labeling-a-probability-data-cube",
    "href": "intro_quicktour.html#labeling-a-probability-data-cube",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.8 Labeling a probability data cube",
    "text": "1.8 Labeling a probability data cube\nAfter removing outliers using local smoothing, the final classification map can be obtained using sits_label_classification(). This function assigns each pixel to the class with the highest probability.\n\n\nR\nPython\n\n\n\n\n# Label the probability file \nlem_map &lt;- sits_label_classification(\n    cube = lem_smooth, \n    output_dir = tempdir_r\n)\nplot(lem_map)\n\n\n\n\n# Label the probability file \nlem_map &lt;- sits_label_classification(\n    cube = lem_smooth, \n    output_dir = tempdir_py\n)\nplot(lem_map)\n\n\n\n\n\n\n\n\nFigure 1.7: Probability map for class Cropland_2_cycles."
  },
  {
    "objectID": "intro_quicktour.html#summary",
    "href": "intro_quicktour.html#summary",
    "title": "\n1  A quick tour of SITS\n",
    "section": "\n1.9 Summary",
    "text": "1.9 Summary\nThis chapter provides an introduction to the main workflow of the sits package, starting from the definition of a data cube from an ARD image collection. Then, we show how to create a regular data cube and how to extract time series using ground truth data. Next, we present a simple way to define machine learning models and how to use them for classifying data cubes. All functions are simple and direct. Once you learn the basic workflow, it becomes easier to understand more detailed examples of sits in the next chapter."
  },
  {
    "objectID": "intro_quicktour.html#references",
    "href": "intro_quicktour.html#references",
    "title": "\n1  A quick tour of SITS\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nL. V. Oldoni, I. D. Sanches, M. C. A. Picoli, R. M. Covre, and J. G. Fronza, “LEM+ dataset: For agricultural remote sensing applications,” Data in Brief, vol. 33, p. 106553, 2020, doi: 10.1016/j.dib.2020.106553."
  },
  {
    "objectID": "intro_examples.html#cerrado-classification",
    "href": "intro_examples.html#cerrado-classification",
    "title": "2  How to use SITS: example scripts",
    "section": "2.1 Cerrado classification",
    "text": "2.1 Cerrado classification"
  },
  {
    "objectID": "intro_visualisation.html#plotting",
    "href": "intro_visualisation.html#plotting",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.1 Plotting",
    "text": "3.1 Plotting\nThe plot() function produces a graphical display of data cubes, time series, models, and SOM maps. For each type of data, there is a dedicated version of the plot() function. See ?plot.sits for details. The plotting of time series, models, and SOM outputs uses the ggplot2 package; maps are plotted using the tmap package. When plotting images and classified maps, users can control the output, with appropriate parameters for each type of image. In this chapter, we provide examples of the options available for plotting different types of maps.\nPlotting and visualisation function in sits use COG overviews if available. COG overviews are reduced-resolution versions of the main image, stored within the same file. Overviews allow for quick rendering at lower zoom levels, improving performance when dealing with large images. Usually, a single GeoTIFF will have many overviews to match different zoom levels.\nIn the case of maps and images, the parameters discussed in the following sections are optional. Simply using plot() with a data cube or classified map as the first parameter works. The function will search for the date with the least cloud cover and will select an RGB product based on the available bands. By default, plot() will try to produce a standard color combination; otherwise, it produces a false-color plot.\n\n3.1.1 Plotting false color maps\nWe refer to false color maps as images that are plotted on a color scale. Usually, these are single bands, indexes such as NDVI or DEMs. For these datasets, the parameters for plot() are:\n\n\nx: data cube containing data to be visualised;\n\nband: band or index to be plotted;\n\npallete: color scheme to be used for false color maps, which should be one of the RColorBrewer palettes. These palletes were designed to be effective for map display by Prof. Cynthia Brewer as described at the Brewer website. By default, optical images use the RdYlGn scheme, SAR images use Greys, and DEM cubes use Spectral.\n\nrev: whether the color pallete should be reversed; TRUE for DEM cubes, and FALSE otherwise.\n\nscale: global scale parameter used by tmap. All font sizes, symbol sizes, border widths, and line widths are controlled by this value. Default is 0.75; users should vary this parameter and see the results.\n\nfirst_quantile: 1st quantile for stretching images (default = 0.05).\n\nlast_quantile: last quantile for stretching images (default = 0.95).\n\nmax_cog_size: for cloud-oriented GeoTIFF files (COG), sets the maximum number of lines or columns of the COG overview to be used for plotting.\n\nThe following optional parameters are available to allow for detailed control over the plot output:\n\n\ngraticules_labels_size: size of coordinate labels (default = 0.8).\n\nlegend_title_size: relative size of legend title (default = 1.0).\n\nlegend_text_size: relative size of legend text (default = 1.0).\n\nlegend_bg_color: color of the legend background (default = “white”).\n\nlegend_bg_alpha: legend opacity (default = 0.5).\n\nlegend_position: where to place the legend (options = “inside” or “outside”, with “inside” except for probability cubes).\n\nThe following example shows a plot of an NDVI index of a data cube. This data cube covers part of MGRS tile 20LMR and contains bands “B02”, “B03”, “B04”, “B05”, “B06”, “B07”, “B08”, “B11”, “B12”, “B8A”, “EVI”, “NBR”, and “NDVI” for the period 2022-01-05 to 2022-12-23. We will use parameters other than their defaults.\n\n\nR\nPython\n\n\n\n\n# set the directory where the data is \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR\", package = \"sitsdata\")\n# read the data cube\nro_20LMR &lt;- sits_cube(\n  source = \"MPC\", \n  collection = \"SENTINEL-2-L2A\",\n  data_dir = data_dir\n)\n# plot the NDVI for date 2022-08-01\nplot(ro_20LMR, \n     band = \"NDVI\", \n     date = \"2022-08-01\", \n     palette = \"Greens\",\n     legend_position = \"outside\",\n     scale = 1.0)\n\n\n\n\n# set the directory where the data is \ndata_dir = sitsdata_dir + \"extdata/Rondonia-20LMR\"\n# read the data cube\nro_20LMR = sits_cube(\n  source = \"MPC\", \n  collection = \"SENTINEL-2-L2A\",\n  data_dir = data_dir\n)\n# plot the NDVI for date 2022-08-01\nplot(ro_20LMR, \n     band = \"NDVI\", \n     date = \"2022-08-01\", \n     palette = \"Greens\",\n     legend_position = \"outside\",\n     scale = 1.0)\n\n\n\n\n\n\n\n\nFigure 3.1: Sentinel-2 NDVI index covering tile 20LMR.\n\n\n\n\n3.1.2 Plotting RGB color composite maps\nFor RGB color composite maps, the parameters for the plot function are:\n\n\nx: data cube containing data to be visualized;\n\nband: band or index to be plotted;\n\ndate: date to be plotted (must be part of the cube timeline);\n\nred: band or index associated with the red color;\n\ngreen: band or index associated to the green color;\n\nblue: band or index associated to the blue color;\n\nscale: global scale parameter used by tmap. All font sizes, symbol sizes, border widths, and line widths are controlled by this value. Default is 0.75; users should vary this parameter and see the results.\n\nfirst_quantile: 1st quantile for stretching images (default = 0.05).\n\nlast_quantile: last quantile for stretching images (default = 0.95).\n\nmax_cog_size: for cloud-optimized GeoTIFF files (COG), sets the maximum number of lines or columns of the COG overview to be used for plotting.\n\nThe optional parameters listed in the previous section are also available. An example follows:\n\n\nR\nPython\n\n\n\n\n# plot a color composite for date 2022-08-01\nplot(ro_20LMR, \n     red = \"B11\", \n     green = \"B8A\",\n     blue = \"B02\",\n     date = \"2022-08-01\", \n     scale = 1.0)\n\n\n\n\n# plot a color composite for date 2022-08-01\nplot(ro_20LMR, \n     red = \"B11\", \n     green = \"B8A\",\n     blue = \"B02\",\n     date = \"2022-08-01\", \n     scale = 1.0)\n\n\n\n\n\n\n\n\nFigure 3.2: Sentinel-2 color composite covering tile 20LMR\n\n\n\nPlotting classified maps\nClassified maps pose an additional challenge for plotting because of the association between labels and colors. In this case, sits allows three alternatives:\n\nPredefined color scheme: sits includes some well-established color schemes such as IBGP, UMD, ESA_CCI_LC, and WORLDCOVER. There is a predefined color table that associates labels commonly used in LUCC classification to colors. Users can also create their color schemes. Please see section “How Colors Work on sits” in this chapter.\nLegend: in this case, users provide a named vector with labels and colors, as shown in the example below.\nPalette: an RColorBrewer categorical palette, which is assigned to labels that are not in the color table.\n\nThe parameters for plot() applied to a classified data cube are:\n\n\nx: data cube containing a classified map;\n\nlegend: legend that associates colors to the classes; NULL by default.\n\npalette: color palette used for undefined colors; Spectral by default.\n\nscale: global scale parameter used by tmap.\n\nThe optional parameters listed in the previous section are also available. For an example of plotting a classified data cube with default color scheme, please see the section “Reading classified images as local data cube” in the “Earth observation data cubes” chapter. In what follows, we show a similar case using a legend.\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LLP\", \n                        package = \"sitsdata\")\n# Read the classified cube\nrondonia_class_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    bands = \"class\",\n    labels = c(\"1\" = \"Burned\", \"2\" = \"Cleared\", \n               \"3\" = \"Degraded\", \"4\" =  \"Natural_Forest\"),\n    data_dir = data_dir\n)\n# Plot the classified cube\nplot(rondonia_class_cube,\n  legend = c(\"Burned\" = \"#a93226\",\n             \"Cleared\" = \"#f9e79f\",\n             \"Degraded\" = \"#d4efdf\",\n             \"Natural_Forest\" = \"#1e8449\"\n             ),\n  scale = 1.0,\n  legend_position = \"outside\"\n)\n\n\n\nFigure 3.3: Sentinel-2 color composite covering tile 20LMR"
  },
  {
    "objectID": "intro_visualisation.html#visualization-of-data-cubes-in-interactive-maps",
    "href": "intro_visualisation.html#visualization-of-data-cubes-in-interactive-maps",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.2 Visualization of data cubes in interactive maps",
    "text": "3.2 Visualization of data cubes in interactive maps\nData cubes and samples can also be shown as interactive maps using sits_view(). This function creates tiled overlays of different kinds of data cubes, allowing comparison between the original, intermediate, and final results. It also includes background maps. The following example creates an interactive map combining the original data cube with the classified map.\n\nsits_view(rondonia_class_cube,\n            legend = c(\"Burned\" = \"#a93226\",\n             \"Cleared\" = \"#f9e79f\",\n             \"Degraded\" = \"#d4efdf\",\n             \"Natural_Forest\" = \"#1e8449\"\n             )\n)\n\n\n\n\n\nFigure 3.4: Leaflet visualization of classification of an area in Rondonia, Brasil"
  },
  {
    "objectID": "intro_visualisation.html#how-colors-work-in-sits",
    "href": "intro_visualisation.html#how-colors-work-in-sits",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.3 How colors work in sits",
    "text": "3.3 How colors work in sits\nIn the examples provided in the book, the color legend is taken from a predefined color palette provided by sits. The default color definition file used by sits includes 220 class names, which can be shown using sits_colors().\n\n# Point default `sits` colors\nsits_colors()\n\n# A tibble: 239 × 2\n   name                             color  \n   &lt;chr&gt;                            &lt;chr&gt;  \n 1 Evergreen_Broadleaf_Forest       #1E8449\n 2 Evergreen_Broadleaf_Forests      #1E8449\n 3 Tree_Cover_Broadleaved_Evergreen #1E8449\n 4 Forest                           #1E8449\n 5 Forests                          #1E8449\n 6 Closed_Forest                    #1E8449\n 7 Closed_Forests                   #1E8449\n 8 Mountainside_Forest              #229C59\n 9 Mountainside_Forests             #229C59\n10 Open_Forest                      #53A145\n# ℹ 229 more rows\n\n\nThese colors are grouped by typical legends used by the Earth observation community, which include “IGBP”, “UMD”, “ESA_CCI_LC”, “WORLDCOVER”, “PRODES”, “PRODES_VISUAL”, “TERRA_CLASS”, and “TERRA_CLASS_PT”. The following commands show the colors associated with the IGBP legend [1].\n\n\n\n\nFigure 3.5: Colors used in the sits package to represeny IGBP legend.\n\n\n\nThe default color table can be extended using sits_colors_set(). As an example of a user-defined color table, consider a definition that covers level 1 of the Anderson Classification System used in the U.S. National Land Cover Data, obtained by defining a set of colors associated with a new legend. The colors should be defined by HEX values, and the color names should consist of a single string; multiple names need to be connected with an underscore(“_“).\n\n# Define a color table based on the Anderson Land Classification System\nus_nlcd &lt;- tibble::tibble(name = character(), color = character())\nus_nlcd &lt;- us_nlcd |&gt;  \n  tibble::add_row(name = \"Urban_Built_Up\", color =  \"#85929E\") |&gt; \n  tibble::add_row(name = \"Agricultural_Land\", color = \"#F0B27A\") |&gt;  \n  tibble::add_row(name = \"Rangeland\", color = \"#F1C40F\") |&gt; \n  tibble::add_row(name = \"Forest_Land\", color = \"#27AE60\") |&gt;  \n  tibble::add_row(name = \"Water\", color = \"#2980B9\") |&gt;  \n  tibble::add_row(name = \"Wetland\", color = \"#D4E6F1\") |&gt; \n  tibble::add_row(name = \"Barren_Land\", color = \"#FDEBD0\") |&gt; \n  tibble::add_row(name = \"Tundra\", color = \"#EBDEF0\") |&gt; \n  tibble::add_row(name = \"Snow_and_Ice\", color = \"#F7F9F9\")\n# Load the color table into `sits`\nsits_colors_set(colors = us_nlcd, legend = \"US_NLCD\")\n# Show the new legend\nsits_colors_show(legend = \"US_NLCD\")\n\n\n\nFigure 3.6: Example of defining colors for the Anderson Land Classification Scheme.\n\n\n\nThe original default sits color table can be restored using sits_colors_reset().\n\n# Reset the color table\nsits_colors_reset()"
  },
  {
    "objectID": "intro_visualisation.html#exporting-colors-to-qgis",
    "href": "intro_visualisation.html#exporting-colors-to-qgis",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "\n3.4 Exporting colors to QGIS",
    "text": "3.4 Exporting colors to QGIS\nTo simplify the process of importing your data into QGIS, the color palette used to display classified maps in sits can be exported as a QGIS style using sits_colors_qgis. The function takes two parameters: (a) cube, a classified data cube; and (b) file, the file where the QGIS style in XML will be written. In this case study, we first retrieve and plot a classified data cube, and then export the colors to a QGIS XML style.\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-Class-2022-Mosaic\", \n                        package = \"sitsdata\")\n\n# labels of the classified image\nlabels &lt;- c(\"1\" = \"Clear_Cut_Bare_Soil\",\n            \"2\" =  \"Clear_Cut_Burned_Area\",\n            \"3\" =   \"Clear_Cut_Vegetation\",\n            \"4\" = \"Forest\",\n            \"5\" =  \"Mountainside_Forest\",\n            \"6\" = \"Riparian_Forest\",\n            \"7\" = \"Seasonally_Flooded\",\n            \"8\" = \"Water\",\n            \"9\" = \"Wetland\" \n)\n# read classified data cube\nro_class &lt;- sits_cube(\n    source = \"MPC\", \n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir, \n    bands = \"class\",\n    labels = labels,\n    version = \"mosaic\"\n)\n# Plot the classified cube\nplot(ro_class, scale = 1.0)\n\n\n\n\n\nFigure 3.7: Classified mosaic for land cover in Rondonia, Brazil for 2022.\n\n\n\nThe file to be read by QGIS is a TIFF file whose location is specified by the data cube, as follows.\n\n# Show the location of the classified map\nro_class[[\"file_info\"]][[1]]$path\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/sitsdata/extdata/Rondonia-Class-2022-Mosaic/SENTINEL-2_MSI_MOSAIC_2022-01-05_2022-12-23_class_mosaic.tif\"\n\n\nThe color schema can be exported to QGIS as follows.\n\n# Export the color schema to QGIS\nsits_colors_qgis(ro_class, file = file.path(tempdir_r,\"qgis_style.xml\"))"
  },
  {
    "objectID": "intro_visualisation.html#references",
    "href": "intro_visualisation.html#references",
    "title": "\n3  Data visualisation in SITS\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Herold, R. Hubald, and A. Di Gregorio, “Translating and evaluating land cover legends using the UN Land Cover Classification System (LCCS),” GOFC-GOLD Florence, Italy, 2009."
  },
  {
    "objectID": "datacubes.html#analysis-ready-data-collections",
    "href": "datacubes.html#analysis-ready-data-collections",
    "title": "Earth observation data cubes",
    "section": "Analysis-ready data collections",
    "text": "Analysis-ready data collections\nAnalysis Ready Data (CEOS-ARD) are satellite data that have been processed to meet the ARD standards defined by the Committee on Earth Observation Satellites (CEOS). ARD data simplify and accelerate the analysis of Earth observation data by providing consistent and high-quality data that are standardized across different sensors and platforms. ARD image processing includes geometric corrections, radiometric corrections, and sometimes atmospheric corrections. Images are georeferenced, meaning they are accurately aligned with a coordinate system. Optical ARD images include cloud and shadow masking information. These masks indicate which pixels are affected by clouds or cloud shadows. For optical sensors, CEOS-ARD images have to be converted to surface reflectance values, which represent the fraction of light that is reflected by the surface. This makes the data more comparable across different times and locations. For SAR images, CEOS-ARD specification require images to undergo Radiometric Terrain Correction (RTC) and be provided in the GammaNought (\\(\\gamma_0\\)) backscatter values. This value which mitigates the variations from diverse observation geometries and is recommended for most land applications.\nARD images are available from various satellite platforms, including Landsat, Sentinel, and commercial satellites. This provides a wide range of spatial, spectral, and temporal resolutions to suit different applications. They are organized as a collection of files, where each pixel contains a single value for each spectral band for a given date. These collections are available in cloud services such as Brazil Data Cube, Digital Earth Africa, and Microsoft’s Planetary Computer. In general, the timelines of the images in an ARD collection are different. Images may still contain cloudy or missing pixels, and bands for the images in the collection may have different resolutions. Figure 1 shows an example of the Landsat ARD image collection.\n\n\n\n\nFigure 1: ARD image collection (source: USGS)."
  },
  {
    "objectID": "datacubes.html#regular-earth-observation-data-cubes",
    "href": "datacubes.html#regular-earth-observation-data-cubes",
    "title": "Earth observation data cubes",
    "section": "Regular Earth observation data cubes",
    "text": "Regular Earth observation data cubes\nA regular EO data cube is a multidimensional array—typically x (longitude), y (latitude), time, and spectral band—containing “analysis-ready” satellite observations that have already been geometrically aligned, radiometrically calibrated, and stored on a common grid. In other words, every pixel in the cube represents the same ground location over a sequence of dates, so the user can retrieve a complete, consistent time series with a single query.\nMachine learning and deep learning (ML/DL) classification algorithms require the input data to be consistent. The dimensionality of the data used for training the model has to be the same as that of the data to be classified. There should be no gaps and no missing values. Thus, to use ML/DL algorithms for remote sensing data, ARD image collections should be converted to regular data cubes. Adapting a previous definition by Appel and Pebesma [1], we consider a regular data cube to have the following definition and properties:\n\nA regular data cube is a four-dimensional data structure with explicit dimensions x (longitude or easting), y (latitude or northing), time, and bands. The spatial, temporal, and attribute dimensions are independent and not interchangeable.\nThe spatial dimensions refer to a coordinate system, such as the grids defined by UTM (Universal Transverse Mercator) or MGRS (Military Grid Reference System). A tile of the grid corresponds to a unique zone of the coordinate system. A data cube may span various tiles and UTM zones.\nThe temporal dimension is a set of continuous and equally spaced intervals.\nFor every combination of dimensions, a cell has a single value.\n\nAll cells of a data cube have the same spatiotemporal extent. The spatial resolution of each cell is the same in the X and Y dimensions. All temporal intervals are the same. Each cell contains a valid set of measures. Each pixel is associated with a unique coordinate in a zone of the coordinate system. For each position in space, the data cube should provide a set of valid time series. For each time interval, the regular data cube should provide a valid 2D image (see Figure Figure 2).\n\n\n\n\nFigure 2: Conceptual view of a data cube.\n\n\n\nCurrently, the only cloud service that provides regular data cubes by default is the Brazil Data Cube (BDC). ARD collections available in other cloud services are not regular in space and time. Bands may have different resolutions, images may not cover the entire timeline, and time intervals may be irregular. For this reason, subsets of these collections need to be converted into regular data cubes before further processing. To produce data cubes for machine learning data analysis, this part of the book describes the steps involved in producing and using regular data cubes:\n\nObtaining data from ARD image collections\nProducing regular data cubes from single- and multi-source data\nRecovering data cubes from local files\nPerforming operations on data cubes"
  },
  {
    "objectID": "datacubes.html#references",
    "href": "datacubes.html#references",
    "title": "Earth observation data cubes",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Appel and E. Pebesma, “On-Demand Processing of Data Cubes from Satellite Image Collections with the gdalcubes Library,” Data, vol. 4, no. 3, 2019, doi: 10.3390/data4030092."
  },
  {
    "objectID": "dc_ardcollections.html#introduction",
    "href": "dc_ardcollections.html#introduction",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nARD (analysis-ready) image collections are organized into spatial partitions. Sentinel-2/2A images follow the Military Grid Reference System (MGRS) tiling system, which divides the world into 60 UTM zones of 8 degrees of longitude. Each zone contains blocks of 6 degrees of latitude. Blocks are split into tiles of 110 \\(\\times\\) 110 km\\(^2\\) with a 10 km overlap. Figure 4.1 shows the MGRS tiling system for a part of the northeastern coast of Brazil, contained in UTM zone 24, block M.\n\n\n\n\nFigure 4.1: MGRS tiling system used by Sentinel-2 images (source: US Army).\n\n\n\nThe Landsat-4/5/7/8/9 satellites use the Worldwide Reference System (WRS-2), which divides the coverage of Landsat satellites into images identified by path and row (Figure 4.2). The path is the descending orbit of the satellite; the WRS-2 system has 233 paths per orbit, and each path has 119 rows, where each row refers to a latitudinal center line of a frame of imagery. Images in WRS-2 are geometrically corrected to the UTM projection.\n\n\n\n\nFigure 4.2: MGRS tiling system used by Sentinel-2 images (source: US Army)."
  },
  {
    "objectID": "dc_ardcollections.html#image-collections-handled-by-sits",
    "href": "dc_ardcollections.html#image-collections-handled-by-sits",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.2 Image collections handled by sits\n",
    "text": "4.2 Image collections handled by sits\n\nIn version 1.5.3,sits supports access to the following ARD image cloud providers:\n\nAmazon Web Services (AWS): Open data Sentinel-2/2A Level-2A collections for the Earth’s land surface.\nBrazil Data Cube (BDC): Open data collections of Sentinel-2/2A, Landsat-8, CBERS-4/4A, and MOD13Q1 products for Brazil. These collections are organized as regular data cubes.\nCopernicus Data Space Ecosystem (CDSE): Open data collections of Sentinel-1 RTC and Sentinel-2/2A images.\nDigital Earth Africa (DEAFRICA): Open data collections of Sentinel-1 RTC, Sentinel-2/2A, Landsat-5/7/8/9 for Africa. Additional products include ALOS_PALSAR mosaics, DEM_COP_30, NDVI_ANOMALY based on Landsat data, and monthly and daily rainfall data from CHIRPS.\nDigital Earth Australia (DEAUSTRALIA): Open data ARD collections of Sentinel-2A/2B and Landsat-5/7/8/9 images, yearly geomedians of Landsat 5/7/8 images; yearly fractional land cover from 1986 to 2024.\nHarmonized Landsat-Sentinel (HLS): HLS, provided by NASA, is an open data collection that processes Landsat 8 and Sentinel-2 imagery to a common standard.\nMicrosoft Planetary Computer (MPC): Open data collections of Sentinel-1 GRD, Sentinel-1 RTC, Sentinel-2/2A, Landsat-4/5/7/8/9 images for the Earth’s land areas. Also supported are the Copernicus DEM-30 and MOD13Q1, MOD10A1, MOD09A1 products, and the Harmonized Landsat-Sentinel collections (HLSL30 and HLSS30).\nSwiss Data Cube (SDC): Collection of Sentinel-2/2A and Landsat-8 images for Switzerland.\nTerrascope: Cloud service with EO products, which includes the ESA World Cover map.\nUSGS: Landsat-4/5/7/8/9 collections available in AWS, which require access payment.\n\nIn addition, sits supports the use of Planet monthly mosaics stored as local files. For a detailed description of the providers and collections supported by sits, please run sits_list_collections()."
  },
  {
    "objectID": "dc_ardcollections.html#accessing-ard-image-collections-in-cloud-providers",
    "href": "dc_ardcollections.html#accessing-ard-image-collections-in-cloud-providers",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.3 Accessing ARD image collections in cloud providers",
    "text": "4.3 Accessing ARD image collections in cloud providers\n\nTo obtain information on ARD image collections from cloud providers, sits uses the SpatioTemporal Asset Catalogue (STAC) protocol, a specification of geospatial information that many large image collection providers have adopted. A ‘spatiotemporal asset’ is any file that represents information about the Earth captured at a specific space and time. To access STAC endpoints, sits uses the rstac R package.\nThe function sits_cube() supports access to image collections from cloud services; it has the following parameters:\n\n\nsource: Name of the provider.\n\ncollection: A collection available in the provider and supported by sits. To find out which collections are supported by sits, see sits_list_collections().\n\nplatform: Optional parameter specifying the platform in collections with multiple satellites.\n\ntiles: Set of tiles of image collection reference system. Either tiles or roi should be specified.\n\nroi: A region of interest. Either: (a) a named vector (lon_min, lon_max, lat_min, lat_max) in WGS 84 coordinates; (b) an sf object; (c) a path to a shapefile polygon; or (d) A named vector (xmin, xmax, ymin, ymax) with XY coordinates. All images intersecting the convex hull of the roi are selected.\n\nbands: Optional parameter with the bands to be used. If missing, all bands from the collection are used.\n\norbit: Optional parameter required only for Sentinel-1 images (default = “descending”).\n\nstart_date: The initial date for the temporal interval containing the time series of images.\n\nend_date: The final date for the temporal interval containing the time series of images.\n\nThe result of sits_cube() is a tibble with a description of the selected images required for further processing. It does not contain the actual data, but only pointers to the images. The attributes of individual image files can be accessed by listing the file_info column of the tibble."
  },
  {
    "objectID": "dc_ardcollections.html#amazon-web-services",
    "href": "dc_ardcollections.html#amazon-web-services",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.4 Amazon Web Services",
    "text": "4.4 Amazon Web Services\nAmazon Web Services (AWS) holds two kinds of collections: open-data and requester-pays. Open-data collections can be accessed without cost. Requester-pays collections require payment from an AWS account. Currently, sits supports the SENTINEL-2-L2A collection, which is open data. The bands at 10 m resolution are B02, B03, B04, and B08. The 20 m bands are B05, B06, B07, B8A, B11, and B12. Bands B01 and B09 are available at 60 m resolution. A CLOUD band is also available. The example below shows how to access one tile of the open-data SENTINEL-2-L2A collection. The tiles parameter allows selecting the desired area according to the MGRS reference system.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n# Create a data cube covering an area in Brazil\ns2_23MMU_cube = sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = \"23MMU\",\n    bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n    start_date = \"2018-07-12\",\n    end_date = \"2019-07-28\"\n)\nplot(s2_23MMU_cube, \n     red = \"B11\", \n     blue = \"B02\", \n     green = \"B8A\", \n     date = \"2018-10-05\"\n)\n\n\n\n\n\n\n\n\nFigure 4.3: Sentinel-2 image in an area of the Northeastern coast of Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#microsoft-planetary-computer",
    "href": "dc_ardcollections.html#microsoft-planetary-computer",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.5 Microsoft Planetary Computer",
    "text": "4.5 Microsoft Planetary Computer\nThe sits package supports access to open-data collection from Microsoft’s Planetary Computer (MPC), including SENTINEL-1-GRD, SENTINEL-1-RTC, SENTINEL-2-L2A, LANDSAT-C2-L2, COP-DEM-GLO-30 (Copernicus Global DEM at 30-meter resolution) and MOD13Q1-6.1 (version 6.1 of the MODIS MOD13Q1 product). Access to the non-open data collections is available for users that have registered with MPC.\n\n4.5.1 SENTINEL-2/2A images in MPC\nThe SENTINEL-2/2A ARD images available in MPC have the same bands and resolutions as those available in AWS (see above). The example below shows how to access the SENTINEL-2-L2A collection.\n\n\nR\nPython\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC &lt;- sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n# Create a data cube covering an area in the Brazilian Amazon\ns2_20LKP_cube_MPC = sits_cube(\n      source = \"MPC\",\n      collection = \"SENTINEL-2-L2A\",\n      tiles = \"20LKP\",\n      bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n      start_date = \"2019-07-01\",\n      end_date = \"2019-07-28\"\n)\n# Plot a color composite of one date of the cube\nplot(s2_20LKP_cube_MPC, red = \"B11\", blue = \"B02\", green = \"B8A\", \n     date = \"2019-07-18\"\n)\n\n\n\n\n\n\n\n\nFigure 4.4: Sentinel-2 image in an area of the state of Rondonia, Brazil.\n\n\n\n\n4.5.2 LANDSAT-C2-L2 images in MPC\nThe LANDSAT-C2-L2 collection provides access to data from the Landsat-4/5/7/8/9 satellites. Images from these satellites have been intercalibrated to ensure data consistency. For compatibility between the different Landsat sensors, the band names are BLUE, GREEN, RED, NIR08, SWIR16, and SWIR22. All images have 30 m resolution. For this collection, tile search is not supported; the roi parameter should be used. The example below shows how to retrieve data from a region of interest covering the city of Brasília in Brazil.\n\n\nR\nPython\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi &lt;- c(lon_min = -43.5526, lat_min = -2.9644, \n         lon_max = -42.5124, lat_max = -2.1671)\n# Select the cube\ns2_L8_cube_MPC &lt;- sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = c(\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"),\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n# Read a ROI that covers part of the Northeastern coast of Brazil\nroi = {\"lon_min\" : -43.5526, \"lat_min\" : -2.9644, \n         \"lon_max\" : -42.5124, \"lat_max\" : -2.1671}\n# Select the cube\ns2_L8_cube_MPC = sits_cube(\n        source = \"MPC\",\n        collection = \"LANDSAT-C2-L2\",\n        bands = [\"BLUE\", \"RED\", \"GREEN\", \"NIR08\", \"SWIR16\", \"CLOUD\"],\n        roi = roi,\n        start_date = \"2019-06-01\",\n        end_date = \"2019-09-01\"\n)\n# Plot the tile that covers the Lencois Maranhenses\nplot(s2_L8_cube_MPC)\n\n\n\n\n\n\n\n\nFigure 4.5: Landsat-8 image in an area in Northeast Brazil.\n\n\n\n\n4.5.3 SENTINEL-1-GRD images in MPC\nSentinel-1 GRD products consist of focused SAR data that has been detected, multi-looked, and projected to ground range using the WGS84 Earth ellipsoid model. GRD images are subject to variations in the radar signal’s intensity due to topographic effects, antenna pattern, range spreading loss, and other radiometric distortions. The most common types of distortions include foreshortening, layover, and shadowing.\nForeshortening occurs when the radar signal strikes a steep terrain slope facing the radar, causing the slope to appear compressed in the image. Features like mountains can appear much steeper than they are, and their true heights can be difficult to interpret. Layover happens when the radar signal reaches the top of a tall feature (like a mountain or building) before it reaches the base. As a result, the top of the feature is displaced towards the radar and appears in front of its base. This results in a reversal of the order of features along the radar line of sight, making the image interpretation challenging. Shadowing occurs when a radar signal is obstructed by a tall object, casting a shadow on the area behind it that the radar cannot illuminate. The shadowed areas appear dark in SAR images, and no information is available from these regions, similar to optical shadows.\nAccess to Sentinel-1 GRD images can be done either by MGRS tiles (tiles) or by region of interest (roi). We recommend using the MGRS tiling system for specifying the area of interest, since when these images are regularized, they will be reprojected into MGRS tiles. By default, only images in descending orbit are selected.\nThe following example shows how to create a data cube of S1 GRD images over a region in Mato Grosso, Brazil, which is a deforested area of the Amazon forest. The resulting cube will not follow any specific projection and its coordinates will be stated as EPSG 4326 (latitude/longitude). Its geometry is derived from the SAR slant-range perspective; thus, it will appear skewed in relation to the Earth’s longitude.\n\n\nR\nPython\n\n\n\n\ncube_s1_grd &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = c(\"VV\"),\n  orbit = \"descending\",\n  tiles = c(\"21LUJ\",\"21LVJ\"),\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_grd =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-GRD\",\n  bands = [\"VV\"],\n  orbit = \"descending\",\n  tiles = [\"21LUJ\",\"21LVJ\"],\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_grd, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.6: Sentinel-1 image in an area in Mato Grosso, Brazil.\n\n\n\nAs explained earlier in this chapter, in areas with large elevation differences, Sentinel-1 GRD images will have geometric distortions. For this reason, whenever possible, we recommend the use of RTC (radiometrically terrain-corrected) images as described in the next section.\n\n4.5.4 SENTINEL-1-RTC images in MPC\nAn RTC SAR image has undergone corrections for both geometric and radiometric distortions caused by the terrain. The purpose of RTC processing is to enhance the interpretability and usability of SAR images for various applications by providing a more accurate representation of the Earth’s surface. The radar backscatter values are normalized to account for these variations, ensuring that the image accurately represents the reflectivity of surface features.\nThe terrain correction addresses geometric distortions caused by the side-looking geometry of SAR imaging, such as foreshortening, layover, and shadowing. It uses a Digital Elevation Model (DEM) to model the terrain and reproject the SAR image from the slant range (radar line of sight) to the ground range (true geographic coordinates). This process aligns the SAR image with the actual topography, providing a more accurate spatial representation.\n\n\nR\nPython\n\n\n\n\ncube_s1_rtc &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = c(\"VV\", \"VH\"),\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\ncube_s1_rtc =  sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-1-RTC\",\n  bands = [\"VV\", \"VH\"],\n  orbit = \"descending\",\n  tiles = \"18NZM\",\n  start_date = \"2021-08-01\",\n  end_date = \"2021-09-30\"\n)\nplot(cube_s1_rtc, band = \"VV\", palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.7: Sentinel-1-RTC image of an area in Colombia.\n\n\n\nThe above image is from the central region of Colombia, a country with large variations in altitude due to the Andes Mountains. Users are invited to compare this image with the one from the SENTINEL-1-GRD collection and observe the significant geometrical distortions of the GRD image compared with the RTC one.\n\n4.5.5 Copernicus DEM 30 meter images in MPC\nThe Copernicus Digital Elevation Model 30-meter global dataset (COP-DEM-GLO-30) is a high-resolution topographic data product provided by the European Space Agency (ESA) under the Copernicus Program. The vertical accuracy of the Copernicus DEM 30-meter dataset is typically within a few meters, but this can vary depending on the region and the original data sources. The primary data source for the Copernicus DEM is data from the TanDEM-X mission, developed by the German Aerospace Center (DLR). TanDEM-X provides high-resolution radar data through interferometric synthetic aperture radar (InSAR) techniques.\nThe Copernicus DEM 30-meter is organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. In sits, access to COP-DEM-GLO-30 images can be done either by MGRS tiles (tiles) or by region of interest (roi). In both cases, the cube is retrieved based on the parts of the grid that intersect the region of interest or the chosen tiles.\n\n\nR\nPython\n\n\n\n\ncube_dem_30 &lt;-  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\ncube_dem_30 =  sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  tiles = \"20LMR\",\n  band = \"ELEVATION\"\n)\nplot(cube_dem_30, band = \"ELEVATION\", palette = \"RdYlGn\", rev = TRUE)\n\n\n\n\n\n\n\n\nFigure 4.8: Copernicus 30-meter DEM of an area in Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#brazil-data-cube",
    "href": "dc_ardcollections.html#brazil-data-cube",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.6 Brazil Data Cube",
    "text": "4.6 Brazil Data Cube\nThe Brazil Data Cube (BDC) is built by Brazil’s National Institute for Space Research (INPE) to provide regular EO data cubes from CBERS, LANDSAT, SENTINEL-2, and TERRA/MODIS satellites for environmental applications. The collections available in the BDC are: LANDSAT-OLI-16D (Landsat-8 OLI, 30 m resolution, 16-day intervals), SENTINEL-2-16D (Sentinel-2A and 2B MSI images at 10 m resolution, 16-day intervals), CBERS-WFI-16D (CBERS-4 WFI, 64 m resolution, 16-day intervals), CBERS-WFI-8D (CBERS-4 and 4A WFI images, 64 m resolution, 8-day intervals), and MOD13Q1-6.1 (MODIS MOD13SQ1 product, collection 6, 250 m resolution, 16-day intervals). For more details, use sits_list_collections(source = \"BDC\").\nThe BDC uses three hierarchical grids based on the Albers Equal Area projection and SIRGAS 2000 datum. The large grid has tiles of 4224.4 \\(\\times4\\) 224.4 km2 and is used for CBERS-4 AWFI collections at 64 m resolution; each CBERS-4 AWFI tile contains images of 6600 \\(\\times\\) 6600 pixels. The medium grid is used for Landsat-8 OLI collections at 30 m resolution; tiles have an extent of 211.2 \\(\\times\\) 211.2 km2, and each image has 7040 \\(\\times\\) 7040 pixels. The small grid covers 105.6 \\(\\times\\) 105.6 km2 and is used for Sentinel-2 MSI collections at 10 m resolutions; each image has 10560 \\(\\times\\) 10560 pixels. The data cubes in the BDC are regularly spaced in time and cloud-corrected [1].\n\n\n\n\nFigure 4.9: Hierarchical BDC tiling system showing (a) large BDC grid overlayed on Brazilian biomes, (b) one learge tile from the grid used for CBERS-4 AWFI data, (c) four medium tiles from the grid used for LANDSAT data; and (d) sixteen small tiles from the grid used for SENTINEL-2 data. Tiles in (b), (c), and (d) are nested.\n\n\n\nTo access the BDC, users must provide their credentials using environment variables, as shown below. Obtaining a BDC access key is free. Users must register at the BDC site to obtain a key. Please include your BDC access key in your .Rprofile.\n\nSys.setenv(BDC_ACCESS_KEY = \"&lt;your_bdc_access_key&gt;\")\n\nIn the example below, the data cube is defined as one tile (“005004”) of CBERS-WFI-16D collection, which contains CBERS AWFI images at 16-day resolution.\n\n\nR\nPython\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = c(\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"),\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n# Define a tile from the CBERS-4/4A AWFI collection\ncbers_tile = sits_cube(\n    source = \"BDC\",\n    collection = \"CBERS-WFI-16D\",\n    tiles = \"005004\",\n    bands = [\"B13\", \"B14\", \"B15\", \"B16\", \"CLOUD\"],\n    start_date = \"2021-05-01\",\n    end_date = \"2021-09-01\")\n# Plot one time instance\nplot(cbers_tile, \n     red = \"B15\", \n     green = \"B16\", \n     blue = \"B13\", \n     date = \"2021-05-09\")\n\n\n\n\n\n\n\n\nFigure 4.10: CBERS-4 WFI image in a Cerrado area in Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#copernicus-data-space-ecosystem-cdse",
    "href": "dc_ardcollections.html#copernicus-data-space-ecosystem-cdse",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.7 Copernicus Data Space Ecosystem (CDSE)",
    "text": "4.7 Copernicus Data Space Ecosystem (CDSE)\nThe Copernicus Data Space Ecosystem (CDSE) is a cloud service designed to support access to Earth observation data from the Copernicus Sentinel missions and other sources. It is designed and maintained by the European Space Agency (ESA) with support from the European Commission.\nConfiguring user access to CDSE involves several steps to ensure proper registration, access to data, and utilization of the platform’s tools and services. Visit the Copernicus Data Space Ecosystem registration page. Complete the registration form with your details, including name, email address, organization, and sector. Confirm your email address through the verification link sent to your inbox.\nAfter registration, you will need to obtain access credentials to the S3 service implemented by CDSE, which can be obtained using the CDSE S3 credentials site. The site will request that you add a new credential. You will receive two keys: an S3 access key and a secret access key. Take note of both and include the following lines in your .Rprofile.\n\nSys.setenv(\n    AWS_ACCESS_KEY_ID = \"your access key\",\n    AWS_SECRET_ACCESS_KEY = \"your secret access key\",\n      AWS_S3_ENDPOINT = \"eodata.dataspace.copernicus.eu\",\n      AWS_VIRTUAL_HOSTING = \"FALSE\"\n)\n\nAfter including these lines in your .Rprofile, restart R for the changes to take effect. By following these steps, users will gain access to the Copernicus Data Space Ecosystem.\n\n4.7.1 SENTINEL-2/2A images in CDSE\nCDSE hosts a global collection of Sentinel-2 Level-2A images, which are processed according to the CEOS Analysis-Ready Data specifications. One example is provided below, where we present a Sentinel-2 image of the Lena River Delta in Siberia during summertime.\n\n\nR\nPython\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = c(\"52XDF\")\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n# obtain a collection of images of a tile covering part of Lena delta\nlena_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = [\"B02\", \"B04\", \"B8A\", \"B11\", \"B12\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-09-01\",\n    tiles = \"52XDF\"\n)\n# plot an image from summertime\nplot(lena_cube, date = \"2023-07-06\", red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 4.11: Sentinel-2 image of the Lena river delta in summertime.\n\n\n\n\n4.7.2 SENTINEL-1-RTC images in CDSE\nAn important product under development at CDSE is the radiometric terrain corrected (RTC) Sentinel-1 images. In CDSE, this product is referred to as normalized terrain backscatter (NRB). The S1-NRB product contains radiometrically terrain corrected (RTC) gamma nought backscatter (γ⁰) processed from Single Look Complex (SLC) Level-1A data. Each acquired polarization is stored in an individual binary image file.\nAll images are projected and gridded into the United States Military Grid Reference System (US-MGRS). The use of the US-MGRS tile grid ensures a very high level of interoperability with Sentinel-2 Level-2A ARD products making it easy to also set up complex analysis systems that exploit both SAR and optical data. While speckle is inherent in SAR acquisitions, speckle filtering is not applied to the S1-NRB product in order to preserve spatial resolution. Some applications (or processing methods) may require spatial or temporal filtering for stationary backscatter estimates.\nFor more details, please refer to the S1-NRB product website. Global coverage is expected to grow as ESA expands the S1-RTC archive. The following example shows an S1-RTC image for the Rift Valley in Ethiopia.\n\n\nR\nPython\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube &lt;- sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = c(\"37NCH\")\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n# retrieve a S1-RTC cube and plot\ns1_cube = sits_cube(\n    source = \"CDSE\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = [\"VV\", \"VH\"],\n    orbit = \"descending\",\n    start_date = \"2023-01-01\",\n    end_date = \"2023-12-31\",\n    tiles = \"37NCH\"\n)\nplot(s1_cube, band = \"VV\", date = c(\"2023-03-03\"), palette = \"Greys\")\n\n\n\n\n\n\n\n\nFigure 4.12: Sentinel-1-RTC image of the Rift Valley in Ethiopia."
  },
  {
    "objectID": "dc_ardcollections.html#digital-earth-africa",
    "href": "dc_ardcollections.html#digital-earth-africa",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.8 Digital Earth Africa",
    "text": "4.8 Digital Earth Africa\nDigital Earth Africa (DEAFRICA) is a cloud service that provides open-access Earth observation data for the African continent. The ARD image collections in sits are:\n\nSentinel-2 level-2A (SENTINEL-2-L2A), organized as MGRS tiles.\nSentinel-1 radiometrically terrain corrected (SENTINEL-1-RTC)\nLandsat-5 (LS5-SR), Landsat-7 (LS7-SR), Landsat-8 (LS8-SR) and Landsat-9 (LS9-SR). All Landsat collections are ARD data and are organized as WRS-2 tiles.\nSAR L-band images produced by PALSAR sensor onboard the Japanese ALOS satellite(ALOS-PALSAR-MOSAIC). Data is organized in a 5\\(^\\circ\\) by 5\\(^\\circ\\) grid with a spatial resolution of 25 meters. Images are available annually from 2007 to 2010 (ALOS/PALSAR) and from 2015 to 2022 (ALOS-2/PALSAR-2).\nEstimates of vegetation condition using NDVI anomalies (NDVI-ANOMALY) compared with the long-term baseline condition. The available measurements are “NDVI_MEAN” (mean NDVI for a month) and “NDVI-STD-ANOMALY” (standardized NDVI anomaly for a month).\nRainfall information provided by Climate Hazards Group InfraRed Precipitation with Station data (CHIRPS) from University of California, Santa Barbara. There are monthly (RAINFALL-CHIRPS-MONTHLY) and daily (RAINFALL-CHIRPS-DAILY) products over Africa.\nDigital elevation model provided by the EC Copernicus program (COP-DEM-30) in 30-meter resolution organized in a 1\\(^\\circ\\) by 1\\(^\\circ\\) grid.\nAnnual geomedian images for Landsat 8 and Landsat 9 (GM-LS8-LS9-ANNUAL (LANDSAT/OLI)`) in the WRS-2 grid system.\nAnnual geomedian images for Sentinel-2 (GM-S2-ANNUAL) in MGRS grid.\nRolling three-month geomedian images for Sentinel-2 (GM-S2-ROLLING) in MGRS grid.\nSemestral geomedian images for Sentinel-2 (GM-S2-SEMIANNUAL) in MGRS grid.\n\nAccess to DEAFRICA Sentinel-2 images can be done using the tiles or roi parameter. In this example, the requested roi produces a cube that contains one MGRS tile (“35LPH”) covering an area of Madagascar that includes the Betsiboka Estuary.\n\n\nR\nPython\n\n\n\n\ndea_s2_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = c(\n    lon_min = 46.1, lat_min = -15.6,\n    lon_max = 46.6, lat_max = -16.1\n  ),\n    bands = c(\"B02\", \"B04\", \"B08\"),\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\ndea_s2_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"SENTINEL-2-L2A\",\n    roi = {\n    \"lon_min\" : 46.1, \"lat_min\" : -15.6,\n    \"lon_max\" : 46.6, \"lat_max\" : -16.1\n  },\n    bands = [\"B02\", \"B04\", \"B08\"],\n    start_date = \"2019-04-01\",\n    end_date = \"2019-05-30\"\n)\nplot(dea_s2_cube, red = \"B04\", blue = \"B02\", green = \"B08\")\n\n\n\n\n\n\n\n\nFigure 4.13: Sentinel-2 image in an area over Madagascar.\n\n\n\nThe next example retrieves a set of ARD Landsat-9 data covering the Serengeti Plain in Tanzania.\n\n\nR\nPython\n\n\n\n\ndea_l9_cube &lt;- sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = c(\n        lon_min = 33.0, lat_min = -3.60, \n        lon_max = 33.6, lat_max = -3.00\n    ),\n    bands = c(\"B04\", \"B05\", \"B06\"),\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\ndea_l9_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"LS9-SR\",\n    roi = {\n        \"lon_min\" : 33.0, \"lat_min\" : -3.60, \n        \"lon_max\" : 33.6, \"lat_max\" : -3.00\n    },\n    bands = [\"B04\", \"B05\", \"B06\"],\n    start_date = \"2023-05-01\",\n    end_date = \"2023-08-30\"\n)\nplot(dea_l9_cube, date = \"2023-06-26\", \n     red = \"B06\", green = \"B05\", blue = \"B04\")\n\n\n\n\n\n\n\n\nFigure 4.14: Landsat-9 image in an area over the Serengeti in Tanzania.\n\n\n\nThe following example shows how to retrieve a subset of the ALOS-PALSAR mosaic for the year 2020. The area is near the Congo-Rwanda border.\n\n\nR\nPython\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = c(\n        lon_min = 28.69, lat_min = -2.35, \n        lon_max = 29.35, lat_max = -1.56\n    ),\n    bands = c(\"HH\", \"HV\"),\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\ndea_alos_cube = sits_cube(\n    source = \"DEAFRICA\",\n    collection = \"ALOS-PALSAR-MOSAIC\",\n    roi = {\n       \"lon_min\" : 28.69, \"lat_min\" : -2.35, \n       \"lon_max\" : 29.35, \"lat_max\" : -1.56\n    },\n    bands = [\"HH\", \"HV\"],\n    start_date = \"2020-01-01\",\n    end_date = \"2020-12-31\"\n)\nplot(dea_alos_cube, band = \"HH\")\n\n\n\n\n\n\n\n\nFigure 4.15: ALOS-PALSAR mosaic in the Congo forest area."
  },
  {
    "objectID": "dc_ardcollections.html#digital-earth-australia",
    "href": "dc_ardcollections.html#digital-earth-australia",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.9 Digital Earth Australia",
    "text": "4.9 Digital Earth Australia\nDigital Earth Australia (DEAUSTRALIA) is an initiative by Geoscience Australia that uses satellite data to monitor and analyze environmental changes and resources across the Australian continent. It provides many datasets offering detailed information on droughts, agriculture, water availability, floods, coastal erosion, and urban development. The DEAUSTRALIA image collections in sits are:\n\nGA_LS5T_ARD_3: ARD images from the Landsat-5 satellite, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, and “CLOUD”.\nGA_LS7E_ARD_3: ARD images from the Landsat-7 satellite, with the same bands as Landsat-5.\nGA_LS8C_ARD_3: ARD images from the Landsat-8 satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR-1”, “SWIR-2”, “PANCHROMATIC”, and “CLOUD”.\nGA_LS9C_ARD_3: ARD images from the Landsat-9 satellite, with the same bands as Landsat-8.\nGA_S2AM_ARD_3: ARD images from the Sentinel-2A satellite, with bands “COASTAL-AEROSOL”, “BLUE”, “GREEN”, “RED”, “RED-EDGE-1”, “RED-EDGE-2”, “RED-EDGE-3”, “NIR-1”, “NIR-2”, “SWIR-2”, “SWIR-3”, and “CLOUD”.\nGA_S2BM_ARD_3: ARD images from the Sentinel-2B satellite, with the same bands as Sentinel-2A.\nGA_LS5T_GM_CYEAR_3: Landsat-5 geomedian images, with bands “BLUE”, “GREEN”, “RED”, “NIR”, “SWIR1”, “SWIR2”, “EDEV”, “SDEV”, “BCDEV”.\nGA_LS7E_GM_CYEAR_3: Landsat-7 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS8CLS9C_GM_CYEAR_3: Landsat-8/9 geomedian images, with the same bands as Landsat-5 geomedian.\nGA_LS_FC_3: Landsat fractional land cover, with bands “BS”, “PV”, “NPV”.\nGA_S2LS_INTERTIDAL_CYEAR_3: Landsat/Sentinel intertidal data, with bands “ELEVATION”, “ELEVATION-UNCERTAINTY”, “EXPOSURE”, “TA-HAT”, “TA-HOT”, “TA-LOT”, “TA-LAT”, “TA-OFFSET-HIGH”, “TA-OFFSET-LOW”, “TA-SPREAD”, “QA-NDWI-CORR”, and “QA-NDWI-FREQ”.\n\nThe following code retrieves an image from Sentinel-2A.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_55KGR &lt;- sits_mgrs_to_roi(\"55KGR\")\n# retrieve the world cover map for the chosen roi\ns2_56KKV &lt;- sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"),\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n# retrieve the world cover map for the chosen tile\ns2_56KKV = sits_cube(\n  source = \"DEAUSTRALIA\",\n  collection = \"GA_S2AM_ARD_3\",\n  tiles = \"56KKV\",\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"NIR-2\", \"SWIR-2\", \"CLOUD\"],\n  start_date = \"2023-09-01\",\n  end_date = \"2023-11-30\"\n)\n# plot the resulting map\nplot(s2_56KKV, green = \"NIR-2\", blue = \"BLUE\", red = \"SWIR-2\", date = \"2023-10-14\")\n\n\n\n\n\n\n\n\nFigure 4.16: Sentinel-2A image from the DEAUSTRALIA collection showing MGRS tile 56KKV."
  },
  {
    "objectID": "dc_ardcollections.html#harmonized-landsat-sentinel",
    "href": "dc_ardcollections.html#harmonized-landsat-sentinel",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.10 Harmonized Landsat-Sentinel",
    "text": "4.10 Harmonized Landsat-Sentinel\nHarmonized Landsat Sentinel (HLS) is a NASA initiative that processes and harmonizes Landsat 8 and Sentinel-2 imagery to a common standard, including atmospheric correction, alignment, resampling, and corrections for BRDF (bidirectional reflectance distribution function). The purpose of the HLS project is to create a unified and consistent dataset that integrates the advantages of both systems, making it easier to work with the data.\nThe NASA Harmonized Landsat and Sentinel (HLS) service provides two image collections:\n\nLandsat 8 OLI Surface Reflectance HLS (HLSL30) – The HLSL30 product includes atmospherically corrected surface reflectance from the Landsat 8 OLI sensors at 30 m resolution. The dataset includes 11 spectral bands.\nSentinel-2 MultiSpectral Instrument Surface Reflectance HLS (HLSS30) – The HLSS30 product includes atmospherically corrected surface reflectance from the Sentinel-2 MSI sensors at 30 m resolution. The dataset includes 12 spectral bands.\n\nThe HLS and Sentinel-2 tiling systems are identical (MGRS). Each tile is 109.8 km wide, with 4,900 m of overlap on each side.\nTo access NASA HLS, users need to register at NASA EarthData, and save their login and password in a ~/.netrc plain text file on Unix (or %HOME%_netrc in Windows). The file must contain the following fields:\n\nmachine urs.earthdata.nasa.gov\nlogin &lt;username&gt;\npassword &lt;password&gt;\n\nWe recommend using the earthdatalogin package to create a .netrc file with earthdatalogin::edl_netrc(). This function creates a properly configured .netrc file in the user’s home directory and sets the environment variable GDAL_HTTP_NETRC_FILE, as shown in the example. As an alternative, we recommend using the HLS collections available in Microsoft Planetary Computer. They are copies of the NASA collections and are faster to access.\n\nlibrary(earthdatalogin)\n\nearthdatalogin::edl_netrc( \nusername = \"&lt;your user name&gt;\", \npassword = \"&lt;your password&gt;\" \n) \n\nImages in NASA HLS can be accessed by region of interest or by tiles. The following example shows an HLS Sentinel-2 image over the Brazilian coast.\n\n\nR\nPython\n\n\n\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n# define a region of interest\nroi = { \"lon_min\" : -45.6422, \"lat_min\" : -24.0335,\n        \"lon_max\" : -45.0840, \"lat_max\" : -23.6178 }\n\n# create a cube from the HLSS30 collection\nhls_cube_s2 = sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = [\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"],\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# plot the cube\nplot(hls_cube_s2, red = \"RED\", green = \"GREEN\", blue = \"BLUE\", date = \"2020-06-20\")\n\n\n\n\n\n\n\n\nFigure 4.17: Sentinel-2 image from NASA HLSS30 collection showing the island of Ilhabela in the coast of Brazil."
  },
  {
    "objectID": "dc_ardcollections.html#eo-products-from-terrascope",
    "href": "dc_ardcollections.html#eo-products-from-terrascope",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "\n4.11 EO products from TERRASCOPE",
    "text": "4.11 EO products from TERRASCOPE\nTerrascope is an online platform for accessing open-source satellite images. This service, operated by VITO, offers a range of free Earth observation data and processing services. Currently, sits supports the World Cover 2021 maps, produced by VITO with support from the European Commission and ESA. The following code shows how to access the World Cover 2021 covering tile “22LBL”. First, we use sits_mgrs_to_roi() to get the region of interest expressed as a bounding box; this box is then entered as the roi parameter in the sits_cube() function. Since the World Cover data is available as a 3\\(^\\circ\\) by 3\\(^\\circ\\) grid, it is necessary to use sits_cube_copy() to extract the exact MGRS tile.\n\n\nR\nPython\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL &lt;- sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 &lt;- sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL &lt;- sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_r\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n# get roi for an MGRS tile\nbbox_22LBL = sits_mgrs_to_roi(\"22LBL\")\n# retrieve the world cover map for the chosen roi\nworld_cover_2021 = sits_cube(\n  source = \"TERRASCOPE\",\n  collection = \"WORLD-COVER-2021\",\n  roi = bbox_22LBL\n)\n# cut the 3 x 3 degree grid to match the MGRS tile 22LBL\nworld_cover_2021_20LBL = sits_cube_copy(\n  cube = world_cover_2021,\n  roi = bbox_22LBL,\n  multicores = 6,\n  output_dir = tempdir_py\n)\n# plot the resulting map\nplot(world_cover_2021_20LBL)\n\n\n\n\n\n\n\n\nFigure 4.18: World Cover 2021 map covering MGRS tile 22LBL."
  },
  {
    "objectID": "dc_ardcollections.html#references",
    "href": "dc_ardcollections.html#references",
    "title": "\n4  Analysis-ready image collections\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nK. R. Ferreira et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products,” Remote Sensing, vol. 12, no. 24, p. 4033, 2020, doi: 10.3390/rs12244033."
  },
  {
    "objectID": "dc_regularize.html#the-need-for-regular-eo-data-cubes",
    "href": "dc_regularize.html#the-need-for-regular-eo-data-cubes",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.1 The need for regular EO data cubes",
    "text": "5.1 The need for regular EO data cubes\nAnalysis Ready Data (ARD) collections are often irregular in space and time. Bands may have different resolutions, images may not cover entire tiles, and time intervals are inconsistent. Clouds and sensor artifacts introduce “holes” in the data, corrupting the time series. If time steps differ or values are missing, batch training breaks and the model learns spurious correlations. Additionally, most machine learning and deep learning libraries expect tensors of identical shape (e.g., n_samples × n_features × time). Regular data cubes guarantee fixed-length feature vectors and GPU-friendly batches. Regularization turns heterogeneous image archives into clean, structured data ready for machine learning models.\nData from ARD collections can be converted into regular data cubes with sits_regularize(), which uses the gdalcubes package [1]. This function has two components:\n\nSpatial harmonization: reproject and resample everything onto the same tiling system and spatial resolution. For example, when Sentinel-1 and Sentinel-2 images are merged in sits, they are projected onto MGRS grid tiles.\nTemporal harmonization: creates equispaced intervals (e.g., 16-day, monthly, or seasonal composites), filling gaps introduced by cloud cover and sensor errors. sits stacks every image within a chosen interval to combine them. It sorts images in increasing order of cloud cover percentage. The least cloud-filled image is taken as a reference, and the others are used to try to fill its gaps. Pixels with persistent cloud cover are marked as NA and are temporally interpolated during computation."
  },
  {
    "objectID": "dc_regularize.html#regularizing-sentinel-2-images",
    "href": "dc_regularize.html#regularizing-sentinel-2-images",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.2 Regularizing Sentinel-2 images",
    "text": "5.2 Regularizing Sentinel-2 images\nIn the following example, we create a non-regular data cube from the Sentinel-2 collection available in Microsoft’s Planetary Computer (MPC). The area lies within the state of Rondônia, Brazil, and is defined by the MGRS tiles 20LKP and 20LLP. We use sits_cube() to retrieve the collection.\n\n\nR\nPython\n\n\n\n\n# Retrieving a non-regular ARD collection from AWS\ns2_cube_rondonia &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = c(\"20LLP\", \"20LKP\"),\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    start_date = as.Date(\"2018-06-30\"),\n    end_date = as.Date(\"2018-08-31\")\n)\n# Show the different timelines of the cube tiles\nsits_timeline(s2_cube_rondonia)\n\n\n\n\n# Retrieving a non-regular ARD collection from AWS\ns2_cube_rondonia =  sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    tiles = [\"20LLP\", \"20LKP\"],\n    bands = [\"B02\", \"B8A\", \"B11\", \"CLOUD\"],\n    start_date = \"2018-06-30\",\n    end_date = \"2018-08-31\"\n)\n# Show the different timelines of the cube tiles\nsits_timeline(s2_cube_rondonia)\n\n\n\n\n\n\n$`20LKP`\n [1] \"2018-07-03\" \"2018-07-08\" \"2018-07-13\" \"2018-07-18\" \"2018-07-23\"\n [6] \"2018-07-28\" \"2018-08-02\" \"2018-08-07\" \"2018-08-12\" \"2018-08-17\"\n[11] \"2018-08-22\" \"2018-08-27\"\n\n$`20LLP`\n [1] \"2018-06-30\" \"2018-07-03\" \"2018-07-05\" \"2018-07-08\" \"2018-07-10\"\n [6] \"2018-07-13\" \"2018-07-15\" \"2018-07-18\" \"2018-07-20\" \"2018-07-23\"\n[11] \"2018-07-25\" \"2018-07-28\" \"2018-07-30\" \"2018-08-02\" \"2018-08-04\"\n[16] \"2018-08-07\" \"2018-08-09\" \"2018-08-12\" \"2018-08-14\" \"2018-08-17\"\n[21] \"2018-08-19\" \"2018-08-22\" \"2018-08-24\" \"2018-08-27\" \"2018-08-29\"\n\n\n\n\nR\nPython\n\n\n\n\n# plot the least cloudy image of the cube\ns2_cube_rondonia |&gt;  \n    dplyr::filter(tile == \"20LLP\") |&gt;  \n    plot()\n\n\n\n\nplot(s2_cube_rondonia)\n\n\n\n\n\n\n\n\nFigure 5.1: Instance of non-regularized Sentinel-2 image covering only part of tile tile 20LLP.\n\n\n\nDifferent satellites—even those within the same mission, such as Sentinel-2A and Sentinel-2B—follow slightly different orbits and acquire data at different times. These are referred to as different acquisition orbits. Due to factors such as the Earth’s rotation and the lack of perfect alignment between Earth’s orbit and the satellites’ paths, some regions are not observed by both satellites during each orbital cycle. As a result, image acquisition timelines can differ between tiles.\nIn our example, tile 20LKP has twelve images within the selected time period, while tile 20LLP has twenty-four. To harmonize these differences, we use the sits_regularize() function, which builds a data cube with a regular timeline and estimates the best available pixel value for each time interval.\nLet’s now examine some technical aspects of sits_regularize(). The period parameter defines the time interval between observations, using the ISO 8601 time period format. This format specifies intervals as P[n]Y[n]M[n]D, where “Y” stands for years, “M” for months, and “D” for days. For example, P1M denotes a one-month interval, and P15D denotes a fifteen-day interval. For each time step, sits_regularize() identifies all available images within the defined window. Then, for each pixel, it sorts these candidate values by increasing cloud cover and selects the first cloud-free value. In this way, the function builds a regular time series for each pixel, even when observations come from different dates or satellites.\nIn this example, we set the regular cube’s spatial resolution to forty meters to speed up processing. For real-world applications, however, we recommend using a resolution of ten meters. We also recommend copying the ARD data to a local directory using sits_cube_copy() before applying regularization. This separates the process of creating a regular data cube into two distinct steps: (a) downloading data from ARD collections and (b) building the data cube from local files. This approach can significantly speed up processing. After sits builds the regular cube, the ARD images can be deleted to save space. Finally, keep in mind that depending on the speed of your Internet connection, sits_cube_copy() may take some time to complete.\n\n\nR\nPython\n\n\n\n\n# set output dir for ARD data if it does not exist \ntempdir_r_s2 &lt;- \"~/sitsbook/tempdir/R/dc_regularize/s2\"\ndir.create(tempdir_r_s2, showWarnings = FALSE)\n\ns2_cube_local &lt;- sits_cube_copy(\n    cube = s2_cube_rondonia,\n    output_dir = tempdir_r_s2\n)\n# set output dir fir regular cube if it does not exist \ntempdir_r_s2_reg &lt;- \"~/sitsbook/tempdir/R/dc_regularize/s2_reg\"\ndir.create(tempdir_r_s2, showWarnings = FALSE)\n\n# Regularize the cube to 16-day intervals\nreg_cube_rondonia &lt;- sits_regularize(\n          cube       = s2_cube_rondonia,\n          output_dir = tempdir_r_s2_reg,\n          res        = 40,\n          period     = \"P16D\",\n          multicores = 6)\n\n# Plot tile 20LLP of the regularized cube with the least cloud cover\n# The pixels of the regular data cube cover the full MGRS tile\nplot(reg_cube_rondonia, tile = \"20LLP\")\n\n\n\n\n# To be completed\n\n\n\n\n\n\n\n\nFigure 5.2: Regularized image for tile Sentinel-2 tile 20LLP."
  },
  {
    "objectID": "dc_regularize.html#regularizing-sentinel-1-images",
    "href": "dc_regularize.html#regularizing-sentinel-1-images",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.3 Regularizing Sentinel-1 images",
    "text": "5.3 Regularizing Sentinel-1 images\nWe have already discussed how different acquisition orbits can result in mismatched timelines. But that is not the only irregularity we need to address. Different satellites may also have different acquisition modes—that is, the way their sensors capture data, including direction, resolution, swath width, and polarization.\nIn the case of SAR (Synthetic Aperture Radar) satellites like Sentinel-1, the acquisition mode determines:\n\nViewing geometry (how the radar observes the ground),\nIncidence angle (the angle between the radar beam and the vertical to the Earth),\nSpatial resolution and coverage area,\nWhether it collects single or dual polarization (e.g., VV, VH).\n\nSAR images are usually captured at an oblique angle (not straight down), resulting in a slanted geometry known as slant range, rather than a map-like view called ground range. As a result, raw SAR images do not align well with optical imagery like Sentinel-2, which uses a nadir (straight-down) viewing geometry.\nTo facilitate the integration of Sentinel-1 and Sentinel-2 data, sits_regularize() reprojects SAR images to the MGRS grid. Internally, it uses the gdalwarp() function (via the gdalcubes backend or an equivalent tool), which supports several interpolation methods:\n\nNearest: Assigns the value of the nearest input pixel (fastest, preserves discrete classes).\nBilinear: Performs linear interpolation from 4 nearest input pixels. Smooths intensity values.\nCubic: Uses 16 surrounding pixels. Smoother, more complex but can introduce artifacts.\n\nBy default, sits applies nearest-neighbor interpolation for categorical or discrete bands (e.g., land cover or polarization labels), and bilinear interpolation for continuous-valued bands (e.g., backscatter intensity). However, the exact interpolation method may vary depending on how the raster cube is created and the backend in use. Advanced users can customize reprojection parameters through sits_config() or environment variables that modify GDAL behavior.\nWe illustrate the spatial harmonization feature of sits_regularize() in the following example, which uses the \"SENTINEL-1-RTC\" collection from the Microsoft Planetary Computer (MPC).\n\n# create an RTC cube from MPC collection for a region in Mato Grosso, Brazil.\ncube_s1_rtc &lt;-  sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    tiles = c(\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-10-01\"\n)\nplot(cube_s1_rtc, band = \"VH\", palette = \"Greys\", scale = 0.7)\n\n\n\n\n\nFigure 5.3: “Original Sentinel-1 image covering tile 22LBL.\n\n\n\nAfter retrieving a non-regular ARD collection from the Microsoft Planetary Computer (MPC), we use sits_regularize() to produce a SAR data cube aligned with MGRS tile “22LBL”. To visualize the SAR data, we generate a multi-date plot of the “VH” polarization band. In this plot, the first date is displayed in red, the second in green, and the third in blue—producing an RGB composite that visually highlights changes over time.\n\n# define the output directory\n# set outout dir if it does not exist \ntempdir_r_sar &lt;- \"~/sitsbook/tempdir/R/dc_regularize/sar\"\ndir.create(tempdir_r_sar, showWarnings = FALSE)\n# create a regular RTC cube from MPC collection for a tile 22LBL.\ncube_s1_reg &lt;- sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P16D\",\n    res = 40,\n    tiles = c(\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_r_sar\n)\nplot(cube_s1_reg, band = \"VH\", palette = \"Greys\", scale = 0.7, \n     dates = c(\"2021-06-06\", \"2021-07-24\", \"2021-09-26\"))\n\n\n\n\n\nFigure 5.4: Regularized Sentinel-1 image covering tile 22LBL."
  },
  {
    "objectID": "dc_regularize.html#summary",
    "href": "dc_regularize.html#summary",
    "title": "\n5  Building regular data cubes\n",
    "section": "\n5.4 Summary",
    "text": "5.4 Summary\nIn this chapter, we learned how to produce regular Earth observation (EO) data cubes from non-regular subsets of ARD collections. Regularization is a key operation when working with time series, as it enables the use of machine learning models on temporally aligned data. In the next chapter, we will discuss how to merge sensors from different data sources and, when necessary, how to combine these datasets with regularization operations."
  },
  {
    "objectID": "dc_regularize.html#references",
    "href": "dc_regularize.html#references",
    "title": "\n5  Building regular data cubes\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Appel and E. Pebesma, “On-Demand Processing of Data Cubes from Satellite Image Collections with the gdalcubes Library,” Data, vol. 4, no. 3, 2019, doi: 10.3390/data4030092."
  },
  {
    "objectID": "dc_merge.html#introduction",
    "href": "dc_merge.html#introduction",
    "title": "\n6  Multi-source EO data cubes\n",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nThis section describes the process of merging collections from different sources. There are many instances when users want to combine different data sources, including:\n\nCombine Sentinel-2A and Sentinel-2B when their collections are stored separately.\nMerge collections of different Landsat satellites.\nJoin Sentinel-1 and Sentinel-2 to increase the number of attributes of time series.\nCombine Landsat and Sentinel HLS (Harmonized Landsat-Sentinel) collections.\nCombine Sentinel-2 cubes with digital elevation models.\n\nExcept in the cases of combining DEMs with Sentinel-2 cubes, and joining Sentinel-1 and Sentinel-2, the other cases require a combination of two operations: sits_merge() and sits_regularize(). The first function combines the timelines of the data cubes, in most cases producing to a non-regular data cube. For this reason, users have to use sits_regularize() to produce a multi-source regular cube."
  },
  {
    "objectID": "dc_merge.html#merging-hls-landsat-and-sentinel-2-collections",
    "href": "dc_merge.html#merging-hls-landsat-and-sentinel-2-collections",
    "title": "\n6  Multi-source EO data cubes\n",
    "section": "\n6.2 Merging HLS Landsat and Sentinel-2 collections",
    "text": "6.2 Merging HLS Landsat and Sentinel-2 collections\nImages from the HLS Landsat and Sentinel-2 collections are accessed separately and can be combined with sits_merge(). We first create two ARD collections, one for HLS Sentinel-2 and one for HLS Landsat over the same area. The two cubes are then merged.\n\n# define a region of interest\nroi &lt;- c(lon_min = -45.6422, lat_min = -24.0335,\n         lon_max = -45.0840, lat_max = -23.6178)\n# Retrieve an HLS Sentinel-2 collection\nhls_cube_s2 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSS30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# create a cube from the HLS Landsat collection\nhls_cube_l8 &lt;- sits_cube(\n  source = \"HLS\",\n  collection = \"HLSL30\",\n  roi = roi,\n  bands = c(\"BLUE\", \"GREEN\", \"RED\", \"CLOUD\"),\n  start_date = as.Date(\"2020-06-01\"),\n  end_date = as.Date(\"2020-09-01\"),\n  progress = FALSE\n)\n# merge the Sentinel-2 and Landsat-8 cubes\nhls_cube_merged &lt;- sits_merge(hls_cube_s2, hls_cube_l8)\n\nComparing the timelines of the original cubes and the merged one, one can see the benefits of the merged collection for time series data analysis.\n\n# Timeline of the Sentinel-2 cube\nsits_timeline(hls_cube_s2)\n\n [1] \"2020-06-15\" \"2020-06-20\" \"2020-06-25\" \"2020-06-30\" \"2020-07-05\"\n [6] \"2020-07-10\" \"2020-07-20\" \"2020-07-25\" \"2020-08-04\" \"2020-08-09\"\n[11] \"2020-08-14\" \"2020-08-19\" \"2020-08-24\" \"2020-08-29\"\n\n\n\n# Timeline of the Landsat-8 cube\nsits_timeline(hls_cube_l8)\n\n[1] \"2020-06-09\" \"2020-06-25\" \"2020-07-11\" \"2020-07-27\" \"2020-08-12\"\n[6] \"2020-08-28\"\n\n\n\n# Timeline of the merged cube\nsits_timeline(hls_cube_merged)\n\n [1] \"2020-06-09\" \"2020-06-15\" \"2020-06-20\" \"2020-06-25\" \"2020-06-30\"\n [6] \"2020-07-05\" \"2020-07-10\" \"2020-07-11\" \"2020-07-20\" \"2020-07-25\"\n[11] \"2020-07-27\" \"2020-08-04\" \"2020-08-09\" \"2020-08-12\" \"2020-08-14\"\n[16] \"2020-08-19\" \"2020-08-24\" \"2020-08-28\" \"2020-08-29\"\n\n\nThe merged cube contains a denser timeline. However, for further use in sits, we need to create a regularized cube to obtain a composite with less clouds. To make processing easier, we copy the original data to a local directory.\n\n# define the output directory\n# set output dir if it does not exist \ntempdir_r_hls &lt;- \"~/sitsbook/tempdir/R/dc_merge/hls\"\ndir.create(tempdir_r_hls, showWarnings = FALSE)\n# copy the hls cube for a local directory for faster regularization\ncube_hls_local &lt;- sits_cube_copy(\n    cube = hls_cube_merged,\n    output_dir = tempdir_r_hls\n)\n\nCopying the merged HLS data to a local directory is an optional procedure, which is recommended in the case of optical data because it speeds the regularization procedure. After the regular data cube is completed, these files can be removed. The next step is to proceed with the regularization. In this example, we select a period of 16-days and keep the original 30-meter resolution of the HLS data.\n\n# define the output directory for the regularized images\n# set output dir if it does not exist \ntempdir_r_hls_reg &lt;- \"~/sitsbook/tempdir/R/dc_merge/hls_reg\"\ndir.create(tempdir_r_hls_reg, showWarnings = FALSE)\n# regularizing a harmonized Landsat-Sentinel data cube \n# plot the cube\ncube_hls_reg &lt;- sits_regularize(\n    cube = cube_hls_local,\n    period = \"P16D\", \n    res = 30,\n    output_dir = tempdir_r_hls_reg\n)\nplot(cube_hls_local, date = \"2020-07-11\")\n\n\n\n\n\nFigure 6.1: Sentinel-2 image obtained from merging NASA HLS Landsat-8 and Sentinel-2 collection for date 2020-06-15 showing the island of Ilhabela in Brazil."
  },
  {
    "objectID": "dc_merge.html#merging-sentinel-1-and-sentinel-2-images",
    "href": "dc_merge.html#merging-sentinel-1-and-sentinel-2-images",
    "title": "\n6  Multi-source EO data cubes\n",
    "section": "\n6.3 Merging Sentinel-1 and Sentinel-2 images",
    "text": "6.3 Merging Sentinel-1 and Sentinel-2 images\nTo combine Sentinel-1 and Sentinel-2 data, the first step is to produce regular data cubes for the same MGRS tiles with compatible time steps. The timelines do not have to be exactly the same, but they need to be close enough so that matching is acceptable and have the same number of time steps. This example uses the regular Sentinel-1 cube for tile “22LBL” produced in the previous sections. The next step is to produce a regular Sentinel-2 data cube for the same tile and regularize it. We start by defining a non-regular data cube from Planetary Computer collections.\n\n# define the output directory\ncube_s2 &lt;-  sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    bands = c(\"B02\", \"B8A\", \"B11\", \"CLOUD\"),\n    tiles = c(\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-09-30\"\n)\nplot(cube_s2, red = \"B11\", green = \"B8A\", blue = \"B02\", date = \"2021-07-07\")\n\n\n\n\n\nFigure 6.2: Sentinel-2 image covering tile 22LBL.\n\n\n\nThe next step is to create a regular data cube for tile “20LBL” and Sentinel-2 data.\n\nif (!file.exists(\"./tempdir/R/dc_merge/s2_opt\"))\n    dir.create(\"./tempdir/R/dc_merge/s2_opt\")\ntempdir_r_s2_opt &lt;- \"./tempdir/R/dc_merge/s2_opt\"\n# define the output directory\ncube_s2_reg &lt;-  sits_regularize(\n    cube = cube_s2,\n    period = \"P16D\",\n    res = 40,\n    tiles = c(\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_r_s2_opt\n)\n\nWe then create a regular data cube of Sentinel-1 images covering the same MGRS tile\n\n# create an RTC cube from MPC collection for a region in Mato Grosso, Brazil.\ncube_s1_rtc &lt;-  sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VV\", \"VH\"),\n    orbit = \"descending\",\n    tiles = c(\"22LBL\"),\n    start_date = \"2021-06-01\",\n    end_date = \"2021-10-01\"\n)\nplot(cube_s1_rtc, band = \"VH\", palette = \"Greys\", scale = 0.7)\n\n\n\n\n\nFigure 6.3: “Original Sentinel-1 image covering tile 22LBL.\n\n\n\nThe next step is to create a regular Sentinel-1 data cube.\n\n# define the output directory\n# set output dir if it does not exist \ntempdir_r_sar &lt;- \"~/sitsbook/tempdir/R/dc_merge/sar\"\ndir.create(tempdir_r_sar, showWarnings = FALSE)\n# create a regular RTC cube from MPC collection for a tile 22LBL.\ncube_s1_reg &lt;- sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P16D\",\n    res = 40,\n    tiles = c(\"22LBL\"),\n    memsize = 12,\n    multicores = 6,\n    output_dir = tempdir_r_sar\n)\nplot(cube_s1_reg, band = \"VH\", palette = \"Greys\", scale = 0.7, \n     dates = c(\"2021-06-06\", \"2021-07-24\", \"2021-09-26\"))\n\nAfter creating the two regular cubes, we can merge them. Considering that the timelines are close enough so that the cubes can be combined, we can use the sits_merge function to produce a combined cube. As an example, we show a plot with both radar and optical bands.\n\n# merge Sentinel-1 and Sentinel-2 cubes\ncube_s1_s2 &lt;- sits_merge(cube_s2_reg, cube_s1_reg)\n# plot a an image with both SAR and optical bands\nplot(cube_s1_s2, red = \"B11\", green = \"B8A\", blue = \"VH\")\n\n\n\n\n\nFigure 6.4: Sentinel-2 and Sentinel-1 RGB composite for tile 22LBL."
  },
  {
    "objectID": "dc_merge.html#combining-multitemporal-data-cubes-with-digital-elevation-models",
    "href": "dc_merge.html#combining-multitemporal-data-cubes-with-digital-elevation-models",
    "title": "\n6  Multi-source EO data cubes\n",
    "section": "\n6.4 Combining multitemporal data cubes with digital elevation models",
    "text": "6.4 Combining multitemporal data cubes with digital elevation models\nIn many applications, especially in regions with large topographical, soil or climatic variations, is is useful to merge multitemporal data cubes with base information such as digital elevation models (DEM). Merging multitemporal satellite images with digital elevation models (DEMs) offers several advantages that enhance the analysis and interpretation of geospatial data. Elevation data provides an additional to the two-dimensional satellite images, which help to distinguish land use and land cover classes which are impacted by altitude gradients. One example is the capacity to distinguish between low-altitude and high-altitude forests. In case where topography changes significantly, DEM information can improve the accuracy of classification algorithms.\nAs an example of DEM integration in a data cube, we will consider an agricultural region of Chile which is located in a narrow area close to the Andes. There is a steep gradient so that the cube benefits from the inclusion of the DEM.\n\ns2_cube_19HBA &lt;- sits_cube(\n  source = \"MPC\",\n  collection = \"SENTINEL-2-L2A\",\n  tiles = \"19HBA\",\n  bands = c(\"B04\", \"B8A\", \"B12\", \"CLOUD\"),\n  start_date = \"2021-01-01\",\n  end_date = \"2021-03-31\"\n)\nplot(s2_cube_19HBA, red = \"B12\", green = \"B8A\", blue = \"B04\")\n\n\n\n\n\nSentinel-2 image covering tile 19HBA.\n\n\n\nThen, we produce a regular data cube to use for classification. In this example, we will use a reduced resolution (30 meters) to expedite processing. In practice, a resolution of 10 meters is recommended.\n\n# set output dir if it does not exist \ntempdir_r_s2_19HBA &lt;- \"~/sitsbook/tempdir/R/dc_merge/s2_19HBA\"\ndir.create(tempdir_r_s2_19HBA, showWarnings = FALSE)\n# Create a regular data cube\ns2_cube_19HBA_reg &lt;- sits_regularize(\n  cube = s2_cube_19HBA,\n  period = \"P16D\",\n  res = 30,\n  output_dir = tempdir_r_s2_19HBA\n)\n\nThe next step is recover the DEM for the area. For this purpose, we will use the Copernicus Global DEM-30, and select the area covered by the tile. As explained in the MPC access section above, the Copernicus DEM tiles are stored as 1\\(^\\circ\\) by 1\\(^\\circ\\) grid. For them to match an MGRS tile, they have to be regularized in a similar way as the Sentinel-1 images, as shown below. To select a DEM, no temporal information is required.\n\n# obtain the DEM cube for \ndem_cube_19HBA &lt;- sits_cube(\n  source = \"MPC\",\n  collection = \"COP-DEM-GLO-30\",\n  bands = \"ELEVATION\",\n  tiles = \"19HBA\"\n)\n\nAfter obtaining the 1\\(^\\circ\\) by 1\\(^\\circ\\) data cube covering the selected tile, the next step is to regularize it. This is done using the sits_regularize() function. This function will produce a DEM which matches exactly the chosen tile.\n\ntempdir_r_dem_19HBA &lt;- \"~/sitsbook/tempdir/R/dc_merge/dem_19HBA\"\ndir.create(tempdir_r_dem_19HBA, showWarnings = FALSE)\n# obtain the DEM cube for \ndem_cube_19HBA_reg &lt;- sits_regularize(\n  cube = dem_cube_19HBA,\n  res = 30,\n  bands = \"ELEVATION\",\n  tiles = \"19HBA\",\n  output_dir = tempdir_r_dem_19HBA\n)\n# plot the DEM reversing the palette \nplot(dem_cube_19HBA_reg, band = \"ELEVATION\", palette = \"Spectral\", rev = TRUE)\n\n\n\n\n\nFigure 6.5: Copernicus DEM-30 covering tile 19HBA.\n\n\n\nThere are two ways to combine multitemporal data cubes with DEM data. The first method takes the DEM as a base information, which is used in combination with the multispectral time series. For exemple, consider a situation of a data cube with 10 bands and 23 time steps, which has a 230-dimensional space. Adding DEM as a base cube will include one dimension to the attribute space. This combination is supported by function sits_add_base_cube. In the resulting cube, the information on the image time series and that of the DEM are stored separately. The data cube metadata will now include a column called base_info.\n\nmerged_cube_base &lt;- sits_add_base_cube(s2_cube_19HBA_reg, dem_cube_19HBA_reg)\nmerged_cube_base$base_info[[1]]\n\n# A tibble: 1 × 11\n  source collection     satellite sensor  tile    xmin   xmax   ymin  ymax crs  \n  &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1 MPC    COP-DEM-GLO-30 TANDEM-X  X-band… 19HBA 199980 309780 5.99e6 6.1e6 EPSG…\n# ℹ 1 more variable: file_info &lt;list&gt;\n\n\nAlthough this combination is conceptually simple, it has drawbacks. Since the attribute space now mixes times series with fixed-time information, the only applicable classification method is random forests. Because of the way random forest works, not all attributes are used by every decision tree. During the training of each tree, at each node, a random subset of features is selected, and the best split is chosen based on this subset rather than all features. Thus, there may be a significant number of decision trees that do use the DEM attribute. As a result, the effect of the DEM information may be underestimated.\nThe alternative is to combine the image data cube and the DEM using sits_merge. In this case, the DEM becomes another band. Although it may look peculiar to replicate the DEM many time to build an artificial time series, there are many advantages in doing so. All classification algorithms available in sits (including the deep learning ones) can be used to classify the resulting cube. For cases where the DEM information is particularly important, such organisation places DEM data at a par with other spectral bands. Users are encouraged to compare the results obtained by direct merging of DEM with spectral bands with the method where DEM is taken as a base cube.\n\nmerged_cube &lt;- sits_merge(s2_cube_19HBA_reg, dem_cube_19HBA_reg)\nmerged_cube$file_info[[1]]\n\n# A tibble: 24 × 13\n   fid   band      date       nrows ncols  xres  yres   xmin   ymin   xmax  ymax\n   &lt;chr&gt; &lt;chr&gt;     &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 1     B04       2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 2 1     B12       2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 3 1     B8A       2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 4 1     ELEVATION 2021-01-03  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 5 2     B04       2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 6 2     B12       2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 7 2     B8A       2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 8 1     ELEVATION 2021-01-19  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n 9 3     B04       2021-02-04  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n10 3     B12       2021-02-04  3660  3660    30    30 199980 5.99e6 309780 6.1e6\n# ℹ 14 more rows\n# ℹ 2 more variables: crs &lt;chr&gt;, path &lt;chr&gt;"
  },
  {
    "objectID": "dc_merge.html#summary",
    "href": "dc_merge.html#summary",
    "title": "\n6  Multi-source EO data cubes\n",
    "section": "\n6.5 Summary",
    "text": "6.5 Summary\nIn this chapter, we provide an overview of methods to merge multisource cubes. Combining multisource data is a powerful means of increasing the information available for classification. Users are encouraged to consider what data sets are available for their areas of interest and to try to merge them."
  },
  {
    "objectID": "dc_localcubes.html#introduction",
    "href": "dc_localcubes.html#introduction",
    "title": "\n7  Data cubes from local files\n",
    "section": "\n7.1 Introduction",
    "text": "7.1 Introduction\nIn many cubes, users need to build data cubes from files installed in locally-accessible computers. The most common case is when one has obtained a data cube by downloading an ARD collection using sits_cube() and producing a regular cube using sits_regularize(). The result will be a data cube stored in a local computer. In this case, STAC services are not available and we need to rely on file-specific information.\nAll regular cubes produced by sits use the same convention for file naming. Images are stored as individual TIFF COGs and follow the structure &lt;satellite&gt;_&lt;sensor&gt;_&lt;tile&gt;_&lt;band&gt;_&lt;date&gt;.tif. In this case, users only need to provide the names of the original cloud provider, the ARD collection from which the data was obtained, and the directory where data is stores. By default, sits scans the directory looking for files that are associated to a regular data cube. We recommend that users store each tile/band/date combination in a single file, since sits does not support multi-image files.\nA special situation is when users have obtained images by other means, such as downloading Planet collections. In this case, they have to organize the files so that each file corresponds to a single tile/date/band combination and that the file name contains information on tile, band and date. We show an example with Planet data in this chapter. Finally, users also may want to retrieve local files associated with processed data cubes, such as probability cubes or classified map. Information on how to proceed is also provided in what follows."
  },
  {
    "objectID": "dc_localcubes.html#building-data-cubes-from-local-files",
    "href": "dc_localcubes.html#building-data-cubes-from-local-files",
    "title": "\n7  Data cubes from local files\n",
    "section": "\n7.2 Building data cubes from local files",
    "text": "7.2 Building data cubes from local files\nTo build a data cube from local files, users must provide information about the original source from which the data was obtained. In this case, sits_cube() needs the parameters:\n\n\nsource, the cloud provider from where the data has been obtained (in this case, the Brazil Data Cube “BDC”);\n\ncollection, the collection of the cloud provider from where the images have been extracted. In this case, data comes from the MOD13Q1 collection 6;\n\ndata_dir, the local directory where the image files are stored;\n\nparse_info (optional), a vector of strings stating how file names store information on “tile”, “band”, and “date”. In this case, local images are stored in files whose names are similar to TERRA_MODIS_012010_EVI_2014-07-28.tif. This file represents an image obtained by the MODIS sensor onboard the TERRA satellite, covering part of tile 012010 in the EVI band for date 2014-07-28."
  },
  {
    "objectID": "dc_localcubes.html#planet-data-as-ard-local-files",
    "href": "dc_localcubes.html#planet-data-as-ard-local-files",
    "title": "\n7  Data cubes from local files\n",
    "section": "Planet data as ARD local files",
    "text": "Planet data as ARD local files\nARD images downloaded from cloud collections to a local computer are not associated with a STAC endpoint that describes them. They must be organized and named to allow sits to create a data cube from them. All local files have to be in the same directory and have the same spatial resolution and projection. Each file must contain a single image band for a single date. Each file name needs to include tile, date, and band information. Users must provide information about the original data source to allow sits to retrieve information about image attributes such as band names, missing values, etc.\nTo be able to read local files, they must belong to a collection registered by sits. All collections known to sits by default are shown using sits_list_collections(). To register a new collection, please see the information provided in the Advanced Topics section.\nThe example shows how to define a data cube using Planet images from the sitsdata package. The dataset contains monthly PlanetScope mosaics for tile “604-1043” for August to October 2022, with bands B01, B02, B04, and B04. In general, sits users need to match the local file names to the values provided by the parse_info parameter. The file names of this dataset use the format PLANETSCOPE_MOSAIC_604-1043_B4_2022-10-01.tif, which fits the default value for parse_info which is c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\") and for delim which is “_“, it is not necessary to set these values when creating a data cube from the local files.\n\n# Define the directory where Planet files are stored\ndata_dir &lt;- system.file(\"extdata/Planet\", package = \"sitsdata\")\n# Create a data cube from local files\nplanet_cube &lt;- sits_cube(\n    source = \"PLANET\",\n    collection = \"MOSAIC\",\n    data_dir = data_dir\n)\n\n# Plot the first instance of the Planet data in natural colors\nplot(planet_cube, red = \"B3\", green = \"B2\", blue = \"B1\")\n\n\n\n\n\nFigure 7.1: Planet image over an area in Colombia."
  },
  {
    "objectID": "dc_localcubes.html#reading-classified-images-as-local-data-cube",
    "href": "dc_localcubes.html#reading-classified-images-as-local-data-cube",
    "title": "\n7  Data cubes from local files\n",
    "section": "Reading classified images as local data cube",
    "text": "Reading classified images as local data cube\nIt is also possible to create local cubes based on results that have been produced by classification or post-classification algorithms. In this case, more parameters are required, and the parameter parse_info is specified differently, as follows:\n\n\nsource: Name of the original data provider.\n\ncollection: Name of the collection from where the data was extracted.\n\ndata_dir: Local directory for the classified images.\n\nband: Band name associated with the type of result. Use: (a) probs for probability cubes produced by sits_classify(); (b) bayes, for cubes produced by sits_smooth(); (c) entropy, least, ratio or margin, according to the method selected when using sits_uncertainty(); (d) variance for those produced by sits_variance(); and (e) class for classified cubes.\n\nstart_date: starting date for the temporal scope of the processed cube, which matches to the start date of the EO data cube that was processed by sits.\n\nend_date: final date for the temporal scope of the processed cube, which matches to the end date of the EO data cube that was processed by sits.\n\nlabels: Labels associated with the names of the classes (not required for cubes produced by sits_uncertainty()).\n\nversion: Version of the result (default = v1).\n\nparse_info: File name parsing information to allow sits to deduce the values of tile, start_date, end_date, band, and version from the file name. Unlike non-classified image files, cubes produced by classification and post-classification have both start_date and end_date.\n\nData cube containing results of sits_classify(), sits_smooth() and sits_variance() are stored as one file per file, organized internally as three-dimensional matrices. The third dimension is associated with the classification labels. Cubes associated with sits_uncertainty() and sits_label_classification() are organized as TIFF COGs containing two-dimensional matrices, one per tile.\nThe following code creates a results cube based on the classification of deforestation in Brazil. This classified cube was obtained by a large data cube of Sentinel-2 images, covering the state of Rondonia, Brazil comprising 40 tiles, 10 spectral bands, and covering the period from 2020-06-01 to 2021-09-11. Samples of four classes were trained by a random forest classifier. Internally, classified images use integers to represent classes. Thus, labels have to be associated to the integers that represent each class name.\n\n# Create a cube based on a classified image \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LLP\", \n                        package = \"sitsdata\")\n# File name  \"SENTINEL-2_MSI_20LLP_2020-06-04_2021-08-26_class_v1.tif\" \nRondonia_class_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    bands = \"class\",\n    labels = c(\"1\" = \"Burned_Area\", \"2\" = \"Cleared_Area\", \n               \"3\" = \"Highly_Degraded\", \"4\" =  \"Forest\"),\n    data_dir = data_dir,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \"start_date\", \"end_date\", \n                   \"band\", \"version\"))\n# Plot the classified cube\nplot(Rondonia_class_cube)\n\n\n\nFigure 7.2: Classified data cube for the year 2020/2021 in Rondonia, Brazil."
  },
  {
    "objectID": "dc_localcubes.html#summary",
    "href": "dc_localcubes.html#summary",
    "title": "\n7  Data cubes from local files\n",
    "section": "\n7.3 Summary",
    "text": "7.3 Summary\nIn this chapter, we explain how to create cubes from local files. Sometimes, users stop a sits session after performing an operation and want to continue from that point onwards. In this case, they can use sits_cube() to retrieve data from local files. This feature is useful, for example, to retrieve data cubes that have been regularized or classified."
  },
  {
    "objectID": "dc_cubeoperations.html#pixel-based-and-neighborhood-based-operations",
    "href": "dc_cubeoperations.html#pixel-based-and-neighborhood-based-operations",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.1 Pixel-based and neighborhood-based operations",
    "text": "8.1 Pixel-based and neighborhood-based operations\nPixel-based operations in remote sensing images refer to image processing techniques that operate on individual pixels or cells in an image without considering their spatial relationships with neighboring pixels. These operations are typically applied to each pixel in the image independently and can be used to extract information on spectral, radiometric, or spatial properties. Pixel-based operations produce spectral indexes which combine data from multiple bands.\nNeighborhood-based operations are applied to groups of pixels in an image. The neighborhood is typically defined as a rectangular or circular region centered on a given pixel. These operations can be used for removing noise, detecting edges, and sharpening, among other uses.\nThe sits_apply() function computes new indices from a desired mathematical operation as a function of the bands available on the cube using any valid R expression. It applies the operation for all tiles and all temporal intervals. There are two types of operations in sits_apply():\n\nPixel-based operations that produce an index based on individual pixels of existing bands. The input bands and indexes should be part of the input data cube and have the same names used in the cube. The new index will be computed for every pixel of all images in the time series. Besides arithmetic operators, the function also accepts vectorized R functions that can be applied to matrices (e.g., sqrt(), log(), and sin()).\nNeighborhood-based operations that produce a derived value based on a window centered around each individual pixel. The available functions are w_median(), w_sum(), w_mean(), w_min(), w_max(), w_sd() (standard deviation), and w_var() (variance). Users set the window size (only odd values are allowed).\n\nThe following examples show how to use sits_apply()."
  },
  {
    "objectID": "dc_cubeoperations.html#computing-vegetation-indexes",
    "href": "dc_cubeoperations.html#computing-vegetation-indexes",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.2 Computing vegetation indexes",
    "text": "8.2 Computing vegetation indexes\nUsing vegetation indexes is an established practice in remote sensing. These indexes aim to improve the discrimination of vegetation structure by combining two wavebands, one where leaf pigments reflect incoming light with another where leaves absorb incoming radiation. Green leaves from natural vegetation such as forests have a strong emissivity rate in the near-infrared bands and low emissivity rates in the red bands of the electromagnetic spectrum. These spectral properties are used to calculate the Normalized Difference Vegetation Index (NDVI), a widely used index that is computed as the normalized difference between the values of infra-red and red bands. Including red-edge bands in Sentinel-2 images has broadened the scope of the bands used to calculate these indices [1], [2]. In what follows, we show examples of vegetation index calculation using a Sentinel-2 data cube.\nFirst, we define a data cube for a tile in the state of Rondonia, Brazil, including bands used to compute different vegetation indexes. We regularize the cube using a target resolution of 60-meters to reduce processing time.\n\n# Create an non-regular data cube from AWS\ns2_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    tiles = \"20LKP\",\n    bands = c(\"B02\", \"B03\", \"B04\", \n              \"B05\", \"B06\", \"B07\", \n              \"B08\", \"B8A\", \"B11\", \n              \"B12\",\"CLOUD\"),\n    start_date = as.Date(\"2018-07-01\"),\n    end_date = as.Date(\"2018-08-31\"))\n\n\n# Regularize the cube to 15 day intervals\nreg_cube &lt;- sits_regularize(\n          cube       = s2_cube,\n          output_dir = tempdir_r,\n          res        = 60,\n          period     = \"P15D\",\n          multicores = 4)\n\nThere are many options for calculating vegetation indexes using Sentinel-2 bands. The most widely used method combines band B08 (785-899 nm) and band B04 (650-680 nm). Recent works in the literature propose using the red-edge bands B05 (698-713 nm), B06 (733-748 nm), and B07 (773-793 nm) for capturing subtle variations in chlorophyll absorption producing indexes, which are called Normalized Difference Vegetation Red-edge indexes (NDRE) [1]. In a recent review, Chaves et al. argue that red-edge bands are important for distinguishing leaf structure and chlorophyll content of different vegetation species [3]. In the example below, we show how to include indexes in the regular data cube with the Sentinel-2 spectral bands.\nWe first calculate the NDVI in the usual way, using bands B08 and B04.\n\n# Calculate NDVI index using bands B08 and B04\nreg_cube &lt;- sits_apply(reg_cube,\n    NDVI = (B08 - B04)/(B08 + B04),\n    output_dir = tempdir_r\n)\n\n\nplot(reg_cube, band = \"NDVI\", palette = \"RdYlGn\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 8.1: NDVI using bands B08 and B04 of Sentinel-2.\n\n\n\nWe now compare the traditional NDVI with other vegetation index computed using red-edge bands. The example below such the NDRE1 index, obtained using bands B06 and B05. Sun et al. argue that a vegetation index built using bands B06 and B07 provides a better approximation to leaf area index estimates than NDVI [2]. Notice that the contrast between forests and deforested areas is more robust in the NDRE1 index than with NDVI.\n\n# Calculate NDRE1 index using bands B06 and B05\nreg_cube &lt;- sits_apply(reg_cube,\n    NDRE1 = (B06 - B05)/(B06 + B05),\n    output_dir = tempdir_r\n)\n\n\n# Plot NDRE1 index\nplot(reg_cube, band = \"NDRE1\",  palette = \"RdYlGn\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nNDRE1 using bands B06 and B05 of Sentinel-2."
  },
  {
    "objectID": "dc_cubeoperations.html#spectral-indexes-for-identifying-burned-areas",
    "href": "dc_cubeoperations.html#spectral-indexes-for-identifying-burned-areas",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.3 Spectral indexes for identifying burned areas",
    "text": "8.3 Spectral indexes for identifying burned areas\nBand combinations can also generate spectral indices for detecting degradation by fires, which are an important element in environmental degradation. Forest fires significantly impact emissions and impoverish natural ecosystems [4]. Fires open the canopy, making the microclimate drier and increasing the amount of dry fuel [5]. One well-established technique for detecting burned areas with remote sensing images is the normalized burn ratio (NBR), the difference between the near-infrared and the short wave infrared band, calculated using bands B8A and B12.\n\n# Calculate the NBR index\nreg_cube &lt;- sits_apply(reg_cube,\n    NBR = (B12 - B8A)/(B12 + B8A),\n    output_dir = tempdir_r\n)\n\n\n# Plot the NBR for the first date\nplot(reg_cube, band = \"NBR\", palette = \"Reds\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 8.2: NBR ratio using Sentinel-2 B11 and B8A."
  },
  {
    "objectID": "dc_cubeoperations.html#support-for-non-normalized-indexes",
    "href": "dc_cubeoperations.html#support-for-non-normalized-indexes",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "Support for non-normalized indexes",
    "text": "Support for non-normalized indexes\nAll data cube operations discussed so far produce normalized indexes. By default, the indexes generated by the sits_apply() function are normalized between -1 and 1, scaled by a factor of 0.0001. Normalized indexes are saved as INT2S (Integer with sign). If the normalized parameter is FALSE, no scaling factor will be applied and the index will be saved as FLT4S (Float with sign). The code below shows an exemple of the non-normalized index, CVI - chlorophyll vegetation index. CVI is a spectral index used to estimate the chlorophyll content and overall health of vegetation. It combines bands in visible and near-infrared (NIR) regions to assess vegetation characteristics. Since CVI is not normalized, we have to set the parameter normalized to FALSE to inform sits_apply() to generate an FLT4S image.\n\n# Calculate the NBR index\nreg_cube &lt;- sits_apply(reg_cube,\n    CVI = (B8A / B03) * (B05 / B03 ),\n    normalized = FALSE, \n    output_dir = tempdir_r\n)\nplot(reg_cube, band = \"CVI\", palette = \"Greens\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 8.3: CVI index using bands B03, B05, and B8A."
  },
  {
    "objectID": "dc_cubeoperations.html#temporal-combination-operations",
    "href": "dc_cubeoperations.html#temporal-combination-operations",
    "title": "\n8  Operations on EO data cubes\n",
    "section": "\n8.4 Temporal combination operations",
    "text": "8.4 Temporal combination operations\nThere are cases when users want to produce results which combine the values a time series associated to each pixel of a data cube using reduction operators. In the context of time series analysis, a reduction operator is a function that reduces a sequence of data points into a single value or a smaller set of values. This process involves summarizing or aggregating the information from the time series in a meaningful way. Reduction operators are often used to extract key statistics or features from the data, making it easier to analyze and interpret.\nTo produce temporal combinations, sits provides sits_reduce, with associated functions:\n\n\nt_max(): maximum value of the series.\n\nt_min(): minimum value of the series\n\nt_mean(): mean of the series.\n\nt_median(): median of the series.\n\nt_sum(): sum of all the points in the series.\n\nt_std(): standard deviation of the series.\n\nt_skewness(): skewness of the series.\n\nt_kurtosis(): kurtosis of the series.\n\nt_amplitude(): difference between maximum and minimum values of the cycle. A small amplitude means a stable cycle.\n\nt_fslope(): maximum value of the first slope of the cycle. Indicates when the cycle presents an abrupt change in the curve. The slope between two values relates the speed of the growth or senescence phases\n\nt_mse(): average spectral energy density. The energy of the time series is distributed by frequency.\n\nt_fqr(): value of the first quartile of the series (0.25).\n\nt_tqr(): value of the third quartile of the series (0.75).\n\nt_iqr(): interquartile range (difference between the third and first quartiles).\n\nThe functions t_sum(), t_std(), t_skewness(), t_kurtosis(), and t_mse() produce values greater than the limit of a two-byte integer. Therefore, we save the images generated by these in floating point format.\nThe following examples show an example temporal reduction operations.\n\ntempdir_r_reduce &lt;- \"./tempdir/R/dc_cubeoperations/reduce\"\nif (!file.exists(\"./tempdir/R/dc_cubeoperations/reduce\"))\n  dir.create(\"./tempdir/R/dc_cubeoperations/reduce\")\n# Calculate the NBR index\nmax_ndvi_cube &lt;- sits_reduce(reg_cube,\n    NDVIMAX = t_max(NDVI),\n    output_dir = tempdir_r_reduce\n)\nplot(max_ndvi_cube, band = \"NDVIMAX\", palette = \"Greens\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 8.4: Maximum NDVI for Sentinel-2 cube."
  },
  {
    "objectID": "dc_cubeoperations.html#spectral-mixture-analysis",
    "href": "dc_cubeoperations.html#spectral-mixture-analysis",
    "title": "\n8  Operations on EO data cubes\n",
    "section": "\n8.5 Spectral mixture analysis",
    "text": "8.5 Spectral mixture analysis\nMany pixels in images of medium-resolution satellites such as Landsat or Sentinel-2 contain a mixture of spectral responses of different land cover types inside a resolution element [6]. In many applications, it is desirable to obtain the proportion of a given class inside a mixed pixel. For this purpose, the literature proposes mixture models; these models represent pixel values as a combination of multiple pure land cover types [7]. Assuming that the spectral response of pure land cover classes (called endmembers) is known, spectral mixture analysis derives new bands containing the proportion of each endmember inside a pixel.\nThe most used method for spectral mixture analysis is the linear model [7]. The main idea behind the linear mixture model is that the observed pixel spectrum can be expressed as a linear combination of the spectra of the pure endmembers, weighted by their respective proportions (or abundances) within the pixel. Mathematically, the model can be represented as: \\[\nR_i = \\sum_{j=1}^N a_{i,j}*x_j + \\epsilon_i, i \\in {1,...M}, M &gt; N,\n\\] where \\(i=1,..M\\) is the set of spectral bands and \\(j=1,..N\\) is the set of land classes. For each pixel, \\(R_i\\) is the reflectance in the i-th spectral band, \\(x_j\\) is the reflectance value due to the j-th endmember, and \\(a_{i,j}\\) is the proportion between the j-th endmember and the i-th spectral band. To solve this system of equations and obtain the proportion of each endmember, sits uses a non-negative least squares (NNLS) regression algorithm, which is available in the R package RStoolbox and was developed by Jakob Schwalb-Willmann, based on the sequential coordinate-wise algorithm (SCA) proposed on Franc et al. [8].\nTo run the mixture model in sits, it is necessary to inform the values of pixels which represent spectral responses of a unique class. These are the so-called “pure” pixels. Because the quality of the resulting endmember images depends on the quality of the pure pixels, they should be chosen carefully and based on expert knowledge of the area. Since sits supports multiple endmember spectral mixture analysis [9], users can specify more than one pure pixel per endmember to account for natural variability.\nIn sits, spectral mixture analysis is done by sits_mixture_model(), which has two mandatory parameters: cube (a data cube) and endmembers, a named table (or equivalent) that defines the pure pixels. The endmembers table must have the following named columns: (a) type, which defines the class associated with an endmember; (b) names, the names of the bands. Each line of the table must contain the value of each endmember for all bands (see example). To improve readability, we suggest that the endmembers parameters be defined as a tribble. A tribble is a tibble with an easier to read row-by-row layout. In the example below, we define three endmembers for classes Forest, Soil, and Water. Note that the values for each band are expressed as integers ranging from 0 to 10,000.\n\n# Define the endmembers for three classes and six bands\nem &lt;- tibble::tribble(\n    ~class,   ~B02, ~B03, ~B04, ~B8A, ~B11, ~B12,\n    \"forest\",  200,  352,  189, 2800, 1340,  546,\n    \"soil\",    400,  650,  700, 3600, 3500, 1800,\n    \"water\",   700, 1100, 1400,  850,   40,   26)\n# Generate the mixture model\nreg_cube &lt;- sits_mixture_model(\n    data = reg_cube,\n    endmembers = em,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r)\n\n\n# Plot the FOREST for the first date using the Greens palette\nplot(reg_cube, band = \"FOREST\", palette = \"Greens\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 8.5: Percentage of forest per pixel estimated by mixture model.\n\n\n\n\n# Plot the water endmember for the first date using the Blues palette\nplot(reg_cube, band = \"WATER\", palette = \"Blues\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 8.6: Percentage of water per pixel estimated by mixture model.\n\n\n\n\n# Plot the SOIL endmember for the first date using the orange red (OrRd) palette \nplot(reg_cube, band = \"SOIL\", palette = \"OrRd\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 8.7: Percentage of soil per pixel estimated by mixture model.\n\n\n\nLinear mixture models (LMM) improve the interpretation of remote sensing images by accounting for mixed pixels and providing a more accurate representation of the Earth’s surface. LMMs provide a more accurate representation of mixed pixels by considering the contributions of multiple land classes within a single pixel. This can lead to improved land cover classification accuracy compared to conventional per-pixel classification methods, which may struggle to accurately classify mixed pixels.\nLMMs also allow for the estimation of the abundances of each land class within a pixel, providing valuable sub-pixel information. This can be especially useful in applications where the spatial resolution of the sensor is not fine enough to resolve individual land cover types, such as monitoring urban growth or studying vegetation dynamics. By considering the sub-pixel composition of land classes, LMMs can provide a more sensitive measure of changes in land cover over time. This can lead to more accurate and precise change detection, particularly in areas with complex land cover patterns or where subtle changes in land cover may occur.\nApplications of spectral mixture analysis in remote sensing include forest degradation [13], wetland surface dynamics [14], and urban area characterization [15]. These models providing valuable information for a wide range of applications, from land mapping and change detection to resource management and environmental monitoring."
  },
  {
    "objectID": "dc_cubeoperations.html#summary",
    "href": "dc_cubeoperations.html#summary",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "\n8.4 Summary",
    "text": "8.4 Summary\nIn this chapter, we learned how to operate on data cubes, including how to compute spectral indexes, estimate mixture models, and do reducing operations. This chapter concludes our overview of how data cubes work in sits. In the next part, we will describe how to work with time series."
  },
  {
    "objectID": "dc_cubeoperations.html#references",
    "href": "dc_cubeoperations.html#references",
    "title": "\n8  Computing NDVI and other spectral indices\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nQ. Xie et al., “Retrieval of crop biophysical parameters from Sentinel-2 remote sensing imagery,” International Journal of Applied Earth Observation and Geoinformation, vol. 80, pp. 187–195, 2019, doi: 10.1016/j.jag.2019.04.019.\n\n\n[2] \nY. Sun, Q. Qin, H. Ren, T. Zhang, and S. Chen, “Red-Edge Band Vegetation Indices for Leaf Area Index Estimation From Sentinel-2/MSI Imagery,” IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no. 2, pp. 826–840, 2020, doi: 10.1109/TGRS.2019.2940826.\n\n\n[3] \nM. Chaves, M. Picoli, and I. Sanches, “Recent Applications of Landsat 8/OLI and Sentinel-2/MSI for Land Use and Land Cover Mapping: A Systematic Review,” Remote Sensing, vol. 12, no. 18, p. 3062, 2020, doi: 10.3390/rs12183062.\n\n\n[4] \nD. C. Nepstad et al., “Large-scale impoverishment of Amazonian forests by logging and fire,” Nature, vol. 398, no. 6727, pp. 505–508, 1999, doi: 10.1038/19066.\n\n\n[5] \nY. Gao, M. Skutsch, J. Paneque-Gálvez, and A. Ghilardi, “Remote sensing of forest degradation: A review,” Environmental Research Letters, vol. 15, no. 10, p. 103001, 2020, doi: 10.1088/1748-9326/abaad7."
  },
  {
    "objectID": "timeseries.html#introduction",
    "href": "timeseries.html#introduction",
    "title": "Satellite image time series",
    "section": "Introduction",
    "text": "Introduction\nThe sits package uses sets of time series data describing properties in spatiotemporal locations of interest. For land classification, these sets consist of samples labeled by experts. The package can also be used for any type of classification, provided that the timeline and bands of the time series used for training match that of the data cubes. In the chapters that follow, we discuss how to work with time series and present methods for improving data quality."
  },
  {
    "objectID": "timeseries.html#the-importance-of-high-quality-training-sets",
    "href": "timeseries.html#the-importance-of-high-quality-training-sets",
    "title": "Satellite image time series",
    "section": "The importance of high-quality training sets",
    "text": "The importance of high-quality training sets\nSelecting good training samples for machine learning classification of satellite images is critical to achieving accurate results. Experience with machine learning methods has shown that the number and quality of training samples are crucial factors in obtaining accurate results [1]. Large and accurate datasets are preferable, regardless of the algorithm used, while noisy training samples can negatively impact classification performance [2]. Thus, it is beneficial to use pre-processing methods to improve the quality of samples and eliminate those that may have been incorrectly labeled or possess low discriminatory power.\nIt is necessary to distinguish between wrongly labeled samples and differences resulting from the natural variability of class signatures. When working in a large geographic region, the variability of vegetation phenology leads to different patterns being assigned to the same label. A related issue is the limitation of crisp boundaries to describe the natural world. Class definitions use idealized descriptions (e.g., “a savanna woodland has tree cover of 50% to 90% ranging from 8 to 15 m in height”). Class boundaries are fuzzy and sometimes overlap, making it hard to distinguish between them. To improve sample quality, sits provides methods for evaluating the training data.\nThe sits package provides three methods for improving training samples. For large datasets, we recommend using both imbalance-reducing and SOM-based algorithms. The SOM-based method identifies potential mislabeled samples and outliers that require further investigation. The methods for balancing training samples reduce bias in favour of classes of high occurrences. The results demonstrate a positive impact on the overall classification accuracy."
  },
  {
    "objectID": "timeseries.html#general-guidance",
    "href": "timeseries.html#general-guidance",
    "title": "Satellite image time series",
    "section": "General guidance",
    "text": "General guidance\nThe complexity and diversity of our planet defy simple label names with hard boundaries. Due to representational and data handling issues, all classification systems have a limited number of categories, which inevitably fail to adequately describe the nuances of the planet’s landscapes. All representation systems are thus limited and application-dependent. As stated by Janowicz [3]: “geographical concepts are situated and context-dependent and can be described from different, equally valid, points of view; thus, ontological commitments are arbitrary to a large extent”.\nThe availability of big data and satellite image time series is a further challenge. In principle, image time series can capture more subtle changes for land classification. Experts must conceive classification systems and training data collections by understanding how time series information relates to actual land change. Methods for quality analysis, such as those presented in this Part, cannot replace user understanding and informed choices."
  },
  {
    "objectID": "timeseries.html#references",
    "href": "timeseries.html#references",
    "title": "Satellite image time series",
    "section": "References",
    "text": "References\n\n\n\n\n[1] A. E. Maxwell, T. A. Warner, and F. Fang, “Implementation of machine-learning classification in remote sensing: An applied review,” International Journal of Remote Sensing, vol. 39, no. 9, pp. 2784–2817, 2018.\n\n\n[2] B. Frenay and M. Verleysen, “Classification in the Presence of Label Noise: A Survey,” IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 5, pp. 845–869, 2014, doi: 10.1109/TNNLS.2013.2292894.\n\n\n[3] K. Janowicz, S. Scheider, T. Pehle, and G. Hart, “Geospatial semantics and linked spatiotemporal data – Past, present, and future,” Semantic Web, vol. 3, no. 4, pp. 321–332, 2012, doi: 10.3233/SW-2012-0077."
  },
  {
    "objectID": "ts_basics.html#data-structure-for-image-time-series",
    "href": "ts_basics.html#data-structure-for-image-time-series",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.1 Data structure for image time series",
    "text": "12.1 Data structure for image time series\n\nIn sits, time series are stored in a tibble data structure. The following code shows the first three lines of a time series tibble containing 1,882 labeled samples of land classes in Mato Grosso state of Brazil. The samples have time series extracted from the MODIS MOD13Q1 product from 2000 to 2016, provided every 16 days at 250 m resolution in the Sinusoidal projection. Based on ground surveys and high-resolution imagery, it includes samples of seven classes: Forest, Cerrado, Pasture, Soy_Fallow, Soy_Cotton, Soy_Corn, and Soy_Millet.\n\n# Samples\ndata(\"samples_matogrosso_mod13q1\")\nsamples_matogrosso_mod13q1[1:4,]\n\n# A tibble: 4 × 7\n  longitude latitude start_date end_date   label   cube     time_series      \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;list&gt;           \n1     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n2     -59.4    -9.31 2014-09-14 2015-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n3     -59.4    -9.31 2013-09-14 2014-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n4     -57.8    -9.76 2006-09-14 2007-08-29 Pasture bdc_cube &lt;tibble [23 × 5]&gt;\n\n\nThe time series tibble contains data and metadata. The first six columns contain spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The first sample has been labeled Pasture at location (-58.5631, -13.8844), being valid for the period (2006-09-14, 2007-08-29). Informing the dates where the label is valid is crucial for correct classification. In this case, the researchers labeling the samples used the agricultural calendar in Brazil. The relevant dates for other applications and other countries will likely differ from those used in the example. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band."
  },
  {
    "objectID": "ts_basics.html#utilities-for-handling-time-series",
    "href": "ts_basics.html#utilities-for-handling-time-series",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.2 Utilities for handling time series",
    "text": "12.2 Utilities for handling time series\nThe package provides functions for data manipulation and displaying information for time series tibbles. For example, summary() shows the labels of the sample set and their frequencies.\n\nsummary(samples_matogrosso_mod13q1)\n\n# A tibble: 7 × 3\n  label      count   prop\n  &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado      379 0.206 \n2 Forest       131 0.0713\n3 Pasture      344 0.187 \n4 Soy_Corn     364 0.198 \n5 Soy_Cotton   352 0.192 \n6 Soy_Fallow    87 0.0474\n7 Soy_Millet   180 0.0980\n\n\nIn many cases, it is helpful to relabel the dataset. For example, there may be situations where using a smaller set of labels is desirable because samples in one label on the original set may not be distinguishable from samples with other labels. We then could use sits_labels()&lt;- to assign new labels. The example below shows how to do relabeling on a time series set shown above; all samples associated with crops are grouped in a single Cropland label.\n\n# Copy the sample set for Mato Grosso \nsamples_new_labels &lt;- samples_matogrosso_mod13q1\n# Show the current labels\nsits_labels(samples_new_labels)\n\n[1] \"Cerrado\"    \"Forest\"     \"Pasture\"    \"Soy_Corn\"   \"Soy_Cotton\"\n[6] \"Soy_Fallow\" \"Soy_Millet\"\n\n# Update the labels\nsits_labels(samples_new_labels) &lt;- c(\"Cerrado\",   \"Forest\",    \n                                     \"Pasture\",   \"Cropland\", \n                                     \"Cropland\", \"Cropland\",\n                                     \"Cropland\")\nsummary(samples_new_labels)\n\n# A tibble: 4 × 3\n  label    count   prop\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 Cerrado    379 0.206 \n2 Cropland   983 0.535 \n3 Forest     131 0.0713\n4 Pasture    344 0.187 \n\n\nSince metadata and the embedded time series use the tibble data format, the functions from dplyr, tidyr, and purrr packages of the tidyverse [1] can be used to process the data. For example, the following code uses sits_select() to get a subset of the sample dataset with two bands (NDVI and EVI) and then uses the dplyr::filter() to select the samples labeled as Cerrado.\n\n# Select NDVI band\nsamples_ndvi &lt;- sits_select(samples_matogrosso_mod13q1, \n                            bands = \"NDVI\")\n# Select only samples with Cerrado label\nsamples_cerrado &lt;- dplyr::filter(samples_ndvi, \n                                 label == \"Cerrado\")"
  },
  {
    "objectID": "ts_basics.html#time-series-visualisation",
    "href": "ts_basics.html#time-series-visualisation",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.3 Time series visualisation",
    "text": "12.3 Time series visualisation\nThe default time series plot in sits combines all samples together in a single temporal interval, even if they belong to different years. This plot shows the spread of values for the time series of each band. The strong red line in the plot indicates the median of the values, while the two orange lines are the first and third interquartile ranges. See ?sits::plot for more details on data visualization in sits.\n\n# Plot cerrado samples together\nplot(samples_cerrado)\n\n\n\nFigure 12.1: Plot of Cerrado samples for NDVI band.\n\n\n\nTo see the spatial distribution of the samples, use sits_view() to create an interactive plot. The spatial visulisation is useful to show where the data has been collected.\n\nsits_view(samples_matogrosso_mod13q1)\n\n\n\n\n\nFigure 12.2: View location of training samples"
  },
  {
    "objectID": "ts_basics.html#patterns-of-training-samples",
    "href": "ts_basics.html#patterns-of-training-samples",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.4 Patterns of training samples",
    "text": "12.4 Patterns of training samples\nWhen dealing with large time series, its is useful to obtain a single plot that captures the essential temporal variability of each class. Following the work on the dtwSat R package [2], we use a generalized additive model (GAM) to obtain a single time series based on statistical approximation. In a GAM, the predictor depends linearly on a smooth function of the predictor variables.\n\\[\ny = \\beta_{i} + f(x) + \\epsilon, \\epsilon \\sim N(0, \\sigma^2).\n\\]\nThe function sits_patterns() uses a GAM to predict an idealized approximation to the time series associated with each class for all bands. The resulting patterns can be viewed using plot().\n\n# Estimate the patterns for each class and plot them\nsamples_matogrosso_mod13q1 |&gt;  \n    sits_patterns() |&gt; \n    plot()\n\n\n\nFigure 12.3: Patterns for Mato Grosso MOD13Q1 samples.\n\n\n\nThe resulting patterns provide some insights over the time series behaviour of each class. The response of the Forest class is quite distinctive. They also show that it should be possible to separate between the single and double cropping classes. There are similarities between the double-cropping classes (Soy_Corn and Soy_Millet) and between the Cerrado and Pasture classes. The subtle differences between class signatures provide hints at possible ways by which machine learning algorithms might distinguish between classes. One example is the difference between the middle-infrared response during the dry season (May to September) to differentiate between Cerrado and Pasture."
  },
  {
    "objectID": "ts_basics.html#geographical-variability-of-training-data",
    "href": "ts_basics.html#geographical-variability-of-training-data",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.5 Geographical variability of training data",
    "text": "12.5 Geographical variability of training data\nWhen working with machine learning classification of Earth observation data, it is important to evaluate if the training samples are well distributed in the study area. Training data often comes from ground surveys made at chosen locations. In large areas, ideally representative samples need to capture spatial variability. In practice, however, ground surveys or other means of data collection are limited to selected areas. In many cases, the geographical distribution of the training data does not cover the study area equally. Such mismatch can be a problem for achieving a good quality classification. As stated by Meyer and Pebesma [3]: “large gaps in geographic space do not always imply large gaps in feature space”.\nMeyer and Pebesma propose using a spatial distance distribution plot, which displays two distributions of nearest-neighbor distances: sample-to-sample and prediction-location-to-sample [3]. The difference between the two distributions reflects the degree of spatial clustering in the reference data. Ideally, the two distributions should be similar. Cases where the sample-to-sample distance distribution does not match prediction-location-to-sample distribution indicate possible problems in training data collection.\nsits implements spatial distance distribution plots with the sits_geo_dist() function. This function gets a training data in the samples parameter, and the study area in the roi parameter expressed as an sf object. Additional parameters are n (maximum number of samples for each distribution) and crs (coordinate reference system for the samples). By default, n is 1000, and crs is “EPSG:4326”. The example below shows how to use sits_geo_dist().\n\n# Read a shapefile for the state of Mato Grosso, Brazil\nmt_shp &lt;- system.file(\"extdata/shapefiles/mato_grosso/mt.shp\",\n                      package = \"sits\")\n# Convert to an sf object\nmt_sf &lt;- sf::read_sf(mt_shp)\n\n# Calculate sample-to-sample and sample-to-prediction distances\ndistances &lt;- sits_geo_dist(\n    samples = samples_modis_ndvi,\n    roi = mt_sf)\n# Plot sample-to-sample and sample-to-prediction distances\nplot(distances)\n\n\n\nFigure 12.4: Distribution of sample-to-sample and sample-to-prediction distances.\n\n\n\nThe plot shows a mismatch between the sample-to-sample and the sample-to-prediction distributions. Most samples are closer to each other than they are close to the location where values need to be predicted. In this case, there are many areas where few or no samples have been collected and where the prediction uncertainty will be higher. In this and similar cases, improving the distribution of training samples is always welcome. If that is not possible, areas with insufficient samples could have lower accuracy. This information must be reported to potential users of classification results."
  },
  {
    "objectID": "ts_basics.html#time-series-from-data-cubes",
    "href": "ts_basics.html#time-series-from-data-cubes",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.6 Time series from data cubes",
    "text": "12.6 Time series from data cubes\nTo get a set of time series in sits, first create a regular data cube and then request one or more time series from the cube using sits_get_data(). This function uses two mandatory parameters: cube and samples. The cube indicates the data cube from which the time series will be extracted. The samples parameter accepts the following data types:\n\nA data.frame with information on latitude and longitude (mandatory), start_date, end_date, and label for each sample point.\nA csv file with columns latitude, longitude, start_date, end_date, and label.\nA shapefile containing either POINTor POLYGON geometries. See details below.\nAn sf object (from the sf package) with POINT or POLYGON geometry information. See details below.\n\nIn the example below, given a data cube, the user provides the latitude and longitude of the desired location. Since the bands, start date, and end date of the time series are missing, sits obtains them from the data cube. The result is a tibble with one time series that can be visualized using plot().\n\n# Obtain a raster cube based on local files\ndata_dir &lt;- system.file(\"extdata/sinop\", package = \"sitsdata\")\nraster_cube &lt;- sits_cube(\n    source     = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir   = data_dir,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\"))\n# Obtain a time series from the raster cube from a point\nsample_latlong &lt;- tibble::tibble(\n    longitude = -55.57320, \n    latitude  = -11.50566)\nseries &lt;- sits_get_data(cube    = raster_cube,\n                        samples = sample_latlong)\nseries\n\n# A tibble: 1 × 7\n  longitude latitude start_date end_date   label   cube        time_series      \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;       &lt;list&gt;           \n1     -55.6    -11.5 2013-09-14 2014-08-29 NoClass MOD13Q1-6.1 &lt;tibble [23 × 3]&gt;\n\n\nA useful case is when a set of labeled samples can be used as a training dataset. In this case, trusted observations are usually labeled and commonly stored in plain text files in comma-separated values (csv) or using shapefiles (shp).\n\n# Retrieve a list of samples described by a csv file\nsamples_csv_file &lt;- system.file(\"extdata/samples/samples_sinop_crop.csv\",\n                                package = \"sits\")\n# Read the csv file into an R object\nsamples_csv &lt;- read.csv(samples_csv_file)\n# Print the first three samples\nsamples_csv[1:3,]\n\n  id longitude  latitude start_date   end_date   label\n1  1 -55.65931 -11.76267 2013-09-14 2014-08-29 Pasture\n2  2 -55.64833 -11.76385 2013-09-14 2014-08-29 Pasture\n3  3 -55.66738 -11.78032 2013-09-14 2014-08-29  Forest\n\n\nTo retrieve training samples for time series analysis, users must provide the temporal information (start_date and end_date). In the simplest case, all samples share the same dates. That is not a strict requirement. It is possible to specify different dates as long as they have a compatible duration. For example, the dataset samples_matogrosso_mod13q1 provided with the sitsdata package contains samples from different years covering the same duration. These samples are from the MOD13Q1 product, which contains the same number of images per year. Thus, all time series in the dataset samples_matogrosso_mod13q1 have the same number of dates.\nGiven a suitably built csv sample file, sits_get_data() requires two parameters: (a) cube, the name of the R object that describes the data cube; (b) samples, the name of the CSV file.\n\n# Get the points from a data cube in raster brick format\npoints &lt;- sits_get_data(cube = raster_cube, \n                        samples = samples_csv_file)\n# Show the tibble with the first three points\npoints[1:3,]\n\n# A tibble: 3 × 7\n  longitude latitude start_date end_date   label    cube        time_series\n      &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;    &lt;chr&gt;       &lt;list&gt;     \n1     -55.8    -11.7 2013-09-14 2014-08-29 Cerrado  MOD13Q1-6.1 &lt;tibble&gt;   \n2     -55.8    -11.7 2013-09-14 2014-08-29 Cerrado  MOD13Q1-6.1 &lt;tibble&gt;   \n3     -55.7    -11.7 2013-09-14 2014-08-29 Soy_Corn MOD13Q1-6.1 &lt;tibble&gt;   \n\n\nUsers can also specify samples by providing shapefiles or sf objects containing POINT or POLYGON geometries. The geographical location is inferred from the geometries associated with the shapefile or sf object. For files containing points, the geographical location is obtained directly. For polygon geometries, the parameter n_sam_pol (defaults to 20) determines the number of samples to be extracted from each polygon. The temporal information can be provided explicitly by the user; if absent, it is inferred from the data cube. If label information is available in the shapefile or sf object, the parameter label_attr is indicates which column contains the label associated with each time series.\nIn what follows, we provide a shapefile with location of forested areas in the Cerrado biome in Brazil. The shapefile is first used to define the region of interest to retrieve a data cube. Then the point locations are used to retrieve the time series.\n\n# Obtain a set of points inside the state of Mato Grosso, Brazil\nshp_file &lt;- system.file(\"extdata/shapefiles/cerrado/cerrado_forested.shp\", \n                        package = \"sits\")\n# Read the shapefile into an \"sf\" object\nsf_shape &lt;- sf::st_read(shp_file)\n\n# Create a data cube based on MOD13Q1 collection from BDC\nmodis_cube &lt;- sits_cube(\n    source      = \"BDC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands       = c(\"NDVI\", \"EVI\"),\n    roi         = sf_shape,\n    start_date  = \"2020-06-01\", \n    end_date    = \"2021-08-29\")\n\n# Read the points from the cube and produce a tibble with time series\nsamples_cerrado_forested &lt;- sits_get_data(\n    cube         = modis_cube, \n    samples      = shp_file, \n    start_date   = \"2020-06-01\",\n    end_date     = \"2021-08-29\", \n    label        = \"Woody Savanna\",\n    multicores   = 4)\n# Display the time series for the locations of Woody Savanna\nsamples_cerrado_forested\n\n\n\n# A tibble: 40 × 7\n   longitude latitude start_date end_date   label         cube       time_series\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;      &lt;list&gt;     \n 1     -51.9    -13.6 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 2     -51.5    -13.6 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 3     -51.4    -12.6 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 4     -51.3    -13.3 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 5     -51.2    -13.8 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 6     -51.1    -13.0 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 7     -50.7    -13.1 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 8     -50.2    -14.3 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n 9     -50.1    -14.2 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n10     -47.3    -11.3 2020-06-09 2021-08-29 Woody Savanna MOD13Q1-6… &lt;tibble&gt;   \n# ℹ 30 more rows"
  },
  {
    "objectID": "ts_basics.html#filtering-time-series",
    "href": "ts_basics.html#filtering-time-series",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.7 Filtering time series",
    "text": "12.7 Filtering time series\nSatellite image time series is generally contaminated by atmospheric influence, geolocation error, and directional effects [4]. Atmospheric noise, sun angle, interferences on observations or different equipment specifications, and the nature of the climate-land dynamics can be sources of variability [5]. Inter-annual climate variability also changes the phenological cycles of the vegetation, resulting in time series whose periods and intensities do not match on a year-to-year basis. To make the best use of available satellite data archives, methods for satellite image time series analysis need to deal with noisy and non-homogeneous datasets.\nThe literature on satellite image time series has several applications of filtering to correct or smooth vegetation index data. The package supports the well-known Savitzky–Golay (sits_sgolay()) and Whittaker (sits_whittaker()) filters. In an evaluation of NDVI time series filtering for estimating phenological parameters in India, Atkinson et al. found that the Whittaker filter provides good results [5]. Zhou et al. found that the Savitzky-Golay filter is suitable for reconstructing tropical evergreen broadleaf forests [6].\n\n12.7.1 Savitzky–Golay filter\nThe Savitzky-Golay filter fits a successive array of \\(2n+1\\) adjacent data points with a \\(d\\)-degree polynomial through linear least squares. The main parameters for the filter are the polynomial degree (\\(d\\)) and the length of the window data points (\\(n\\)). It generally produces smoother results for a larger value of \\(n\\) and/or a smaller value of \\(d\\) [7]. The optimal value for these two parameters can vary from case to case. In sits, the parameter order sets the order of the polynomial (default = 3), the parameter length sets the size of the temporal window (default = 5), and the parameter scaling sets the temporal expansion (default = 1). The following example shows the effect of Savitsky-Golay filter on a point extracted from the MOD13Q1 product, ranging from 2000-02-18 to 2018-01-01.\n\n# Take NDVI band of the first sample dataset\npoint_ndvi &lt;- sits_select(point_mt_6bands, bands = \"NDVI\")\n# Apply Savitzky Golay filter\npoint_sg &lt;- sits_sgolay(point_ndvi, length = 5)\n# Merge the point and plot the series\nsits_merge(point_sg, point_ndvi) |&gt; plot()\n\n[[1]]\n\n\n\n\nFigure 12.5: Savitzky-Golay filter applied on a multi-year NDVI time series.\n\n\n\n\n12.7.2 Whittaker filter\nThe Whittaker smoother attempts to fit a curve representing the raw data, but is penalized if subsequent points vary too much [8]. The Whittaker filter balances the residual to the original data and the smoothness of the fitted curve. The filter has one parameter: \\(\\lambda{}\\) that works as a smoothing weight parameter. The following example shows the effect of the Whittaker filter on a point extracted from the MOD13Q1 product, ranging from 2000-02-18 to 2018-01-01. The lambda parameter controls the smoothing of the filter. By default, it is set to 0.5, a small value. The example shows the effect of a larger smoothing parameter.\n\n# Take NDVI band of the first sample dataset\npoint_ndvi &lt;- sits_select(point_mt_6bands, bands = \"NDVI\")\n# Apply Whitakker filter\npoint_whit &lt;- sits_whittaker(point_ndvi, lambda = 0.5)\n# Merge the point and plot the series\nsits_merge(point_whit, point_ndvi) |&gt; plot()\n\n[[1]]\n\n\n\n\nFigure 12.6: Whittaker filter applied on a multi-year NDVI time series.\n\n\n\nSimilar to what is observed in the Savitsky-Golay filter, high values of the smoothing parameter lambda produce an over-smoothed time series that reduces the capacity of the time series to represent natural variations in crop growth. For this reason, low smoothing values are recommended when using sits_whittaker()."
  },
  {
    "objectID": "ts_basics.html#summary",
    "href": "ts_basics.html#summary",
    "title": "\n12  Basic operations on image time series\n",
    "section": "\n12.8 Summary",
    "text": "12.8 Summary\nIn this chapter, we presented the data structure used by sits to store pixel-based image time series. The text also shows how to retrieve time series from data cubes, as well as utilities available for visualisation, filtering and extracting patterns and geographical distribution. In the next chapters, we discuss how to improve the quality of training samples."
  },
  {
    "objectID": "ts_basics.html#references",
    "href": "ts_basics.html#references",
    "title": "\n12  Basic operations on image time series\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nH. Wickham and G. Grolemund, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc., 2017.\n\n\n[2] \nV. Maus, G. Câmara, M. Appel, and E. Pebesma, “dtwSat: Time-Weighted Dynamic Time Warping for Satellite Image Time Series Analysis in R,” Journal of Statistical Software, vol. 88, no. 5, pp. 1–31, 2019, doi: 10.18637/jss.v088.i05.\n\n\n[3] \nH. Meyer and E. Pebesma, “Machine learning-based global maps of ecological variables and the challenge of assessing them,” Nature Communications, vol. 13, no. 1, p. 2208, 2022, doi: 10.1038/s41467-022-29838-9.\n\n\n[4] \nE. F. Lambin and M. Linderman, “Time series of remote sensing data for land change science,” IEEE Transactions on Geoscience and Remote Sensing, vol. 44, no. 7, pp. 1926–1928, 2006.\n\n\n[5] \nP. M. Atkinson, C. Jeganathan, J. Dash, and C. Atzberger, “Inter-comparison of four models for smoothing satellite sensor time-series data to estimate vegetation phenology,” Remote Sensing of Environment, vol. 123, pp. 400–417, 2012.\n\n\n[6] \nJ. Zhou, L. Jia, M. Menenti, and B. Gorte, “On the performance of remote sensing time series reconstruction methods: A spatial comparison,” Remote Sensing of Environment, vol. 187, pp. 367–384, 2016.\n\n\n[7] \nJ. Chen, Per. Jönsson, M. Tamura, Z. Gu, B. Matsushita, and L. Eklundh, “A simple method for reconstructing a high-quality NDVI time-series data set based on the Savitzky–Golay filter,” Remote Sensing of Environment, vol. 91, no. 3, pp. 332–344, 2004, doi: 10.1016/j.rse.2004.03.014.\n\n\n[8] \nC. Atzberger and P. H. Eilers, “Evaluating the effectiveness of smoothing algorithms in the absence of ground reference measurements,” International Journal of Remote Sensing, vol. 32, no. 13, pp. 3689–3709, 2011."
  },
  {
    "objectID": "ts_cluster.html#introduction",
    "href": "ts_cluster.html#introduction",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.1 Introduction",
    "text": "13.1 Introduction\nGiven a set of training samples, experts should first cross-validate the training set to assess their inherent prediction error. The results show whether the data is internally consistent. Since cross-validation does not predict actual model performance, this chapter provides additional tools for improving the quality of training sets. More detailed information is available on Chapter Validation and accuracy measurements."
  },
  {
    "objectID": "ts_cluster.html#dataset-used-in-this-chapter",
    "href": "ts_cluster.html#dataset-used-in-this-chapter",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.2 Dataset used in this chapter",
    "text": "13.2 Dataset used in this chapter\nThe examples of this chapter use the cerrado_2classes data, a set of time series for the Cerrado region of Brazil, the second largest biome in South America with an area of more than 2 million \\(km^2\\). The data contains 746 samples divided into 2 classes (Cerrado and Pasture). Each time series covers 12 months (23 data points) from MOD13Q1 product, and has 2 bands (EVI, and NDVI).\n\n# Show the summary of the cerrado_2_classes dataset\nsummary(cerrado_2classes)\n\n# A tibble: 2 × 3\n  label   count  prop\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 Cerrado   400 0.536\n2 Pasture   346 0.464"
  },
  {
    "objectID": "ts_cluster.html#hierarchical-clustering-for-sample-quality-control",
    "href": "ts_cluster.html#hierarchical-clustering-for-sample-quality-control",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.3 Hierarchical clustering for sample quality control",
    "text": "13.3 Hierarchical clustering for sample quality control\nThe package provides two clustering methods to assess sample quality: Agglomerative Hierarchical Clustering (AHC) and Self-organizing Maps (SOM). These methods have different computational complexities. AHC has a computational complexity of \\(\\mathcal{O}(n^2)\\), given the number of time series \\(n\\), whereas SOM complexity is linear. For large data, AHC requires substantial memory and running time; in these cases, SOM is recommended. This section describes how to run AHC in sits. The SOM-based technique is presented in the next section.\nAHC computes the dissimilarity between any two elements from a dataset. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. This approach is helpful for exploring samples due to its visualization power and ease of use [1]. In sits, AHC is implemented using sits_cluster_dendro().\n\n# Take a set of patterns for 2 classes\n# Create a dendrogram, plot, and get the optimal cluster based on ARI index\nclusters &lt;- sits_cluster_dendro(\n    samples = cerrado_2classes, \n    bands = c(\"NDVI\", \"EVI\"),\n    dist_method = \"dtw_basic\",\n    linkage =  \"ward.D2\")\n\n\n\nFigure 13.1: Example of hierarchical clustering for a two class set.\n\n\n\nThe sits_cluster_dendro() function has one mandatory parameter (samples), with the samples to be evaluated. Optional parameters include bands, dist_method, and linkage. The dist_method parameter specifies how to calculate the distance between two time series. We recommend a metric that uses dynamic time warping (DTW) [2], as DTW is a reliable method for measuring differences between satellite image time series [3]. The options available in sits are based on those provided by package dtwclust, which include dtw_basic, dtw_lb, and dtw2. Please check ?dtwclust::tsclust for more information on DTW distances.\nThe linkage parameter defines the distance metric between clusters. The recommended linkage criteria are: complete or ward.D2. Complete linkage prioritizes the within-cluster dissimilarities, producing clusters with shorter distance samples, but results are sensitive to outliers. As an alternative, Ward proposes to use the sum-of-squares error to minimize data variance [4]; his method is available as ward.D2 option to the linkage parameter. To cut the dendrogram, the sits_cluster_dendro() function computes the adjusted rand index (ARI) [5], returning the height where the cut of the dendrogram maximizes the index. In the example, the ARI index indicates that there are six clusters. The result of sits_cluster_dendro() is a time series tibble with one additional column called “cluster”. The function sits_cluster_frequency() provides information on the composition of each cluster.\n\n# Show clusters samples frequency\nsits_cluster_frequency(clusters)\n\n         \n            1   2   3   4   5   6 Total\n  Cerrado 203  13  23  80   1  80   400\n  Pasture   2 176  28   0 140   0   346\n  Total   205 189  51  80 141  80   746\n\n\nThe cluster frequency table shows that each cluster has a predominance of either Cerrado or Pasture labels, except for cluster 3, which has a mix of samples from both labels. Such confusion may have resulted from incorrect labeling, inadequacy of selected bands and spatial resolution, or even a natural confusion due to the variability of the land classes. To remove cluster 3, use dplyr::filter(). The resulting clusters still contain mixed labels, possibly resulting from outliers. In this case, sits_cluster_clean() removes the outliers, leaving only the most frequent label. After cleaning the samples, the resulting set of samples is likely to improve the classification results.\n\n# Remove cluster 3 from the samples\nclusters_new &lt;- dplyr::filter(clusters, cluster != 3)\n# Clear clusters, leaving only the majority label\nclean &lt;- sits_cluster_clean(clusters_new)\n# Show clusters samples frequency\nsits_cluster_frequency(clean)\n\n         \n            1   2   4   5   6 Total\n  Cerrado 203   0  80   0  80   363\n  Pasture   0 176   0 140   0   316\n  Total   203 176  80 140  80   679"
  },
  {
    "objectID": "ts_cluster.html#summary",
    "href": "ts_cluster.html#summary",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "\n13.4 Summary",
    "text": "13.4 Summary\nIn this chapter, we present hierarchical clustering to improve the quality of training data. This method works well for up to four classes. Because of its quadratical computational complexity, it is not practical to use it for data sets with many classes. In this case, we suggest the use of self-organized maps (SOM) as shown in the next chapter."
  },
  {
    "objectID": "ts_cluster.html#references",
    "href": "ts_cluster.html#references",
    "title": "\n13  Hierarchical clustering of time series\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nE. Keogh, J. Lin, and W. Truppel, “Clustering of time series subsequences is meaningless: Implications for previous and future research,” in Data Mining, 2003. ICDM 2003. Third IEEE International Conference on, 2003, pp. 115–122.\n\n\n[2] \nF. Petitjean, J. Inglada, and P. Gancarski, “Satellite Image Time Series Analysis Under Time Warping,” IEEE Transactions on Geoscience and Remote Sensing, vol. 50, no. 8, pp. 3081–3095, 2012, doi: 10.1109/TGRS.2011.2179050.\n\n\n[3] \nV. Maus, G. Camara, R. Cartaxo, A. Sanchez, F. M. Ramos, and G. R. Queiroz, “A Time-Weighted Dynamic Time Warping Method for Land-Use and Land-Cover Mapping,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 9, no. 8, pp. 3729–3739, 2016, doi: 10.1109/JSTARS.2016.2517118.\n\n\n[4] \nJ. H. Ward, “Hierarchical grouping to optimize an objective function,” Journal of the American statistical association, vol. 58, no. 301, pp. 236–244, 1963.\n\n\n[5] \nW. M. Rand, “Objective Criteria for the Evaluation of Clustering Methods,” Journal of the American Statistical Association, vol. 66, no. 336, pp. 846–850, 1971, doi: 10.1080/01621459.1971.10482356."
  },
  {
    "objectID": "ts_som.html#introduction",
    "href": "ts_som.html#introduction",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.1 Introduction",
    "text": "14.1 Introduction\nThe sits package provides a clustering technique based on self-organizing maps (SOM) as an alternative to hierarchical clustering for quality control of training samples. SOM is a dimensionality reduction technique [1], where high-dimensional data is mapped into a two-dimensional map, keeping the topological relations between data patterns. As shown in Figure 14.1, the SOM 2D map is composed of units called neurons. Each neuron has a weight vector, with the same dimension as the training samples. At the start, neurons are assigned a small random value and then trained by competitive learning. The algorithm computes the distances of each member of the training set to all neurons and finds the neuron closest to the input, called the best matching unit.\n\n\n\n\nFigure 14.1: SOM 2D map creation (source: [2]).\n\n\n\nThe input data for quality assessment is a set of training samples, which are high-dimensional data; for example, a time series with 25 instances of 4 spectral bands has 100 dimensions. When projecting a high-dimensional dataset into a 2D SOM map, the units of the map (called neurons) compete for each sample. Each time series will be mapped to one of the neurons. Since the number of neurons is smaller than the number of classes, each neuron will be associated with many time series. The resulting 2D map will be a set of clusters. Given that SOM preserves the topological structure of neighborhoods in multiple dimensions, clusters that contain training samples with a given label will usually be neighbors in 2D space. The neighbors of each neuron of a SOM map provide information on intraclass and interclass variability, which is used to detect noisy samples. The methodology of using SOM for sample quality assessment is discussed in detail in the reference paper [2].\n\n\n\n\nFigure 14.2: Using SOM for class noise reduction (source: [2])"
  },
  {
    "objectID": "ts_som.html#dataset-used-in-this-chapter",
    "href": "ts_som.html#dataset-used-in-this-chapter",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.2 Dataset used in this chapter",
    "text": "14.2 Dataset used in this chapter\nThe examples of this chapter use samples_cerrado_mod13q1, a set of time series from the Cerrado region of Brazil. The data ranges from 2000 to 2017 and includes 50,160 samples divided into 12 classes (Dense_Woodland, Dunes, Fallow_Cotton, Millet_Cotton, Pasture, Rocky_Savanna, Savanna, Savanna_Parkland, Silviculture, Soy_Corn, Soy_Cotton, and Soy_Fallow). Each time series covers 12 months (23 data points) from MOD13Q1 product, and has 4 bands (EVI, NDVI, MIR, and NIR). We use bands NDVI and EVI for faster processing.\n\n# Take only the NDVI and EVI bands\nsamples_cerrado_mod13q1_2bands &lt;- sits_select(\n    data = samples_cerrado_mod13q1, \n    bands = c(\"NDVI\", \"EVI\"))\n\n# Show the summary of the samples\nsummary(samples_cerrado_mod13q1_2bands)\n\n# A tibble: 12 × 3\n   label            count    prop\n   &lt;chr&gt;            &lt;int&gt;   &lt;dbl&gt;\n 1 Dense_Woodland    9966 0.199  \n 2 Dunes              550 0.0110 \n 3 Fallow_Cotton      630 0.0126 \n 4 Millet_Cotton      316 0.00630\n 5 Pasture           7206 0.144  \n 6 Rocky_Savanna     8005 0.160  \n 7 Savanna           9172 0.183  \n 8 Savanna_Parkland  2699 0.0538 \n 9 Silviculture       423 0.00843\n10 Soy_Corn          4971 0.0991 \n11 Soy_Cotton        4124 0.0822 \n12 Soy_Fallow        2098 0.0418"
  },
  {
    "objectID": "ts_som.html#creating-the-som-map",
    "href": "ts_som.html#creating-the-som-map",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.3 Creating the SOM map",
    "text": "14.3 Creating the SOM map\nTo perform the SOM-based quality assessment, the first step is to run sits_som_map(), which uses the kohonen R package to compute a SOM grid [3], controlled by five parameters. The grid size is given by grid_xdim and grid_ydim. The starting learning rate is alpha, which decreases during the interactions. To measure the separation between samples, use distance (either “dtw” or “euclidean”). The number of iterations is set by rlen. When using sits_som_map() in machines which have multiprocessing support for the OpenMP protocol, setting the learning mode parameter mode to “patch” improves processing time. In Windows, please use “online”.\nWe suggest using the Dynamic Time Warping (“dtw”) metric as the distance measure. It is a technique used to measure the similarity between two temporal sequences that may vary in speed or timing [4]. The core idea of DTW is to find the optimal alignment between two sequences by allowing non-linear mapping of one sequence onto another. In time series analysis, DTW matches two series slightly out of sync. This property is useful in land use studies for matching time series of agricultural areas [5].\n\n# Clustering time series using SOM\nsom_cluster &lt;- sits_som_map(samples_cerrado_mod13q1_2bands,\n    grid_xdim = 15,\n    grid_ydim = 15,\n    alpha = 1.0,\n    distance = \"dtw\",\n    rlen = 20)\n\n\n# Plot the SOM map\nplot(som_cluster)\n\n\n\nFigure 14.3: SOM map for the Cerrado samples.\n\n\n\nThe output of the sits_som_map() is a list with three elements: (a) data, the original set of time series with two additional columns for each time series: id_sample (the original id of each sample) and id_neuron (the id of the neuron to which it belongs); (b) labelled_neurons, a tibble with information on the neurons. For each neuron, it gives the prior and posterior probabilities of all labels which occur in the samples assigned to it; and (c) the SOM grid. To plot the SOM grid, use plot(). The neurons are labelled using majority voting.\nThe SOM grid shows that most classes are associated with neurons close to each other, although there are exceptions. Some Pasture neurons are far from the main cluster because the transition between open savanna and pasture areas is not always well defined and depends on climate and latitude. Also, the neurons associated with Soy_Fallow are dispersed in the map, indicating possible problems in distinguishing this class from the other agricultural classes. The SOM map can be used to remove outliers, as shown below."
  },
  {
    "objectID": "ts_som.html#measuring-confusion-between-labels-using-som",
    "href": "ts_som.html#measuring-confusion-between-labels-using-som",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.4 Measuring confusion between labels using SOM",
    "text": "14.4 Measuring confusion between labels using SOM\nThe second step in SOM-based quality assessment is understanding the confusion between labels. The function sits_som_evaluate_cluster() groups neurons by their majority label and produces a tibble. Neurons are grouped into clusters, and there will be as many clusters as there are labels. The results shows the percentage of samples of each label in each cluster. Ideally, all samples of each cluster would have the same label. In practice, cluster contain samples with different label. This information helps on measuring the confusion between samples.\n\n# Produce a tibble with a summary of the mixed labels\nsom_eval &lt;- sits_som_evaluate_cluster(som_cluster)\n# Show the result\nsom_eval \n\n# A tibble: 66 × 4\n   id_cluster cluster        class          mixture_percentage\n        &lt;int&gt; &lt;chr&gt;          &lt;chr&gt;                       &lt;dbl&gt;\n 1          1 Dense_Woodland Dense_Woodland            78.1   \n 2          1 Dense_Woodland Pasture                    5.56  \n 3          1 Dense_Woodland Rocky_Savanna              8.95  \n 4          1 Dense_Woodland Savanna                    3.88  \n 5          1 Dense_Woodland Silviculture               3.48  \n 6          1 Dense_Woodland Soy_Corn                   0.0249\n 7          2 Dunes          Dunes                    100     \n 8          3 Fallow_Cotton  Dense_Woodland             0.169 \n 9          3 Fallow_Cotton  Fallow_Cotton             49.5   \n10          3 Fallow_Cotton  Millet_Cotton             13.9   \n# ℹ 56 more rows\n\n\nMany labels are associated with clusters where there are some samples with a different label. Such confusion between labels arises because sample labeling is subjective and can be biased. In many cases, interpreters use high-resolution data to identify samples. However, the actual images to be classified are captured by satellites with lower resolution. In our case study, a MOD13Q1 image has pixels with 250 m resolution. As such, the correspondence between labeled locations in high-resolution images and mid to low-resolution images is not direct. The confusion by sample label can be visualized in a bar plot using plot(), as shown below. The bar plot shows some confusion between the labels associated with the natural vegetation typical of the Brazilian Cerrado (Savanna, Savanna_Parkland, Rocky_Savanna). This mixture is due to the large variability of the natural vegetation of the Cerrado biome, which makes it difficult to draw sharp boundaries between classes. Some confusion is also visible between the agricultural classes. The Fallow_Cotton class is a particularly difficult one since many of the samples assigned to this class are confused with Soy_Cotton and Millet_Cotton.\n\n# Plot the confusion between clusters\nplot(som_eval)\n\n\n\nFigure 14.4: Confusion between classes as measured by SOM."
  },
  {
    "objectID": "ts_som.html#detecting-noisy-samples-using-som",
    "href": "ts_som.html#detecting-noisy-samples-using-som",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.5 Detecting noisy samples using SOM",
    "text": "14.5 Detecting noisy samples using SOM\nThe third step in the quality assessment uses the discrete probability distribution associated with each neuron, which is included in the labeled_neurons tibble produced by sits_som_map(). This approach associates probabilities with frequency of occurrence. More homogeneous neurons (those with one label has high frequency) are assumed to be composed of good quality samples. Heterogeneous neurons (those with two or more classes with significant frequencies) are likely to contain noisy samples. The algorithm computes two values for each sample:\n\nprior probability: the probability that the label assigned to the sample is correct, considering the frequency of samples in the same neuron. For example, if a neuron has 20 samples, of which 15 are labeled as Pasture and 5 as Forest, all samples labeled Forest are assigned a prior probability of 25%. This indicates that Forest samples in this neuron may not be of good quality.\nposterior probability: the probability that the label assigned to the sample is correct, considering the neighboring neurons. Take the case of the above-mentioned neuron whose samples labeled Pasture have a prior probability of 75%. What happens if all the neighboring neurons have Forest as a majority label? To answer this question, we use Bayesian inference to estimate if these samples are noisy based on the surrounding neurons [2].\n\nTo identify noisy samples, we take the result of the sits_som_map() function as the first argument to the function sits_som_clean_samples(). This function finds out which samples are noisy, which are clean, and which need to be further examined by the user. It requires the prior_threshold and posterior_threshold parameters according to the following rules:\n\nIf the prior probability of a sample is less than prior_threshold, the sample is assumed to be noisy and tagged as “remove”;\nIf the prior probability is greater or equal to prior_threshold and the posterior probability calculated by Bayesian inference is greater or equal to posterior_threshold, the sample is assumed not to be noisy and thus is tagged as “clean”;\nIf the prior probability is greater or equal to prior_threshold and the posterior probability is less than posterior_threshold, we have a situation when the sample is part of the majority level of those assigned to its neuron, but its label is not consistent with most of its neighbors. This is an anomalous condition and is tagged as “analyze”. Users are encouraged to inspect such samples to find out whether they are in fact noisy or not.\n\nThe default value for both prior_threshold and posterior_threshold is 60%. The sits_som_clean_samples() has an additional parameter (keep), which indicates which samples should be kept in the set based on their prior and posterior probabilities. The default for keep is c(\"clean\", \"analyze\"). As a result of the cleaning, about 900 samples have been considered to be noisy and thus to be possibly removed. We first show the complete distribution of the samples and later remove the noisy ones.\n\nall_samples &lt;- sits_som_clean_samples(\n    som_map = som_cluster, \n    prior_threshold = 0.6,\n    posterior_threshold = 0.6,\n    keep = c(\"clean\", \"analyze\", \"remove\"))\n# Print the sample distribution based on evaluation\nplot(all_samples)\n\n\n\nFigure 14.5: Distribution of samples using som evaluation.\n\n\n\nWe now remove the noisy samples to improve the quality of the training set.\n\nnew_samples &lt;- sits_som_clean_samples(\n    som_map = som_cluster, \n    prior_threshold = 0.6,\n    posterior_threshold = 0.6,\n    keep = c(\"clean\", \"analyze\"))\n# Print the new sample distribution\nsummary(new_samples)\n\n# A tibble: 9 × 3\n  label            count   prop\n  &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n1 Dense_Woodland    8519 0.220 \n2 Dunes              550 0.0142\n3 Pasture           5509 0.142 \n4 Rocky_Savanna     5508 0.142 \n5 Savanna           7651 0.197 \n6 Savanna_Parkland  1619 0.0418\n7 Soy_Corn          4595 0.119 \n8 Soy_Cotton        3515 0.0907\n9 Soy_Fallow        1309 0.0338\n\n\nAll samples of the classes which had the highest confusion with others(Fallow_Cotton, Silviculture, and Millet_Cotton) are marken as noisy been removed. Classes Fallow_Cotton and Millet_Cotton are not distinguishable from other crops. Samples of class Silviculture (planted forests) have removed since they have been confused with natural forests and woodlands in the SOM map. Further analysis includes calculating the SOM map and confusion matrix for the new set, as shown in the following example.\n\n# Produce a new SOM map with the cleaned samples\nnew_cluster &lt;- sits_som_map(\n   data = new_samples,\n   grid_xdim = 15,\n   grid_ydim = 15,\n   alpha = 1.0,\n   rlen = 20,\n   distance = \"dtw\")\n\n\n# Evaluate the mixture in the new SOM clusters\nnew_cluster_mixture &lt;- sits_som_evaluate_cluster(new_cluster)\n# Plot the mixture information.\nplot(new_cluster_mixture)\n\n\n\nFigure 14.6: Cluster confusion plot for samples cleaned by SOM.\n\n\n\nAs expected, the new confusion map shows a significant improvement over the previous one. This result should be interpreted carefully since it may be due to different effects. The most direct interpretation is that Millet_Cotton and Silviculture cannot be easily separated from the other classes, given the current attributes (a time series of NDVI and EVI indices from MODIS images). In such situations, users should consider improving the number of samples from the less represented classes, including more MODIS bands, or working with higher resolution satellites. The results of the SOM method should be interpreted based on the users’ understanding of the ecosystems and agricultural practices of the study region.\nThe SOM-based analysis discards samples that can be confused with samples of other classes. After removing noisy samples or uncertain classes, the dataset obtains a better validation score since there is less confusion between classes. Users should analyse the results with care. Not all discarded samples are low-quality ones. Confusion between samples of different classes can result from inconsistent labeling or from the lack of capacity of satellite data to distinguish between chosen classes. When many samples are discarded, as in the current example, revising the whole classification schema is advisable. The aim of selecting training data should always be to match the reality on the ground to the power of remote sensing data to identify differences. No analysis procedure can replace actual user experience and knowledge of the study region."
  },
  {
    "objectID": "ts_som.html#summary",
    "href": "ts_som.html#summary",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "\n14.6 Summary",
    "text": "14.6 Summary\nIn this chapter, we discuss the use of SOM as a proven clustering method for removing noisy samples and those that cannot be easily distinguishable from other samples of other classes. Experience with sits indicates that using SOM is a good way to assess data quality. In the next section, we focus on a complementary method of removing sample imbalance."
  },
  {
    "objectID": "ts_som.html#references",
    "href": "ts_som.html#references",
    "title": "\n14  Self-organized maps for sample quality control\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nT. Kohonen, “The self-organizing map,” Proceedings of the IEEE, vol. 78, no. 9, pp. 1464–1480, 1990, doi: 10.1109/5.58325.\n\n\n[2] \nL. A. Santos, K. R. Ferreira, G. Camara, M. C. A. Picoli, and R. E. Simoes, “Quality control and class noise reduction of satellite image time series,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 177, pp. 75–88, 2021, doi: 10.1016/j.isprsjprs.2021.04.014.\n\n\n[3] \nR. Wehrens and J. Kruisselbrink, “Flexible Self-Organizing Maps in kohonen 3.0,” Journal of Statistical Software, vol. 87, no. 1, pp. 1–18, 2018, doi: 10.18637/jss.v087.i07.\n\n\n[4] \nD. Berndt and J. Clifford, “Using Dynamic Time Warping to Find Patterns in Time Series,” 1994, [Online]. Available: https://www.semanticscholar.org/paper/Using-Dynamic-Time-Warping-to-Find-Patterns-in-Time-Berndt-Clifford/1ac57524ba2d2a69c1bb6defed7352a06fd7050d.\n\n\n[5] \nV. Maus, G. Camara, R. Cartaxo, F. M. Ramos, A. Sanchez, and G. Q. Ribeiro, “Open boundary dynamic time warping for satellite image time series classification,” in 2015 IEEE international geoscience and remote sensing symposium (IGARSS), 2015, pp. 3349–3352, doi: 10.1109/IGARSS.2015.7326536."
  },
  {
    "objectID": "ts_balance.html#introduction",
    "href": "ts_balance.html#introduction",
    "title": "\n15  Reducing sample imbalance\n",
    "section": "\n15.1 Introduction",
    "text": "15.1 Introduction\nMany training samples for Earth observation data analysis are imbalanced. This situation arises when the distribution of samples associated with each label is uneven. Sample imbalance is an undesirable property of a training set since machine learning algorithms tend to be more accurate for classes with many samples. The instances belonging to the minority group are misclassified more often than those belonging to the majority group. Thus, reducing sample imbalance can positively affect classification accuracy [1]."
  },
  {
    "objectID": "ts_balance.html#dataset-using-in-this-chapter",
    "href": "ts_balance.html#dataset-using-in-this-chapter",
    "title": "\n15  Reducing sample imbalance\n",
    "section": "\n15.2 Dataset using in this chapter",
    "text": "15.2 Dataset using in this chapter\nThe examples of this chapter use samples_cerrado_mod13q1, a set of time series from the Cerrado region of Brazil. The data ranges from 2000 to 2017 and includes 50,160 samples divided into 12 classes (Dense_Woodland, Dunes, Fallow_Cotton, Millet_Cotton, Pasture, Rocky_Savanna, Savanna, Savanna_Parkland, Silviculture, Soy_Corn, Soy_Cotton, and Soy_Fallow). Each time series covers 12 months (23 data points) from MOD13Q1 product, and has 4 bands (EVI, NDVI, MIR, and NIR). We use bands NDVI and EVI for faster processing.\n\n# Take only the NDVI and EVI bands\nsamples_cerrado_mod13q1_2bands &lt;- sits_select(\n    data = samples_cerrado_mod13q1, \n    bands = c(\"NDVI\", \"EVI\"))\n\n# Show the summary of the samples\nsummary(samples_cerrado_mod13q1_2bands)\n\n# A tibble: 12 × 3\n   label            count    prop\n   &lt;chr&gt;            &lt;int&gt;   &lt;dbl&gt;\n 1 Dense_Woodland    9966 0.199  \n 2 Dunes              550 0.0110 \n 3 Fallow_Cotton      630 0.0126 \n 4 Millet_Cotton      316 0.00630\n 5 Pasture           7206 0.144  \n 6 Rocky_Savanna     8005 0.160  \n 7 Savanna           9172 0.183  \n 8 Savanna_Parkland  2699 0.0538 \n 9 Silviculture       423 0.00843\n10 Soy_Corn          4971 0.0991 \n11 Soy_Cotton        4124 0.0822 \n12 Soy_Fallow        2098 0.0418 \n\n\nThe Cerrado dataset is highly imbalanced. The three most frequent labels (Dense Woodland, Savanna, and Pasture) include 53% of all samples, while the three least frequent labels (Millet-Cotton, Silviculture, and Dunes) comprise only 2.5% of the dataset. This is a good dataset to investigate the impact of rebalancing."
  },
  {
    "objectID": "ts_balance.html#producing-a-balanced-training-set",
    "href": "ts_balance.html#producing-a-balanced-training-set",
    "title": "\n15  Reducing sample imbalance\n",
    "section": "\n15.3 Producing a balanced training set",
    "text": "15.3 Producing a balanced training set\nThe function sits_reduce_imbalance() deals with training set imbalance; it increases the number of samples of least frequent labels, and reduces the number of samples of most frequent labels. Oversampling requires generating synthetic samples. The package uses the SMOTE method that estimates new samples by considering the cluster formed by the nearest neighbors of each minority label. SMOTE takes two samples from this cluster and produces a new one by randomly interpolating them [2].\nTo perform undersampling, sits_reduce_imbalance() builds a SOM map for each majority label based on the required number of samples to be selected. Each dimension of the SOM is set to ceiling(sqrt(new_number_samples/4)) to allow a reasonable number of neurons to group similar samples. After calculating the SOM map, the algorithm extracts four samples per neuron to generate a reduced set of samples that approximates the variation of the original one.\nThe sits_reduce_imbalance() algorithm has two parameters: n_samples_over and n_samples_under. The first parameter indicates the minimum number of samples per class. All classes with samples less than its value are oversampled. The second parameter controls the maximum number of samples per class; all classes with more samples than its value are undersampled. The following example uses sits_reduce_imbalance() with the Cerrado samples used in the previous chapter. We generate a balanced dataset where all classes have a minimum of 1000 and and a maximum of 1500 samples.\n\n# Reducing imbalances in the Cerrado dataset\nbalanced_samples &lt;- sits_reduce_imbalance(\n    samples = samples_cerrado_mod13q1_2bands,\n    n_samples_over = 1000,\n    n_samples_under = 1500,\n    multicores = 4)\n\n\n# Show summary of balanced samples\n# Some classes have more than 1500 samples due to the SOM map\n# Each label has between 10% and 6% of the full set\nsummary(balanced_samples)\n\n# A tibble: 12 × 3\n   label            count   prop\n   &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n 1 Dense_Woodland    1596 0.0974\n 2 Dunes             1000 0.0610\n 3 Fallow_Cotton     1000 0.0610\n 4 Millet_Cotton     1000 0.0610\n 5 Pasture           1592 0.0971\n 6 Rocky_Savanna     1476 0.0901\n 7 Savanna           1600 0.0976\n 8 Savanna_Parkland  1564 0.0954\n 9 Silviculture      1000 0.0610\n10 Soy_Corn          1588 0.0969\n11 Soy_Cotton        1568 0.0957\n12 Soy_Fallow        1404 0.0857\n\n\nTo assess the impact of reducing imbalance, we use the SOM cluster technique described in the previous chapter. In synthesis, SOM builds clusters out of the training data. Ideally, each cluster would be composed by a samples of a single class. Mixed clusters indicate possible confusion between samples of different classes. We fist build a SOM using sits_som_map() and then assess the results with sits_som_evaluate_clusters().\n\n# Clustering time series using SOM\nsom_cluster_bal &lt;- sits_som_map(\n    data = balanced_samples,\n    grid_xdim = 15,\n    grid_ydim = 15,\n    alpha = 1.0,\n    distance = \"dtw\",\n    rlen = 20,\n    mode = \"pbatch\")\n\n\n# Produce a tibble with a summary of the mixed labels\nsom_eval &lt;- sits_som_evaluate_cluster(som_cluster_bal)\n\n\n\nWarning: Removed 38 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\nFigure 15.1: Confusion by cluster for the balanced dataset\n\n\n\nAs shown in Figure 15.1, the balanced dataset shows less confusion per label than the unbalanced one. In this case, many classes that were confused with others in the original confusion map are now better represented. Reducing sample imbalance should be tried as an alternative to reducing the number of samples of the classes using SOM. In general, users should balance their training data for better performance."
  },
  {
    "objectID": "ts_balance.html#summary",
    "href": "ts_balance.html#summary",
    "title": "\n15  Reducing sample imbalance\n",
    "section": "\n15.4 Summary",
    "text": "15.4 Summary\nReducing imbalance is an important method to improve quality of training data. As a general rule, users should work with balanced sets, since experiments with sits show an improvement of classification accuracy in almost all cases."
  },
  {
    "objectID": "ts_balance.html#references",
    "href": "ts_balance.html#references",
    "title": "\n15  Reducing sample imbalance\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nJ. M. Johnson and T. M. Khoshgoftaar, “Survey on deep learning with class imbalance,” Journal of Big Data, vol. 6, no. 1, p. 27, 2019, doi: 10.1186/s40537-019-0192-5.\n\n\n[2] \nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, no. 1, pp. 321–357, 2002."
  },
  {
    "objectID": "classification.html#considerations-on-model-choice",
    "href": "classification.html#considerations-on-model-choice",
    "title": "Machine learning for image time series",
    "section": "Considerations on model choice",
    "text": "Considerations on model choice\nThe results should not be taken as an indication of which method performs better. The most crucial factor for achieving a good result is the quality of the training data [2]. Experience shows that classification quality depends on the training samples and how well the model matches these samples. For examples of ML for classifying large areas, please see the papers by the authors [3]–[6].\nIn the specific case of satellite image time series, Russwurm et al. present a comparative study between seven deep neural networks for the classification of agricultural crops, using random forest as a baseline [7]. The data is composed of Sentinel-2 images over Britanny, France. Their results indicate a slight difference between the best model (attention-based transformer model) over TempCNN and random forest. Attention-based models obtain accuracy ranging from 80-81%, TempCNN gets 78-80%, and random forest obtains 78%. Based on this result and also on the authors’ experience, we make the following recommendations:\n\nRandom forest provides a good baseline for image time series classification and should be included in users’ assessments.\nXGBoost is a worthy alternative to random forest. In principle, XGBoost is more sensitive to data variations at the cost of possible overfitting.\nTempCNN is a reliable model with reasonable training time, which is close to the state-of-the-art in deep learning classifiers for image time series.\nAttention-based models (TAE and LightTAE) can achieve the best overall performance with well-designed and balanced training sets and hyperparameter tuning.\n\nThe best means of improving classification performance is to provide an accurate and reliable training dataset. Accuracy improvements resulting from using deep learning methods instead of random forest of xgboost are on the order of 3-5%, while gains when using good training data improve results by 10-30%. As a basic rule, make sure you have good quality samples before training and classification.\nIn the chapters that follow, we first present the sits machine learning algorithms, then show how to classify data cubes and how to smooth the classification results. We also discuss tuning of deep learning algorithms and methods for uncertainty assessment and ensemble prediction."
  },
  {
    "objectID": "classification.html#references",
    "href": "classification.html#references",
    "title": "Machine learning for image time series",
    "section": "References",
    "text": "References\n\n\n\n\n[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.\n\n\n[2] A. E. Maxwell, T. A. Warner, and F. Fang, “Implementation of machine-learning classification in remote sensing: An applied review,” International Journal of Remote Sensing, vol. 39, no. 9, pp. 2784–2817, 2018.\n\n\n[3] M. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007.\n\n\n[4] M. C. A. Picoli et al., “CBERS data cube: A powerful technology for mapping and monitoring Brazilian biomes.” in ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2020, vol. V–3–2020, pp. 533–539, doi: 10.5194/isprs-annals-V-3-2020-533-2020.\n\n\n[5] R. Simoes et al., “Land use and cover maps for Mato Grosso State in Brazil from 2001 to 2017,” Scientific Data, vol. 7, no. 1, p. 34, 2020, doi: 10.1038/s41597-020-0371-4.\n\n\n[6] K. R. Ferreira et al., “Earth Observation Data Cubes for Brazil: Requirements, Methodology and Products,” Remote Sensing, vol. 12, no. 24, p. 4033, 2020, doi: 10.3390/rs12244033.\n\n\n[7] M. Rußwurm, C. Pelletier, M. Zollner, S. Lefèvre, and M. Körner, “BreizhCrops: A Time Series Dataset for Crop Type Mapping,” 2020, [Online]. Available: http://arxiv.org/abs/1905.11893."
  },
  {
    "objectID": "cl_machinelearning.html#data-used-in-this-chapter",
    "href": "cl_machinelearning.html#data-used-in-this-chapter",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.1 Data used in this chapter",
    "text": "16.1 Data used in this chapter\nThe following examples show how to train machine learning methods and apply them to classify a single time series. We use the set samples_matogrosso_mod13q1, containing time series samples from the Brazilian Mato Grosso state obtained from the MODIS MOD13Q1 product. It has 1,892 samples and nine classes (Cerrado, Forest, Pasture, Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet). Each time series covers 12 months (23 data points) with six bands (NDVI, EVI, BLUE, RED, NIR, MIR). The samples are arranged along an agricultural year, starting in September and ending in August. The dataset was used in the paper “Big Earth observation time series analysis for monitoring Brazilian agriculture” [1], being available in the R package sitsdata."
  },
  {
    "objectID": "cl_machinelearning.html#common-interface-to-machine-learning-and-deep-learning-models",
    "href": "cl_machinelearning.html#common-interface-to-machine-learning-and-deep-learning-models",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.2 Common interface to machine learning and deep learning models",
    "text": "16.2 Common interface to machine learning and deep learning models\nThe sits_train() function provides a standard interface to all machine learning models. This function takes two mandatory parameters: the training data (samples) and the ML algorithm (ml_method). After the model is estimated, it can classify individual time series or data cubes with sits_classify(). In what follows, we show how to apply each method to classify a single time series. Then, in Chapter Image classification in data cubes, we discuss how to classify data cubes.\nSince sits is aimed at remote sensing users who are not machine learning experts, it provides a set of default values for all classification models. These settings have been chosen based on testing by the authors. Nevertheless, users can control all parameters for each model. Novice users can rely on the default values, while experienced ones can fine-tune model parameters to meet their needs. Model tuning is discussed at the end of this Chapter.\nWhen a set of time series organized as tibble is taken as input to the classifier, the result is the same tibble with one additional column (predicted), which contains the information on the labels assigned for each interval. The results can be shown in text format using the function sits_show_prediction() or graphically using plot()."
  },
  {
    "objectID": "cl_machinelearning.html#random-forest",
    "href": "cl_machinelearning.html#random-forest",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.3 Random forest",
    "text": "16.3 Random forest\nRandom forest is a machine learning algorithm that uses an ensemble learning method for classification tasks. The algorithm consists of multiple decision trees, each trained on a different subset of the training data and with a different subset of features. To make a prediction, each decision tree in the forest independently classifies the input data. The final prediction is made based on the majority vote of all the decision trees. The randomness in the algorithm comes from the random subsets of data and features used to train each decision tree, which helps to reduce overfitting and improve the accuracy of the model. This classifier measures the importance of each feature in the classification task, which can be helpful in feature selection and data visualization. For an in-depth discussion of the robustness of random forest method for satellite image time series classification, please see Pelletier et al [2].\n\n\n\n\nFigure 16.1: Random forest algorithm (source: Venkata Jagannath in Wikipedia).\n\n\n\nsits provides sits_rfor(), which uses the R randomForest package [3]; its main parameter is num_trees, which is the number of trees to grow with a default value of 100. The model can be visualized using plot().\n\nset.seed(290356)\n\n\n# Train the Mato Grosso samples with random forest model\nrfor_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_rfor(num_trees = 100))\n# Plot the most important variables of the model\nplot(rfor_model)\n\n\n\n\n\nFigure 16.2: Most important variables in random forest model.\n\n\n\nThe most important explanatory variables are the NIR (near infrared) band on date 17 (2007-05-25) and the MIR (middle infrared) band on date 22 (2007-08-13). The NIR value at the end of May captures the growth of the second crop for double cropping classes. Values of the MIR band at the end of the period (late July to late August) capture bare soil signatures to distinguish between agricultural and natural classes. This corresponds to summertime when the ground is drier after harvesting crops.\n\n# Classify using random forest model and plot the result\npoint_class &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model  = rfor_model)\nplot(point_class, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.3: Classification of time series using random forest.\n\n\n\nThe result shows that the area started as a forest in 2000, was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2009 onwards. This behavior is consistent with expert evaluation of land change process in this region of Amazonia.\nRandom forest is robust to outliers and can deal with irrelevant inputs [4]. The method tends to overemphasize some variables because its performance tends to stabilize after part of the trees is grown [4]. In cases where abrupt change occurs, such as deforestation mapping, random forest (if properly trained) will emphasize the temporal instances and bands that capture such quick change."
  },
  {
    "objectID": "cl_machinelearning.html#support-vector-machine",
    "href": "cl_machinelearning.html#support-vector-machine",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.4 Support vector machine",
    "text": "16.4 Support vector machine\nThe support vector machine (SVM) classifier is a generalization of a linear classifier that finds an optimal separation hyperplane that minimizes misclassification [5]. Since a set of samples with \\(n\\) features defines an n-dimensional feature space, hyperplanes are linear \\({(n-1)}\\)-dimensional boundaries that define linear partitions in that space. If the classes are linearly separable on the feature space, there will be an optimal solution defined by the maximal margin hyperplane, which is the separating hyperplane that is farthest from the training observations [6]. The maximal margin is computed as the smallest distance from the observations to the hyperplane. The solution for the hyperplane coefficients depends only on the samples that define the maximum margin criteria, the so-called support vectors.\n\n\n\n\nFigure 16.4: Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. (source: Larhmam in Wikipedia).\n\n\n\nFor data that is not linearly separable, SVM includes kernel functions that map the original feature space into a higher dimensional space, providing nonlinear boundaries to the original feature space. Despite having a linear boundary on the enlarged feature space, the new classification model generally translates its hyperplane to a nonlinear boundary in the original attribute space. Kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space; thus, they improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been applied to classify remote sensing data [7].\nIn sits, SVM is implemented as a wrapper of e1071 R package that uses the LIBSVM implementation [8]. The sits package adopts the one-against-one method for multiclass classification. For a \\(q\\) class problem, this method creates \\({q(q-1)/2}\\) SVM binary models, one for each class pair combination, testing any unknown input vectors throughout all those models. A voting scheme computes the overall result.\nThe example below shows how to apply SVM to classify time series using default values. The main parameters are kernel, which controls whether to use a nonlinear transformation (default is radial), cost, which measures the punishment for wrongly-classified samples (default is 10), and cross, which sets the value of the k-fold cross validation (default is 10).\n\nset.seed(290356)\n\n\n# Train an SVM model\nsvm_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_svm())\n# Classify using the SVM model and plot the result\npoint_class &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model = svm_model)\n# Plot the result\nplot(point_class, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.5: Classification of time series using SVM.\n\n\n\nThe SVM classifier is less stable and less robust to outliers than the random forest method. In this example, it tends to misclassify some of the data. In 2008, it is likely that the correct land class was still Pasture rather than Soy_Millet as produced by the algorithm, while the Soy_Cotton class in 2012 is also inconsistent with the previous and latter classification of Soy_Corn."
  },
  {
    "objectID": "cl_machinelearning.html#extreme-gradient-boosting",
    "href": "cl_machinelearning.html#extreme-gradient-boosting",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.5 Extreme gradient boosting",
    "text": "16.5 Extreme gradient boosting\nXGBoost (eXtreme Gradient Boosting) [9] is an implementation of gradient boosted decision trees designed for speed and performance. It is an ensemble learning method, meaning it combines the predictions from multiple models to produce a final prediction. XGBoost builds trees one at a time, where each new tree helps to correct errors made by previously trained tree. Each tree builds a new model to correct the errors made by previous models. Using gradient descent, the algorithm iteratively adjusts the predictions of each tree by focusing on instances where previous trees made errors. Models are added sequentially until no further improvements can be made.\nAlthough random forest and boosting use trees for classification, there are significant differences. While random forest builds multiple decision trees in parallel and merges them together for a more accurate and stable prediction, XGBoost builds trees one at a time, where each new tree helps to correct errors made by previously trained tree. XGBoost is often preferred for its speed and performance, particularly on large datasets, and is well-suited for problems where precision is paramount. Random Forest, on the other hand, is simpler to implement, more interpretable, and can be more robust to overfitting, making it a good choice for general-purpose applications.\n\n\n\n\nFigure 16.6: Flow chart of XGBoost algorithm (source: Guo et al., Applied Sciences, 2020).\n\n\n\nThe boosting method starts from a weak predictor and then improves performance sequentially by fitting a better model at each iteration. It fits a simple classifier to the training data and uses the residuals of the fit to build a predictor. Typically, the base classifier is a regression tree. Although random forest and boosting use trees for classification, there are significant differences. The performance of random forest generally increases with the number of trees until it becomes stable. Boosting trees apply finer divisions over previous results to improve performance [4]. Some recent papers show that it outperforms random forest for remote sensing image classification [10]. However, this result is not generalizable since the quality of the training dataset controls actual performance.\nIn sits, the XGBoost method is implemented by the sits_xbgoost() function, based on XGBoost R package, and has five hyperparameters that require tuning. The sits_xbgoost() function takes the user choices as input to a cross-validation to determine suitable values for the predictor.\nThe learning rate eta varies from 0.0 to 1.0 and should be kept small (default is 0.3) to avoid overfitting. The minimum loss value gamma specifies the minimum reduction required to make a split. Its default is 0; increasing it makes the algorithm more conservative. The max_depth value controls the maximum depth of the trees. Increasing this value will make the model more complex and likely to overfit (default is 6). The subsample parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The nrounds parameter controls the maximum number of boosting interactions; its default is 100, which has proven to be enough in most cases. To follow the convergence of the algorithm, users can turn the verbose parameter on. In general, the results using the extreme gradient boosting algorithm are similar to the random forest method.\n\nset.seed(290356)\n\n\n# Train using  XGBoost\nxgb_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_xgboost(verbose = 0))\n# Classify using SVM model and plot the result\npoint_class_xgb &lt;- sits_classify(\n    data = point_mt_mod13q1, \n    ml_model = xgb_model)\nplot(point_class_xgb, bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.7: Classification of time series using XGBoost."
  },
  {
    "objectID": "cl_machinelearning.html#deep-learning-using-multilayer-perceptron",
    "href": "cl_machinelearning.html#deep-learning-using-multilayer-perceptron",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.6 Deep learning using multilayer perceptron",
    "text": "16.6 Deep learning using multilayer perceptron\nTo support deep learning methods, sits uses the torch R package, which takes the Facebook torch C++ library as a back-end. Machine learning algorithms that use the R torch package are similar to those developed using PyTorch. The simplest deep learning method is multilayer perceptron (MLP), which are feedforward artificial neural networks. An MLP consists of three kinds of nodes: an input layer, a set of hidden layers, and an output layer. The input layer has the same dimension as the number of features in the dataset. The hidden layers attempt to approximate the best classification function. The output layer decides which class should be assigned to the input.\nIn sits, MLP models can be built using sits_mlp(). Since there is no established model for generic classification of satellite image time series, designing MLP models requires parameter customization. The most important decisions are the number of layers in the model and the number of neurons per layer. These values are set by the layers parameter, which is a list of integer values. The size of the list is the number of layers, and each element indicates the number of nodes per layer.\nThe choice of the number of layers depends on the inherent separability of the dataset to be classified. For datasets where the classes have different signatures, a shallow model (with three layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. Models with many hidden layers may take a long time to train and may not converge. We suggest to start with three layers and test different options for the number of neurons per layer before increasing the number of layers. In our experience, using three to five layers is a reasonable compromise if the training data has a good quality. Further increase in the number of layers will not improve the model.\nMLP models also need to include the activation function. The activation function of a node defines the output of that node given an input or set of inputs. Following standard practices [11], we use the relu activation function.\nThe optimization method (optimizer) represents the gradient descent algorithm to be used. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function [12]. Since gradient descent plays a key role in deep learning model fitting, developing optimizers is an important topic of research [13]. Many optimizers have been proposed in the literature, and recent results are reviewed by Schmidt et al. [14]. The Adamw optimizer provides a good baseline and reliable performance for general deep learning applications [15]. By default, all deep learning algorithms in sits use Adamw.\nAnother relevant parameter is the list of dropout rates (dropout). Dropout is a technique for randomly dropping units from the neural network during training [16]. By randomly discarding some neurons, dropout reduces overfitting. Since a cascade of neural nets aims to improve learning as more data is acquired, discarding some neurons may seem like a waste of resources. In practice, dropout prevents an early convergence to a local minimum [11]. We suggest users experiment with different dropout rates, starting from small values (10-30%) and increasing as required.\nThe following example shows how to use sits_mlp(). The default parameters have been chosen based on a modified version of [17], which proposes using multilayer perceptron as a baseline for time series classification. These parameters are: (a) Three layers with 512 neurons each, specified by the parameter layers; (b) Using the “relu” activation function; (c) dropout rates of 40%, 30%, and 20% for the layers; (d) the “optimizer_adamw” as optimizer (default value); (e) a number of training steps (epochs) of 100; (f) a batch_size of 64, which indicates how many time series are used for input at a given step; and (g) a validation percentage of 20%, which means 20% of the samples will be randomly set aside for validation.\nTo simplify the output, the verbose option has been turned off. After the model has been generated, we plot its training history.\n\nset.seed(290356)\n# Train using an MLP model\n# This is an example of how to set parameters\n# First-time users should test default options first\nmlp_model &lt;- sits_train(\n    samples = samples_matogrosso_mod13q1, \n    ml_method = sits_mlp(\n        optimizer        = torch::optim_adamw, \n        layers           = c(512, 512, 512),\n        dropout_rates    = c(0.40, 0.30, 0.20),\n        epochs           = 80,\n        batch_size       = 64,\n        verbose          = FALSE,\n        validation_split = 0.2))\n# Show training evolution\nplot(mlp_model)\n\n\n\n\n\nFigure 16.8: Evolution of training accuracy of MLP model.\n\n\n\nThen, we classify a 16-year time series using the multilayer perceptron model.\n\n# Classify using MLP model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(mlp_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.9: Classification of time series using MLP.\n\n\n\nIn theory, multilayer perceptron model can capture more subtle changes than random forest and XGBoost In this specific case, the result is similar to theirs. Although the model mixes the Soy_Corn and Soy_Millet classes, the distinction between their temporal signatures is quite subtle. Also it suggests the need to improve the number of samples. In this example, the MLP model shows an increase in sensitivity compared to previous models. We recommend to compare different configurations since the MLP model is sensitive to changes in its parameters."
  },
  {
    "objectID": "cl_machinelearning.html#temporal-convolutional-neural-network-tempcnn",
    "href": "cl_machinelearning.html#temporal-convolutional-neural-network-tempcnn",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.7 Temporal Convolutional Neural Network (TempCNN)",
    "text": "16.7 Temporal Convolutional Neural Network (TempCNN)\nConvolutional neural networks (CNN) are deep learning methods that apply convolution filters (sliding windows) to the input data sequentially. The Temporal Convolutional Neural Network (TempCNN) is a neural network architecture specifically designed to process sequential data such as time series. In the case of time series, a 1D CNN applies a moving temporal window to the time series to produce another time series as the result of the convolution.\nThe TempCNN architecture for satellite image time series classification is proposed by Pelletier et al. [18]. It has three 1D convolutional layers and a final softmax layer for classification. The authors combine different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalization. In the TempCNN reference paper [18], the authors favourably compare their model with the Recurrent Neural Network proposed by Russwurm and Körner [19]. Figure 16.10 shows the architecture of the TempCNN model. TempCNN applies one-dimensional convolutions on the input sequence to capture temporal dependencies, allowing the network to learn long-term dependencies in the input sequence. Each layer of the model captures temporal dependencies at a different scale. Due to its multi-scale approach, TempCNN can capture complex temporal patterns in the data and produce accurate predictions.\n\n\n\n\nFigure 16.10: Structure of tempCNN architecture (source: [18]).\n\n\n\nThe function sits_tempcnn() implements the model. The first parameter is the optimizer used in the backpropagation phase for gradient descent. The default is adamw which is considered as a stable and reliable optimization function. The parameter cnn_layers controls the number of 1D-CNN layers and the size of the filters applied at each layer; the default values are three CNNs with 128 units. The parameter cnn_kernels indicates the size of the convolution kernels; the default is kernels of size 7. Activation for all 1D-CNN layers uses the “relu” function. The dropout rates for each 1D-CNN layer are controlled individually by the parameter cnn_dropout_rates. The validation_split controls the size of the test set relative to the full dataset. We recommend setting aside at least 20% of the samples for validation.\n\nset.seed(290356)\nlibrary(torch)\n# Train using tempCNN\ntempcnn_model &lt;- sits_train(\n    samples_matogrosso_mod13q1, \n    sits_tempcnn(\n        optimizer            = torch::optim_adamw,\n        cnn_layers           = c(256, 256, 256),\n        cnn_kernels          = c(7, 7, 7),\n        cnn_dropout_rates    = c(0.2, 0.2, 0.2),\n        epochs               = 80,\n        batch_size           = 64,\n        validation_split     = 0.2,\n        verbose              = FALSE))\n# Show training evolution\nplot(tempcnn_model)\n\n\n\n\n\nFigure 16.11: Training evolution of TempCNN model.\n\n\n\nUsing the TempCNN model, we classify a 16-year time series.\n\n# Classify using TempCNN model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(tempcnn_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.12: Classification of time series using TempCNN\n\n\n\nThe result has important differences from the previous ones. The TempCNN model indicates the Soy_Cotton class as the most likely one in 2004. While this result is possibly wrong, it shows that the time series for 2004 is different from those of Forest and Pasture classes. One possible explanation is that there was forest degradation in 2004, leading to a signature that is a mix of forest and bare soil. In this case, including forest degradation samples could improve the training data. In our experience, TempCNN models are a reliable way of classifying image time series [20]. Recent work which compares different models also provides evidence that TempCNN models have satisfactory behavior, especially in the case of crop classes [21]."
  },
  {
    "objectID": "cl_machinelearning.html#attention-based-models",
    "href": "cl_machinelearning.html#attention-based-models",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.8 Attention-based models",
    "text": "16.8 Attention-based models\nAttention-based deep learning models are a class of models that use a mechanism inspired by human attention to focus on specific parts of input during processing. These models have been shown to be effective for various tasks such as machine translation, image captioning, and speech recognition.\nThe basic idea behind attention-based models is to allow the model to selectively focus on different input parts at different times. This can be done by introducing a mechanism that assigns weights to each element of the input, indicating the relative importance of that element to the current processing step. The model can then use them to compute a weighted sum of the input. The results capture the model’s attention on specific parts of the input.\nAttention-based models have become one of the most used deep learning architectures for problems that involve sequential data inputs, e.g., text recognition and automatic translation. The general idea is that not all inputs are alike in applications such as language translation. Consider the English sentence “Look at all the lonely people”. A sound translation system needs to relate the words “look” and “people” as the key parts of this sentence to ensure such link is captured in the translation. A specific type of attention models, called transformers, enables the recognition of such complex relationships between input and output sequences [22].\nThe basic structure of transformers is the same as other neural network algorithms. They have an encoder that transforms textual input values into numerical vectors and a decoder that processes these vectors to provide suitable answers. The difference is how the values are handled internally. In an MLP, all inputs are treated equally at first; based on iterative matching of training and test data, the backpropagation technique feeds information back to the initial layers to identify the most suitable combination of inputs that produces the best output.\nConvolutional nets (CNN) combine input values that are close in time (1D) or space (2D) to produce higher-level information that helps to distinguish the different components of the input data. For text recognition, the initial choice of deep learning studies was to use recurrent neural networks (RNN) that handle input sequences.\nHowever, neither MLPs, CNNs, or RNNs have been able to capture the structure of complex inputs such as natural language. The success of transformer-based solutions accounts for substantial improvements in natural language processing.\nThe two main differences between transformer models and other algorithms are positional encoding and self-attention. Positional encoding assigns an index to each input value, ensuring that the relative locations of the inputs are maintained throughout the learning and processing phases. Self-attention compares every word in a sentence to every other word in the same sentence, including itself. In this way, it learns contextual information about the relation between the words. This conception has been validated in large language models such as BERT [23] and GPT-3 [24].\nThe application of attention-based models for satellite image time series analysis is proposed by Garnot et al. [25] and Russwurm and Körner [21]. A self-attention network can learn to focus on specific time steps and image features most relevant for distinguishing between different classes. The algorithm tries to identify which combination of individual temporal observations is most relevant to identify each class. For example, crop identification will use observations that capture the onset of the growing season, the date of maximum growth, and the end of the growing season. In the case of deforestation, the algorithm tries to identify the dates when the forest is being cut. Attention-based models are a means to identify events that characterize each land class.\nThe first model proposed by Garnot et al. is a full transformer-based model [25]. Considering that image time series classification is easier than natural language processing, Garnot et al. also propose a simplified version of the full transformer model [26]. This simpler model uses a reduced way to compute the attention matrix, reducing time for training and classification without loss of quality of the result.\nIn sits, the full transformer-based model proposed by Garnot et al. [25] is implemented using sits_tae(). The default parameters are those proposed by the authors. The default optimizer is optim_adamw, as also used in the sits_tempcnn() function.\n\n# Train a machine learning model using TAE\ntae_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_tae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = torch::optim_adamw,\n                          validation_split     = 0.2,\n                          verbose              = FALSE))\n# Show training evolution\nplot(tae_model)\n\n\n\n\n\nFigure 16.13: Training evolution of Temporal Self-Attention model.\n\n\n\nThen, we classify a 16-year time series using the TAE model.\n\n# Classify using DL model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(tae_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.14: Classification of time series using TAE.\n\n\n\nGarnot and co-authors also proposed the Lightweight Temporal Self-Attention Encoder (LTAE) [26], which the authors claim can achieve high classification accuracy with fewer parameters compared to other neural network models. It is a good choice for applications where computational resources are limited. The sits_lighttae() function implements this algorithm. The most important parameter to be set is the learning rate lr. Values ranging from 0.001 to 0.005 should produce good results. See also the section below on model tuning.\n\n# Train a machine learning model using TAE\nltae_model &lt;- sits_train(samples_matogrosso_mod13q1, \n                       sits_lighttae(\n                          epochs               = 80,\n                          batch_size           = 64,\n                          optimizer            = torch::optim_adamw,\n                          opt_hparams = list(lr = 0.001),\n                          validation_split     = 0.2))\n# Show training evolution\nplot(ltae_model)\n\n\n\n\n\nFigure 16.15: Training evolution of Lightweight Temporal Self-Attention model.\n\n\n\nThen, we classify a 16-year time series using the LightTAE model.\n\n# Classify using DL model and plot the result\npoint_mt_mod13q1 |&gt;  \n    sits_classify(ltae_model) |&gt;  \n    plot(bands = c(\"NDVI\", \"EVI\"))\n\n\n\n\n\nFigure 16.16: Classification of time series using LightTAE.\n\n\n\nThe behaviour of both sits_tae() and sits_lighttae() is similar to that of sits_tempcnn(). It points out the possible need for more classes and training data to better represent the transition period between 2004 and 2010. One possibility is that the training data associated with the Pasture class is only consistent with the time series between the years 2005 to 2008. However, the transition from Forest to Pasture in 2004 and from Pasture to Agriculture in 2009-2010 is subject to uncertainty since the classifiers do not agree on the resulting classes. In general, deep learning temporal-aware models are more sensitive to class variability than random forest and extreme gradient boosters."
  },
  {
    "objectID": "cl_machinelearning.html#summary",
    "href": "cl_machinelearning.html#summary",
    "title": "\n16  Machine learning algorithms\n",
    "section": "\n16.9 Summary",
    "text": "16.9 Summary\nIn this chapter, we present a basic description of the machine learning algorithms available in sits. The basic distinction is between time-sensitive algorithms such as LightTAE and TempCNN and those who do not consider temporal order of values, such as random forests. In practice, we suggest that users take random forests as their baseline and then use LightTAE or TempCNN to try to to improve classification accuracy."
  },
  {
    "objectID": "cl_machinelearning.html#references",
    "href": "cl_machinelearning.html#references",
    "title": "\n16  Machine learning algorithms\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nM. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007.\n\n\n[2] \nC. Pelletier, S. Valero, J. Inglada, N. Champion, and G. Dedieu, “Assessing the robustness of Random Forests to map land cover with high resolution satellite image time series over large areas,” Remote Sensing of Environment, vol. 187, pp. 156–168, 2016, doi: 10.1016/j.rse.2016.10.010.\n\n\n[3] \nJ. S. Wright et al., “Rainforest-initiated wet season onset over the southern Amazon,” Proceedings of the National Academy of Sciences, 2017, doi: 10.1073/pnas.1621516114.\n\n\n[4] \nT. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Data Mining, Inference, and Prediction. New York: Springer, 2009.\n\n\n[5] \nC. Cortes and V. Vapnik, “Support-vector networks,” Machine learning, vol. 20, no. 3, pp. 273–297, 1995.\n\n\n[6] \nG. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning: With Applications in R. New York, EUA: Springer, 2013.\n\n\n[7] \nG. Mountrakis, J. Im, and C. Ogole, “Support vector machines in remote sensing: A review,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 66, no. 3, pp. 247–259, 2011.\n\n\n[8] \nC.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector machines,” ACM transactions on intelligent systems and technology (TIST), vol. 2, no. 3, p. 27, 2011.\n\n\n[9] \nT. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 785–794, doi: 10.1145/2939672.2939785.\n\n\n[10] \nH. Jafarzadeh, M. Mahdianpari, E. Gill, F. Mohammadimanesh, and S. Homayouni, “Bagging and Boosting Ensemble Classifiers for Classification of Multispectral, Hyperspectral and PolSAR Data: A Comparative Evaluation,” Remote Sensing, vol. 13, no. 21, p. 4405, 2021, doi: 10.3390/rs13214405.\n\n\n[11] \nI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.\n\n\n[12] \nS. Ruder, “An overview of gradient descent optimization algorithms,” CoRR, vol. abs/1609.04747, 2016, [Online]. Available: http://arxiv.org/abs/1609.04747.\n\n\n[13] \nL. Bottou, F. E. Curtis, and J. Nocedal, “Optimization Methods for Large-Scale Machine Learning,” SIAM Review, vol. 60, no. 2, pp. 223–311, 2018, doi: 10.1137/16M1080173.\n\n\n[14] \nR. M. Schmidt, F. Schneider, and P. Hennig, “Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers,” in Proceedings of the 38th International Conference on Machine Learning, 2021, pp. 9367–9376, [Online]. Available: https://proceedings.mlr.press/v139/schmidt21a.html.\n\n\n[15] \nD. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization.” arXiv, 2017, doi: 10.48550/arXiv.1412.6980.\n\n\n[16] \nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n\n\n[17] \nZ. Wang, W. Yan, and T. Oates, “Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline,” 2017.\n\n\n[18] \nC. Pelletier, G. I. Webb, and F. Petitjean, “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,” Remote Sensing, vol. 11, no. 5, 2019.\n\n\n[19] \nM. Russwurm and M. Korner, “Multi-temporal land cover classification with sequential recurrent encoders,” ISPRS International Journal of Geo-Information, vol. 7, no. 4, p. 129, 2018.\n\n\n[20] \nR. Simoes et al., “Satellite Image Time Series Analysis for Big Earth Observation Data,” Remote Sensing, vol. 13, no. 13, p. 2428, 2021, doi: 10.3390/rs13132428.\n\n\n[21] \nM. Rußwurm, C. Pelletier, M. Zollner, S. Lefèvre, and M. Körner, “BreizhCrops: A Time Series Dataset for Crop Type Mapping,” 2020, [Online]. Available: http://arxiv.org/abs/1905.11893.\n\n\n[22] \nA. Vaswani et al., “Attention is All you Need,” in Advances in Neural Information Processing Systems, 2017, vol. 30, [Online]. Available: https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n\n\n[23] \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” arXiv, 2019, doi: 10.48550/arXiv.1810.04805.\n\n\n[24] \nT. B. Brown et al., “Language Models are Few-Shot Learners.” arXiv, 2020, doi: 10.48550/arXiv.2005.14165.\n\n\n[25] \nV. Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12322–12331, doi: 10.1109/CVPR42600.2020.01234.\n\n\n[26] \nV. S. F. Garnot and L. Landrieu, “Lightweight Temporal Self-attention for Classifying Satellite Images Time Series,” in Advanced Analytics and Learning on Temporal Data, 2020, pp. 171–181."
  },
  {
    "objectID": "cl_tuning.html#introduction",
    "href": "cl_tuning.html#introduction",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.1 Introduction",
    "text": "17.1 Introduction\nModel tuning is the process of selecting the best set of hyperparameters for a specific application. When using deep learning models for image classification, it is a highly recommended step to enable a better fit of the algorithm to the training data. Hyperparameters are parameters of the model that are not learned during training but instead are set prior to training and affect the behavior of the model during training. Examples include the learning rate, batch size, number of epochs, number of hidden layers, number of neurons in each layer, activation functions, regularization parameters, and optimization algorithms.\nDeep learning model tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model on a given task. This is done by training and evaluating the model with different sets of hyperparameters to select the set that gives the best performance.\nDeep learning algorithms try to find the optimal point representing the best value of the prediction function that, given an input \\(X\\) of data points, predicts the result \\(Y\\). In our case, \\(X\\) is a multidimensional time series, and \\(Y\\) is a vector of probabilities for the possible output classes. For complex situations, the best prediction function is time-consuming to estimate. For this reason, deep learning methods rely on gradient descent methods to speed up predictions and converge faster than an exhaustive search [1]. All gradient descent methods use an optimization algorithm adjusted with hyperparameters such as the learning and regularization rates [2]. The learning rate controls the numerical step of the gradient descent function, and the regularization rate controls model overfitting. Adjusting these values to an optimal setting requires using model tuning methods."
  },
  {
    "objectID": "cl_tuning.html#how-sits-performs-model-tuning",
    "href": "cl_tuning.html#how-sits-performs-model-tuning",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.2 How SITS performs model tuning",
    "text": "17.2 How SITS performs model tuning\nTo reduce the learning curve, sits provides default values for all machine learning and deep learning methods, ensuring a reasonable baseline performance. However, refining model hyperparameters might be necessary, especially for more complex models such as sits_lighttae() or sits_tempcnn(). To that end, the package provides the sits_tuning() function.\nThe most straightforward approach to model tuning is to run a grid search; this involves defining a range for each hyperparameter and then testing all possible combinations. This approach leads to a combinatorial explosion and thus is not recommended. Instead, Bergstra and Bengio propose randomly chosen trials [3]. Their paper shows that randomized trials are more efficient than grid search trials, selecting adequate hyperparameters at a fraction of the computational cost. The sits_tuning() function follows Bergstra and Bengio by using a random search on the chosen hyperparameters.\nExperiments with image time series show that other optimizers may have better performance for the specific problem of land classification. For this reason, the authors developed the torchopt R package, which includes several recently proposed optimizers, including Madgrad [4], and Yogi [5]. Using the sits_tuning() function allows testing these and other optimizers available in torch and torch_opt packages.\nThe sits_tuning() function takes the following parameters:\n\n\nsamples: Training dataset to be used by the model.\n\nsamples_validation: Optional dataset containing time series to be used for validation. If missing, the next parameter will be used.\n\nvalidation_split: If samples_validation is not used, this parameter defines the proportion of time series in the training dataset to be used for validation (default is 20%).\n\nml_method(): Deep learning method (either sits_mlp(), sits_tempcnn(), sits_tae() or sits_lighttae()).\n\nparams: Defines the optimizer and its hyperparameters by calling sits_tuning_hparams(), as shown in the example below.\n\ntrials: Number of trials to run the random search.\n\nmulticores: Number of cores to be used for the procedure.\n\nprogress: Show a progress bar?\n\nThe sits_tuning_hparams() function inside sits_tuning() allows defining optimizers and their hyperparameters, including lr (learning rate), eps (controls numerical stability), and weight_decay (controls overfitting). The default values for eps and weight_decay in all sits deep learning functions are 1e-08 and 1e-06, respectively. The default lr for sits_lighttae() and sits_tempcnn() is 0.005.\nUsers have different ways to randomize the hyperparameters, including:\n\n\nchoice() (a list of options);\n\nuniform (a uniform distribution);\n\nrandint (random integers from a uniform distribution);\n\nnormal(mean, sd) (normal distribution);\n\nbeta(shape1, shape2) (beta distribution);\n\nloguniform(max, min) (loguniform distribution).\n\nWe suggest to use the log-uniform distribution to search over a wide range of values that span several orders of magnitude. This is common for hyperparameters like learning rates, which can vary from very small values (e.g., 0.0001) to larger values (e.g., 1.0) in a logarithmic manner. By default, sits_tuning() uses a loguniform distribution between 10^-2 and 10^-4 for the learning rate and the same distribution between 10^-2 and 10^-8 for the weight decay."
  },
  {
    "objectID": "cl_tuning.html#tuning-a-lighttae-model",
    "href": "cl_tuning.html#tuning-a-lighttae-model",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.3 Tuning a LightTAE model",
    "text": "17.3 Tuning a LightTAE model\nOur fist example is tuning a Lightweight Temporal Attention Enconder model [6] on the MOD13Q1 dataset for the state of MatoGrosso. To recall, this data set contains time series samples from the Brazilian Mato Grosso state obtained from the MODIS MOD13Q1 product. It has 1,892 samples and nine classes (Cerrado, Forest, Pasture, Soy_Corn, Soy_Cotton, Soy_Fallow, Soy_Millet) [7] and is available in the R package sitsdata.\n\ntuned_mt &lt;- sits_tuning(\n     samples = samples_matogrosso_mod13q1,\n     ml_method = sits_lighttae(),\n     params = sits_tuning_hparams(\n         optimizer = torch::optim_adamw,\n         opt_hparams = list(\n             lr = loguniform(10^-2, 10^-4),\n             weight_decay = loguniform(10^-2, 10^-8)\n             )\n         ),\n     trials = 40,\n     multicores = 6,\n     progress = FALSE)\n\nThe result is a tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The best results obtain accuracy values between 0.978 and 0.970, as shown below. The best result is obtained by a learning rate of 0.0013 and a weight decay of 3.73e-07. The worst result has an accuracy of 0.891, which shows the importance of the tuning procedure.\n\n# Obtain accuracy, kappa, lr, and weight decay for the 5 best results\n# Hyperparameters are organized as a list\nhparams_5 &lt;- tuned_mt[1:5,]$opt_hparams\n# Extract learning rate and weight decay from the list\nlr_5 &lt;- purrr::map_dbl(hparams_5, function(h) h$lr)\nwd_5 &lt;- purrr::map_dbl(hparams_5, function(h) h$weight_decay)\n\n# Create a tibble to display the results\nbest_5 &lt;- tibble::tibble(\n    accuracy = tuned_mt[1:5,]$accuracy,\n    kappa = tuned_mt[1:5,]$kappa,\n    lr    = lr_5,\n    weight_decay = wd_5)\n# Print the best five combination of hyperparameters\nbest_5\n\n# A tibble: 5 × 4\n  accuracy kappa       lr weight_decay\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n1    0.978 0.974 0.00136  0.000000373 \n2    0.975 0.970 0.00269  0.0000000861\n3    0.973 0.967 0.00162  0.00218     \n4    0.970 0.964 0.000378 0.00000868  \n5    0.970 0.964 0.00198  0.00000275"
  },
  {
    "objectID": "cl_tuning.html#tuning-a-tempcnn-model",
    "href": "cl_tuning.html#tuning-a-tempcnn-model",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.4 Tuning a TempCNN model",
    "text": "17.4 Tuning a TempCNN model\nIn the example, we use sits_tuning() to find good hyperparameters to train the sits_tempcnn() algorithm for a dataset for measuring deforestation in Rondonia (samples_deforestation_rondonia) available in package sitsdata. This dataset consists of 6,007 samples collected from Sentinel-2 images covering the state of Rondonia. There are nine classes: Clear_Cut_Bare_Soil, Clear_Cut_Burned_Area, Mountainside_Forest, Forest, Riparian_Forest, Clear_Cut_Vegetation, Water, Wetland, and Seasonally_Flooded. Each time series contains values from Sentinel-2/2A bands B02, B03, B04, B05, B06, B07, B8A, B08, B11 and B12, from 2022-01-05 to 2022-12-23 in 16-day intervals. The samples are intended to detect deforestation events and have been collected by remote sensing experts using visual interpretation.\nThe hyperparameters for the sits_tempcnn() method include the size of the layers, convolution kernels, dropout rates, learning rate, and weight decay. Please refer to the description of the Temporal CNN algorithm in Chapter Machine learning for data cubes.\n\ntuned_tempcnn &lt;- sits_tuning(\n  samples = samples_deforestation_rondonia,\n  ml_method = sits_tempcnn(),\n  params = sits_tuning_hparams(\n    cnn_layers = choice(c(256, 256, 256), c(128, 128, 128), c(64, 64, 64)),\n    cnn_kernels = choice(c(3, 3, 3), c(5, 5, 5), c(7, 7, 7)),\n    cnn_dropout_rates = choice(c(0.15, 0.15, 0.15), c(0.2, 0.2, 0.2),\n                               c(0.3, 0.3, 0.3), c(0.4, 0.4, 0.4)),\n    optimizer = torch::optim_adamw,\n    opt_hparams = list(\n      lr = loguniform(10^-2, 10^-4),\n      weight_decay = loguniform(10^-2, 10^-8)\n    )\n  ),\n  trials = 50,\n  multicores = 4\n)\n\nThe result of sits_tuning() is tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The five best results obtain accuracy values between 0.939 and 0.908, as shown below. The best result is obtained by a learning rate of 3.76e-04 and a weight decay of 1.5e-04, and three CNN layers of size 256, kernel size of 5, and dropout rates of 0.2.\n\n# Obtain accuracy, kappa, cnn_layers, cnn_kernels, and cnn_dropout_rates the best result\ncnn_params &lt;- tuned_tempcnn[1,c(\"accuracy\", \"kappa\", \"cnn_layers\", \"cnn_kernels\", \"cnn_dropout_rates\"), ]\n# Learning rates and weight decay are organized as a list\nhparams_best &lt;- tuned_tempcnn[1,]$opt_hparams[[1]]\n# Extract learning rate and weight decay \nlr_wd &lt;- tibble::tibble(lr_best = hparams_best$lr, \n                        wd_best = hparams_best$weight_decay)\n# Print the best parameters \ndplyr::bind_cols(cnn_params, lr_wd)\n\n# A tibble: 1 × 7\n  accuracy kappa cnn_layers       cnn_kernels cnn_dropout_rates  lr_best wd_best\n     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n1    0.939 0.929 c(256, 256, 256) c(5, 5, 5)  c(0.2, 0.2, 0.2)  0.000376 1.53e-4"
  },
  {
    "objectID": "cl_tuning.html#summary",
    "href": "cl_tuning.html#summary",
    "title": "\n17  Deep learning model tuning\n",
    "section": "\n17.5 Summary",
    "text": "17.5 Summary\nFor large datasets, the tuning process is time-consuming. Despite this cost, it is recommended to achieve the best performance. In general, tuning hyperparameters for models such as sits_tempcnn() and sits_lighttae() will result in a slight performance improvement over the default parameters on overall accuracy. The performance gain will be stronger in the less well represented classes, where significant gains in producer’s and user’s accuracies are possible. When detecting change in less frequent classes, tuning can make a substantial difference in the results."
  },
  {
    "objectID": "cl_tuning.html#references",
    "href": "cl_tuning.html#references",
    "title": "\n17  Deep learning model tuning\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nY. Bengio, “Practical recommendations for gradient-based training of deep architectures,” arXiv:1206.5533 [cs], 2012, [Online]. Available: http://arxiv.org/abs/1206.5533.\n\n\n[2] \nR. M. Schmidt, F. Schneider, and P. Hennig, “Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers,” in Proceedings of the 38th International Conference on Machine Learning, 2021, pp. 9367–9376, [Online]. Available: https://proceedings.mlr.press/v139/schmidt21a.html.\n\n\n[3] \nJ. Bergstra and Y. Bengio, “Random Search for Hyper-Parameter Optimization,” Journal of Machine Learning Research, vol. 13, no. 10, pp. 281–305, 2012, [Online]. Available: http://jmlr.org/papers/v13/bergstra12a.html.\n\n\n[4] \nA. Defazio and S. Jelassi, “Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization.” arXiv, 2021, doi: 10.48550/arXiv.2101.11075.\n\n\n[5] \nM. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar, “Adaptive Methods for Nonconvex Optimization,” in Advances in Neural Information Processing Systems, 2018, vol. 31, [Online]. Available: https://proceedings.neurips.cc/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html.\n\n\n[6] \nV. S. F. Garnot and L. Landrieu, “Lightweight Temporal Self-attention for Classifying Satellite Images Time Series,” in Advanced Analytics and Learning on Temporal Data, 2020, pp. 171–181.\n\n\n[7] \nM. Picoli et al., “Big earth observation time series analysis for monitoring Brazilian agriculture,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 145, pp. 328–339, 2018, doi: 10.1016/j.isprsjprs.2018.08.007."
  },
  {
    "objectID": "cl_rasterclassification.html#data-cube-for-case-study",
    "href": "cl_rasterclassification.html#data-cube-for-case-study",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "\n18.1 Data cube for case study",
    "text": "18.1 Data cube for case study\nThe examples of this chapter use a pre-built data cube of Sentinel-2 images, available in the package sitsdata. These images are from the SENTINEL-2-L2A collection in Microsoft Planetary Computer (MPC). The data consists of bands BO2, B8A, and B11, and indexes NDVI, EVI and NBR in a small area of \\(1200 \\times 1200\\) pixels in the state of Rondonia. As explained in Chapter Earth observation data cubes, we need to inform sits how to parse these file names to obtain tile, date, and band information. Image files are named according to the convention “satellite_sensor_tile_band_date” (e.g., SENTINEL-2_MSI_20LKP_BO2_2020_06_04.tif) which is the default format in sits.\n\n# Files are available in a local directory \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR/\", \n                        package = \"sitsdata\")\n# Read data cube\nrondonia_20LMR &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir\n)\n\n# Plot the cube\nplot(rondonia_20LMR, date = \"2022-07-16\", band = \"NDVI\")\n\n\n\n\n\nFigure 18.1: Color composite image of the cube for date 2022-07-16."
  },
  {
    "objectID": "cl_rasterclassification.html#training-data-for-the-case-study",
    "href": "cl_rasterclassification.html#training-data-for-the-case-study",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "\n18.2 Training data for the case study",
    "text": "18.2 Training data for the case study\nThis case study uses the training dataset samples_deforestation_rondonia, available in package sitsdata. This dataset consists of 6007 samples collected from Sentinel-2 images covering the state of Rondonia. There are nine classes: Clear_Cut_Bare_Soil, Clear_Cut_Burned_Area, Mountainside_Forest, Forest, Riparian_Forest, Clear_Cut_Vegetation, Water, Wetland, and Seasonally_Flooded. Each time series contains values from Sentinel-2/2A bands B02, B03, B04, B05, B06, B07, B8A, B08, B11 and B12, from 2022-01-05 to 2022-12-23 in 16-day intervals. The samples are intended to detect deforestation events and have been collected by remote sensing experts using visual interpretation.\n\nlibrary(sitsdata)\n# Obtain the samples \ndata(\"samples_deforestation_rondonia\")\n# Show the contents of the samples\nsummary(samples_deforestation_rondonia)\n\n# A tibble: 9 × 3\n  label                 count   prop\n  &lt;chr&gt;                 &lt;int&gt;  &lt;dbl&gt;\n1 Clear_Cut_Bare_Soil     944 0.157 \n2 Clear_Cut_Burned_Area   983 0.164 \n3 Clear_Cut_Vegetation    603 0.100 \n4 Forest                  964 0.160 \n5 Mountainside_Forest     211 0.0351\n6 Riparian_Forest        1247 0.208 \n7 Seasonally_Flooded      731 0.122 \n8 Water                   109 0.0181\n9 Wetland                 215 0.0358\n\n\nIt is helpful to plot the basic patterns associated with the samples to understand the training set better. The function sits_patterns() uses a generalized additive model (GAM) to predict a smooth, idealized approximation to the time series associated with each class for all bands. Since the data cube used in the classification has 10 bands, we obtain the indexes NDVI, EVI, and NBR before showing the patterns.\n\n# Generate indexes \nsamples_deforestation_indices &lt;- samples_deforestation_rondonia |&gt; \n    sits_apply(NDVI = (B08 - B04)/(B08 + B04)) |&gt; \n    sits_apply(NBR = (B08 - B12) / (B08 + B12)) |&gt; \n    sits_apply(EVI = 2.5 * (B08 - B04) / ((B08 + 6.0 * B04 - 7.5 * B02) + 1.0)) \n\n# Generate and plot patterns\nsamples_deforestation_indices |&gt; \n    sits_select(bands = c(\"NDVI\", \"EVI\", \"NBR\")) |&gt; \n    sits_patterns() |&gt; \n    plot()\n\n\n\n\n\nFigure 18.2: Time series patterns for deforestation study in Rondonia.\n\n\n\nThe patterns show different temporal responses for the selected classes. They match the typical behavior of deforestation in the Amazon. In most cases, the forest is cut at the start of the dry season (May/June). At the end of the dry season, some clear-cut areas are burned to clean the remains; this action is reflected in the steep fall of the response of B11 values of burned area samples after August. The areas where native trees have been cut but some vegatation remain (“Clear_Cut_Vegetation”) have values in the B8A band that increase during the period."
  },
  {
    "objectID": "cl_rasterclassification.html#training-machine-learning-models",
    "href": "cl_rasterclassification.html#training-machine-learning-models",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "Training machine learning models",
    "text": "Training machine learning models\nThe next step is to train a machine learning model to illustrate CPU-based classification. We build a random forest model using sits_train() and then create a plot to find out what are the most important variables for the model.\n\n# set the seed to get the same result\nset.seed(03022024)\n\n# Train model using random forest model\nrfor_model &lt;- sits_train(\n  samples_deforestation_rondonia,\n  ml_method = sits_rfor()\n)\n# plot the model results\nplot(rfor_model)\n\n\n\n\n\nFigure 18.3: Most relevant variables of the random forest model.\n\n\n\nThe figure shows bands and dates represent relevant inflection points in the image time series."
  },
  {
    "objectID": "cl_rasterclassification.html#classification-of-machine-learning-models-in-cpus",
    "href": "cl_rasterclassification.html#classification-of-machine-learning-models-in-cpus",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "Classification of machine learning models in CPUs",
    "text": "Classification of machine learning models in CPUs\nBy default, all classification algorithms in sits use CPU-based parallel processing, done internally by the package. The algorithms are adaptable; the only requirement for users is to inform the configuration of their machines. To achieve efficiency, sits implements a fault-tolerant multitasking procedure, using a cluster of independent workers linked to a virtual machine. To avoid communication overhead, all large payloads are read and stored independently; direct interaction between the main process and the workers is kept at a minimum. Details of CPU-based parallel processing in sits can be found in the Technical annex.\nTo classify both data cubes and sets of time series, use sits_classify(), which uses parallel processing to speed up the performance, as described at the end of this Chapter. Its most relevant parameters are: (a) data, either a data cube or a set of time series; (b) ml_model, a trained model using one of the machine learning methods provided; (c) multicores, number of CPU cores that will be used for processing; (d) memsize, memory available for classification; (e) output_dir, directory where results will be stored; (f) version, for version control. To follow the processing steps, turn on the parameters verbose to print information and progress to get a progress bar. The classification result is a data cube with a set of probability layers, one for each output class. Each probability layer contains the model’s assessment of how likely each pixel belongs to the related class. The probability cube can be visualized with plot(). In this example, we show only the probabilities associated to label “Forest”.\n\n# Classify data cube to obtain a probability cube\nrondonia_20LMR_probs &lt;- sits_classify(\n    data     = rondonia_20LMR,\n    ml_model = rfor_model,\n    output_dir = tempdir_r,\n    version = \"rf-raster\",\n    multicores = 4,\n    memsize = 16)\n\nplot(rondonia_20LMR_probs, labels = \"Forest\", palette = \"YlGn\")\n\n\n\n\n\nFigure 18.4: Probability map for class Forest for random forest model.\n\n\n\nThe probability cube provides information on the output values of the algorithm for each class. Most probability maps contain outliers or misclassified pixels. The labeled map generated from the pixel-based time series classification method exhibits several misclassified pixels, which are small patches surrounded by a different class. This occurrence of outliers is a common issue that arises due to the inherent nature of this classification approach. Regardless of their resolution, mixed pixels are prevalent in images, and each class exhibits considerable data variability. As a result, these factors can lead to outliers that are more likely to be misclassified. To overcome this limitation, sits employs post-processing smoothing techniques that leverage the spatial context of the probability cubes to refine the results. These techniques will be discussed in the Chapter Bayesian smoothing for post-processing. In what follows, we will generate the smoothed cube to illustrate the procedure.\n\n# Smoothen a  probability cube\nrondonia_20LMR_bayes &lt;- sits_smooth(\n    cube    = rondonia_20LMR_probs,\n    output_dir = tempdir_r,\n    version = \"rf-raster\",\n    multicores = 4,\n    memsize = 16)\nplot(rondonia_20LMR_bayes, labels = c(\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n\nFigure 18.5: Smoothed probability map for class Forest.\n\n\n\nIn general, users should perform a post-processing smoothing after obtaining the probability maps in raster format. After the post-processing operation, we apply sits_label_classification() to obtain a map with the most likely class for each pixel. For each pixel, the sits_label_classification() function takes the label with highest probability and assigns it to the resulting map. The output is a labelled map with classes.\n\n# Generate a thematic map\nrondonia_20LMR_class &lt;- sits_label_classification(\n    cube = rondonia_20LMR_bayes,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r,\n    version = \"rf-raster\")\n\n# Plot the thematic map\nplot(rondonia_20LMR_class, legend_text_size = 0.7)\n\n\n\n\n\nFigure 18.6: Final map of deforestation obtained by random forest model."
  },
  {
    "objectID": "cl_rasterclassification.html#training-and-running-deep-learning-models",
    "href": "cl_rasterclassification.html#training-and-running-deep-learning-models",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "Training and running deep learning models",
    "text": "Training and running deep learning models\nThe next examples show how to run deep learning models in sits. The case study uses the Temporal CNN model [1], which is described in Chapter Machine learning for data cubes. Deep learning time series classification methods in sits, which include sits_tempcnn(), sits_mlp(), sits_lightae() and sits_tae(), are written using the torch package, which is an adaptation of pyTorch to the R environment. These algorithms can use a CUDA-compatible NVDIA GPU if one is available and has been properly configured. Please refer to the torch installation guide for details on how to configure torch to use GPUs. If no GPU is available, these algorithms will run on regular CPUs, using the same parallelization methods described in the traditional machine learning methods. Typically, there is a 10-fold performance increase when running torch based methods in GPUs relative to their processing time in GPU.\nWe take the same data cube and training data used in the previous examples and use a Temporal CNN method. The first step is to obtain a deep learning model using the sits_tempcnn() algorithm. We use the tuned parameters obtained in the example of the previous chapter.\n\ntcnn_model &lt;- sits_train(\n  samples_deforestation_rondonia,\n  sits_tempcnn(\n    cnn_layers = c(256, 256, 256),\n    cnn_kernels = c(5, 5, 5),\n    cnn_dropout_rates = c(0.2, 0.2, 0.2),\n    opt_hparams = list(\n      lr = 0.0004,\n      weight_decay = 0.00015\n    )\n  )\n)\n\nAfter training the model, we classify the data cube. If a GPU is available, users need to provide the additional parameter gpu_memory to sits_classify(). This information will be used by sits to optimize access to the GPU and speed up processing.\n\nrondonia_20LMR_probs_tcnn &lt;- sits_classify(\n    rondonia_20LMR,\n    ml_model = tcnn_model,\n    output_dir = tempdir_r,\n    version = \"tcnn-raster\",\n    gpu_memory = 16,\n    multicores = 6,\n    memsize = 24\n)\nplot(rondonia_20LMR_probs_tcnn, labels = c(\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n\nFigure 18.7: Probability map for class Forest for TempCNN model.\n\n\n\nAfter classification, we can smooth the probability cube. It is useful to compare the smoothed map for class Forest resulting from the TempCNN model with that produced by random forests. The TempCNN model tends to show more confidence in its predictions than the random forests one. This is a feature of the model more than an intrinsic property of the training data or the data cube.\n\n# Smoothen the probability map \nrondonia_20LMR_bayes_tcnn &lt;- sits_smooth(\n    rondonia_20LMR_probs_tcnn,\n    output_dir = \"./tempdir/chp9\",\n    version = \"tcnn-raster\",\n    multicores = 6,\n    memsize = 24\n)\nplot(rondonia_20LMR_bayes_tcnn, labels = c(\"Forest\"),  palette = \"YlGn\")\n\n\n\n\n\nFigure 18.8: Probability map for class Forest for TempCNN model.\n\n\n\nWe then label the resulting smoothed probabilities to obtain a classified map.\n\n# Obtain the final labelled map\nrondonia_20LMR_class_tcnn &lt;- sits_label_classification(\n    rondonia_20LMR_bayes_tcnn,\n    output_dir = \"./tempdir/chp9\",\n    version = \"tcnn-raster\",\n    multicores = 6,\n    memsize = 24\n)\nplot(rondonia_20LMR_class_tcnn)\n\n\n\n\n\nFigure 18.9: Probability map for class Forest for TempCNN model."
  },
  {
    "objectID": "cl_rasterclassification.html#summary",
    "href": "cl_rasterclassification.html#summary",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "\n18.3 Summary",
    "text": "18.3 Summary\nThis chapter presents a detailed example of how to train models and apply them to raster classification in sits. The procedure is simple and direct, using a workflow that combines sits_train(), sits_classify(), sits_smooth() and sits_label_classification(). For traditional machine learning models, such as random forests, the code is optimized for CPU processing and users will see good performance. In case of deep learning model, sits is optimized for GPU processing. The typical time for classifying a 10-band Sentinel-2 tile in a CPU using random forests is 20 minutes in a moderately sized machine (16 cores, 64 GB RAM). The same performance is expected when running deep learning models in a standard GPU with 16 GB memory."
  },
  {
    "objectID": "cl_rasterclassification.html#references",
    "href": "cl_rasterclassification.html#references",
    "title": "\n18  Classification of raster data cubes\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nC. Pelletier, G. I. Webb, and F. Petitjean, “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,” Remote Sensing, vol. 11, no. 5, 2019."
  },
  {
    "objectID": "cl_smoothing.html#introduction",
    "href": "cl_smoothing.html#introduction",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.1 Introduction",
    "text": "19.1 Introduction\nMachine learning algorithms rely on training samples that are derived from “pure” pixels, hand-picked by users to represent the desired output classes. Given the presence of mixed pixels in images regardless of resolution, and the considerable data variability within each class, these classifiers often produce results with outliers or misclassified pixels. Therefore, post-processing techniques have become crucial to refine the labels of a classified image [1]. Post-processing methods reduce salt-and-pepper and border effects, where single pixels or small groups of pixels are classified differently from their larger surrounding areas; these effects lead to visual discontinuity and inconsistency. By mitigating these errors and minimizing noise, post-processing improves the quality of the initial classification results, bringing a significant gain in the overall accuracy and interpretability of the final output [2].\nThe sits package uses a time-first, space-later approach. Since machine learning classifiers in sits are mostly pixel-based, it is necessary to complement them with spatial smoothing methods. These methods improve the accuracy of land classification by incorporating spatial and contextual information into the classification process. The smoothing method available in sits uses an Empirical Bayes approach, adjusted to the specific properties of land classification. The assumption is that class probabilities at the local level should be similar and provide the baseline for comparison with the pixel values produced by the classifier. Based on these two elements, Bayesian smoothing adjusts the probabilities for the pixels, considering a spatial dependence [3]."
  },
  {
    "objectID": "cl_smoothing.html#the-need-for-post-processing",
    "href": "cl_smoothing.html#the-need-for-post-processing",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.2 The need for post-processing",
    "text": "19.2 The need for post-processing\nThe main idea behind our post-processing method is that a pixel-based classification should take into account its neighborhood pixels. Consider the figure blow which shows a class assignment produced by a random forest algorithm on a image time series. The classified map has been produced by taking, for each pixel, the class of higher probability produced by the algorithm. The resulting map has many noisy areas with a high spatial variability of class assignments. This happens more frequently in two cases: (a) small clusters of pixels of one class inside a larger area of a different class; (b) transition zones between classes. In general, images of heterogeneous landscapes with high spatial variability have many mixed pixels, whose spectral response combines different types of land cover in a single ground resolved cell. For example, many pixels in the border between areas of classes Forest and Clear_Cut_Bare_Soil are wrongly assigned to the Clear_Cut_Vegetation class. This wrong assignment occurs because these pixels have a mixed response. Inside the ground cell captured by the sensor as a single pixel value, there are both trees and bare soil areas. Such results are undesirable and need to be corrected by post-processing.\n\n\n\n\nFigure 19.1: Detail of labelled map produced by pixel-based random forest without smoothing.\n\n\n\nTo maintain consistency and coherence in our class representations, we should minimise small variations or misclassifications. We incorporate spatial coherence as a post-processing step to accomplish this. The probabilities associated with each pixel will change based on statistical inference, which depends on the values for each neighbourhood. Using the recalculated probabilities for each pixel, we get a better version of the final classified map. Consider the figure below, which is the result of Bayesian smoothing on the random forest algorithm outcomes. The noisy border pixels between two large areas of the same class have been removed. We have also removed small clusters of pixels belonging to one class inside larger areas of other classes. The outcome is a more uniform map, like the ones created through visual interpretation or object-based analysis. Details like narrow vegetation corridors or small forest roads might be missing in the smoothed image. However, the improved spatial consistency of the final map compensates for such losses, due to the removal of misclassified pixels that have mixed spectral responses.\n\n\n\n\nFigure 19.2: Detail of labelled map produced by pixel-based random forest after smoothing."
  },
  {
    "objectID": "cl_smoothing.html#empirical-bayesian-estimation",
    "href": "cl_smoothing.html#empirical-bayesian-estimation",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.3 Empirical Bayesian estimation",
    "text": "19.3 Empirical Bayesian estimation\nThe Bayesian estimate is based on the probabilities produced by the classifiers. Let \\(p_{i,k} \\geq 0\\) be the prior probability of the \\(i\\)-th pixel belonging to class \\(k \\in \\{1, \\ldots, m\\}\\). The probabilities \\(p_{i,k}\\) are the classifier’s output, being subject to noise, outliers, and classification errors. Our estimation aims to remove these effects and obtain values that approximate the actual class probability better. We convert the class probability values \\(p_{i,k}\\) to log-odds values using the logit function, to transform probability values ranging from \\(0\\) to \\(1\\) to values from negative infinity to infinity. The conversion from probabilities logit values is helpful to support our assumption of normal distribution.\n\\[\n    x_{i,k} = \\log \\left(\\frac{p_{i,k}}{1 - p_{i,k}}\\right)\n\\] We assume that the logit of the prior probability of the pixels \\(i\\) associated to class \\(k\\) is described by a Gaussian distribution function\n\\[\\begin{equation}\nx_{i,k} = \\log\\left( \\frac{\\pi_{i,k}}{1-\\pi_{i,k}} \\right) \\sim N(m_{i,k}, s^2_{i,k})\n\\end{equation}\\]\nwhere \\(m_{i,k}\\) represents the local mean value and \\(s^2_{i,k}\\) the local class variance. The local mean and variance are computed based on the local neighborhood of the point. We express the likelihood as a conditional Gaussian distribution of the logit \\(x_{i,k}\\) of the observed values \\(p_{i,k}\\) over \\(\\mu_{i,k}\\): \\[\\begin{equation}\n(x_{i,k} | \\mu_{i,k}) = \\log(p_{i,k}/(1-p_{i,k})) \\sim N(\\mu_{i,k}, \\sigma^2).\n\\end{equation}\\]\nIn the above equation, \\(\\mu_{i,k}\\) is the posterior expected mean of the logit probability associated to the \\(i-th\\) pixel. The variance \\(\\sigma^2_{k}\\) will be estimated based on user expertise and taken as a hyperparameter to control the smoothness of the resulting estimate. The standard Bayesian updating [4] leads to the posterior distribution which can be expressed as a weighted mean\n\\[\n{E}[\\mu_{i,k} | x_{i,k}] =\n\\Biggl [ \\frac{s^2_{i,k}}{\\sigma^2_{k} +s^2_{i,k}} \\Biggr ] \\times\nx_{i,k} +\n\\Biggl [ \\frac{\\sigma^2_{k}}{\\sigma^2_{k} +s^2_{i,k}} \\Biggr ] \\times m_{i,k},\n\\] where:\n\n\n\\(x_{i,k}\\) is the logit value for pixel \\(i\\) and class \\(k\\).\n\n\\(m_{i,k}\\) is the average of logit values for pixels of class \\(k\\) in the neighborhood of pixel \\(i\\).\n\n\\(s^2_{i,k}\\) is the variance of logit values for pixels of class \\(k\\) in the neighborhood of pixel \\(i\\).\n\n\\(\\sigma^2_{k}\\) is an user-derived hyperparameter which estimates the variance for class \\(k\\), expressed in logits.\n\nThe above equation is a weighted average between the value \\(x_{i,k}\\) for the pixel and the mean \\(m_{i,k}\\) for the neighboring pixels. When the variance \\(s^2_{i,k}\\) for the neighbors is too high, the algorithm gives more weight to the pixel value \\(x_{i,k}\\). When class variance \\(\\sigma^2_k\\) increases, the results gives more weight to the neighborhood mean \\(m_{i,k}\\).\nBayesian smoothing for land classification assumes that image patches with similar characteristics have a dominant class. This dominant class has higher average probabilities and lower variance than other classes. A pixel assigned to a different class will likely exhibit high local variance in such regions. As a result, post-processing should adjust the class of this pixel to match the dominant class.\nThere is usually no prior information to specify \\(m_{i,k}\\) and \\(s^2_{i,k}\\). Because of that, we adopt an Empirical Bayes (EB) approach to obtain estimates of these prior parameters by considering the pixel neighborhood. However, using a standard symmetrical neighborhood for each pixel, based uniquely on the distance between locations, would not produce reasonable results for border pixels. For this reason, our EB estimates uses non-isotropic neighbourhood, as explained below."
  },
  {
    "objectID": "cl_smoothing.html#using-non-isotropic-neighborhoods",
    "href": "cl_smoothing.html#using-non-isotropic-neighborhoods",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.4 Using non-isotropic neighborhoods",
    "text": "19.4 Using non-isotropic neighborhoods\nThe fundamental idea behind Bayesian smoothing for land classification is that individual pixels area related to those close to it. Each pixel usually has the same class as most of its neighbors. These closeness relations are expressed in similar values of class probability. If we find a pixel assigned to Water surrounded by pixels labeled as Forest, such pixel may have been wrongly labelled. To check if the pixel has been mislabeled, we look at the class probabilities for the pixels and its neighbors. There are possible situations:\n\nThe outlier has a class probability distribution very different from its neighbors. For example, its probability for belonging to the Water class is 80% while that of being a Forest is 20%. If we also consider that Water pixels have a smaller variance, since water areas have a strong signal in multispectral images, our post-processing method will not change the pixel’s label.\nThe outlier has a class probability distribution similar from its neighbors. Consider a case where a pixel has a 47% probability for Water and 43% probability for Forest. This small difference indicates that we need to look at the neighborhood to improve the information produced by the classifier. In these cases, the post-processing estimate may change the pixel’s label.\n\nPixels in the border between two areas of different classes pose a challenge. Only some of their neighbors belong to the same class as the pixel. To address this issue, we employ a non-isotropic definition of a neighborhood to estimate the prior class distribution. For instance, consider a boundary pixel with a neighborhood defined by a 7 x 7 window, located along the border between Forest and Grassland classes. To estimate the prior probability of the pixel being labeled as a Forest, we should only take into account the neighbors on one side of the border that are likely to be correctly classified as Forest. Pixels on the opposite side of the border should be disregarded, since they are unlikely to belong to the same spatial process. In practice, we use only half of the pixels in the 7 x 7 window, opting for those that have a higher probability of being named as Forest. For the prior probability of the Grassland class, we reverse the selection and only consider those on the opposite side of the border.\nAlthough this choice of neighborhood may seem unconventional, it is consistent with the assumption of non-continuity of the spatial processes describing each class. A dense forest patch, for example, will have pixels with strong spatial autocorrelation for values of the Forest class; however, this spatial autocorrelation doesn’t extend across its border with other land classes."
  },
  {
    "objectID": "cl_smoothing.html#effect-of-the-hyperparameter",
    "href": "cl_smoothing.html#effect-of-the-hyperparameter",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.5 Effect of the hyperparameter",
    "text": "19.5 Effect of the hyperparameter\nThe parameter \\(\\sigma^2_k\\) controls the level of smoothness. If \\(\\sigma^2_k\\) is zero, the value \\({E}[\\mu_{i,k} | x_{i,k}]\\) will be equal to the pixel value \\(x_{i,k}\\). The parameter \\(\\sigma^2_k\\) expresses confidence in the inherent variability of the distribution of values of a class \\(k\\). The smaller the parameter \\(\\sigma^2_k\\), the more we trust the estimated probability values produced by the classifier for class \\(k\\). Conversely, higher values of \\(\\sigma^2_k\\) indicate lower confidence in the classifier outputs and improved confidence in the local averages.\nConsider the following two-class example. Take a pixel with probability \\(0.4\\) (logit \\(x_{i,1} = -0.4054\\)) for class A and probability \\(0.6\\) (logit \\(x_{i,2} = 0.4054\\)) for class B. Without post-processing, the pixel will be labeled as class B. Consider that the local average is \\(0.6\\) (logit \\(m_{i,1} = 0.4054\\)) for class A and \\(0.4\\) (logit \\(m_{i,2} = -0.4054\\)) for class B. This is a case of an outlier classified originally as class B in the midst of a set of class A pixels.\nGiven this situation, we apply the proposed method. Suppose the local variance of logits to be \\(s^2_{i,1} = 5\\) for class A and \\(s^2_{i,2} = 10\\) and for class B. This difference is to be expected if the local variability of class A is smaller than that of class B. To complete the estimate, we need to set the parameter \\(\\sigma^2_{k}\\), representing our belief in the variability of the probability values for each class.\nSetting \\(\\sigma^2_{k}\\) will be based on our confidence in the local variability of each class around pixel \\({i}\\). If we considered the local variability to be high, we can take both \\(\\sigma^2_1\\) for class A and \\(\\sigma^2_2\\) for class B to be both 10. In this case, the Bayesian estimated probability for class A is \\(0.52\\) and for class B is \\(0.48\\) and the pixel will be relabeled as being class A.\nBy contrast, if we consider local variability to be high If we set \\(\\sigma^2\\) to be 5 for both classes A and B, the Bayesian probability estimate will be \\(0.48\\) for class A and \\(0.52\\) for class B. In this case, the original class will be kept. Therefore, the result is sensitive to the subjective choice of the hyperparameter. In the example below, we will show how to use the local logit variance to set the appropriate values of \\(\\sigma^2\\)."
  },
  {
    "objectID": "cl_smoothing.html#running-bayesian-smoothing",
    "href": "cl_smoothing.html#running-bayesian-smoothing",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.6 Running Bayesian smoothing",
    "text": "19.6 Running Bayesian smoothing\nWe now show how to run Bayesian smoothing on a data cube covering an area of Sentinel-2 tile “20LLQ” in the period 2020-06-04 to 2021-08-26. The training data has six classes: (a) Forest for natural tropical forest; (b) Water for lakes and rivers; (c) “Wetlands” for areas where water covers the soil in the wet season; (d) Clear_Cut_Burned_Area for areas where fires cleared the land after tree removal; (e) Clear_Cut_Bare_Soil where the forest has been completely removed; (f) Clear_Cut_Vegetation where some vegetation remains after most trees have been removed. To simplify the example, our input is the probability cube generated by a random forest model. We recover the probability data cube and then plot the the results of the machine learning method for classes Forest, Clear_Cut_Bare_Soil, Clear_Cut_Vegetation, and Clear_Cut_Burned_Area.\n\n# define the classes of the probability cube\nlabels &lt;- c(\"1\" = \"Water\", \n            \"2\" = \"Clear_Cut_Burned_Area\", \n            \"3\" = \"Clear_Cut_Bare_Soil\",\n            \"4\" = \"Clear_Cut_Vegetation\", \n            \"5\" = \"Forest\", \n            \"6\" = \"Wetland\"\n)\n# directory where the data is stored \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LLQ/\", package = \"sitsdata\")\n# create a probability data cube from a file \nrondonia_20LLQ_probs &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = \"probs\",\n    labels = labels,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \n                   \"start_date\", \"end_date\", \"band\", \"version\"))\n\n\n# plot the probabilities for water and forest\nplot(rondonia_20LLQ_probs, \n     labels = c(\"Forest\", \"Clear_Cut_Bare_Soil\"))\n\n\n\nFigure 19.3: Probability map produced for classes Forest and Clear_Cut_Bare_Soil.\n\n\n\n\nplot(rondonia_20LLQ_probs, \n     labels = c(\"Clear_Cut_Vegetation\", \"Clear_Cut_Burned_Area\"))\n\n\n\nFigure 19.4: Probability map produced for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area.\n\n\n\nThe probability map for Forest shows high values associated with compact patches and linear stretches in riparian areas. Class Clear_Cut_Bare_Soil is mostly composed of dense areas of high probability whose geometrical boundaries result from forest cuts. Areas of class Clear_Cut_Vegetation are is less well-defined than the others; this is to be expected since this is a transitional class between a natural forest and areas of bare soil. Patches associated to class Clear_Cut_Burned_Area include both homogeneous areas of high probability and areas of mixed response. Since classes have different behaviours, the post-processing procedure should enable users to control how to handle outliers and border pixels of each class.\nThe next step is to show the labelled map resulting from the raw class probabilites. We produce a classification map by taking the class of higher probability to each pixel, without considering the spatial context. There are many places with the so-called “salt-and-pepper” effect which result from misclassified pixels. The non-smoothed labelled map shows the need for post-processing, since it contains a significant number of outliers and areas with mixed labelling.\n\n# Generate the thematic map\nrondonia_20LLQ_class &lt;- sits_label_classification(\n    cube = rondonia_20LLQ_probs,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r,\n    version = \"no_smooth\")\n\n\n# Plot the result\nplot(rondonia_20LLQ_class,\n     legend_text_size = 0.8, legend_position = \"outside\")\n\n\n\nFigure 19.5: Classified map without smoothing."
  },
  {
    "objectID": "cl_smoothing.html#assessing-the-local-logit-variance",
    "href": "cl_smoothing.html#assessing-the-local-logit-variance",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.7 Assessing the local logit variance",
    "text": "19.7 Assessing the local logit variance\nTo determine appropriate settings for the \\(\\sigma^2_{k}\\) hyperparameter for each class to perform Bayesian smoothing, it is useful to calculate the local logit variances for each class. For each pixel, we estimate the local variance \\(s^2_{i,k}\\) by considering the non-isotropic neighborhood. The local logit variances are estimated by sits_variance(); Its main parameters are: (a) cube, a probability data cube; (b) window_size, dimension of the local neighbourhood; (c) neigh_fraction, the percentage of pixels in the neighbourhood used to calculate the variance. The example below uses half of the pixels of a \\(7\\times 7\\) window to estimate the variance. The chosen pixels will be those with the highest probability pixels to be more representative of the actual class distribution. The output values are the logit variances in the vicinity of each pixel.\nThe choice of the \\(7 \\times 7\\) window size is a compromise between having enough values to estimate the parameters of a normal distribution and the need to capture local effects for class patches of small sizes. Classes such as Water tend to be spatially limited; a bigger window size could result in invalid values for their respective normal distributions.\n\n# calculate variance\nrondonia_20LLQ_var &lt;- sits_variance(\n    cube = rondonia_20LLQ_probs,\n    window_size = 7,\n    neigh_fraction = 0.50,\n    output_dir = tempdir_r, \n    multicores = 4,\n    memsize = 16\n)\n\n\n# Plot variance map for classes Forest and Clear_Cut_Bare_Soil\nplot(rondonia_20LLQ_var, \n     labels = c(\"Forest\", \"Clear_Cut_Bare_Soil\"), \n     palette = \"Spectral\", \n     rev = TRUE\n)\n\n\n\nFigure 19.6: Variance map for classes Forest and Clear_Cut_Bare_Soil.\n\n\n\n\n\n\n\nFigure 19.7: Variance map for classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area.\n\n\n\nComparing the variance maps with the probability maps, one sees that areas of high probability of classes Forest and Clear_Cut_Bare_Soil are mostly made of compact patches. Recall these are the two dominant classes in the area, and deforestation is a process that converts forest to bare soil. Many areas of high logit variance for these classes are related to border pixels which have a mixed response. Areas of large patches of high logit variance for these classes are associated to lower class probabilities and will not be relevant to the final result.\nBy contrast, the transitional classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area have a different spatial pattern of their probability and logit variance. The first has a high spatial variability, since pixels of this class arise when the forest has not been completely removed and there is some remaining vegetation after trees are cut. The extent of remaining vegetation after most trees have been removed is not uniform. For this reason, many areas of high local logit variance of class Clear_Cut_Vegetation are located in mixed patches inside pixels of class Forest and on the border between Forest and Clear_Cut_Bare_Soil. This situation is consistent with the earlier observation that transitional classes may appear as artificial effects of mixed pixels in borders between other classes.\nInstances of class ClearCut_Burned_Area arise following a forest fire. Most pixels of this class tend to form mid-sized to large spatial clusters, because of how forest fires start and propagate. It is desirable to preserve the contiguity of the burned areas and remove pixels of other classes inside these clusters. Isolated points of class ClearCut_Burned_Area can be removed without significant information loss.\nThe distinct patterns of these classes are measured quantitatively by the summary() function. For variance cubes, this function provides information on the logit variance values of the higher inter-quartile values.\n\n# get the summary of the logit variance\nsummary(rondonia_20LLQ_var)\n\n     Water Clear_Cut_Burned_Area Clear_Cut_Bare_Soil Clear_Cut_Vegetation\n75%   4.22                  0.25                0.39                 0.54\n80%   4.76                  0.31                0.49                 0.67\n85%   5.07                  0.38                0.63                 0.87\n90%   5.36                  0.50                0.91                 1.19\n95%   5.86                  0.75                1.77                 1.85\n100% 20.01                  8.28               11.48                 9.20\n     Forest Wetland\n75%    1.11  0.2900\n80%    1.72  0.3500\n85%    2.84  0.4300\n90%    4.22  0.6000\n95%    5.11  1.3605\n100%  18.80 10.4300\n\n\nThe summary statistics show that most local variance values are low, which is an expected result. Areas of low variance correspond to pixel neighborhoods of high logit values for one of the classes and low logit values for the others. High values of the local variances are relevant in areas of confusion between classes."
  },
  {
    "objectID": "cl_smoothing.html#using-the-variance-to-select-values-of-hyperparameters",
    "href": "cl_smoothing.html#using-the-variance-to-select-values-of-hyperparameters",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.8 Using the variance to select values of hyperparameters",
    "text": "19.8 Using the variance to select values of hyperparameters\nWe make the following recommendations for setting the \\(\\sigma^2_{k}\\) parameter, based on the local logit variance:\n\nSet the \\(\\sigma^2_{k}\\) parameter with high values (in the 95%-100% range) to increase the neighborhood influence compared with the probability values for each pixel. Such choice will produce denser spatial clusters and remove “salt-and-pepper” outliers.\nSet the \\(\\sigma^2_{k}\\) parameter with low values (in the 75%-80% range) to reduce the neighborhood influence, for classes that we want to preserve their original spatial shapes.\n\nConsider the case of forest areas and watersheds. If an expert wishes to have compact areas classified as forests without many outliers inside them, she will set the \\(\\sigma^2\\) parameter for the class Forest to be high. For comparison, to avoid that small watersheds with few similar neighbors being relabeled, it is advisable to avoid a strong influence of the neighbors, setting \\(\\sigma^2\\) to be as low as possible. In contrast, transitional classes such as Clear_Cut_Vegetation are likely to be associated with some outliers; use large \\(\\sigma^2_{k}\\) for them.\nTo remove the outliers and classification errors, we run a smoothing procedure with sits_smooth() with parameters: (a) cube, a probability cube produced by sits_classify(); (b) window_size, the local window to compute the neighborhood probabilities; (d) neigh_fraction, fraction of local neighbors used to calculate local statistics; (e) smoothness, a vector with estimates of the prior variance of each class; (f) multicores, number of CPU cores that will be used for processing; (g) memsize, memory available for classification; (h) output_dir, a directory where results will be stored; (i) version, for version control. The resulting cube can be visualized with plot().\nThe parameters window_size and neigh_fraction control how many pixels in a neighborhood the Bayesian estimator will use to calculate the local statistics. For example, setting window size to \\(7\\) and neigh_fraction to \\(0.50\\) (the defaults) ensures that \\(25\\) samples are used to estimate the local statistics. The smoothness values for the classes are set as recommended above.\n\n# Compute Bayesian smoothing\nrondonia_20LLQ_smooth &lt;- sits_smooth(\n    cube = rondonia_20LLQ_probs,\n    window_size = 7,\n    neigh_fraction = 0.50,\n    smoothness = c(\"Water\" = 5.0, \n                   \"Clear_Cut_Burned_Area\" = 9.5, \n                   \"Clear_Cut_Bare_Soil\" = 0.5, \n                   \"Clear_Cut_Vegetation\" =  15, \n                   \"Forest\" = 2.5, \n                   \"Wetland\" = 0.40),\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r\n)\n\n\n# Plot the result\nplot(rondonia_20LLQ_smooth, labels = c(\"Clear_Cut_Vegetation\", \"Forest\"))\n\n\n\nFigure 19.8: Probability maps after bayesian smoothing.\n\n\n\nBayesian smoothing has removed some of the local variability associated with misclassified pixels that differ from their neighbors, specially in the case of transitional classes such as Clear_Cut_Vegetation. The smoothing impact is best appreciated by comparing the labeled map produced without smoothing to the one that follows the procedure, as shown below.\n\n# Generate the thematic map\nrondonia_20LLQ_class_v2 &lt;- sits_label_classification(\n    cube = rondonia_20LLQ_smooth,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r,\n    version = \"smooth\")\n\n\n# plot the thematic map\nplot(rondonia_20LLQ_class_v2, legend_position = \"outside\")\n\n\n\nFigure 19.9: Final classification map after Bayesian smoothing with 7 x 7 window, using high smoothness values.\n\n\n\nIn the smoothed map, outliers inside forest areas and in the class borders have been removed. The salt-and-pepper effect associated to transitional classes has also been replaced by more coherent estimates. The smoothed map shown much improvements compared with the non-smoothed one."
  },
  {
    "objectID": "cl_smoothing.html#summary",
    "href": "cl_smoothing.html#summary",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "\n19.9 Summary",
    "text": "19.9 Summary\nPost-processing is a desirable step in any classification process. Bayesian smoothing improves the borders between the objects created by the classification and removes outliers that result from pixel-based classification. It is a reliable method that should be used in most situations."
  },
  {
    "objectID": "cl_smoothing.html#references",
    "href": "cl_smoothing.html#references",
    "title": "\n19  Bayesian smoothing for classification post-processing\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nX. Huang, Q. Lu, L. Zhang, and A. Plaza, “New postprocessing methods for remote sensing image classification: A systematic study,” IEEE Transactions on Geoscience and Remote Sensing, vol. 52, no. 11, pp. 7140–7159, 2014.\n\n\n[2] \nK. Schindler, “An overview and comparison of smooth labeling methods for land-cover classification,” IEEE transactions on geoscience and remote sensing, vol. 50, no. 11, pp. 4534–4545, 2012.\n\n\n[3] \nG. Camara et al., “Bayesian Inference for Post-Processing of Remote-Sensing Image Classification,” Remote Sensing, vol. 16, no. 23, p. 4572, 2024, doi: 10.3390/rs16234572.\n\n\n[4] \nA. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin, Bayesian Data Analysis, Third Edition. CRC Press, 2014."
  },
  {
    "objectID": "cl_reclassification.html#introduction",
    "href": "cl_reclassification.html#introduction",
    "title": "\n20  Map reclassification\n",
    "section": "\n20.1 Introduction",
    "text": "20.1 Introduction\nReclassification of a remote sensing map refers to changing the classes assigned to different pixels in the image. The purpose of reclassification is to modify the information contained in the image to better suit a specific use case. In sits, reclassification involves assigning new classes to pixels based on additional information from a reference map. Users define rules according to the desired outcome. These rules are then applied to the classified map to produce a new map with updated classes."
  },
  {
    "objectID": "cl_reclassification.html#reclassifying-a-deforestation-map",
    "href": "cl_reclassification.html#reclassifying-a-deforestation-map",
    "title": "\n20  Map reclassification\n",
    "section": "\n20.2 Reclassifying a deforestation map",
    "text": "20.2 Reclassifying a deforestation map\nTo illustrate the reclassification in sits, we take a classified data cube stored in the sitsdata package. As discussed in Chapter Data cubes from local files, sits can create a data cube from a classified image file. Users need to provide the original data source and collection, the directory where data is stored (data_dir), the information on how to retrieve data cube parameters from file names (parse_info), and the labels used in the classification.\n\n\n\n\nFigure 20.1: Original classification map.\n\n\n\nThe above map shows the total extent of deforestation by clear cuts estimated by the sits random forest algorithm in an area in Rondonia, Brazil, based on a time series of Sentinel-2 images for the period 2020-06-04 to 2021-08-26. Suppose we want to estimate the deforestation that occurred from June 2020 to August 2021. We need a reference map containing information on forest cuts before 2020.\nIn this example, we use as a reference the PRODES deforestation map of Amazonia created by Brazil’s National Institute for Space Research (INPE). This map is produced by visual interpretation. PRODES measures deforestation every year, starting from August of one year to July of the following year. It contains classes that represent the natural world (Forest, Water, NonForest, and NonForest2) and classes that capture the yearly deforestation increments. These classes are named “dYYYY” and “rYYYY”; the first refers to deforestation in a given year (e.g., “d2008” for deforestation for August 2007 to July 2008); the second to places where the satellite data is not sufficient to determine the land class (e.g., “r2010” for 2010). This map is available on package sitsdata, as shown below.\n\n# Set the directory for PRODES data \ndata_dir &lt;- system.file(\"extdata/PRODES\", package = \"sitsdata\")\n# Recover the PRODES classified cube\nprodes_2021 &lt;- sits_cube(\n    source = \"USGS\",\n    collection = \"LANDSAT-C2L2-SR\",\n    data_dir = data_dir,\n    parse_info = c(\"product\", \"sensor\", \n                   \"tile\", \"start_date\", \"end_date\",\n                   \"band\", \"version\"),\n    bands = \"class\",\n    version = \"v20220606\",\n    labels = c(\"1\" = \"Forest\", \"2\" = \"Water\", \"3\" = \"NonForest\",\n               \"4\" = \"NonForest2\", \"6\" = \"d2007\", \"7\" = \"d2008\",\n               \"8\" = \"d2009\", \"9\" = \"d2010\", \"10\" = \"d2011\", \n               \"11\" = \"d2012\", \"12\" = \"d2013\", \"13\" = \"d2014\", \n               \"14\" = \"d2015\", \"15\" = \"d2016\", \"16\" = \"d2017\",\n               \"17\" = \"d2018\", \"18\" = \"r2010\", \"19\" = \"r2011\",\n               \"20\" = \"r2012\", \"21\" = \"r2013\", \"22\" = \"r2014\", \n               \"23\" = \"r2015\", \"24\" = \"r2016\", \"25\" = \"r2017\", \n               \"26\" = \"r2018\", \"27\" = \"d2019\", \"28\" = \"r2019\", \n               \"29\" = \"d2020\", \"31\" = \"r2020\", \"32\" = \"Clouds2021\",\n               \"33\" = \"d2021\", \"34\" = \"r2021\")\n    )\n\nSince the labels of the deforestation map are specialized and are not part of the default sits color table, we define a legend for better visualization of the different deforestation classes.\n\n# Use the RColorBrewer palette \"YlOrBr\" for the deforestation years\ncolors &lt;- grDevices::hcl.colors(n = 15, palette = \"YlOrBr\")\n# Define the legend for the deforestation map\ndef_legend &lt;- c(\n    \"Forest\" = \"forestgreen\", \"Water\" = \"dodgerblue3\", \n    \"NonForest\" = \"bisque2\", \"NonForest2\" = \"bisque2\",\n    \"d2007\" = colors[1],  \"d2008\" = colors[2],\n    \"d2009\" = colors[3],  \"d2010\" = colors[4], \n    \"d2011\" = colors[5],  \"d2012\" = colors[6],\n    \"d2013\" = colors[7],  \"d2014\" = colors[8],\n    \"d2015\" = colors[9],  \"d2016\" = colors[10],\n    \"d2017\" = colors[11], \"d2018\" = colors[12],\n    \"d2019\" = colors[13], \"d2020\" = colors[14], \n    \"d2021\" = colors[15], \"r2010\" = \"lightcyan\",\n    \"r2011\" = \"lightcyan\", \"r2012\"= \"lightcyan\", \n    \"r2013\" = \"lightcyan\", \"r2014\" = \"lightcyan\", \n    \"r2015\" = \"lightcyan\", \"r2016\" = \"lightcyan\", \n    \"r2017\" = \"lightcyan\", \"r2018\" = \"lightcyan\", \n    \"r2019\" = \"lightcyan\", \"r2020\" = \"lightcyan\",\n    \"r2021\" = \"lightcyan\", \"Clouds2021\" = \"lightblue2\")\n\nUsing this new legend, we can visualize the PRODES deforestation map.\n\nsits_view(prodes_2021, legend = def_legend)\n\n\n\n\n\nFigure 20.2: Deforestation map produced by PRODES.\n\n\n\nTaking the PRODES map as our reference, we can include new labels in the classified map produced by sits using sits_reclassify(). The new class “Deforestation_Mask” will be applied to all pixels that PRODES considers that have been deforested before July 2020. We also include a Non_Forest class to include all pixels that PRODES takes as not covered by native vegetation, such as wetlands and rocky areas. The PRODES classes will be used as a mask over the sits deforestation map.\nThe sits_reclassify() operation requires the parameters: (a) cube, the classified data cube whose pixels will be reclassified; (b) mask, the reference data cube used as a mask; (c) rules, a named list. The names of the rules list will be the new label. Each new label is associated with a mask vector that includes the labels of the reference map that will be joined. sits_reclassify() then compares the original and reference map pixel by pixel. For each pixel of the reference map whose labels are in one of the rules, the algorithm relabels the original map. The result will be a reclassified map with the original labels plus the new labels that have been masked using the reference map.\n\n# Reclassify cube\nrondonia_def_2021 &lt;- sits_reclassify(\n    cube = rondonia_class,\n    mask = prodes_2021,\n    rules = list(\n        \"Non_Forest\" = mask %in% c(\"NonForest\", \"NonForest2\"),\n        \"Deforestation_Mask\" = mask %in% c(\n            \"d2007\", \"d2008\", \"d2009\",\n            \"d2010\", \"d2011\", \"d2012\",\n            \"d2013\", \"d2014\", \"d2015\",\n            \"d2016\", \"d2017\", \"d2018\",\n            \"d2019\", \"d2020\",\n            \"r2010\", \"r2011\", \"r2012\",\n            \"r2013\", \"r2014\", \"r2015\",\n            \"r2016\", \"r2017\", \"r2018\",\n            \"r2019\", \"r2020\", \"r2021\"),\n        \"Water\" = mask == \"Water\"),\n    memsize = 8,\n    multicores = 2,\n    output_dir = tempdir_r,\n    version = \"reclass\")\n\n# Plot the reclassified map\nplot(rondonia_def_2021,\n     legend_text_size = 0.7)\n\n\n\nFigure 20.3: Deforestation map by sits masked by PRODES map.\n\n\n\nThe reclassified map has been split into deforestation before mid-2020 (using the PRODES map) and the areas classified by sits that are taken as being deforested from mid-2020 to mid-2021. This allows experts to measure how much deforestation occurred in this period according to sits and compare the result with the PRODES map."
  },
  {
    "objectID": "cl_reclassification.html#summary",
    "href": "cl_reclassification.html#summary",
    "title": "\n20  Map reclassification\n",
    "section": "\n20.3 Summary",
    "text": "20.3 Summary\nIn this chapter, we describe a useful operation that can be applied to classified maps. The sits_reclassify() function is not restricted to comparing deforestation maps. It can be used in any case that requires masking of a result based on a reference map."
  },
  {
    "objectID": "cl_uncertainty.html#introduction",
    "href": "cl_uncertainty.html#introduction",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.1 Introduction",
    "text": "21.1 Introduction\nLand classification tasks have unique characteristics that differ from other machine learning domains, such as image recognition and natural language processing. The main challenge for land classification is to describe the diversity of the planet’s landscapes in a handful of labels. However, the diversity of the world’s ecosystem makes all classification systems to be biased approximations of reality. As stated by Murphy: “The gradation of properties in the world means that our smallish number of categories will never map perfectly onto all objects” [1]. For this reason, sits provides tools to improve classifications using a process called active learning."
  },
  {
    "objectID": "cl_uncertainty.html#active-learning",
    "href": "cl_uncertainty.html#active-learning",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.2 Active learning",
    "text": "21.2 Active learning\nActive learning is an iterative process of sample selection, labeling, and model retraining. The following steps provide a general overview of how to use active learning:\n\nCollect initial training samples: Start by collecting a small set of representative training samples that cover the range of land classes of interest.\nTrain a machine learning model: Use the initial training samples to train a machine learning model to classify remote sensing data.\nClassify the data cube using the model.\nIdentify areas of uncertainty.\nSelect samples for re-labeling: Select a set of unlabeled samples that the model is most uncertain about, i.e., those that the model is least confident in classifying.\nLabel the selected samples: The user labels the selected samples, adding them to the training set.\nRetrain the model: The model is retrained using the newly labeled samples, and the process repeats itself, starting at step 2.\nStop when the classification accuracy is satisfactory: The iterative process continues until the classification accuracy reaches a satisfactory level.\n\nIn traditional classification methods, experts provide a set of training samples and use a machine learning algorithm to produce a map. By contrast, the active learning approach puts the human in the loop [2]. At each iteration, an unlabeled set of samples is presented to the user, which assigns classes to them and includes them in the training set [3]. The process is repeated until the expert is satisfied with the result, as shown in Figure 21.1.\n\n\n\n\nFigure 21.1: Active learning approach (source: [3]).\n\n\n\nActive learning aims to reduce bias and errors in sample selection and, as such, improve the accuracy of the result. At each interaction, experts are asked to review pixels where the machine learning classifier indicates a high uncertainty value. Sources of classification uncertainty include missing classes and or mislabeled samples. In sits, active learning is supported by functions sits_uncertainty() and sits_uncertainty_sampling()."
  },
  {
    "objectID": "cl_uncertainty.html#measuring-uncertainty",
    "href": "cl_uncertainty.html#measuring-uncertainty",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.3 Measuring uncertainty",
    "text": "21.3 Measuring uncertainty\nUncertainty refers to the degree of doubt or ambiguity in the accuracy of the classification results. Several sources of uncertainty can arise during land classification using satellite data, including:\n\nClassification errors: These can occur when the classification algorithm misinterprets the spectral, spatial or temporal characteristics of the input data, leading to the misclassification of land classes.\nAmbiguity in classification schema: The definition of land classes can be ambiguous or subjective, leading to inconsistencies in the classification results.\nVariability in the landscape: Natural and human-induced variations in the landscape can make it difficult to accurately classify land areas.\nLimitations of the data: The quality and quantity of input data can influence the accuracy of the classification results.\n\nQuantifying uncertainty in land classification is important for ensuring that the results are reliable and valid for decision-making. Various methods, such as confusion and error matrices, can be used to estimate and visualize the level of uncertainty in classification results. Additionally, incorporating uncertainty estimates into decision-making processes can help to identify regions where further investigation or data collection is needed.\nThe function sits_uncertainty() calculates the uncertainty cube based on the probabilities produced by the classifier. It takes a probability cube as input. The uncertainty measure is relevant in the context of active learning. It helps to increase the quantity and quality of training samples by providing information about the model’s confidence. The supported types of uncertainty are ‘entropy’, ‘least’, ‘margin’, and ‘ratio’.\nLeast confidence sampling is the difference between no uncertainty (100% confidence) and the probability of the most likely class, normalized by the number of classes. Let \\(P_1(i)\\) be the higher class probability for pixel \\(i\\). Then least confidence sampling is expressed as\n\\[\n\\theta_{LC} = (1 - P_1(i)) * \\frac{n}{n-1}.\n\\]\nMargin of confidence is the difference between the two most confident predictions, expressed from 0% (no uncertainty) to 100% (maximum uncertainty). Let \\(P_1(i)\\) and \\(P_2(i)\\) be the two higher class probabilities for pixel \\(i\\). Then, the margin of confidence is expressed as\n\\[\n\\theta_{MC} = (P_1(i) - P_2(i)).\n\\]\nThe ratio of confidence is the measure of the ratio between the two most confident predictions, expressed in a range from 0% (no uncertainty) to 100% (maximum uncertainty). Let \\(P_1(i)\\) and \\(P_2(i)\\) be the two higher class probabilities for pixel \\(i\\). Then, the ratio of confidence is expressed as \\[\n\\theta_{RC} = \\frac{P_2(i)}{P_1(i)}.\n\\]\nEntropy is a measure of uncertainty used by Claude Shannon in his classic work “A Mathematical Theory of Communication”. It is related to the amount of variability in the probabilities associated with a pixel. The lower the variability, the lower the entropy. Let \\(P_k(i)\\) be the probability of class \\(k\\) for pixel \\(i\\). The entropy is calculated as \\[\n\\theta_{E} = \\frac{-\\sum_{k=1}^K P_k(i) * log_2(P_k(i))}{log_2{n}}.\n\\]\nThe parameters for sits_uncertainty() are: cube, a probability data cube; type, the uncertainty measure (default is least). As with other processing functions, multicores is the number of cores to run the function and memsize is the maximum overall memory (in GB) to run the function, output_dir is the output directory for image files, and version is the result version."
  },
  {
    "objectID": "cl_uncertainty.html#using-uncertainty-measures-for-active-learning",
    "href": "cl_uncertainty.html#using-uncertainty-measures-for-active-learning",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.4 Using uncertainty measures for active learning",
    "text": "21.4 Using uncertainty measures for active learning\nThe following case study shows how uncertainty measures can be used in the context of active learning. The study area is a subset of one Sentinel-2 tile in the state of Rondonia, Brazil. The work aims to detect deforestation in Brazilian Amazonia.\nThe study area is close to the Samuel Hydroelectric Dam, located on the Madeira River in the Brazilian state of Rondônia. Building the dam led to a loss of 56,000 ha of native forest. The dam’s construction caused the displacement of several indigenous communities and traditional populations, leading to social and cultural disruption. Additionally, flooding large forest areas resulted in losing habitats and biodiversity, including several endangered species. The dam has altered the natural flow of the Madeira River, leading to changes in water quality and temperature and affecting the aquatic life that depends on the river. The changes in river flow have also impacted the navigation and transportation activities of the local communities [4].\nThe first step is to produce a regular data cube for the chosen area from 2020-06-01 to 2021-09-01. To reduce processing time and storage, we use only three bands (B02, B8A, and B11) plus the cloud band, and take a small area inside the tile. After obtaining a regular cube, we plot the study area in two dates during the temporal interval of the data cube. The first image is taken at the beginning of the dry season in 2020-07-04, when the inundation area of the dam was covered by shallow water.\n\n# Select a S2 tile\ns2_cube_ro &lt;- sits_cube(\n      source = \"AWS\",\n      collection = \"SENTINEL-S2-L2A-COGS\",\n      tiles = \"20LMR\",\n      bands = c(\"B02\", \"B8A\", \"B11\", \"SCL\"),\n      start_date = as.Date(\"2020-06-01\"),\n      end_date = as.Date(\"2021-09-01\"),\n      progress = FALSE\n)\n\n# Select a small area inside the tile\nroi = c(lon_max = -63.25790, lon_min = -63.6078, \n        lat_max = -8.72290, lat_min = -8.95630)\n# Regularize the small area cube\ns2_reg_cube_ro &lt;- sits_regularize(\n  cube = s2_cube_ro,\n  output_dir = tempdir_r,\n  res = 20,\n  roi = roi,\n  period = \"P16D\",\n  multicores = 4,\n  progress = FALSE\n)\n\n\nplot(s2_reg_cube_ro, \n     red = \"B11\", \n     green = \"B8A\", \n     blue = \"B02\",\n     date = \"2020-07-04\")\n\n\n\nFigure 21.2: Area in Rondonia near Samuel dam in July 2020.\n\n\n\nThe second image is from 2020-11-09 and shows that most of the inundation area dries during the dry season. In early November 2020, after the end of the dry season, the inundation area is dry and has a response similar to bare soil and burned areas. The Madeira River can be seen running through the dried inundation area.\n\nplot(s2_reg_cube_ro, \n     red = \"B11\", \n     green = \"B8A\", \n     blue = \"B02\", \n     date = \"2020-11-09\")\n\n\n\nFigure 21.3: Area in Rondonia near Samuel dam in November 2020.\n\n\n\nThe third image is from 2021-08-08. In early August 2021, after the wet season, the inundation area is again covered by a shallow water layer. Several burned and clear-cut areas can also be seen in the August 2021 image compared with the July 2020 one. Given the contrast between the wet and dry seasons, correct land classification of this area is hard.\n\nplot(s2_reg_cube_ro, red = \"B11\", green = \"B8A\", blue = \"B02\", date = \"2021-08-08\")\n\n\n\nFigure 21.4: Area in Rondonia near Samuel dam in August 2021.\n\n\n\nThe next step is to classify this study area using a training set with 480 times series collected over the state of Rondonia (Brazil) for detecting deforestation. The training set uses 4 classes (Burned_Area, Forest,Highly_Degraded, andCleared_Area`). The cube is classified using a Random Forest model, post-processed by Bayesian smoothing, and then labeled.\n\nset.seed(290356)\nlibrary(sitsdata)\n# Load the training set\ndata(\"samples_prodes_4classes\")\n# Select the same three bands used in the data cube\nsamples_4classes_3bands &lt;- sits_select(\n    data = samples_prodes_4classes, \n    bands = c(\"B02\", \"B8A\", \"B11\"))\n\n# Train a random forest model \nrfor_model &lt;- sits_train(\n    samples = samples_4classes_3bands, \n    ml_method = sits_rfor())\n\n# Classify the small area cube\ns2_cube_probs &lt;- sits_classify(\n    data = s2_reg_cube_ro,\n    ml_model = rfor_model,\n    output_dir = tempdir_r,\n    memsize = 15,\n    multicores = 5)\n\n# Post-process the probability cube\ns2_cube_bayes &lt;- sits_smooth(\n    cube = s2_cube_probs,\n    output_dir = tempdir_r,\n    memsize = 16,\n    multicores = 4)\n\n# Label the post-processed  probability cube\ns2_cube_label &lt;- sits_label_classification(\n    cube = s2_cube_bayes,\n    output_dir = tempdir_r,\n    memsize = 16,\n    multicores = 4)\n\nplot(s2_cube_label)\n\n\n\nFigure 21.5: First classified map for area in Rondonia near Samuel dam.\n\n\n\nThe resulting map correctly identifies the forest area and the deforestation. However, it misclassifies the area covered by the Samuel hydroelectric dam. The reason is the lack of samples for classes related to surface water and wetlands. To improve the classification, we need to improve our samples. To do that, the first step is to calculate the uncertainty of the classification.\n\n# Calculate the uncertainty cube\ns2_cube_uncert &lt;- sits_uncertainty(\n    cube = s2_cube_bayes,\n    type = \"margin\",\n    output_dir = tempdir_r,\n    memsize = 16,\n    multicores = 4)\n\nplot(s2_cube_uncert)\n\n\n\nFigure 21.6: Uncertainty map for classification in Rondonia near Samuel dam.\n\n\n\nAs expected, the places of highest uncertainty are those covered by surface water or associated with wetlands. These places are likely to be misclassified. For this reason, sits provides sits_uncertainty_sampling(), which takes the uncertainty cube as its input and produces a tibble with locations in WGS84 with high uncertainty. The function has three parameters: n, the number of uncertain points to be included; min_uncert, the minimum value of uncertainty for pixels to be included in the list; and sampling_window, which defines a window where only one sample will be selected. The aim of sampling_window is to improve the spatial distribution of the new samples by avoiding points in the same neighborhood to be included. After running the function, we can use sits_view() to visualize the location of the samples.\n\n# Find samples with high uncertainty\nnew_samples &lt;- sits_uncertainty_sampling(\n    uncert_cube = s2_cube_uncert,\n    n = 20,\n    min_uncert = 0.5,\n    sampling_window = 10)\n\n\nsits_view(new_samples)\n\n\n\n\n\nFigure 21.7: Location of uncertain pixels for classification in Rondonia near Samuel dam.\n\n\n\nThe visualization shows that the samples are located in the areas covered by the Samuel data. Thus, we designate these samples as Wetlands. A more detailed evaluation, which is recommended in practice, requires analysing these samples with an exploration software such as QGIS and individually labeling each sample. In our case, we will take a direct approach for illustration purposes.\n\n# Label the new samples\nnew_samples$label &lt;- \"Wetland\"\n# Obtain the time series from the regularized cube \nnew_samples_ts &lt;- sits_get_data(\n    cube = s2_reg_cube_ro,\n    samples = new_samples)\n\n# Join the new samples with the original ones with 4 classes\nsamples_round_2 &lt;- dplyr::bind_rows(\n    samples_4classes_3bands,\n    new_samples_ts)\n\n# Train a RF model with the new sample set\nrfor_model_v2 &lt;- sits_train(\n    samples = samples_round_2, \n    ml_method = sits_rfor())\n\n# Classify the small area cube\ns2_cube_probs_v2 &lt;- sits_classify(\n    data = s2_reg_cube_ro,\n    ml_model = rfor_model_v2,\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Post-process the probability cube\ns2_cube_bayes_v2 &lt;- sits_smooth(\n    cube = s2_cube_probs_v2,\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Label the post-processed  probability cube\ns2_cube_label_v2 &lt;- sits_label_classification(\n    cube = s2_cube_bayes_v2,\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\n# Plot the second version of the classified cube\nplot(s2_cube_label_v2)\n\n\n\nFigure 21.8: Classified map for area in Rondonia near Samuel dam with added samples.\n\n\n\nThe results show a significant quality gain over the earlier classification. There are still some areas of confusion in the exposed soils inside the inundation area, some of which have been classified as burned areas. It is also useful to show the uncertainty map associated with the second model.\n\n# Calculate the uncertainty cube\ns2_cube_uncert_v2 &lt;- sits_uncertainty(\n    cube = s2_cube_bayes_v2,\n    type = \"margin\",\n    output_dir = tempdir_r,\n    version = \"v2\",\n    memsize = 16,\n    multicores = 4)\n\nplot(s2_cube_uncert_v2)\n\n\n\nFigure 21.9: Uncertainty map for classification in Rondonia near Samuel dam - improved model.\n\n\n\nAs the new uncertainty map shows, there is a significant improvement in the quality of the classification. The remaining areas of high uncertainty are those affected by the contrast between the wet and dry seasons close to the inundation area. These areas are low-laying places that sometimes are covered by water and sometimes are bare soil areas throughout the year, depending on the intensity of the rainy season. To further improve the classification quality, we could obtain new samples of those uncertain areas, label them, and add them to samples."
  },
  {
    "objectID": "cl_uncertainty.html#summary",
    "href": "cl_uncertainty.html#summary",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "\n21.5 Summary",
    "text": "21.5 Summary\nIn general, as this Chapter shows, combining uncertainty measurements with active learning is a recommended practice for improving classification results. This approach offers several important benefits for remote sensing image classification, especially in contexts where obtaining high-quality labeled data is expensive and time-consuming. Uncertainty maps direct expert to places where new samples are required. In this way, combnining uncertainty with active learning minimizes the number of samples that need to be manually labeled by selecting the most informative ones."
  },
  {
    "objectID": "cl_uncertainty.html#references",
    "href": "cl_uncertainty.html#references",
    "title": "\n21  Uncertainty and active learning\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nG. L. Murphy, The Big Book of Concepts. Cambridge, MA, USA: Bradford Books, 2002.\n\n\n[2] \nR. Monarch, Human-in-the-Loop Machine Learning. Shelter Island, NY: Manning Publications, 2021.\n\n\n[3] \nM. M. Crawford, D. Tuia, and H. L. Yang, “Active Learning: Any Value for Classification of Remotely Sensed Data?” Proceedings of the IEEE, vol. 101, no. 3, pp. 593–608, 2013, doi: 10.1109/JPROC.2012.2231951.\n\n\n[4] \nP. M. Fearnside, “Brazil’s Samuel Dam: Lessons for Hydroelectric Development Policy and the Environment in Amazonia,” Environmental Management, vol. 35, no. 1, pp. 1–19, 2005, doi: 10.1007/s00267-004-0100-3."
  },
  {
    "objectID": "cl_ensembleprediction.html#introduction-to-ensemble-prediction",
    "href": "cl_ensembleprediction.html#introduction-to-ensemble-prediction",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.1 Introduction to ensemble prediction",
    "text": "22.1 Introduction to ensemble prediction\nEnsemble prediction is a powerful technique for combining predictions from multiple models to produce more accurate and robust predictions. Errors of individual models cancel out or are reduced when combined with the predictions of other models. As a result, ensemble predictions can lead to better overall accuracy and reduce the risk of overfitting. This can be especially useful when working with complex or uncertain data. By combining the predictions of multiple models, users can identify which features or factors are most important for making accurate predictions. When using ensemble methods, choosing diverse models with different sources of error is essential to ensure that the ensemble predictions are more precise and robust.\nThe sits package provides sits_combine_predictions() to estimate ensemble predictions using probability cubes produced by sits_classify() and optionally post-processed with sits_smooth(). There are two ways to make ensemble predictions from multiple models:\n\nAveraging: In this approach, the predictions of each model are averaged to produce the final prediction. This method works well when the models have similar accuracy and errors.\nUncertainty: Predictions from different models are compared in terms of their uncertainties on a pixel-by-pixel basis; predictions with lower uncertainty are chosen as being more likely to be valid.\n\nIn what follows, we will use the same sample dataset and data cube used in Chapter Image classification in data cubes to illustrate how to produce an ensemble prediction. The dataset samples_deforestation_rondonia consists of 6,007 samples collected from Sentinel-2 images covering the state of Rondonia. Each time series contains values from all Sentinel-2/2A spectral bands for year 2022 in 16-day intervals. The data cube is a subset of the Sentinel-2 tile “20LMR” which contains all spectral bands, plus spectral indices “NVDI”, “EVI” and “NBR” for the year 2022."
  },
  {
    "objectID": "cl_ensembleprediction.html#recovering-the-data-cube",
    "href": "cl_ensembleprediction.html#recovering-the-data-cube",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.2 Recovering the data cube",
    "text": "22.2 Recovering the data cube\nThe first step is to recover the data cube which is available in the sitsdata package, and to select only the spectral bands.\n\n# Files are available in a local directory \ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR/\", package = \"sitsdata\")\n# Read data cube\nro_cube_20LMR &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir\n)\n# reduce the number of bands\nro_cube_20LMR &lt;- sits_select(\n  data = ro_cube_20LMR,\n  bands = c(\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"B8A\")\n)\n# plot one time step of the cube\nplot(ro_cube_20LMR, blue = \"B02\", green = \"B8A\", red = \"B11\", date = \"2022-08-17\")\n\n\n#|label: fig-ensprd-rgb \n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: \"Subset of Sentinel-2 tile 20LMR.\"\nknitr::include_graphics(\"./images/ensprd-rgb.png\")"
  },
  {
    "objectID": "cl_ensembleprediction.html#classification-using-random-forests",
    "href": "cl_ensembleprediction.html#classification-using-random-forests",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.3 Classification using Random Forests",
    "text": "22.3 Classification using Random Forests\nWe will train three models: Random Forests (RF), Light Temporal Attention Encoder (LTAE), and Temporal Convolution Neural Networks (TempCNN), classify the cube with them, and then combine their results. The example uses all spectral bands. We first run the RF classification.\n\n# train a random forest model\nrfor_model &lt;- sits_train(samples_deforestation_rondonia, sits_rfor())\n# classify the data cube\nro_cube_20LMR_rfor_probs &lt;- sits_classify(\n    ro_cube_20LMR, \n    rfor_model,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_rfor_probs, labels = \"Forest\")\n\n\n#|label: fig-ensprd-plot-rfor\n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: | \n#|  Plot of probabilities for class Forest produced by random forests algorithm.\nknitr::include_graphics(\"./images/ensprd-plot-rfor.png\")\n\n\n\n\n\nro_cube_20LMR_rfor_variance &lt;- sits_variance(\n    ro_cube_20LMR_rfor_probs, \n    window_size = 9,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\nsummary(ro_cube_20LMR_rfor_variance)\n\n\n\n     Clear_Cut_Bare_Soil Clear_Cut_Burned_Area Clear_Cut_Vegetation  Forest\n75%                 4.62                  4.95                0.420  1.0800\n80%                 5.17                  5.38                0.510  1.4900\n85%                 5.57                  5.70                0.670  2.2115\n90%                 5.93                  6.03                1.261  4.4000\n95%                 6.64                  6.69                5.290  7.1000\n100%               17.31                 13.45               13.950 27.2900\n     Mountainside_Forest Riparian_Forest Seasonally_Flooded   Water Wetland\n75%                0.690          0.9100               0.37  0.5300    4.58\n80%                1.960          1.3800               0.47  0.5300    5.21\n85%                3.830          2.5815               0.61  1.1645    5.60\n90%                5.241          4.3600               0.95  2.3800    5.96\n95%                6.220          6.2000               2.19  7.4215    6.45\n100%              11.900         31.3200              18.27 42.9700   23.91\n\n\nBased on the variance values, we apply the smoothness hyperparameter according to the recommendations proposed before. We choose values of \\(\\sigma^2_{k}\\) that reflect our prior expectation of the spatial patterns of each class. For classes Clear_Cut_Vegetation and Clear_Cut_Burned_Area, to produce denser spatial clusters and remove “salt-and-pepper” outliers, we take \\(\\sigma^2_{k}\\) values in 95%-100% range. In the case of the most frequent classes Forest and Clear_Cut_Bare_Soil we want to preserve their original spatial shapes as much as possible; the same logic applies to less frequent classes Water and Wetland. For this reason, we set \\(\\sigma^2_{k}\\) values in the 75%-80% range for these classes. The class spatial patterns correspond to our prior expectations.\n\nro_cube_20LMR_rfor_bayes &lt;- sits_smooth(\n    ro_cube_20LMR_rfor_probs,\n    output_dir = tempdir_r, \n    smoothness = c(  \n      \"Clear_Cut_Bare_Soil\" = 5.25, \n      \"Clear_Cut_Burned_Area\" = 15.0, \n      \"Clear_Cut_Vegetation\" =  12.0, \n      \"Forest\" = 1.8, \n      \"Mountainside_Forest\" = 6.5,\n      \"Riparian_Forest\" = 6.0, \n      \"Seasonally_Flooded\" = 3.5,\n      \"Water\" = 1.5, \n      \"Wetland\" = 5.5),\n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\nro_cube_20LMR_rfor_class &lt;- sits_label_classification(\n    ro_cube_20LMR_rfor_bayes,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"rfor\"\n)\nplot(ro_cube_20LMR_rfor_class, \n      legend_text_size = 0.7, legend_position = \"outside\")\n\n\n#|label: fig-ensprd-map-rfor\n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: \"Plot of classified map produced by Random Forests algorithm.\"\nknitr::include_graphics(\"./images/ensprd-map-rfor.png\")"
  },
  {
    "objectID": "cl_ensembleprediction.html#classification-using-temporal-convolution-neural-networks-tempcnn",
    "href": "cl_ensembleprediction.html#classification-using-temporal-convolution-neural-networks-tempcnn",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.4 Classification using Temporal Convolution Neural Networks (TempCNN)",
    "text": "22.4 Classification using Temporal Convolution Neural Networks (TempCNN)\nThe next step is to classify the same area using a tempCNN algorithm, as shown below.\n\n# train a tempcnn model\ntcnn_model &lt;- sits_train(samples_deforestation_rondonia, \n                         sits_tempcnn())\n\n# classify the data cube\nro_cube_20LMR_tcnn_probs &lt;- sits_classify(\n    ro_cube_20LMR, \n    tcnn_model,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 8,\n    gpu_memory = 8,\n    version = \"tcnn\"\n)\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_tcnn_probs, labels = \"Forest\")\n\n\n#|label: fig-ensprd-probs-tcnn\n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: \"Plot of probabilities for class Forest produced by TempCNN algorithm.\"\nknitr::include_graphics(\"./images/ensprd-probs-tcnn.png\")\n\n\n\n\n\nro_cube_20LMR_tcnn_variance &lt;- sits_variance(\n    ro_cube_20LMR_tcnn_probs,\n    window_size = 9,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"tcnn\"\n)\nsummary(ro_cube_20LMR_tcnn_variance)\n\n\n\n     Clear_Cut_Bare_Soil Clear_Cut_Burned_Area Clear_Cut_Vegetation Forest\n75%               1.1900                 1.150                 2.17  2.390\n80%               1.5200                 1.392                 2.54  3.050\n85%               2.0800                 1.730                 3.00  4.030\n90%               3.3800                 2.370                 3.80  5.821\n95%               6.9225                 3.820                 5.35 10.560\n100%             36.8900                22.630                32.26 44.410\n     Mountainside_Forest Riparian_Forest Seasonally_Flooded  Water Wetland\n75%                 1.40           2.600             2.5225  0.550  1.4400\n80%                 1.63           3.122             3.1000  0.680  1.7800\n85%                 1.90           3.910             4.1115  0.900  2.2600\n90%                 2.32           5.300             5.9500  1.390  3.0400\n95%                 3.16           8.440             9.6105  9.544  4.7205\n100%               11.21          56.980            41.7900 53.920 50.7400\n\n\n\nro_cube_20LMR_tcnn_bayes &lt;- sits_smooth(\n    ro_cube_20LMR_tcnn_probs,\n    output_dir = tempdir_r, \n    window_size = 11,\n    smoothness = c(  \n      \"Clear_Cut_Bare_Soil\" = 1.5, \n      \"Clear_Cut_Burned_Area\" = 20.0, \n      \"Clear_Cut_Vegetation\" =  25.0, \n      \"Forest\" = 4.0, \n      \"Mountainside_Forest\" = 3.0,\n      \"Riparian_Forest\" = 40.0, \n      \"Seasonally_Flooded\" = 30.0,\n      \"Water\" = 1.0, \n      \"Wetland\" = 2.0),\n    multicores = 2,\n    memsize = 6,\n    version = \"tcnn\"\n)\nro_cube_20LMR_tcnn_class &lt;- sits_label_classification(\n    ro_cube_20LMR_tcnn_bayes,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 6,\n    version = \"tcnn\"\n)\nplot(ro_cube_20LMR_tcnn_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n#|label: fig-ensprd-class-tcnn\n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: | \n#|   Land classification in Rondonia using tempCNN.\nknitr::include_graphics(\"./images/ensprd-class-tcnn.png\")"
  },
  {
    "objectID": "cl_ensembleprediction.html#classification-using-lightweight-temporal-attention-encoder-ltae",
    "href": "cl_ensembleprediction.html#classification-using-lightweight-temporal-attention-encoder-ltae",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.5 Classification using Lightweight Temporal Attention Encoder (LTAE)",
    "text": "22.5 Classification using Lightweight Temporal Attention Encoder (LTAE)\nThe third model is the Light Temporal Attention Encoder (LTAE), which has been discussed in the Machine Learning chapter.\n\n# train a tempcnn model\nltae_model &lt;- sits_train(samples_deforestation_rondonia, sits_lighttae())\n\n# classify the data cube\nro_cube_20LMR_ltae_probs &lt;- sits_classify(\n    ro_cube_20LMR, \n    ltae_model,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 8,\n    gpu_memory = 8,\n    version = \"ltae\"\n)\n# plot the probabilities for class Forest\nplot(ro_cube_20LMR_ltae_probs, labels = \"Forest\")\n\n\n#|label: fig-ensprd-probs-ltae\n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: | \n#|   Land classification in Rondonia using tempCNN.\nknitr::include_graphics(\"./images/ensprd-probs-ltae.png\")\n\n\n\n\nWe then compute the variance of the probability cube produced by the LTAE algorithm.\n\nro_cube_20LMR_ltae_variance &lt;- sits_variance(\n    ro_cube_20LMR_ltae_probs,\n    window_size = 9,\n    output_dir = tempdir_r, \n    multicores = 6,\n    memsize = 24,\n    version = \"ltae\"\n)\nsummary(ro_cube_20LMR_ltae_variance)\n\n\n\n     Clear_Cut_Bare_Soil Clear_Cut_Burned_Area Clear_Cut_Vegetation Forest\n75%               0.7300                  0.51               2.2800   1.97\n80%               1.1100                  0.70               2.8320   2.86\n85%               1.8000                  1.00               3.5300   3.84\n90%               3.1710                  1.48               4.4310   5.07\n95%               5.7205                  2.50               5.7605   7.11\n100%             29.6300                 13.77              14.9000  24.17\n     Mountainside_Forest Riparian_Forest Seasonally_Flooded   Water Wetland\n75%                 0.42           1.010             3.6425  0.3000   0.660\n80%                 0.52           1.560             4.3800  0.3400   1.042\n85%                 0.67           2.470             5.2315  0.4000   1.880\n90%                 1.00           3.871             6.5210  0.5400   3.340\n95%                 1.68           6.080             8.7400  8.1905   6.360\n100%               12.74          24.430            31.3100 44.8400  43.080\n\n\nWe use the same rationale for selecting the smoothness parameter for the Bayesian smoothing operation as in the cases above.\n\nro_cube_20LMR_ltae_bayes &lt;- sits_smooth(\n    ro_cube_20LMR_tcnn_probs,\n    output_dir = tempdir_r, \n    window_size = 11,\n    smoothness = c(  \n      \"Clear_Cut_Bare_Soil\" = 1.2, \n      \"Clear_Cut_Burned_Area\" = 10.0, \n      \"Clear_Cut_Vegetation\" = 15.0, \n      \"Forest\" = 4.0, \n      \"Mountainside_Forest\" = 8.0,\n      \"Riparian_Forest\" = 25.0, \n      \"Seasonally_Flooded\" = 30.0,\n      \"Water\" = 0.3, \n      \"Wetland\" = 1.8),\n    multicores = 2,\n    memsize = 6,\n    version = \"ltae\"\n)\n# generate the classified map\nro_cube_20LMR_ltae_class &lt;- sits_label_classification(\n    ro_cube_20LMR_ltae_bayes,\n    output_dir = tempdir_r, \n    multicores = 2,\n    memsize = 6,\n    version = \"ltae\"\n)\n# plot the classified map\nplot(ro_cube_20LMR_ltae_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n#|label: fig-ensprd-class-ltae\n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: | \n#|   Land classification in Rondonia using tempCNN.\nknitr::include_graphics(\"./images/ensprd-class-ltae.png\")"
  },
  {
    "objectID": "cl_ensembleprediction.html#differences-between-model-results",
    "href": "cl_ensembleprediction.html#differences-between-model-results",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.6 Differences between model results",
    "text": "22.6 Differences between model results\nTo understand the differences between the results, it is useful to compare the resulting class areas produced by the different algorithms, expressed in \\(km^2\\).\n\n# get the summary of the map produced by RF\nsum1 &lt;- summary(ro_cube_20LMR_rfor_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum1) &lt;- c(\"class\", \"rfor\")\n# get the summary of the map produced by TCNN\nsum2 &lt;- summary(ro_cube_20LMR_tcnn_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum2) &lt;- c(\"class\", \"tcnn\")\n# get the summary of the map produced by LTAE\nsum3 &lt;- summary(ro_cube_20LMR_ltae_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum3) &lt;- c(\"class\", \"ltae\")\n# compare class areas of maps produced by the three models\npred_class_areas &lt;- dplyr::inner_join(sum1, sum2, by = \"class\") |&gt; \n  dplyr::inner_join(sum3, by = \"class\")\npred_class_areas\n\n\n\n# A tibble: 9 × 4\n  class                     rfor    tcnn    ltae\n  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Clear_Cut_Bare_Soil    81       67      66    \n2 Clear_Cut_Burned_Area   1.6      3.8     3.9  \n3 Clear_Cut_Vegetation   19       22      22    \n4 Forest                280      240     240    \n5 Mountainside_Forest     0.0008   0.064   0.053\n6 Riparian_Forest        46       39      38    \n7 Seasonally_Flooded     72      130     130    \n8 Water                  63       68      67    \n9 Wetland                13       10      11    \n\n\nThe study area presents many challenges for land classification, given the presence of wetlands, riparian forests and seasonally-flooded areas. The results show the algorithms obtain quite different results, since each model has different sensitivities. The RF method is biased towards the most frequent classes, especially for Clear_Cut_Bare_Soil and Forest. The area estimated by RF for class Clear_Cut_Burned_Area is the smallest of the three models. Most pixels assigned by LTAE and TCNN as burned areas are assigned by RF as being areas of bare soil. The RF algorithm tends to be more conservative. The reason is because RF decision-making uses values from single attributes (values of a single band in a given time instance), while LTAE and TCNN consider the relations between instances of the time series. Since the RF model is sensitive to the response of images at the end of the period, it tends to focus on values that indicate the presence of forests and bare soils during the dry season, which peaks in August. The LTAE and TCNN models are more balanced to the overall separation of classes in the entire attribute space, and produces larger estimates of riparian and seasonally flooded forest than the other methods. In contrast, both LTAE and TCNN make more mistakes than RF in including flooded areas in the center-left part of the image on the left side of the rives as Clear_Cut_Vegetation when the right label would be riparian or flooded forests."
  },
  {
    "objectID": "cl_ensembleprediction.html#combining-the-different-predictions",
    "href": "cl_ensembleprediction.html#combining-the-different-predictions",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.7 Combining the different predictions",
    "text": "22.7 Combining the different predictions\nGiven the differences and complementaries between the three predicted outcomes, combining them using sits_combine_predictions() is useful. This function takes the following arguments: (a) cubes, a list of the cubes to be combined. These cubes should be probability cubes generated by which optionally may have been smoothened; (b) type, which indicates how to combine the probability maps. The options are average, which performs a weighted mean of the probabilities, and uncertainty, which uses the uncertainty cubes to combine the predictions; (c) weights, a vector of weights to be used to combine the predictions when average is selected; (d) uncertainty_cubes, a list of uncertainty cubes associated to the predictions; (e) multicores, number of cores to be used; (f) memsize, RAM used in the classification; (g) output_dir, the directory where the classified raster files will be written.\n\n# Combine the three predictions by taking the average of the probabilities for each class\nro_cube_20LMR_average_probs &lt;- sits_combine_predictions(\n  cubes = list(ro_cube_20LMR_tcnn_bayes, \n               ro_cube_20LMR_rfor_bayes, \n               ro_cube_20LMR_ltae_bayes),\n  type = \"average\",\n  version = \"average-rfor-tcnn-ltae\",\n  output_dir = tempdir_r,\n  weights = c(0.33, 0.34, 0.33),\n  memsize = 16,\n  multicores = 4\n)\n# Label the average probability cube\nro_cube_20LMR_average_class &lt;- sits_label_classification(\n    cube = ro_cube_20LMR_average_probs,\n    output_dir = tempdir_r,\n    version = \"average-rfor-tcnn-ltae\",\n    memsize = 16,\n    multicores = 4\n)\n# plot the classified map\nplot(ro_cube_20LMR_average_class,  \n     legend_text_size = 0.7, legend_position = \"outside\")\n\n\n#|label: fig-ensprd-class-ave\n#|echo: false\n#|out.width: \"100%\"\n#|fig.align: \"center\"\n#|fig.cap: | \n#|   Land classification in Rondonia using the average of the probabilities produced by random forests, TempCNN and LTAE models.\nknitr::include_graphics(\"./images/ensprd-class-ave.png\")\n\n\n\n\nWe can also consider the class areas produced by the ensemble combination and compare them to the original estimates, expressed in \\(km^2\\).\n\n# get the summary of the map produced by LTAE\nsum4 &lt;- summary(ro_cube_20LMR_average_class) |&gt; \n    dplyr::select(\"class\", \"area_km2\")\ncolnames(sum4) &lt;- c(\"class\", \"ave\")\n# compare class areas of non-smoothed and smoothed maps\npred_ave_rfor_tcnn_ltae &lt;- dplyr::inner_join(sum1, sum2, by = \"class\") |&gt; \n  dplyr::inner_join(sum3, by = \"class\") |&gt; \n    dplyr::inner_join(sum4, by = \"class\")\npred_ave_rfor_tcnn_ltae\n\n\n\n# A tibble: 9 × 5\n  class                     rfor    tcnn    ltae     ave\n  &lt;chr&gt;                    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Clear_Cut_Bare_Soil    81       67      66      70    \n2 Clear_Cut_Burned_Area   1.6      3.8     3.9     3.2  \n3 Clear_Cut_Vegetation   19       22      22      19    \n4 Forest                280      240     240     250    \n5 Mountainside_Forest     0.0008   0.064   0.053   0.038\n6 Riparian_Forest        46       39      38      41    \n7 Seasonally_Flooded     72      130     130     120    \n8 Water                  63       68      67      67    \n9 Wetland                13       10      11      11    \n\n\nAs expected, the ensemble map combines information from the three models. Taking the RF model prediction as a base, there is a reduction in the areas of classes Clear_Cut_Bare_Soil and Forest, confirming the tendency of the RF model to overemphasize the most frequent classes. The LTAE and TempCNN models are more sensitive to class variations and capture time-varying classes such as Riparian_Forest and Clear_Cut_Burned_Area in more detail than the RF model. However, both TempCNN and LTAE tend to confuse the deforestation-related class Clear_Cut_Vegetation and the natural class Riparian_Forest more than the RF model. This effect is evident in the left bank of the Madeira river in the centre-left region of the image. Also, both the LTAE and TempCNN maps are more grainy and have more spatial variability than the RF map.\nThe average map provides a compromise between RF’s strong empahsis on the most frequent classes and the tendency of deep learning methods to produce outliers based on temporal relationship. The average map is less grainy and more spatially consistent than the LTAE and TempCNN maps, while introducing variability which is not present in the RF map."
  },
  {
    "objectID": "cl_ensembleprediction.html#summary",
    "href": "cl_ensembleprediction.html#summary",
    "title": "\n22  Ensemble prediction with multiple models\n",
    "section": "\n22.8 Summary",
    "text": "22.8 Summary\nThis chapter shows the possibilities of ensemble prediction. There are many ways to get better results than those presented here. Increasing the number of spectral bands would improve the final accuracy. Also, Bayesian smoothing for deep learning models should not rely on default parameters; rather it needs to rely on variance analysis, increase the spatial window and provide more informed hyperparameters. In general, ensemble prediction should be consider in all situations where one is not satisfied with the results of individual models. Combining model output increses the reliability of the result and thus shouls be considered in all situations where similar classes are present."
  },
  {
    "objectID": "validation.html#basic-measures-of-accuracy",
    "href": "validation.html#basic-measures-of-accuracy",
    "title": "Validation and accuracy measurement",
    "section": "Basic measures of accuracy",
    "text": "Basic measures of accuracy\nThe metrics of producer’s accuracy, user’s accuracy, F1 score, and overall accuracy are derived from the confusion matrix (also known as the error matrix), which compares the classified map data to reference (ground truth) data. Each of these metrics provides a different perspective on the quality of classification.\nProducer’s Accuracy (PA) measures the probability that a reference (true) sample is correctly classified. It measures omission error — how often real features of a class are missed in the classification. It is measured the proportion of correctly classified instances relative to the total number of reference samples in a given class.\n\\[\n    \\text{PA}_i = \\frac{\\text{Correctly classified samples of class } i}{\\text{Total reference samples of class } i}\n    = \\frac{n_{ii}}{\\sum_j n_{ji}}\n\\]\nUser’s Accuracy (UA) is the probability that a sample classified as a given class actually belongs to that class in the reference data. It measures commission error — how often a class on the map includes misclassified pixels.\n\\[\n    \\text{UA}_i = \\frac{\\text{Correctly classified samples of class } i}{\\text{Total classified samples as class } i}\n    = \\frac{n_{ii}}{\\sum_j n_{ij}}\n\\]\nOverall Accuracy (OA) is the proportion of all samples that are correctly classified. It provides a general measure of classification success across all classes.\n\\[\n    \\text{OA} = \\frac{\\text{Correctly classified samples of class}}{\\text{Total classified samples}} = \\frac{\\sum_i n_{ii}}{N}\n\\]\nThe F1 score is a metric that balances user’s and producer’s accuracy into a single number. It is especially useful as a trade-off between false positives and false negatives. For a given class \\(i\\), the F1 score is the harmonic mean of UA and PA.\n\\[\nF1_i = 2 \\cdot \\frac{UA_i \\cdot PA_i}{UA_i + PA_i}\n\\]\nBy convention, we express the confusion matrix by placing the reference data (assumed to be the ground truth) in the columns and the respective map labels in the lines. In this way the UA and PA values are easily computed from the matrix.\n\n\nTable 1: Confusion matrix example with two classes\n\n\n\nReference class A\nReference class B\nUser Acc.\n\n\n\n\nMap class A\n40\n10\n0.80\n\n\nMap class B\n5\n45\n0.90\n\n\nProd Acc\n0.89\n0.82\n\n\n\nF1 score\n0.84\n0.86\n\n\n\n\n\nIn the data shown in Table 1, the producer’s accuracy (PA) for class A is 0.89 (40/45) and for class B is 0.82 (45/55). The user’s accuracy (UA) for class A is 0.80 (40/50) and for class B is 0.90 (45/50). The F1 score for class A is 0.84 and for class B is 0.87. The overall accuracy (OA) is 0.85 ((40 + 45) / 100)."
  },
  {
    "objectID": "validation.html#cross-validation",
    "href": "validation.html#cross-validation",
    "title": "Validation and accuracy measurement",
    "section": "Cross-validation",
    "text": "Cross-validation\nCross-validation is a widely used statistical technique for assessing the generalization performance of machine learning models. Its primary purpose is to provide an unbiased estimate of a model’s ability to perform on independent, unseen data, thereby helping to prevent overfitting.\nThe first step is to partition the available data into two distinct subsets: one for training and another for validation. The training set is used to develop the classification model, while the validation set is reserved exclusively for assessing the model’s performance. This separation is crucial to avoid biased accuracy estimates. Ideally, the partitioning should ensure that each land cover class is proportionally represented—this can be achieved through stratified random sampling.\nHowever, oerformance estimates obtained via cross-validation may not fully reflect the conditions encountered in real dara, especially when there is a significant distributional shift between the training data and real-world scenarios. Since this is a common situation is land data, measures of cross-validation are not a reliable prediction of map accuracy."
  },
  {
    "objectID": "validation.html#map-accuracy-measures",
    "href": "validation.html#map-accuracy-measures",
    "title": "Validation and accuracy measurement",
    "section": "Map accuracy measures",
    "text": "Map accuracy measures\nOnce the classifier is trained and the image is classified, a reliable set of reference data must be collected or prepared. This reference, or ground truth data, serves as the benchmark for assessing classification quality. It can be derived from field surveys, high-resolution satellite imagery, or expert interpretation, and it must be representative, accurate, and temporally consistent with the classified image.\nThe map accuracy assessment and area estimation procedures in sits follow the best practices outlined by Olofsson et al. [1], which provide a statistically rigorous framework for evaluating land cover and land change maps. These guidelines emphasize the importance of using probability sampling, transparent documentation, and unbiased estimation techniques to ensure that both classification accuracy and area statistics are reliable and reproducible.\nBy adhering to these principles, the assessment of classification results not only quantifies accuracy but also supports statistically sound area estimation, enabling robust and policy-relevant applications of remote sensing data."
  },
  {
    "objectID": "validation.html#references",
    "href": "validation.html#references",
    "title": "Validation and accuracy measurement",
    "section": "References",
    "text": "References\n\n\n\n\n[1] P. Olofsson, G. M. Foody, M. Herold, S. V. Stehman, C. E. Woodcock, and M. A. Wulder, “Good practices for estimating area and assessing accuracy of land change,” Remote Sensing of Environment, vol. 148, pp. 42–57, 2014."
  },
  {
    "objectID": "val_kfold.html#introduction",
    "href": "val_kfold.html#introduction",
    "title": "\n23  Cross-validation of training data\n",
    "section": "\n23.1 Introduction",
    "text": "23.1 Introduction\nCross-validation is a technique to estimate the inherent prediction error of a model [1]. Since cross-validation uses only the training samples, its results are not accuracy measures unless the samples have been carefully collected to represent the diversity of possible occurrences of classes in the study area [2]. In practice, when working in large areas, it is hard to obtain random stratified samples which cover the different variations in land classes associated with the ecosystems of the study area. Thus, cross-validation should be taken as a measure of model performance on the training data and not an estimate of overall map accuracy.\nCross-validation uses part of the available samples to fit the classification model and a different part to test it. The k-fold validation method splits the data into \\(k\\) partitions with approximately the same size and proceeds by fitting the model and testing it \\(k\\) times. At each step, we take one distinct partition for the test and the remaining \\({k-1}\\) for training the model and calculate its prediction error for classifying the test partition. A simple average gives us an estimation of the expected prediction error. The recommended choices of \\(k\\) are \\(5\\) or \\(10\\) [1]."
  },
  {
    "objectID": "val_kfold.html#using-k-fold-validation-in-sits",
    "href": "val_kfold.html#using-k-fold-validation-in-sits",
    "title": "\n23  Cross-validation of training data\n",
    "section": "\n23.2 Using k-fold validation in SITS",
    "text": "23.2 Using k-fold validation in SITS\nsits_kfold_validate() supports k-fold validation in sits. The result is the confusion matrix and the accuracy statistics (overall and by class). In the examples below, we use multiprocessing to speed up the results. The parameters of sits_kfold_validate are:\n\n\nsamples: training samples organized as a time series tibble;\n\nfolds: number of folds, or how many times to split the data (default = 5);\n\nml_method: ML/DL method to be used for the validation (default = random forest);\n\nmulticores: number of cores to be used for parallel processing (default = 2).\n\nBelow we show an example of cross-validation on the samples_matogrosso_mod13q1 dataset.\n\nrfor_validate_mt &lt;- sits_kfold_validate(\n    samples = samples_matogrosso_mod13q1,\n    folds = 5,\n    ml_method = sits_rfor(),\n    multicores = 5\n)\nrfor_validate_mt\n\n\nrfor_validate_mt &lt;- readRDS(\"./etc/rfor_validate_mt.rds\")\nrfor_validate_mt\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Pasture Soy_Corn Soy_Millet Soy_Cotton Cerrado Forest Soy_Fallow\n  Pasture        340        3          6          0       0      0          0\n  Soy_Corn         1      346          7         17       0      0          0\n  Soy_Millet       0       10        164          0       0      0          2\n  Soy_Cotton       1        5          2        335       0      0          0\n  Cerrado          2        0          0          0     378      2          0\n  Forest           0        0          0          0       1    129          0\n  Soy_Fallow       0        0          1          0       0      0         85\n\nOverall Statistics\n                             \n Accuracy : 0.9673           \n   95% CI : ( 0.9582, 0.975 )\n                             \n    Kappa : 0.9606           \n\nStatistics by Class:\n\n                     Class: Pasture Class: Soy_Corn Class: Soy_Millet\nProd Acc (Recall)            0.9884          0.9505            0.9111\nUser Acc (Precision)         0.9742          0.9326            0.9318\nF1 score                     0.9812          0.9415            0.9213\n                     Class: Soy_Cotton Class: Cerrado Class: Forest\nProd Acc (Recall)               0.9517         0.9974        0.9847\nUser Acc (Precision)            0.9767         0.9895        0.9923\nF1 score                        0.9640         0.9934        0.9885\n                     Class: Soy_Fallow\nProd Acc (Recall)               0.9770\nUser Acc (Precision)            0.9884\nF1 score                        0.9827\n\n\nThe results show a good validation, reaching 96% accuracy. However, this accuracy does not guarantee a good classification result. It only shows if the training data is internally consistent. In the next chapters, we present additional methods for measuring classification accuracy."
  },
  {
    "objectID": "val_kfold.html#summary",
    "href": "val_kfold.html#summary",
    "title": "\n23  Cross-validation of training data\n",
    "section": "\n23.3 Summary",
    "text": "23.3 Summary\nCross-validation measures how well the model fits the training data. Using these results to measure classification accuracy is only valid if the training data is a good sample of the entire dataset. Training data is subject to various sources of bias. In land classification, some classes are much more frequent than others, so the training dataset will be imbalanced. Regional differences in soil and climate conditions for large areas will lead the same classes to have different spectral responses. Field analysts may be restricted to places they have access (e.g., along roads) when collecting samples. An additional problem is mixed pixels. Expert interpreters select samples that stand out in fieldwork or reference images. Border pixels are unlikely to be chosen as part of the training data. For all these reasons, cross-validation results do not measure classification accuracy."
  },
  {
    "objectID": "val_kfold.html#references",
    "href": "val_kfold.html#references",
    "title": "\n23  Cross-validation of training data\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nT. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Data Mining, Inference, and Prediction. New York: Springer, 2009.\n\n\n[2] \nA. M. J.-C. Wadoux, G. B. M. Heuvelink, S. de Bruin, and D. J. Brus, “Spatial cross-validation is not the right way to evaluate map accuracy,” Ecological Modelling, vol. 457, p. 109692, 2021, doi: 10.1016/j.ecolmodel.2021.109692."
  },
  {
    "objectID": "val_map.html#introduction",
    "href": "val_map.html#introduction",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.1 Introduction",
    "text": "24.1 Introduction\nStatistically robust and transparent approaches for assessing accuracy are essential parts of the land classification process. The sits package supports the good practice recommendations for designing and implementing an accuracy assessment of a change map and estimating the area based on reference sample data. These recommendations address three components: sampling design, reference data collection, and accuracy estimates [1].\nCrucially, Olofsson et al. argue that area estimation should be based on the reference data, not solely on the classified map. This is because map-derived area estimates are often biased due to classification errors. By weighting the confusion matrix according to the sampling design, one can produce unbiased estimates of the area occupied by each land cover class, along with corresponding standard errors and confidence intervals. This correction is particularly important in studies where accurate area estimation informs policy or decision-making processes."
  },
  {
    "objectID": "val_map.html#example-data-set",
    "href": "val_map.html#example-data-set",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.2 Example data set",
    "text": "24.2 Example data set\nOur study area is the state of Rondonia (RO) in the Brazilian Amazon, which has a total area of . According to official Brazilian government statistics, as of 2021, there are of tropical forests in RO, which corresponds to 53% of the state’s total area. Significant human occupation started in 1970, led by settlement projects promoted by then Brazil’s military government [2]. Small and large-scale cattle ranching occupies most deforested areas. Deforestation in Rondonia is highly fragmented, partly due to the original occupation by small settlers. Such fragmentation poses considerable challenges for automated methods to distinguish between clear-cut and highly degraded areas. While visual interpreters rely upon experience and field knowledge, researchers must carefully train automated methods to achieve the same distinction.\nWe used Sentinel-2 and Sentinel-2A ARD (analysis ready) images from 2022-01-01 to 2022-12-31. Using all 10 spectral bands, we produced a regular data cube with a 16-day interval, with 23 instances per year. The best pixels for each period were selected to obtain as low cloud cover as possible. Persistent cloud cover pixels remaining in each period are then temporally interpolated to obtain estimated values. As a result, each pixel is associated with a valid time series. To fully cover RO, we used 41 MGRS tiles; the final data cube has 1.1 TB.\nThe work considered nine LUCC classes: (a) stable natural land cover, including Forest and Water; (b) events associated with clear-cuts, including Clear_Cut_Vegetation, Clear_Cut_Bare_Soil, and Clear_Cut_Burned_Area; (c) natural areas with seasonal variability, Wetland, Seasonally_Flooded_Forest, and Riparian_Forest; (d) stable forest areas subject to topographic effects, including Mountainside_Forest.\nIn this chapter, we will take the classification map as our starting point for accuracy assessment. This map can be retrieved from the sitsdata package as follows.\n\n# define the classes of the probability cube\nlabels &lt;- c(\"1\" = \"Clear_Cut_Bare_Soil\",\n            \"2\" = \"Clear_Cut_Burned_Area\", \n            \"3\" = \"Mountainside_Forest\", \n            \"4\" = \"Forest\", \n            \"5\" = \"Riparian_Forest\", \n            \"6\" = \"Clear_Cut_Vegetation\", \n            \"7\" = \"Water\",\n            \"8\" = \"Seasonally_Flooded\",\n            \"9\" = \"Wetland\")\n\n# directory where the data is stored \ndata_dir &lt;- system.file(\"extdata/Rondonia-Class-2022-Mosaic/\", package = \"sitsdata\")\n# create a probability data cube from a file \nrondonia_2022_class &lt;- sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = \"class\",\n    labels = labels,\n    version = \"mosaic\"\n)\n\n\n# plot the classification map\nplot(rondonia_2022_class)\n\n\n\n\n\nFigure 24.1: Classified mosaic for land cover in Rondonia, Brazil for 2022."
  },
  {
    "objectID": "val_map.html#sampling-design",
    "href": "val_map.html#sampling-design",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.3 Sampling design",
    "text": "24.3 Sampling design\nA key recommendation is the use of probability-based sampling designs—such as stratified random sampling—for the selection of reference data. This approach guarantees that each sample unit has a known, non-zero probability of selection, enabling the derivation of statistically valid and unbiased estimates. Moreover, stratified sampling is particularly effective in improving the precision of area estimates, especially when class distributions are imbalanced.\nThe reference data used for validation must be collected independently of the classification process and should be carefully labeled according to a set of clear, mutually exclusive, and exhaustive class definitions. These definitions must be consistently applied across both map labels and reference labels to avoid ambiguities during comparison.\nSampling designs use established statistical methods aimed at providing unbiased estimates. Based on a chosen design, sits supports a selection of random samples per class. These samples should be evaluated accurately using high-quality reference data, ideally collected through field visits or using high-resolution imagery. In this way, we get a reference classification that is more accurate than the map classification being evaluated.\nFollowing the recommended best practices for estimating accuracy of LUCC maps [1], sits uses Cochran’s method for stratified random sampling [3]. The method divides the population into homogeneous subgroups, or strata, and then applying random sampling within each stratum. In the case of LUCC, we take the classification map as the basis for the stratification. The area occupied by each class is considered as an homogeneous subgroup. Cochran’s method for stratified random sampling helps to increase the precision of the estimates by reducing the overall variance, particularly when there is significant variability between strata but relatively less variability within each stratum.\nTo determine the overall number of samples to measure accuracy, we use the following formula [3]:\n\\[\nn = \\left( \\frac{\\sum_{i=1}^L W_i S_i}{S(\\hat{O})} \\right)^2\n\\] where\n\n\n\\(L\\) is the number of classes\n\n\\(S(\\hat{O})\\) is the expected standard error of the accuracy estimate, expressed as a proportion\n\n\\(S_i\\) is the standard deviation of stratum \\(i\\)\n\n\n\\(W_i\\) is is the mapped proportion of area of class \\(i\\)\n\n\nThe standard deviation per class (stratum) is estimated based on the expected user’s accuracy \\(U_i\\) for each class as\n\\[\nS_i = \\sqrt{U_i(1 - U_i)}\n\\]\nTherefore, the total number of samples depends on the assumptions about the user’s accuracies \\(U_i\\) and the expected standard error \\(S(\\hat{O})\\). Once the sample size is estimated, there are several methods for allocating samples per class [1]. One option is proportional allocation, when sample size in each stratum is proportional to the stratum’s size in the population. In land use mapping, some classes often have small areas compared to the more frequent ones. Using proportional allocation, rare classes will have small sample sizes decreasing their accuracy. Another option is equal allocation, where all classes will have the same number of samples; however, equal allocation may fail to capture the natural variation of classes with large areas.\nAs alternatives to proportional and equal allocation, [1] suggests ad-hoc approaches where each class is assigned a minimum number of samples. He proposes three allocations where 50, 75 and 100 sample units are allocated to the less common classes, and proportional allocation is used for more frequent ones. These allocation methods should be considered as suggestions, and users should be flexible to select alternative sampling designs.\nThe allocation methods proposed by [1] are supported by function sits_sampling_design(), which has the following parameters:\n\n\ncube: a classified data cube;\n\nexpected_ua: a named vector with the expected user’s accuracies for each class;\n\nalloc_options: fixed sample allocation for rare classes;\n\nstd_err: expected standard error of the accuracy estimate;\n\nrare_class_prop: proportional area limit to determine which are the rare classes.\n\nIn the case of Rondonia, the following sampling design was adopted.\n\nro_sampling_design &lt;- sits_sampling_design(\n    cube = rondonia_2022_class,\n    expected_ua = c(\n        \"Clear_Cut_Bare_Soil\" = 0.75,\n        \"Clear_Cut_Burned_Area\" = 0.70, \n        \"Mountainside_Forest\" = 0.70, \n        \"Forest\" = 0.75,  \n        \"Riparian_Forest\" = 0.70, \n        \"Clear_Cut_Vegetation\" = 0.70,  \n        \"Water\" = 0.70, \n        \"Seasonally_Flooded\" = 0.70, \n         \"Wetland\" = 0.70\n    ),\n    alloc_options = c(120, 100),\n    std_err = 0.01,\n    rare_class_prop = 0.1\n)\n# show sampling desing\nro_sampling_design\n\n\n\n                      prop        expected_ua std_dev equal alloc_120 alloc_100\nClear_Cut_Bare_Soil   0.3841309   0.75        0.433   210   438       496      \nClear_Cut_Burned_Area 0.004994874 0.7         0.458   210   120       100      \nMountainside_Forest   0.004555433 0.7         0.458   210   120       100      \nForest                0.538726    0.75        0.433   210   614       696      \nRiparian_Forest       0.005482552 0.7         0.458   210   120       100      \nClear_Cut_Vegetation  0.009201698 0.7         0.458   210   120       100      \nWater                 0.007682599 0.7         0.458   210   120       100      \nSeasonally_Flooded    0.007677294 0.7         0.458   210   120       100      \nWetland               0.03754864  0.7         0.458   210   120       100      \n                      alloc_prop\nClear_Cut_Bare_Soil   727       \nClear_Cut_Burned_Area 9         \nMountainside_Forest   9         \nForest                1019      \nRiparian_Forest       10        \nClear_Cut_Vegetation  17        \nWater                 15        \nSeasonally_Flooded    15        \nWetland               71"
  },
  {
    "objectID": "val_map.html#stratified-random-sampling",
    "href": "val_map.html#stratified-random-sampling",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.4 Stratified random sampling",
    "text": "24.4 Stratified random sampling\nThe next step is to chose one of the options for sampling design to generate a set of points for stratified sampling. These points can then be used for accuracy assessment. This is achieved by function sits_stratified_sampling() which takes the following parameters:\n\n\ncube: a classified data cube;\n\nsampling_design: the output of function sits_sampling_design();\n\nalloc: one of the sampling allocation options produced by sits_sampling_design();\n\noverhead: additional proportion of number of samples per class (see below);\n\nmulticores: number of cores to run the function in parallel;\n\nshp_file: name of shapefile to save results for later use (optional);\n\nprogress: show progress bar?\n\nIn the example below, we chose the “alloc_120” option from the sampling design to generate a set of stratified samples. The output of the function is an sf object with points with location (latitude and longitude) and class assigned in the map. We can also generate a SHP file with the sample information. The script below shows how to use sits_stratified_sampling() and also how to convert an sf object to a SHP file.\n\nro_samples_sf &lt;- sits_stratified_sampling(\n    cube = rondonia_2022_class,\n    sampling_design = ro_sampling_design,\n    alloc = \"alloc_120\",\n    multicores = 4\n)\n\n\n# save sf object as SHP file\nsf::st_write(ro_samples_sf, \n    file.path(tempdir_r, \"ro_samples.shp\"), \n    append = FALSE\n)\n\nUsing the SHP file, users can visualize the points in a standard GIS such as QGIS. For each point, they will indicate what is the correct class. In this way, they will obtain a confusion matrix which will be used for accuracy assessment. The overhead parameter is useful for users to discard border or doubtful pixels where the interpreter cannot be confident of her class assignment. By discarding points whose attribution is uncertain, they will improve the quality of the assessment.\nAfter all sampling points are labelled in QGIS (or similar), users should produce a CSV file, a SHP file, a data frame, or an sf object, with at least three columns: latitude, longitude and label. See the next section for an example on how to use this data set for accuracay assessment."
  },
  {
    "objectID": "val_map.html#accuracy-assessment-of-classified-images",
    "href": "val_map.html#accuracy-assessment-of-classified-images",
    "title": "\n24  Map accuracy assessment\n",
    "section": "Accuracy assessment of classified images",
    "text": "Accuracy assessment of classified images\nTo measure the accuracy of classified images, sits_accuracy() uses an area-weighted technique, following the best practices proposed by Olofsson et al. [4]. The need for area-weighted estimates arises because the land classes are not evenly distributed in space. In some applications (e.g., deforestation) where the interest lies in assessing how much of the image has changed, the area mapped as deforested is likely to be a small fraction of the total area. If users disregard the relative importance of small areas where change is taking place, the overall accuracy estimate will be inflated and unrealistic. For this reason, Olofsson et al. argue that “mapped areas should be adjusted to eliminate bias attributable to map classification error, and these error-adjusted area estimates should be accompanied by confidence intervals to quantify the sampling variability of the estimated area” [4].\nWith this motivation, when measuring the accuracy of classified images, sits_accuracy() follows the procedure set by Olofsson et al. [4]. Given a classified image and a validation file, the first step calculates the confusion matrix in the traditional way, i.e., by identifying the commission and omission errors. Then it calculates the unbiased estimator of the proportion of area in cell \\(i,j\\) of the error matrix\n\\[\n\\hat{p_{i,j}} = W_i\\frac{n_{i,j}}{n_i},\n\\]\nwhere the total area of the map is \\(A_{tot}\\), the mapping area of class \\(i\\) is \\(A_{m,i}\\) and the proportion of area mapped as class \\(i\\) is \\(W_i = {A_{m,i}}/{A_{tot}}\\).\nAdjusting for area size allows producing an unbiased estimation of the total area of class \\(j\\), defined as a stratified estimator \\[\n\\hat{A_j} = A_{tot}\\sum_{i=1}^KW_i\\frac{n_{i,j}}{n_i}.\n\\]\nThis unbiased area estimator includes the effect of false negatives (omission error) while not considering the effect of false positives (commission error). The area estimates also allow for an unbiased estimate of the user’s and producer’s accuracy for each class. Following Olofsson et al. [4], we provide the 95% confidence interval for \\(\\hat{A_j}\\).\nTo produce the adjusted area estimates for classified maps, sits_accuracy() uses the following parameters:\n\n\ndata: a classified data cube;\n\nvalidation: a CSV file, SHP file, GPKG file, sf object or data frame containing at least three columns: latitude, longitude and label, containing a set of well-selected labeled points obtained from the samples suggested by sits_stratified_sample().\n\nIn the example below, we use a validation set produced by the researchers which produced the Rondonia data set, described above. We selected this data set both to serve as an example of sits_accuracy() and to illustrate the pitfalls of using visual interpretation of results of image time series classification. In this case, the validation team used an image from a single date late in 2022 to assess the results. This choice is not adequate for assessing results of time series classification. In many cases, including the example used in this chapter, the training set includes transitional classes such as Clear_Cut_Burned_Area and Clear_Cut_Vegetation. The associated samples refer to events that occur in specific times of the year. An area may start the year as a Forest land cover, only to be cut and burned during the peak of the dry season and later be completely clean. The classifier will recognize the signs of burned area and will signal that such event occurred. When using only a single date to evaluate the classification results, this correct estimate by the classifier will be missed by the interpreter. For this reason, the results shown below are merely illustrative and do not reflect a correct accuracy assessment.\nThe validation team used QGIS to produce a CSV file with validation data, which is then used to assess the area accuracy using the best practices recommended by [1].\n\n# Get ground truth points\nvalid_csv &lt;- system.file(\"extdata/Rondonia-Class-2022-Mosaic/rondonia_samples_validation.csv\", package = \"sitsdata\")\n# Calculate accuracy according to Olofsson's method\narea_acc &lt;- sits_accuracy(rondonia_2022_class, \n                          validation = valid_csv,\n                          multicores = 4)\n\n\n# Print the area estimated accuracy \narea_acc\n\nArea Weighted Statistics\nOverall Accuracy = 0.84\n\nArea-Weighted Users and Producers Accuracy\n                      User Producer\nClear_Cut_Bare_Soil   0.82     1.00\nClear_Cut_Burned_Area 0.88     0.08\nMountainside_Forest   0.69     0.05\nForest                0.85     1.00\nRiparian_Forest       0.66     0.58\nClear_Cut_Vegetation  0.82     0.24\nWater                 0.97     0.67\nSeasonally_Flooded    0.86     0.68\nWetland               0.87     0.69\n\nMapped Area x Estimated Area (ha)\n                      Mapped Area (ha) Error-Adjusted Area (ha)\nClear_Cut_Bare_Soil          9537617.8                7787913.8\nClear_Cut_Burned_Area         124018.1                1383784.0\nMountainside_Forest           113107.2                1665469.0\nForest                      13376070.4               11377193.6\nRiparian_Forest               136126.7                 155704.6\nClear_Cut_Vegetation          228469.7                 766171.1\nWater                         190751.9                 275599.8\nSeasonally_Flooded            190620.2                 241225.8\nWetland                       932298.3                1176018.6\n                      Conf Interval (ha)\nClear_Cut_Bare_Soil            321996.87\nClear_Cut_Burned_Area          278746.61\nMountainside_Forest            299925.62\nForest                         333181.28\nRiparian_Forest                 60452.25\nClear_Cut_Vegetation           186476.04\nWater                           78786.79\nSeasonally_Flooded              58098.50\nWetland                        163726.86\n\n\nThe confusion matrix is also available, as follows.\n\narea_acc$error_matrix\n\n                       \n                        Clear_Cut_Bare_Soil Clear_Cut_Burned_Area\n  Clear_Cut_Bare_Soil                   415                    65\n  Clear_Cut_Burned_Area                   1                    42\n  Mountainside_Forest                     1                     0\n  Forest                                  0                     0\n  Riparian_Forest                         4                     0\n  Clear_Cut_Vegetation                    1                    17\n  Water                                   0                     0\n  Seasonally_Flooded                      0                     0\n  Wetland                                 0                     2\n                       \n                        Mountainside_Forest Forest Riparian_Forest\n  Clear_Cut_Bare_Soil                     0      0               0\n  Clear_Cut_Burned_Area                   0      0               0\n  Mountainside_Forest                    22      9               0\n  Forest                                 95    680               3\n  Riparian_Forest                         4      5             111\n  Clear_Cut_Vegetation                    0      0               0\n  Water                                   0      0               3\n  Seasonally_Flooded                      0      0               1\n  Wetland                                 0      0               1\n                       \n                        Clear_Cut_Vegetation Water Seasonally_Flooded Wetland\n  Clear_Cut_Bare_Soil                     10     3                  1      15\n  Clear_Cut_Burned_Area                    1     0                  1       3\n  Mountainside_Forest                      0     0                  0       0\n  Forest                                  19     2                  0       3\n  Riparian_Forest                         43     0                  0       0\n  Clear_Cut_Vegetation                    82     0                  0       0\n  Water                                    0   121                  1       0\n  Seasonally_Flooded                       0     1                118      18\n  Wetland                                  4     0                  6      88\n\n\nThese results show the challenges of conducting validation assessments with image time series. While stable classes like Forest and Clear_Cut_Bare_Soil exhibit high user’s accuracy (UA) and producer’s accuracy (PA), the transitional classes (Clear_Cut_Burned_Area and Clear_Cut_Vegetation) have low PA. This discrepancy is not a true reflection of classification accuracy, but rather a result of inadequate visual interpretation practices. As mentioned earlier, the visual interpretation for quality assessment utilised only a single date, a method traditionally used for single images, but ineffective for image time series.\nA detailed examination of the confusion matrix reveals a clear distinction between natural areas (e.g., Forest and Riparian_Forest) and areas associated with deforestation (e.g., Clear_Cut_Bare_Soil and Clear_Cut_Burned_Area). The low producer’s accuracy values for transitional classes Clear_Cut_Burned_Area and Clear_Cut_Vegetation are artefacts of the validation procedure. Validation relied on only one date near the end of the calendar year, causing transitional classes to be overlooked."
  },
  {
    "objectID": "val_map.html#summary",
    "href": "val_map.html#summary",
    "title": "\n24  Map accuracy assessment\n",
    "section": "\n24.5 Summary",
    "text": "24.5 Summary\nThis chapter provides an example of the recommended statistical methods for designing stratified samples for accuracy assessment. However, these sampling methods depend on perfect or near-perfect validation by end-users. Ensuring best practices in accuracy assessment involves a well-designed sample set and a sample interpretation that aligns with the classifier’s training set.\n\n\n\n\n[1] \nP. Olofsson, G. M. Foody, M. Herold, S. V. Stehman, C. E. Woodcock, and M. A. Wulder, “Good practices for estimating area and assessing accuracy of land change,” Remote Sensing of Environment, vol. 148, pp. 42–57, 2014.\n\n\n[2] \nD. S. Alves, M. I. S. Escada, J. L. G. Pereira, and C. de Albuquerque Linhares, “Land use intensification and abandonment in Rondônia, Brazilian Amazônia,” International Journal of Remote Sensing, vol. 24, no. 4, pp. 899–903, 2003, doi: 10.1080/0143116021000015807.\n\n\n[3] \nW. G. Cochran, Sampling techniques. john wiley & sons, 1977.\n\n\n[4] \nP. Olofsson, G. M. Foody, S. V. Stehman, and C. E. Woodcock, “Making better use of accuracy data in land change studies: Estimating accuracy and area and quantifying uncertainty using stratified estimation,” Remote Sensing of Environment, vol. 129, pp. 122–131, 2013, doi: 10.1016/j.rse.2012.10.031."
  },
  {
    "objectID": "vector_datacubes.html",
    "href": "vector_datacubes.html",
    "title": "Vector data cubes",
    "section": "",
    "text": "The concept of vector data cubes represents a multidimensional data structure designed to manage, analyze, and query vector-based geospatial data across spatial, temporal, and thematic dimensions. This model extends the well-established framework of raster data cubes—commonly used in remote sensing and Earth observation—to the domain of vector data, which comprises geometries such as points, lines, and polygons that represent discrete spatial features.\nVector data cubes are structured to encapsulate geospatial features that evolve over time and carry associated attributes. Each element of the cube corresponds to a spatiotemporal observation of a vector object, characterized by its geometry, timestamp, and a set of descriptive attributes. Formally, a vector data cube can be described as a collection of tuples\n\\[\nVC = {(g_i, t_j, a_{i_j})}\n\\]\nwhere \\(g_i\\) denotes the spatial geometry, \\(t_j\\) are temporal instances, and \\(a_{i_j})\\) the vector of thematic attributes associated with that geometry at that point in time.\nThe spatial dimension captures the geometric representation of real-world entities (e.g., land parcels, administrative units), while the temporal dimension allows for tracking changes over time, and the attribute dimensions store thematic variables of interest (e.g., land use class, vegetation type). Vector data cubes are particularly advantageous in contexts where object-based spatial representations are more appropriate than gridded data.\nThe sits package supports a restricted version of vector data cubes. In sits, the polygons that make up a vector data cube are calculated using all temporal instances of the data cube. One of the key measures used to build vectors from raster data cube is the local similarity between pixel values. In sits, these similares are computed from all bands and all dates of the data cube.\nThis representation is useful for classification of land parcels whose boundaries have been computed by segmentation algorithms or which are part of a land cadastre. The basic idea is that sits will combine raster and vector information by classifying polygons based on time series defined by their interior pixels. In the following chapters, we will describe how sits can attach a vector data structure (e.g., a polygon shapefile) to an existing raster data cube. We will also show how to extract polygons based on segmentation methods such as SLIC superpixels algorithm."
  },
  {
    "objectID": "vec_obia.html#image-segmentation-in-sits",
    "href": "vec_obia.html#image-segmentation-in-sits",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "Image segmentation in sits",
    "text": "Image segmentation in sits\nThe first step of the OBIA procedure in sits is to select a data cube to be segmented and function that performs the segmentation. For this purpose, sits provides a generic sits_segment() function, which allows users to select different segmentation algorithms. The sits_segment() function has the following parameters:\n\n\ncube: a regular data cube.\n\nseg_fn: function to apply the segmentation\n\nroi: spatial region of interest in the cube\n\nstart_date: starting date for the space-time segmentation\n\nend_date: final date for the space-time segmentation\n\nmemsize: memory available for processing\n\nmulticores: number of cores available for processing\n\noutput_dir: output directory for the resulting cube\n\nversion: version of the result\n\nprogress: show progress bar?\n\nIn sits version 1.4.2, there is only one segmentation function available (sits_slic) which implements the extended version of the Simple Linear Iterative Clustering (SLIC) which is described below. In future versions of sits, we expect to include additional functions that support spatio-temporal segmentation."
  },
  {
    "objectID": "vec_obia.html#simple-linear-iterative-clustering-algorithm",
    "href": "vec_obia.html#simple-linear-iterative-clustering-algorithm",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "Simple linear iterative clustering algorithm",
    "text": "Simple linear iterative clustering algorithm\nAfter building the multidimensional space, we use the Simple Linear Iterative Clustering (SLIC) algorithm [1] that clusters pixels to efficiently generate compact, nearly uniform superpixels. This algorithm has been adapted by Nowosad and Stepinski [2] to work with multispectral images. SLIC uses spectral similarity and proximity in the image space to segment the image into superpixels. Superpixels are clusters of pixels with similar spectral responses that are close together, which correspond to coherent object parts in the image. Here’s a high-level view of the extended SLIC algorithm:\n\nThe algorithm starts by dividing the image into a grid, where each cell of the grid will become a superpixel.\nFor each cell, the pixel in the center becomes the initial “cluster center” for that superpixel.\nFor each pixel, the algorithm calculates a distance to each of the nearby cluster centers. This distance includes both a spatial component (how far the pixel is from the center of the superpixel in terms of x and y coordinates) and a spectral component (how different the pixel’s spectral values are from the average values of the superpixel). The spectral distance is calculated using all the temporal instances of the bands.\nEach pixel is assigned to the closest cluster. After all pixels have been assigned to clusters, the algorithm recalculates the cluster centers by averaging the spatial coordinates and spectral values of all pixels within each cluster.\nSteps 3-4 are repeated for a set number of iterations, or until the cluster assignments stop changing.\n\nThe outcome of the SLIC algorithm is a set of superpixels which try to capture the to boundaries of objects within the image. The SLIC implementation in sits 1.4.1 uses the supercells R package [2]. The parameters for the sits_slic() function are:\n\n\ndist_fn: metric used to calculate the distance between values. By default, the “euclidean” metric is used. Alternatives include “jsd” (Jensen-Shannon distance), and “dtw” (dynamic time warping) or one of 46 distance and similarity measures implemented in the R package philentropy [3].\n\navg_fn: function to calculate a value of each superpixel. There are two internal functions implemented in C++ - “mean” and “median”. It is also possible to provide a user-defined R function that returns one value based on an R vector.\n\nstep: distance, measured in the number of cells, between initial superpixels’ centers.\n\ncompactness: A value that controls superpixels’ density. Larger values cause clusters to be more compact.\n\nminarea: minimal size of the output superpixels (measured in number of cells)."
  },
  {
    "objectID": "vec_obia.html#example-of-slic-based-segmentation-and-classification",
    "href": "vec_obia.html#example-of-slic-based-segmentation-and-classification",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "Example of SLIC-based segmentation and classification",
    "text": "Example of SLIC-based segmentation and classification\nTo show an example of SLIC-based segmentation, we first build a data cube, using images available in the sitsdata package.\n\n# directory where files are located\ndata_dir &lt;- system.file(\"extdata/Rondonia-20LMR\", package = \"sitsdata\")\n# Builds a cube based on existing files\ncube_20LMR &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-2-L2A\",\n    data_dir = data_dir,\n    bands = c(\"B02\", \"B8A\", \"B11\")\n)\nplot(cube_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", date = \"2022-07-16\")\n\n\n\n\n\nFigure 25.1: RGB composite image for part of tile 20LMR in Rondonia, Brasil.\n\n\n\nThe following example produces a segmented image. For the SLIC algorithm, we take the initial separation between cluster centres (step) to be 20 pixels, the compactness to be 1, and the minimum area for each superpixel (min_area) to be 20 pixels.\n\n# segment a cube using SLIC\n# Files are available in a local directory \nsegments_20LMR &lt;- sits_segment(\n    cube = cube_20LMR,\n    output_dir = tempdir_r,\n    seg_fn = sits_slic(\n        step = 20,\n        compactness = 1,\n        dist_fun = \"euclidean\",\n        iter = 20,\n        minarea = 20\n    )\n)\nplot(segments_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", \n          date = \"2022-07-16\")\n\n\n\n\n\nFigure 25.2: Image with segments for part of tile 20LMR in Rondonia, Brasil.\n\n\n\nIt is useful to visualize the segments in a leaflet together with the RGB image using sits_view().\n\nsits_view(segments_20LMR, red = \"B11\", green = \"B8A\", blue = \"B02\", \n          dates = \"2022-07-16\")\n\n\n\n\n\nFigure 25.3: Detail of segementation of image in Amazonia\n\n\n\nAfter obtaining the segments, the next step is to classify them. This is done by first training a classification model. This case study uses the training dataset samples_deforestation_rondonia, available in package sitsdata. This dataset consists of 6007 samples collected from Sentinel-2 images covering the state of Rondonia. There are nine classes: Clear_Cut_Bare_Soil, Clear_Cut_Burned_Area, Mountainside_Forest, Forest, Riparian_Forest, Clear_Cut_Vegetation, Water, Wetland, and Seasonally_Flooded. Each time series contains values from Sentinel-2/2A bands B02, B03, B04, B05, B06, B07, B8A, B08, B11 and B12, from 2022-01-05 to 2022-12-23 in 16-day intervals. The samples are intended to detect deforestation events and have used in earlier examples of the book. For the training, we select the same bands as those of the data cube.\n\nsamples_deforestation &lt;- sits_select(samples_deforestation_rondonia,\n                                     bands = c(\"B02\", \"B8A\", \"B11\"))\ntcnn_model &lt;- sits_train(samples_deforestation, sits_tempcnn())\n\nThe segment classification procedure applies the model to a number of user-defined samples inside each segment. Each of these samples is then assigned a set of probability values, one for each class. We then obtain the median value of the probabilities for each class and normalize them. The output of the procedure is a vector data cube containing a set of classified segments. The most relevsnt parameters for the sits_classify() function are:\n\n\ndata: vector cube being classified\n\nml_model: machine learning model\n\noutput_dir: directory where results are stored\n\nn_sam_pol: number of samples to choose for each polygon\n\nmemsize: memory available for classification in GB.\n\nmulticores: number of cores to be used for classification\n\ngpu_memory: GPU memory available (if there is a GPU)\n\n\nversion: version name (to control for multiple versions)\n\n\nsegments_20LMR_probs &lt;- sits_classify(\n    data = segments_20LMR,\n    ml_model = tcnn_model,\n    output_dir = tempdir_r,\n    n_sam_pol = 10,\n    gpu_memory = 2,\n    memsize = 24,\n    multicores = 1,\n    version = \"tcnn\"\n)\n\nAfter computing the probabilities for the segments, we can visualize them.\n\nplot(segments_20LMR_probs, labels = \"Forest\")\n\n\n\n\n\nFigure 25.4: Probability maps for the Forest class inside segments.\n\n\n\nFinally, we can compute the most likely class for each of the segments.\n\nsegments_20LMR_class &lt;- sits_label_classification(\n    segments_20LMR_probs,\n    output_dir = tempdir_r,\n    memsize = 24,\n    multicores = 6,\n    version = \"tcnn\"\n)\n\nTo view the classified segments together with the original image, use plot() or sits_view(), as in the following example.\n\n# view the classified segments\nsits_view(\n    segments_20LMR_class, \n    red = \"B11\", \n    green = \"B8A\", \n    blue = \"B02\", \n    dates = \"2022-07-16\",\n)\n# put the original segments on top\nsits_view(segments_20LMR, add = TRUE)\n\n\n\n\n\nFigure 25.5: Detail of labeled segments for image in Rondonia, Brazil."
  },
  {
    "objectID": "vec_obia.html#summary",
    "href": "vec_obia.html#summary",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "\n25.1 Summary",
    "text": "25.1 Summary\nOBIA analysis applied to image time series is a worthy and efficient technique for land classification, combining the desirable sharp object boundary properties required by land use and cover maps with the analytical power of image time series."
  },
  {
    "objectID": "vec_obia.html#references",
    "href": "vec_obia.html#references",
    "title": "\n25  Object-based time series image analysis\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nR. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk, “SLIC Superpixels Compared to State-of-the-Art Superpixel Methods,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 11, pp. 2274–2282, 2012, doi: 10.1109/TPAMI.2012.120.\n\n\n[2] \nJ. Nowosad and T. F. Stepinski, “Extended SLIC superpixels algorithm for applications to non-imagery geospatial rasters,” International Journal of Applied Earth Observation and Geoinformation, vol. 112, p. 102935, 2022, doi: 10.1016/j.jag.2022.102935.\n\n\n[3] \nH.-G. Drost, “Philentropy: Information Theory and Distance Quantification with R,” Journal of Open Source Software, vol. 3, no. 26, p. 765, 2018, doi: 10.21105/joss.00765."
  },
  {
    "objectID": "vec_creating.html#introduction",
    "href": "vec_creating.html#introduction",
    "title": "\n23  Creating vector data cube from local files\n",
    "section": "\n23.1 Introduction",
    "text": "23.1 Introduction\nIn many situations, user may want to create vector data cubes from local files. The most common case is when the want to recover a vector file has been previously created. An alternative is when users want to join a vector data file to a raster data cube. Recalll that sits uses a restricted notion of vector data cubes, which are single polygon sets that cover an entire period of time. One example are farm boundaries in case of agricultural statistics.\nTo be used for classification, vector cubes are associated to a raster cube. In this way, classification is done on a polygon-by-polygon basis. Given a polygon which matches part of a raster cube, sits uses pixels inside the polygon to obtain a set of time series that are used for classification, as shown in the example below.\nIn what follows, we illustrate the procedure of linking vector and raster files by creating segments from a raster data cube and then recovering them later."
  },
  {
    "objectID": "vec_creating.html#producing-a-vector-data-cube",
    "href": "vec_creating.html#producing-a-vector-data-cube",
    "title": "\n23  Creating vector data cube from local files\n",
    "section": "\n23.2 Producing a vector data cube",
    "text": "23.2 Producing a vector data cube\nWe take the MODIS data set for a small area in Mato Grosso, Brazil available in the sits package. For details on how to retrieve raster data cubes from local files, please see chapter “Data cubes from local files”.\n\n# Retrieve a local cube based with MODIS data\n# data directory\ndata_dir &lt;- system.file(\"extdata/raster/mod13q1\", package = \"sits\")\n# local cube\nmodis_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir,\n    parse_info = c(\"satellite\", \"sensor\", \"tile\", \"band\", \"date\")\n)\n\nThen, we produce a vector data cube associated to the raster data cube using sits_segment, as described in the previous chapter.\n\n# segment the vector cube\nsegs_cube &lt;- sits_segment(\n    cube = modis_cube,\n    output_dir = tempdir_r \n)\n## plot the segmented cube\nplot(segs_cube)\n\nno bands provided - using a best guess color composite\n\n\n\n\n\nThe resulting vector file is stored in the tempdir directory, following a convention set by sits that vector files need to be stored in the geopackage format, and should contained closed polygons which are geometrically consistent. Listing the resulting file, one can better understand the convention.\n\nlist.files(tempdir_r, pattern = \"*segments*\")\n\n[1] \"TERRA_MODIS_012010_2013-09-14_2014-08-29_segments_v1.gpkg\"\n\n\nThe segments file is a geopackage whose title contains optional information on satellite and sensor and mandatory information on tile, start_date, end_date, band (in this case segments) and version. Any geopackage polygon file that contains these required items can be imported and linked to a corresponding raster file, as shown below."
  },
  {
    "objectID": "vec_creating.html#linking-raster-and-vector-data",
    "href": "vec_creating.html#linking-raster-and-vector-data",
    "title": "\n23  Creating vector data cube from local files\n",
    "section": "\n23.3 Linking raster and vector data",
    "text": "23.3 Linking raster and vector data\nNow consider the inverse situation, where a set of a segments is already stored locally and users want to recover them and associate to a raster data cube. In the same way as we\nTo do so, they need to use sits_cube(), which the following minimum parameters:\n\n\nsource: data source of the raster data (in this case, the Brazil Data Cube).\n\ncollection: ARD collection associated to the images (in the example, `MOD13Q1-6.1”).\n\nraster_cube: raster data cube to associate with the polygons.\n\nvector_dir: directory where the vectors are stored.\n\nvector_band: kind of vector cube. Either segments for polygons, probs for class probabilities associated to each polygon, or class for the classified areas.\n\nparse_info: where to find metadata on the file title. The minimum required elements are tile, start_date, end_date, band and version. The start and end dates must match those of the raster cube.\n\ndelim: delimiter character for breaking down the file into parts (“_” by default.)\n\nIn the example below, we recover the file we produced before. We could replace it by any other polygon file in geopackage format that matches the parse_info information listed above. As satellite and sensor are optional, we use place holders from them.\n\n# recover the local segmented cube\nlocal_segs_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir_r,\n    vector_band = \"segments\",\n    parse_info =  c(\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\n# plot the recover model and compare with previous plot\nplot(local_segs_cube)\n\nThe resulting plot should be the same as the original one. We now train a model with the random forests algorithm, and use the result to classify the segments. Then we show how to recover the classified data from the local files which was produced by the classifier.\n\n# classify the segments\n# create a random forest model\nrfor_model &lt;- sits_train(samples_modis_ndvi, sits_rfor())\nprobs_vector_cube &lt;- sits_classify(\n    data = segs_cube,\n    ml_model = rfor_model,\n    output_dir = tempdir_r,\n    n_sam_pol = 10\n)\nplot(probs_vector_cube, labels = \"Forest\")\n\n\n\n\nIn the same way as before, we can recover the probability cube from local files using sits_cube().\n\n# recover vector cube\nlocal_probs_vector_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir_r,\n    vector_band = \"probs\",\n    parse_info =  c(\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\nplot(local_probs_vector_cube, labels = \"Forest\")\n\nFinally, we produce a classified map.\n\n# label the segments\nclass_vector_cube &lt;- sits_label_classification(\n    cube = probs_vector_cube,\n    output_dir = tempdir(),\n)\nplot(class_vector_cube)\n\n\n\n\nWe then recover the map from a local file.\n\n# recover vector cube\nlocal_class_vector_cube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    raster_cube = modis_cube,\n    vector_dir = tempdir(),\n    vector_band = \"class\",\n    parse_info =  c(\"X1\", \"X2\", \"tile\", \"start_date\", \"end_date\", \"band\", \"version\")\n)\nplot(local_class_vector_cube)"
  },
  {
    "objectID": "vec_creating.html#summary",
    "href": "vec_creating.html#summary",
    "title": "\n23  Creating vector data cube from local files\n",
    "section": "\n23.4 Summary",
    "text": "23.4 Summary\nIn this chapter, we show how mix vector and raster data cubes. For the operation to succeed, the vector cube must be a geopackage file whose polygons and contained inside the raster data cube and files need to provide adequate metadata. The polygon file can come from outside sources, provided it spatio-temporal coordinates match those of the raster data cube to which it is being merged. This allows much flexibility to users to combine external polygon files to sits."
  },
  {
    "objectID": "annex.html",
    "href": "annex.html",
    "title": "Advanced Topics",
    "section": "",
    "text": "The next chapters contains technical details on sits and are intended mainly for developers. It is intended to support those that want to understand how the package works and also want to contribute to its development. It includes general principles of sits software development, describes how parallel processing works. It shows how to support new STAC-based cloud collections and how to include new methods for machine learning in sits. It discusses how to export data to other packages and provides a comparison between SITS and Google Earth Engine."
  },
  {
    "objectID": "annex_export.html#exporting-time-series-data-to-sf",
    "href": "annex_export.html#exporting-time-series-data-to-sf",
    "title": "\n27  Exporting data to other packages\n",
    "section": "\n27.1 Exporting time series data to sf",
    "text": "27.1 Exporting time series data to sf\nThe sf package is the backbone of geospatial vector processing in R [1]. To export time series tibbles from sits to sf, the function sits_as_sf() creates an sf POINT object with the locations of each sample and includes the time_series column as a list. Each row in the sf object contains the time series associated to the sample.\n\n# Export a sits tibble to sf\nsf_obj &lt;- sits_as_sf(samples_modis_ndvi)\n# Display the sf object\nsf_obj\n\nSimple feature collection with 1218 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -60.4875 ymin: -17.4373 xmax: -51.565 ymax: -9.3126\nGeodetic CRS:  WGS 84\n# A tibble: 1,218 × 9\n   crs                  geometry longitude latitude start_date end_date  \n * &lt;chr&gt;             &lt;POINT [°]&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;    \n 1 EPSG:4326 (-55.1852 -10.8378)     -55.2   -10.8  2013-09-14 2014-08-29\n 2 EPSG:4326   (-57.794 -9.7573)     -57.8    -9.76 2006-09-14 2007-08-29\n 3 EPSG:4326 (-51.9412 -13.4198)     -51.9   -13.4  2014-09-14 2015-08-29\n 4 EPSG:4326 (-55.9643 -10.0621)     -56.0   -10.1  2005-09-14 2006-08-29\n 5 EPSG:4326  (-54.554 -10.3749)     -54.6   -10.4  2013-09-14 2014-08-29\n 6 EPSG:4326 (-52.4572 -10.9512)     -52.5   -11.0  2013-09-14 2014-08-29\n 7 EPSG:4326 (-52.1443 -13.9981)     -52.1   -14.0  2013-09-14 2014-08-29\n 8 EPSG:4326 (-57.6907 -13.3382)     -57.7   -13.3  2015-09-14 2016-08-28\n 9 EPSG:4326 (-54.7034 -16.4265)     -54.7   -16.4  2015-09-14 2016-08-28\n10 EPSG:4326 (-56.7898 -11.4209)     -56.8   -11.4  2011-09-14 2012-08-28\n# ℹ 1,208 more rows\n# ℹ 3 more variables: label &lt;chr&gt;, cube &lt;chr&gt;, time_series &lt;list&gt;"
  },
  {
    "objectID": "annex_export.html#exporting-data-cubes-to-stars",
    "href": "annex_export.html#exporting-data-cubes-to-stars",
    "title": "\n27  Exporting data to other packages\n",
    "section": "\n27.2 Exporting data cubes to stars",
    "text": "27.2 Exporting data cubes to stars\nThe stars R package handles spatiotemporal arrays. It provides a general framework for working with raster and vector data. Data cubes in sits can be converted to stars objects using sits_as_stars(). By default, the stars object will be loaded in memory. This can result in heavy memory usage. To produce a stars.proxy object, users have to select a single date, since stars does not allow proxy objects to be created with two dimensions.\n\n# create a data cube from a local set of TIFF files\n# this is a cube with 23 instances and one band (\"NDVI\")\ndata_dir &lt;- system.file(\"extdata/raster/mod13q1\", package = \"sits\")\ncube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir\n)\nstars_object &lt;- sits_as_stars(cube)\n# plot the first date of the stars object\nplot(stars_object[,,,,1])"
  },
  {
    "objectID": "annex_export.html#exporting-data-cubes-to-terra",
    "href": "annex_export.html#exporting-data-cubes-to-terra",
    "title": "\n27  Exporting data to other packages\n",
    "section": "\n27.3 Exporting data cubes to terra",
    "text": "27.3 Exporting data cubes to terra\nThe terra package in R is a high-performance framework for spatial raster and vector data analysis. It was developed as the successor to the older raster package, offering a faster, more memory-efficient, and flexible API for working with geographic data. To export data cubes to terra, sits uses sits_as_terra() function which takes information about files, bands and dates in a data cube to produce an object of class SpatRaster in terra. Because terra does not understand multi-tiles and multi-temporal cubes, users have to select a tile and a date from the data cube. By default, all bands are included in the terra object, but users can also select which bands to export.\n\n# create a data cube from a local set of TIFF files\n# this is a cube with 23 instances and one band (\"NDVI\")\ndata_dir &lt;- system.file(\"extdata/raster/mod13q1\", package = \"sits\")\ncube &lt;- sits_cube(\n    source = \"BDC\",\n    collection = \"MOD13Q1-6.1\",\n    data_dir = data_dir\n)\nterra_object &lt;- sits_as_terra(cube, date = \"2013-09-14\")\n# plot the first date of the stars object\nterra_object\n\nclass       : SpatRaster \ndimensions  : 147, 255, 1  (nrow, ncol, nlyr)\nresolution  : 231.6564, 231.6564  (x, y)\nextent      : -6073798, -6014726, -1312333, -1278280  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \nsource      : TERRA_MODIS_012010_NDVI_2013-09-14.jp2 \nname        : TERRA_MODIS_012010_NDVI_2013-09-14 \n\n\n\n\n\n\n[1] \nE. Pebesma, “Simple Features for R: Standardized Support for Spatial Vector Data,” The R Journal, vol. 10, no. 1, p. 439, 2018, doi: 10.32614/RJ-2018-009."
  },
  {
    "objectID": "annex_gee.html#introduction",
    "href": "annex_gee.html#introduction",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.1 Introduction",
    "text": "28.1 Introduction\nThis section presents a side-by-side exploration of the sits and Google Earth Engine (gee) APIs, focusing on their respective capabilities in handling satellite data. The exploration is structured around three key examples: (1) creating a mosaic, (2) calculating the Normalized Difference Vegetation Index (NDVI), and (3) performing a Land Use and Land Cover (LULC) classification. Each example demonstrates how these tasks are executed using sits and gee, offering a clear view of their methodologies and highlighting the similarities and the unique approaches each API employs."
  },
  {
    "objectID": "annex_gee.html#creating-a-mosaic",
    "href": "annex_gee.html#creating-a-mosaic",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.2 Creating a Mosaic",
    "text": "28.2 Creating a Mosaic\nA common application among scientists and developers in the field of Remote Sensing is the creation of satellite image mosaics. These mosaics are formed by combining two or more images, typically used for visualization in various applications. In this example, we will demonstrate how to create an image mosaic using sits and gee APIs.\nIn this example, a Region of Interest (ROI) is defined using a bounding box with longitude and latitude coordinates. Below are the code snippets for specifying this ROI in both sits and gee environments.\n\n\nSITS\nGEE\n\n\n\n\nroi &lt;- c(\"lon_min\" = -63.410, \"lat_min\" = -9.783,\n         \"lon_max\" = -62.614, \"lat_max\" = -9.331)\n\n\n\n\nroi = ee.Geometry.Rectangle([-63.410,-9.783,-62.614,-9.331]);\n\n\n\n\nNext, we will load the satellite imagery. For this example, we used data from Sentinel-2. In sits, several providers offer Sentinel-2 ARD images. In this example, we will use images provided by the Microsoft Planetary Computer (MPC).\n\n\nSITS\nGEE\n\n\n\n\ndata &lt;- sits_cube(\n  source     = \"MPC\",\n  collection = \"SENTINEL-2-L2A\",\n  bands      = c(\"B02\", \"B03\", \"B04\"),\n  tiles      = c(\"20LNQ\", \"20LMQ\"),\n  start_date = \"2024-08-01\",\n  end_date   = \"2024-08-03\"\n)\n\n\n\n\ndata = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n  .filterDate('2024-08-01', '2024-08-03')\n  .filter(ee.Filter.inList('MGRS_TILE', ['20LNQ', '20LMQ']))\n  .select(['B4', 'B3', 'B2']);\n\n\n\n\nsits provides search filters for a collection as parameters in the sits_cube() function, whereas gee offers these filters as methods of an ImageCollection object.\nIn sits, we will use the sits_mosaic() function to create mosaics of our images. In gee, we will take the mosaic() method. In sits, sits_mosaic() crops the mosaic based on the roi parameter. In gee, cropping is performed using the clip() method. We will use the same roi that was used to filter the images to perform the cropping on the mosaic. See the following code:\n\n\nSITS\nGEE\n\n\n\n\nmosaic &lt;- sits_mosaic(\n  cube       = data,\n  roi        = roi,\n  multicores = 4,\n  output_dir = tempdir()\n)\n\n\n\n\nmosaic = data.mosaic().clip(roi);\n\n\n\n\nFinally, the results can be visualized in an interactive map.\nsits\n\n# Visualization in SITS\nsits_view(\n  x     = mosaic,\n  red   = \"B04\",\n  green = \"B03\",\n  blue  = \"B02\"\n)\n\n\n\n\n\n\ngee\n\n# Visualization in GEE\n# Define view region\n#\nMap.centerObject(roi, 10);\n\n# Add mosaic Image\nMap.addLayer(mosaic, {\n      min: 0, \n      max: 3000\n}, 'Mosaic');"
  },
  {
    "objectID": "annex_gee.html#calculating-ndvi",
    "href": "annex_gee.html#calculating-ndvi",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.3 Calculating NDVI",
    "text": "28.3 Calculating NDVI\nThis example demonstrates how to generate time-series of Normalized Difference Vegetation Index (NDVI) using both the sits and gee APIs. In this example, a Region of Interest (ROI) is defined using the sinop_roi.shp file. Below are the code snippets for specifying this file in both sits and gee environments.\nTo reproduce the example, you can download the shapefile using this link. In sits, you can just use it. In gee, it would be required to upload the file in your user space.\n\n\nSITS\nGEE\n\n\n\n\nroi_data &lt;- \"sinop_roi.shp\"\n\n\n\n\nroi_data = ee.FeatureCollection(\"/path/to/sinop_roi\");\n\n\n\n\nNext, we load the satellite imagery. For this example, we use data from Landsat-8. In sits, this data is retrieved from the Brazil Data Cube, although other sources are available. For gee, the data provided by the platform is used. In sits, when the data is loaded, all necessary transformations to make the data ready for use (e.g., factor, offset, cloud masking) are applied automatically. In gee, users are responsible for performing these transformations themselves.\n\n\nSITS\nGEE\n\n\n\n\ndata &lt;- sits_cube(\n    source      = \"BDC\",\n    collection  = \"LANDSAT-OLI-16D\",\n    bands       = c(\"RED\", \"NIR08\", \"CLOUD\"),\n    roi         = roi_data,\n    start_date  = \"2019-05-01\",\n    end_date    = \"2019-07-01\"\n)\n\n\n\n\ndata = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n  .filterBounds(roi_data)\n  .filterDate(\"2019-05-01\", \"2019-07-01\")\n  .select([\"SR_B4\", \"SR_B5\", \"QA_PIXEL\"]);\n\n# factor and offset\ndata = data.map(function(image) {\n  opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n  return image.addBands(opticalBands, null, true);\n});\n\ndata = data.map(function(image) {\n  # Select the pixel_qa band\n  qa = image.select('QA_PIXEL');\n\n  // Create a mask to identify cloud and cloud shadow\n  cloudMask = qa.bitwiseAnd(1 &lt;&lt; 5).eq(0) # Clouds\n           .and(qa.bitwiseAnd(1 &lt;&lt; 3).eq(0)); # Cloud shadows\n\n  # Apply the cloud mask to the image\n  return image.updateMask(cloudMask);\n});\n\n\n\n\nAfter loading the satellite imagery, the NDVI can be generated. In sits, a function allows users to specify the formula used to create a new attribute, in this case, NDVI. In gee, a callback function is used, where the NDVI is calculated for each image.\n\n\nSITS\nGEE\n\n\n\n\ndata_ndvi &lt;- sits_apply(\n  data        = data,\n  NDVI        = (NIR08 - RED) / (NIR08 + RED),\n  output_dir  = tempdir(),\n  multicores  = 4,\n  progress    = TRUE\n)\n\n\n\n\nvar data_ndvi = data.map(function(image) {\n  var ndvi = image.normalizedDifference([\"SR_B5\", \"SR_B4\"]).rename('NDVI');\n\n  return image.addBands(ndvi);\n});\n\ndata_ndvi = data_ndvi.select(\"NDVI\");\n\n\n\n\nThe results are clipped to the ROI defined at the beginning of the example to facilitate visualization. In both APIs, you can define a ROI before performing the operation to optimize resource usage. However, in this example, the data is cropped after the calculation.\n\n\nSITS\nGEE\n\n\n\n\ndata_ndvi &lt;- sits_mosaic(\n  cube       = data_ndvi,\n  roi        = roi_data,\n  output_dir = tempdir(),\n  multicores = 4\n)\n\n\n\n\ndata_ndvi = data_ndvi.map(function(image) {\n  return image.clip(roi_data);\n});\n\n\n\n\nFinally, the results can be visualized in an interactive map.\nsits\n\nsits_view(data_ndvi, band = \"NDVI\", date = \"2019-05-25\", opacity = 1)\n\n\n\n\n\n\ngee\n\n# Define view region\nMap.centerObject(roi_data, 10);\n# Add classification map (colors from sits)\nMap.addLayer(data_ndvi, {\n      min: 0, \n      max: 1, \n      palette: [\"red\", 'white', 'green']\n}, \"NDVI Image\");"
  },
  {
    "objectID": "annex_gee.html#land-use-and-land-cover-lulc-classification",
    "href": "annex_gee.html#land-use-and-land-cover-lulc-classification",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "Land Use and Land Cover (LULC) Classification",
    "text": "Land Use and Land Cover (LULC) Classification\nThis example demonstrates how to perform Land Use and Land Cover (LULC) classification using satellite image time series and machine-learning models in both sits and gee.\nThis example defines the region of interest (ROI) using a shapefile named sinop_roi.shp. Below are the code snippets for specifying this file in both sits and gee environments. To reproduce the example, you can download the shapefile using this link. In sits, you can just use it. In gee, it would be required to upload the file in your user space.\n\n\nSITS\nGEE\n\n\n\n\nroi_data &lt;- \"sinop_roi.shp\"\n\n\n\n\nroi_data = ee.FeatureCollection(\"/path/to/sinop_roi\");\n\n\n\n\nTo train a classifier, sample data with labels representing the behavior of each class to be identified is necessary. In this example, we use a small set with 18 samples. The following code snippets show how these samples are defined in each environment.\nIn sits, labels can be of type string, whereas gee requires labels to be integers. To accommodate this difference, two versions of the same sample set were created: (1) one with string labels for use with sits, and (2) another with integer labels for use with gee.\nTo download these samples, you can use the following links: samples_sinop_crop for sits or samples_sinop_crop for gee\n\n\nSITS\nGEE\n\n\n\n\nsamples &lt;- \"samples_sinop_crop.shp\"\n\n\n\n\nsamples = ee.FeatureCollection(\"samples_sinop_crop_gee\");\n\n\n\n\nNext, we load the satellite imagery. For this example, we use data from MOD13Q1 . In sits, this data is retrieved from the Brazil Data Cube, but other sources are also available. In gee, the platform directly provides this data. In sits, all necessary data transformations for classification tasks are handled automatically. In contrast, gee requires users to manually transform the data into the correct format.\nIn the gee code, transforming all images into bands mimics the approach used by sits for non-temporal classifiers. However, this method is not inherently scalable in gee and may need adjustments for larger datasets or more bands. Additionally, for temporal classifiers like TempCNN, other transformations are necessary and must be manually implemented ingee . In contrast, sits provides a consistent API experience, regardless of the data size or machine learning algorithm.\n\n\nSITS\nGEE\n\n\n\n\ndata &lt;- sits_cube(\n    source      = \"BDC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands       = c(\"NDVI\"),\n    roi         = roi_data,\n    start_date  = \"2013-09-01\",\n    end_date    = \"2014-08-29\"\n)\n\n\n\n\ndata = ee.ImageCollection(\"MODIS/061/MOD13Q1\")\n  .filterBounds(roi_data)\n  .filterDate(\"2013-09-01\", \"2014-09-01\")\n  .select([\"NDVI\"]);\n\n# Transform all images to bands\ndata = data.toBands();\n\n\n\n\nIn this example, we’ll use a Random Forest classifier to create a LULC map. To train the classifier, we need sample data linked to time-series. This step shows how to extract and associate time-series with samples.\n\n\nSITS\nGEE\n\n\n\n\nsamples_ts &lt;- sits_get_data(\n    cube       = data,\n    samples    = samples,\n    multicores = 4\n)\n\n\n\n\nsamples_ts = data.sampleRegions({\n  collection: samples,\n  properties: [\"label\"]\n});\n\n\n\n\nWith the time-series data extracted for each sample, we can now train the random forests classifier.\n\n\nSITS\nGEE\n\n\n\n\nclassifier &lt;- sits_train(\n    samples_ts, sits_rfor(num_trees = 100)\n)\n\n\n\n\nvar classifier = ee.Classifier.smileRandomForest(100).train({\n  features: samples_ts,\n  classProperty: \"label\",\n  inputProperties: data.bandNames()\n});\n\n\n\n\nNow, it is possible to generate the classification map using the trained model. In sits, the classification process starts with a probability map. This map provides the probability of each class for every pixel, offering insights into the classifier’s performance. It also allows for refining the results using methods like Bayesian probability smoothing. After generating the probability map, it is possible to produce the class map, where each pixel is assigned to the class with the highest probability.\nIn gee, while it is possible to generate probabilities, it is not strictly required to produce the classification map. Yet, as of the date of this document, there is no out-of-the-box solution available for using these probabilities to enhance classification results.\n\n\nSITS\nGEE\n\n\n\n\nprobabilities &lt;- sits_classify(\n    data       = data,\n    ml_model   = classifier,\n    multicores = 4,\n    roi        = roi_data,\n    output_dir = tempdir()\n)\n\nclass_map &lt;- sits_label_classification(\n    cube       = probabilities,\n    output_dir = tempdir(),\n    multicores = 4\n)\n\n\n\n\n#  Get the probabilities maps\nprobs_map = data.classify(classifier.setOutputMode(\"MULTIPROBABILITY\"));\n#  Get the classified map\nclass_map = data.classify(classifier);\n\n\n\n\nThe results are clipped to the ROI defined at the beginning of the example to facilitate visualization. In both APIs, it’s possible to define an ROI before processing. However, this was not done in this example.\n\n\nSITS\nGEE\n\n\n\n\nclass_map &lt;- sits_mosaic(\n    cube       = class_map,\n    roi        = roi_data,\n    output_dir = tempdir(),\n    multicores = 4\n)\n\n\n\n\nclass_map = class_map.clip(roi_data);\n\n\n\n\nFinally, the results can be visualized on an interactive map.\nsits\n\nsits_view(class_map, opacity = 1)\n\n\n\n\n\n\ngee\n\n# Define view region\nMap.centerObject(roi_data, 10);\n# Add classification map (colors from sits)\nMap.addLayer(class_map, {\n      min: 1,\n      max: 4,\n      palette: [\"#FAD12D\", \"#1E8449\", \"#D68910\", \"#a2d43f\"]\n}, \"Classification map\");"
  },
  {
    "objectID": "annex_gee.html#summary",
    "href": "annex_gee.html#summary",
    "title": "\n28  SITS and GEE: side-by-side comparison\n",
    "section": "\n28.4 Summary",
    "text": "28.4 Summary\nThis chapter provides an overview of the main differences between the APIs of sits and gee. The sits authors recognize their debt to gee, which was the first application with a coherent API to deal with big EO data. By making a coherent API, gee showed the way forward to big EO application developers. To a large extent, the improvements of sits in relation to gee stem from a careful analysis of Google’s API, which allowed an appraisal of its strengths and drawbacks."
  },
  {
    "objectID": "annex_api.html#general-principles",
    "href": "annex_api.html#general-principles",
    "title": "29  Developing new functions in SITS",
    "section": "29.1 General principles",
    "text": "29.1 General principles\nNew functions that build on the sits API should follow the general principles below.\n\nThe target audience for sits is the community of remote sensing experts with Earth Sciences background who want to use state-of-the-art data analysis methods with minimal investment in programming skills. The design of the sits API considers the typical workflow for land classification using satellite image time series and thus provides a clear and direct set of functions, which are easy to learn and master.\nFor this reason, we welcome contributors that provide useful additions to the existing API, such as new ML/DL classification algorithms. In case of a new API function, before making a pull request please raise an issue stating your rationale for a new function.\nMost functions in sits use the S3 programming model with a strong emphasis on generic methods wich are specialized depending on the input data type. See for example the implementation of the sits_bands() function.\nPlease do not include contributed code using the S4 programming model. Doing so would break the structure and the logic of existing code. Convert your code from S4 to S3.\nUse generic functions as much as possible, as they improve modularity and maintenance. If your code has decision points using if-else clauses, such as if A, do X; else do Y consider using generic functions.\nFunctions that use the torch package use the R6 model to be compatible with that package. See for example, the code in sits_tempcnn.R and api_torch.R. To convert pyTorch code to R and include it is straightforward. Please see the Technical Annex of the sits on-line book.\nThe sits code relies on the packages of the tidyverse to work with tables and list. We use dplyr and tidyr for data selection and wrangling, purrr and slider for loops on lists and table, lubridate to handle dates and times."
  },
  {
    "objectID": "annex_api.html#adherence-to-the-sits-data-types",
    "href": "annex_api.html#adherence-to-the-sits-data-types",
    "title": "29  Developing new functions in SITS",
    "section": "29.2 Adherence to the sits data types",
    "text": "29.2 Adherence to the sits data types\nThe sits package in built on top of three data types: time series tibble, data cubes and models. Most sits functions have one or more of these types as inputs and one of them as return values. The time series tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. All time series tibbles are objects of class sits.\nThe cube data type is designed to store metadata about image files. In principle, images which are part of a data cube share the same geographical region, have the same bands, and have been regularized to fit into a pre-defined temporal interval. Data cubes in sits are organized by tiles. A tile is an element of a satellite’s mission reference system, for example MGRS for Sentinel-2 and WRS2 for Landsat. A cube is a tibble where each row contains information about data covering one tile. Each row of the cube tibble contains a column named file_info; this column contains a list that stores a tibble\nThe cube data type is specialised in raster_cube (ARD images), vector_cube (ARD cube with segmentation vectors). probs_cube (probabilities produced by classification algorithms on raster data), probs_vector_cube(probabilites generated by vector classification of segments), uncertainty_cube (cubes with uncertainty information), and class_cube (labelled maps). See the code in sits_plot.R as an example of specialisation of plot to handle different classes of raster data.\nAll ML/DL models in sits which are the result of sits_train belong to the ml_model class. In addition, models are assigned a second class, which is unique to ML models (e.g, rfor_model, svm_model) and generic for all DL torch based models (torch_model). The class information is used for plotting models and for establishing if a model can run on GPUs."
  },
  {
    "objectID": "annex_api.html#literal-values-error-messages-and-testing",
    "href": "annex_api.html#literal-values-error-messages-and-testing",
    "title": "29  Developing new functions in SITS",
    "section": "29.3 Literal values, error messages, and testing",
    "text": "29.3 Literal values, error messages, and testing\nThe internal sits code has no literal values, which are all stored in the YAML configuration files ./inst/extdata/config.yml and ./inst/extdata/config_internals.yml. The first file contains configuration parameters that are relevant to users, related to visualisation and plotting; the second contains parameters that are relevant only for developers. These values are accessible using the .conf function. For example, the value of the default size for ploting COG files is accessed using the command .conf[\"plot\", \"max_size\"].\nError messages are also stored outside of the code in the YAML configuration file ./inst/extdata/config_messages.yml. These values are accessible using the .conf function. For example, the error associated to an invalid NA value for an input parameter is accessible using th function .conf(\"messages\", \".check_na_parameter\").\nWe strive for high code coverage (&gt; 90%). Every parameter of all sits function (including internal ones) is checked for consistency. Please see api_check.R."
  },
  {
    "objectID": "annex_ml.html#introduction",
    "href": "annex_ml.html#introduction",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.1 Introduction",
    "text": "30.1 Introduction\nThis section provides guidance for experts that want to include new methods for machine learning that work in connection with sits. The discussion below assumes familiarity with the R language. Developers should consult Hadley Wickham’s excellent book Advanced R, especially Chapter 10 on “Function Factories”."
  },
  {
    "objectID": "annex_ml.html#common-features-for-all-classification-algorithms",
    "href": "annex_ml.html#common-features-for-all-classification-algorithms",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.2 Common features for all classification algorithms",
    "text": "30.2 Common features for all classification algorithms\nAll machine learning and deep learning algorithm in sits follow the same logic; all models are created by sits_train(). This function has two parameters: (a) samples, a set of time series with the training samples; (b) ml_method, a function that fits the model to the input data. The result is a function that is passed on to sits_classify() to classify time series or data cubes. The structure of sits_train() is simple, as shown below.\n\nsits_train &lt;- function(samples, ml_method){\n    # train a ml classifier with the given data\n    result &lt;- ml_method(samples)\n    # return a valid machine learning method\n    return(result)\n}\n\nIn R terms, sits_train() is a function factory, i.e., a function that makes functions. Such behavior is possible because functions are first-class objects in R. In other words, they can be bound to a name in the same way that variables are. A second propriety of R is that functions capture (enclose) the environment in which they are created. In other words, when a function is returned as a result of another function, the internal variables used to create it are available inside its environment. In programming language, this technique is called “closure”.\nThe following definition from Wikipedia captures the purpose of clousures: “Operationally, a closure is a record storing a function together with an environment. The environment is a mapping associating each free variable of the function with the value or reference to which the name was bound when the closure was created. A closure allows the function to access those captured variables through the closure’s copies of their values or references, even when the function is invoked outside their scope.”\nIn sits, the properties of closures are used as a basis for making training and classification independent. The return of sits_train() is a model that contains information on how to classify input values, as well as information on the samples used to train the model."
  },
  {
    "objectID": "annex_ml.html#transforming-samples-into-predictors-for-ml-models",
    "href": "annex_ml.html#transforming-samples-into-predictors-for-ml-models",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.3 Transforming samples into predictors for ML models",
    "text": "30.3 Transforming samples into predictors for ML models\nTo ensure all models work in the same fashion, machine learning functions in sits also share the same data structure for prediction. This data structure is created by sits_predictors(), which transforms the time series tibble into a set of values suitable for using as training data, as shown in the following example.\n\ndata(\"samples_matogrosso_mod13q1\", package = \"sitsdata\")\npred &lt;- sits_predictors(samples_matogrosso_mod13q1)\npred\n\nThe predictors tibble is organized as a combination of the “X” and “Y” values used by machine learning algorithms. The first two columns are sample_id and label. The other columns contain the data values, organized by band and time. For machine learning methods that are not time-sensitive, such as random forest, this organization is sufficient for training. In the case of time-sensitive methods such as tempCNN, further arrangements are necessary to ensure the tensors have the right dimensions. Please refer to the sits_tempcnn() source code for an example of how to adapt the prediction table to appropriate torch tensor."
  },
  {
    "objectID": "annex_ml.html#data-normalization",
    "href": "annex_ml.html#data-normalization",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.4 Data normalization",
    "text": "30.4 Data normalization\nSome ML algorithms, such as svm, require data normalization. Therefore, the sits_predictors() code is usually combined with methods that extract statistical information and then normalize the data, as in the example below.\n\n # Data normalization\nml_stats &lt;- sits_stats(samples)\n# extract the training samples\ntrain_samples  &lt;- sits_predictors(samples)\n# normalize the training samples\ntrain_samples  &lt;- sits_pred_normalize(pred = train_samples, stats = ml_stats)"
  },
  {
    "objectID": "annex_ml.html#implementing-the-lightgbm-algorithm",
    "href": "annex_ml.html#implementing-the-lightgbm-algorithm",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.5 Implementing the LightGBM algorithm",
    "text": "30.5 Implementing the LightGBM algorithm\nThe following example shows the implementation of the LightGBM algorithm, designed to efficiently handle large-scale datasets and perform fast training and inference [1]. Gradient boosting is a machine learning technique that builds an ensemble of weak prediction models, typically decision trees, to create a stronger model. LightGBM specifically focuses on optimizing the training and prediction speed, making it particularly suitable for large datasets. The example builds a model using the lightgbm package. This model will then be applied later to obtain a classification.\nSince LightGBM is a gradient boosting model, it uses part of the data as testing data to improve the model’s performance. The split between the training and test samples is controlled by a parameter, as shown in the following code extract.\n\n# split the data into training and validation datasets\n# create partitions different splits of the input data\ntest_samples &lt;- sits_pred_sample(train_samples,\n                                 frac = validation_split\n)\n# Remove the lines used for validation\nsel &lt;- !(train_samples$sample_id %in% test_samples$sample_id)\ntrain_samples &lt;- train_samples[sel, ]\n\nTo include the lightgbm package as part of sits, we need to create a new training function which is compatible with the other machine learning methods of the package and will be called by sits_train(). For compatibility, this new function will be called sits_lightgbm(). Its implementation uses two functions from the lightgbm: (a) lgb.Dataset(), which transforms training and test samples into internal structures; (b) lgb.train(), which trains the model.\nThe parameters of lightgbm::lgb.train() are: (a) boosting_type, boosting algorithm; (b) objective, classification objective (c) num_iterations, number of runs; (d) max_depth, maximum tree depth; (d) min_samples_leaf, minimum size of data in one leaf (to avoid overfitting); (f) learning_rate, learning rate of the algorithm; (g) n_iter_no_change, number of successive iterations to stop training when validation metrics do not improve; (h) validation_split, fraction of training data to be used as validation data.\n\n# install \"lightgbm\" package if not available \nif (!require(\"lightgbm\")) install.packages(\"lightgbm\")\n# create a function in sits style for LightGBM algorithm\nsits_lightgbm &lt;- function(samples = NULL,\n                          boosting_type = \"gbdt\",\n                          objective = \"multiclass\",\n                          min_samples_leaf = 10,\n                          max_depth = 6,\n                          learning_rate = 0.1,\n                          num_iterations = 100,\n                          n_iter_no_change = 10,\n                          validation_split = 0.2, ...){\n\n    # function that returns a model based on training data\n    train_fun &lt;- function(samples) {\n        # Extract the predictors\n        train_samples &lt;- sits_predictors(samples)\n        \n        # find number of labels\n        labels &lt;- sits_labels(samples)\n        n_labels &lt;- length(labels)\n        # lightGBM uses numerical labels starting from 0\n        int_labels &lt;- c(1:n_labels) - 1\n        # create a named vector with integers match the class labels\n        names(int_labels) &lt;- labels\n        \n        # add number of classes to lightGBM params\n        # split the data into training and validation datasets\n        # create partitions different splits of the input data\n        test_samples &lt;- sits_pred_sample(train_samples,\n                                         frac = validation_split\n        )\n        \n        # Remove the lines used for validation\n        sel &lt;- !(train_samples$sample_id %in% test_samples$sample_id)\n        train_samples &lt;- train_samples[sel, ]\n        \n        # transform the training data to LGBM dataset\n        lgbm_train_samples &lt;- lightgbm::lgb.Dataset(\n            data = as.matrix(train_samples[, -2:0]),\n            label = unname(int_labels[train_samples[[2]]])\n        )\n        # transform the test data to LGBM dataset\n        lgbm_test_samples &lt;- lightgbm::lgb.Dataset(\n            data = as.matrix(test_samples[, -2:0]),\n            label = unname(int_labels[test_samples[[2]]])\n        )\n        # set the parameters for the lightGBM training\n        lgb_params &lt;- list(\n            boosting_type = boosting_type,\n            objective = objective,\n            min_samples_leaf = min_samples_leaf,\n            max_depth = max_depth,\n            learning_rate = learning_rate,\n            num_iterations = num_iterations,\n            n_iter_no_change = n_iter_no_change,\n            num_class = n_labels\n        )\n        # call method and return the trained model\n        lgbm_model &lt;- lightgbm::lgb.train(\n            data    = lgbm_train_samples,\n            valids  = list(test_data = lgbm_test_samples),\n            params  = lgb_params,\n            verbose = -1,\n            ...\n        )\n        # serialize the model for parallel processing\n        lgbm_model_string &lt;- lgbm_model$save_model_to_string(NULL)\n        # construct model predict closure function and returns\n        predict_fun &lt;- function(values) {\n            # reload the model (unserialize)\n            lgbm_model &lt;- lightgbm::lgb.load(model_str = lgbm_model_string)\n            # predict probabilities\n            prediction &lt;- stats::predict(lgbm_model,\n                               newdata = as.matrix(values),\n                               type = \"response\"\n            )\n            # adjust the names of the columns of the probs\n            colnames(prediction) &lt;- labels\n            # retrieve the prediction results\n            return(prediction)\n        }\n        # Set model class\n        # This tells sits that the resulting function\n        # will be handled as a prediction model\n        class(predict_fun) &lt;- c(\"lightgbm_model\", \n                                \"sits_model\", \n                                class(predict_fun))\n        return(predict_fun)\n    }\n    result &lt;- sits_factory_function(samples, train_fun)\n    return(result)\n}"
  },
  {
    "objectID": "annex_ml.html#understanding-how-models-are-organized-in-sits",
    "href": "annex_ml.html#understanding-how-models-are-organized-in-sits",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.6 Understanding how models are organized in SITS",
    "text": "30.6 Understanding how models are organized in SITS\nThe above code has two nested functions: train_fun() and predict_fun(). When sits_lightgbm() is called, train_fun() transforms the input samples into predictors and uses them to train the algorithm, creating a model (lgbm_model). This model is included as part of the function’s closure and becomes available at classification time. Inside train_fun(), we include predict_fun(), which applies the lgbm_model object to classify to the input values. The train_fun object is then returned as a closure, using the sits_factory_function constructor. This function allows the model to be called either as part of sits_train() or to be called independently, with the same result.\n\nsits_factory_function &lt;- function(data, fun) {\n    # if no data is given, we prepare a\n    # function to be called as a parameter of other functions\n    if (purrr::is_null(data)) {\n        result &lt;- fun\n    } else {\n        # ...otherwise compute the result on the input data\n        result &lt;- fun(data)\n    }\n    return(result)\n}\n\nAs a result, the following calls are equivalent.\n\n# building a model using sits_train\nlgbm_model &lt;- sits_train(samples, sits_lightgbm())\n# building a model directly\nlgbm_model &lt;- sits_lightgbm(samples)"
  },
  {
    "objectID": "annex_ml.html#model-serialization-for-parallel-processing",
    "href": "annex_ml.html#model-serialization-for-parallel-processing",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.7 Model serialization for parallel processing",
    "text": "30.7 Model serialization for parallel processing\nThere is one additional requirement for the algorithm to be compatible with sits. Data cube processing algorithms in sits run in parallel. For this reason, once the classification model is trained, it is serialized, as shown in the following line. The serialized version of the model is exported to the function closure, so it can be used at classification time.\n\n# serialize the model for parallel processing\nlgbm_model_string &lt;- lgbm_model$save_model_to_string(NULL)\n\nDuring classification, predict_fun() is called in parallel by each CPU. At this moment, the serialized string is transformed back into a model, which is then run to obtain the classification, as shown in the code.\n\n# unserialize the model\nlgbm_model &lt;- lightgbm::lgb.load(model_str = lgbm_model_string)"
  },
  {
    "objectID": "annex_ml.html#case-study",
    "href": "annex_ml.html#case-study",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.8 Case study",
    "text": "30.8 Case study\nTo illustrate this separation between training and classification, the new algorithm developed in the chapter using lightgbm will be used to classify a data cube.\n\ndata(\"samples_matogrosso_mod13q1\", package = \"sitsdata\")\n# Create a data cube using local files\nsinop &lt;- sits_cube(\n  source = \"BDC\", \n  collection  = \"MOD13Q1-6.1\",\n  data_dir = system.file(\"extdata/sinop\", package = \"sitsdata\"),  \n  parse_info = c(\"X1\", \"X2\", \"tile\", \"band\", \"date\")\n)\n# The data cube has only \"NDVI\" and \"EVI\" bands \n# Select the bands NDVI and EVI\nsamples_2bands &lt;- sits_select(\n    data = samples_matogrosso_mod13q1, \n    bands = c(\"NDVI\", \"EVI\")\n)\n# train lightGBM model\nlgb_model &lt;- sits_train(samples_2bands, sits_lightgbm())\n\n# Classify the data cube\nsinop_probs &lt;- sits_classify(\n    data = sinop, \n    ml_model = lgb_model,\n    multicores = 1,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n# Perform spatial smoothing\nsinop_bayes &lt;- sits_smooth(\n    cube = sinop_probs,\n    multicores = 2,\n    memsize = 8,\n    output_dir = tempdir_r\n)\n# Label the smoothed file \nsinop_map &lt;- sits_label_classification(\n    cube = sinop_bayes, \n    output_dir = tempdir_r\n)\n# plot the result\nplot(sinop_map, title = \"Sinop Classification Map\")\n\n\n\n\n\nFigure 30.1: Classification map for Sinop using LightGBM"
  },
  {
    "objectID": "annex_ml.html#summary",
    "href": "annex_ml.html#summary",
    "title": "\n30  Including new methods for machine learning\n",
    "section": "\n30.9 Summary",
    "text": "30.9 Summary\nUsing function factories that produce closures, sits keeps the classification function independent of the machine learning or deep learning algorithm. This policy allows independent proposal, testing, and development of new classification methods. It also enables improvements on parallel processing methods without affecting the existing classification methods.\n\n\n\n\n[1] \nG. Ke et al., “Lightgbm: A highly efficient gradient boosting decision tree,” in Advances in neural information processing systems, 2017, pp. 3146–3154."
  },
  {
    "objectID": "annex_stac.html",
    "href": "annex_stac.html",
    "title": "28  Supporting STAC-based ARD catalogs",
    "section": "",
    "text": "If you want to include a STAC-based catalogue not yet supported by sits, we encourage you to look at existing implementations of catalogues such as Microsoft Planetary Computer (MPC), Digital Earth Africa (DEA) and AWS. STAC-based catalogues in sits are associated to YAML description files, which are available in the directory .inst/exdata/sources. For example, the YAML file config_source_mpc.yml describes the contents of the MPC collections supported by sits. Please first provide an YAML file which lists the detailed contents of the new catalogue you wish to include. Follow the examples provided.\nAfter writing the YAML file, you need to consider how to access and query the new catalogue. The entry point for access to all catalogues is the sits_cube.stac_cube() function, which in turn calls a sequence of functions which are described in the generic interface api_source.R. Most calls of this API are handled by the functions of api_source_stac.R which provides an interface to the rstac package and handles STAC queries.\nsources:\n    AWS                     :\n        s3_class            : [\"aws_cube\", \"stac_cube\", \"eo_cube\",\n                               \"raster_cube\"]\n        service             : \"STAC\"\n        url                 : \"https://earth-search.aws.element84.com/v1/\"\n        collections         :\n            SENTINEL-2-L2A  :\n                bands       :\n                    B01     : &aws_msi_60m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  60\n                        band_name    : \"coastal\"\n                        data_type     : \"INT2S\"\n                    B02     : &aws_msi_10m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  10\n                        band_name    : \"blue\"\n                        data_type     : \"INT2S\"\n                    B03     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"green\"\n                    B04     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"red\"\n                    B05     : &aws_msi_20m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  20\n                        band_name    : \"rededge1\"\n                        data_type     : \"INT2S\"\n                    B06     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"rededge2\"\n                    B07     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"rededge3\"\n                    B08     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"nir\"\n                    B8A     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"nir08\"\n                    B09     :\n                        &lt;&lt;: *aws_msi_60m\n                        band_name    : \"nir09\"\n                    B11     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"swir16\"\n                    B12     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"swir22\"\n                    CLOUD   :\n                        bit_mask     : false\n                        band_name    : \"scl\"\n                        values       :\n                            0        : \"missing_data\"\n                            1        : \"defective pixel\"\n                            2        : \"shadows\"\n                            3        : \"cloud shadows\"\n                            4        : \"vegetation\"\n                            5        : \"non-vegetated\"\n                            6        : \"water\"\n                            7        : \"unclassified\"\n                            8        : \"cloud medium\"\n                            9        : \"cloud high\"\n                            10       : \"thin cirrus\"\n                            11       : \"snow or ice\"\n                        interp_values: [0, 1, 2, 3, 8, 9, 10]\n                        resolution  : 20\n                        data_type     : \"INT1U\"\n                satellite   : \"SENTINEL-2\"\n                sensor      : \"MSI\"\n                platforms   :\n                    SENTINEL-2A: \"sentinel-2a\"\n                    SENTINEL-2B: \"sentinel-2b\"\n                collection_name: \"sentinel-2-l2a\"\n                access_vars :\n                   AWS_DEFAULT_REGION   : \"us-west-2\"\n                   AWS_S3_ENDPOINT      : \"s3.amazonaws.com\"\n                   AWS_NO_SIGN_REQUEST  : true\n                open_data       : true\n                open_data_token : false\n                metadata_search : \"tile\"\n                ext_tolerance   : 0\n                grid_system     : \"MGRS\"\n                dates           : \"2015 to now\"\nEach STAC catalogue is different. The STAC specification allows providers to implement their data descriptions with specific information. For this reason, the generic API described in api_source.R needs to be specialized for each provider. Whenever a provider needs specific implementations of parts of the STAC protocol, we include them in separate files. For example, api_source_mpc.R implements specific quirks of the MPC platform. Similarly, specific support for CDSE (Copernicus Data Space Environment) is available in api_source_cdse.R."
  },
  {
    "objectID": "annex_parallel.html",
    "href": "annex_parallel.html",
    "title": "\n32  How parallel processing works in SITS\n",
    "section": "",
    "text": "This section provides an overview of how sits_classify(), sits_smooth(), and sits_label_classification() process images in parallel. To achieve efficiency, sits implements a fault-tolerant multitasking procedure for big Earth observation data classification. The learning curve is shortened as there is no need to learn how to do multiprocessing. Image classification in sits is done by a cluster of independent workers linked to a virtual machine. To avoid communication overhead, all large payloads are read and stored independently; direct interaction between the main process and the workers is kept at a minimum.\nThe classification procedure benefits from the fact that most images available in cloud collections are stored as COGs (cloud-optimized GeoTIFF). COGs are regular GeoTIFF files organized in regular square blocks to improve visualization and access for large datasets. Thus, data requests can be optimized to access only portions of the images. All cloud services supported by sits use COG files. The classification algorithm in sits uses COGs to ensure optimal data access, reducing I/O demand as much as possible.\nThe approach for parallel processing in sits, depicted in Figure @ref(fig:par), has the following steps:\n\nBased on the block size of individual COG files, calculate the size of each chunk that must be loaded in memory, considering the number of bands and the timeline’s length. Chunk access is optimized for the efficient transfer of data blocks.\nDivide the total memory available by the chunk size to determine how many processes can run in parallel.\nEach core processes a chunk and produces a subset of the result.\nRepeat the process until all chunks in the cube have been processed.\nCheck that subimages have been produced correctly. If there is a problem with one or more subimages, run a failure recovery procedure to ensure all data is processed.\nAfter generating all subimages, join them to obtain the result.\n\n\n\n{#fig-annex-parallel, fig-align=‘center’ width=90% height=90%}\n\n\nThis approach has many advantages. It has no dependencies on proprietary software and runs in any virtual machine that supports R. Processing is done in a concurrent and independent way, with no communication between workers. Failure of one worker does not cause the failure of big data processing. The software is prepared to resume classification processing from the last processed chunk, preventing failures such as memory exhaustion, power supply interruption, or network breakdown.\nTo reduce processing time, it is necessary to adjust sits_classify(), sits_smooth(), and sits_label_classification() according to the capabilities of the host environment. The memsize parameter controls the size of the main memory (in GBytes) to be used for classification. A practical approach is to set memsize to the maximum memory available in the virtual machine for classification and to choose multicores as the largest number of cores available. Based on the memory available and the size of blocks in COG files, sits will access the images in an optimized way. In this way, sits tries to ensure the best possible use of the available resources."
  },
  {
    "objectID": "annex_stac.html#introduction",
    "href": "annex_stac.html#introduction",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.1 Introduction",
    "text": "31.1 Introduction\nA useful facility in sits is its capability to access different ARD collections in many cloud services. Moreover, sits is able to make the data ready for use without user intervention. Each ARD collection has different conventions from image attributes such band names, maximum and minimum value, encoding, scale factor and cloud masking. In sits, when the data is loaded, all necessary transformations are applied automatically.\nTo make data transformation transparent to users, sits relies on YAML configuration files and a flexible interface to the rstac package. In what follows, we show how configuration files are defined and how the sits code uses them."
  },
  {
    "objectID": "annex_stac.html#ard-collections-configuration-files",
    "href": "annex_stac.html#ard-collections-configuration-files",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.2 ARD collections configuration files",
    "text": "31.2 ARD collections configuration files\nAll STAC-based catalogues supported by sits are associated to YAML description files, which are available in the directory exdata/sources in the sits package. The example below shows how to accessthe YAML file config_source_mpc.yml describes the contents of the MPC collections supported by sits.\n\nlibrary(yaml)\n# location of YAML configuration file for MPC\nmpc_yaml_file &lt;- system.file(\"extdata/sources/config_source_mpc.yml\", package = \"sits\")\n# convert to R list object\nmpc_yaml &lt;- yaml::read_yaml(mpc_yaml_file)\n\nAccess to other configuration files is done in a similar way, replacing mpc in the above code by one of aws, bdc, cdse, deafrica, deaustralia, hls30, mpc, planet, or terrascope."
  },
  {
    "objectID": "annex_stac.html#explaining-the-contents-of-an-yaml-file",
    "href": "annex_stac.html#explaining-the-contents-of-an-yaml-file",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.3 Explaining the contents of an YAML file",
    "text": "31.3 Explaining the contents of an YAML file\nTo describe in detail the contents of an YAML configuration file for ARD catalogues, we take the example of the SENTINEL-2-L2A catalogue in AWS, shown below.\nsources:\n    AWS                     :\n        s3_class            : [\"aws_cube\", \"stac_cube\", \"eo_cube\",\n                               \"raster_cube\"]\n        service             : \"STAC\"\n        url                 : \"https://earth-search.aws.element84.com/v1/\"\n        collections         :\n            SENTINEL-2-L2A  :\n                bands       :\n                    B01     : &aws_msi_60m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  60\n                        band_name    : \"coastal\"\n                        data_type     : \"INT2S\"\n                    B02     : &aws_msi_10m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  10\n                        band_name    : \"blue\"\n                        data_type     : \"INT2S\"\n                    B03     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"green\"\n                    B04     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"red\"\n                    B05     : &aws_msi_20m\n                        missing_value: -9999\n                        minimum_value: 0\n                        maximum_value: 10000\n                        scale_factor : 0.0001\n                        offset_value : 0\n                        resolution  :  20\n                        band_name    : \"rededge1\"\n                        data_type     : \"INT2S\"\n                    B06     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"rededge2\"\n                    B07     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"rededge3\"\n                    B08     :\n                        &lt;&lt;: *aws_msi_10m\n                        band_name    : \"nir\"\n                    B8A     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"nir08\"\n                    B09     :\n                        &lt;&lt;: *aws_msi_60m\n                        band_name    : \"nir09\"\n                    B11     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"swir16\"\n                    B12     :\n                        &lt;&lt;: *aws_msi_20m\n                        band_name    : \"swir22\"\n                    CLOUD   :\n                        bit_mask     : false\n                        band_name    : \"scl\"\n                        values       :\n                            0        : \"missing_data\"\n                            1        : \"defective pixel\"\n                            2        : \"shadows\"\n                            3        : \"cloud shadows\"\n                            4        : \"vegetation\"\n                            5        : \"non-vegetated\"\n                            6        : \"water\"\n                            7        : \"unclassified\"\n                            8        : \"cloud medium\"\n                            9        : \"cloud high\"\n                            10       : \"thin cirrus\"\n                            11       : \"snow or ice\"\n                        interp_values: [0, 1, 2, 3, 8, 9, 10]\n                        resolution  : 20\n                        data_type     : \"INT1U\"\n                satellite   : \"SENTINEL-2\"\n                sensor      : \"MSI\"\n                platforms   :\n                    SENTINEL-2A: \"sentinel-2a\"\n                    SENTINEL-2B: \"sentinel-2b\"\n                collection_name: \"sentinel-2-l2a\"\n                access_vars :\n                   AWS_DEFAULT_REGION   : \"us-west-2\"\n                   AWS_S3_ENDPOINT      : \"s3.amazonaws.com\"\n                   AWS_NO_SIGN_REQUEST  : true\n                open_data       : true\n                open_data_token : false\n                metadata_search : \"tile\"\n                ext_tolerance   : 0\n                grid_system     : \"MGRS\"\n                dates           : \"2015 to now\"\nThe main keyword is sources, which tells sits that comes below is an ARD config file. The next level contains the source name (AWS) by which the cloud provided is referred to in sits. At the next level we find the following keywords:\n\ns3_class: this is the internal S3 class used by sits to deal with data from this provider. In this case, the value aws_cube is specific to the provider and is used by sits to support modularity in STAC access. For MPC, for example, sits uses mpc_cube. The value stac_cube indicates that this catalogue is accessible via STAC. The values eo_cube and raster_cube indicate that this collection consists of raster data.\nservice: for STAC-based collections, use “STAC”.\nurl: STAC endpoint for the collection.\ncollections: keyword for collections associated to the service and described at the next level.\n\nIn the next level, we find the list of collections supported by the cloud service. In this level, we shown one collection (SENTINEL-2-L2A). Below this level, the keywords are:\n\nbands: describes the bands associated with the satellite. The value for each band is the one used by sits(e.g. “B01”) while band_name is the name of the band user by AWS. This correspondence ensures that the same band in different providers has a single name in sits. For each band, we need to provide missing_value, minimum_value, maximum_value, scale_factor, offset_value, resolution, band_name and data_type.\nsatellite: name of the satellite\nsensor: sensor designation\nplatform: in case of multiple platforms associated to the satellite constellation, provide their names.\ncollection_name: name used by AWS to designate the collection.\naccess_vars: environmental variable used to access the collection.\nopen_data: is the collection available as open data?\nopen_data_token: is a token required to access the collection?\nmetadata_search: does the collection allow searching by tiles or only by ROI?\ngrid_system: tiling system used (e.g, “MGRS” or “WRS2”)\n\nCustom YAML files can be placed in the user’s configuration file, which is a YAML file located at the place indicated by the SITS_CONFIG_USER_FILE environment variable."
  },
  {
    "objectID": "annex_stac.html#acessing-stac-collections",
    "href": "annex_stac.html#acessing-stac-collections",
    "title": "\n31  Supporting STAC-based ARD catalogs\n",
    "section": "\n31.4 Acessing STAC collections",
    "text": "31.4 Acessing STAC collections\nAfter writing the YAML file, you need to consider how to access and query the new catalogue. The entry point for access to all catalogues is the sits_cube.stac_cube() function, which in turn calls a sequence of functions which are described in the generic interface api_source.R. Most calls of this API are handled by the functions of api_source_stac.R which provides an interface to the rstac package and handles STAC queries.\nThe STAC specification is flexible aand allows providers to implement their data descriptions with specific information. Thus, STAC specifications may differ among the cloud providers. For example, many providers do not allow queries by tiles. For this reason, the generic API described in api_source.R needs to be specialized for each provider. Whenever a provider needs specific implementations of parts of the STAC protocol, we include them in separate files. For example, api_source_mpc.R implements specific quirks of the MPC platform. Similarly, specific support for CDSE (Copernicus Data Space Environment) is available in api_source_cdse.R. We suggest developers interested in including a new cloud service to follow one current implementation for a cloud provider step-by-step. With a little bit of effort, it will become clear to developers how to achieve their needs."
  },
  {
    "objectID": "dc_mixture.html#introduction",
    "href": "dc_mixture.html#introduction",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.1 Introduction",
    "text": "9.1 Introduction\nMany pixels in images of medium-resolution satellites such as Landsat or Sentinel-2 contain a mixture of spectral responses of different land cover types inside a resolution element [1]. In many applications, it is desirable to obtain the proportion of a given class inside a mixed pixel. For this purpose, the literature proposes mixture models; these models represent pixel values as a combination of multiple pure land cover types [2]. Assuming that the spectral response of pure land cover classes (called endmembers) is known, spectral mixture analysis derives new bands containing the proportion of each endmember inside a pixel.\nApplications of spectral mixture analysis in remote sensing include forest degradation [6], wetland surface dynamics [7], and urban area characterization [8]. These models providing valuable information for a wide range of applications, from land mapping and change detection to resource management and environmental monitoring."
  },
  {
    "objectID": "dc_mixture.html#methods",
    "href": "dc_mixture.html#methods",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.2 Methods",
    "text": "9.2 Methods\nThe most used method for spectral mixture analysis is the linear model [2]. The main idea behind the linear mixture model is that the observed pixel spectrum can be expressed as a linear combination of the spectra of the pure endmembers, weighted by their respective proportions (or abundances) within the pixel. Mathematically, the model can be represented as: \\[\nR_i = \\sum_{j=1}^N a_{i,j}*x_j + \\epsilon_i, i \\in {1,...M}, M &gt; N,\n\\] where \\(i=1,..M\\) is the set of spectral bands and \\(j=1,..N\\) is the set of land classes. For each pixel, \\(R_i\\) is the reflectance in the i-th spectral band, \\(x_j\\) is the reflectance value due to the j-th endmember, and \\(a_{i,j}\\) is the proportion between the j-th endmember and the i-th spectral band. To solve this system of equations and obtain the proportion of each endmember, sits uses a non-negative least squares (NNLS) regression algorithm, which is available in the R package RStoolbox and was developed by Jakob Schwalb-Willmann, based on the sequential coordinate-wise algorithm (SCA) proposed on Franc et al. [9]."
  },
  {
    "objectID": "dc_mixture.html#running-mixture-models-in-sits",
    "href": "dc_mixture.html#running-mixture-models-in-sits",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.3 Running mixture models in SITS",
    "text": "9.3 Running mixture models in SITS\nTo run the mixture model in sits, it is necessary to inform the values of pixels which represent spectral responses of a unique class. These are the so-called “pure” pixels. Because the quality of the resulting endmember images depends on the quality of the pure pixels, they should be chosen carefully and based on expert knowledge of the area. Since sits supports multiple endmember spectral mixture analysis [10], users can specify more than one pure pixel per endmember to account for natural variability.\nIn sits, spectral mixture analysis is done by sits_mixture_model(), which has two mandatory parameters: cube (a data cube) and endmembers, a named table (or equivalent) that defines the pure pixels. The endmembers table must have the following named columns: (a) type, which defines the class associated with an endmember; (b) names, the names of the bands. Each line of the table must contain the value of each endmember for all bands (see example). To improve readability, we suggest that the endmembers parameters be defined as a tribble. A tribble is a tibble with an easier to read row-by-row layout. In the example below, we define three endmembers for classes Forest, Soil, and Water. Note that the values for each band are expressed as integers ranging from 0 to 10,000. We use the same data cube which was used in the previous chapter.\n\n# Create an non-regular data cube from AWS\ns2_cube &lt;- sits_cube(\n    source = \"AWS\",\n    collection = \"SENTINEL-S2-L2A-COGS\",\n    tiles = \"20LKP\",\n    bands = c(\"B02\", \"B03\", \"B04\", \n              \"B05\", \"B06\", \"B07\", \n              \"B08\", \"B8A\", \"B11\", \n              \"B12\",\"CLOUD\"),\n    start_date = as.Date(\"2018-07-01\"),\n    end_date = as.Date(\"2018-08-31\"))\n\n\n# Regularize the cube to 15 day intervals\nreg_cube &lt;- sits_regularize(\n          cube       = s2_cube,\n          output_dir = tempdir_r,\n          res        = 60,\n          period     = \"P15D\",\n          multicores = 4)\n\n\n# Define the endmembers for three classes and six bands\nem &lt;- tibble::tribble(\n    ~class,   ~B02, ~B03, ~B04, ~B8A, ~B11, ~B12,\n    \"forest\",  200,  352,  189, 2800, 1340,  546,\n    \"soil\",    400,  650,  700, 3600, 3500, 1800,\n    \"water\",   700, 1100, 1400,  850,   40,   26)\n# Generate the mixture model\nreg_cube &lt;- sits_mixture_model(\n    data = reg_cube,\n    endmembers = em,\n    multicores = 4,\n    memsize = 12,\n    output_dir = tempdir_r)\n\n\n# Plot the FOREST for the first date using the Greens palette\nplot(reg_cube, band = \"FOREST\", palette = \"Greens\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 9.1: Percentage of forest per pixel estimated by mixture model.\n\n\n\n\n# Plot the water endmember for the first date using the Blues palette\nplot(reg_cube, band = \"WATER\", palette = \"Blues\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 9.2: Percentage of water per pixel estimated by mixture model.\n\n\n\n\n# Plot the SOIL endmember for the first date using the orange red (OrRd) palette \nplot(reg_cube, band = \"SOIL\", palette = \"OrRd\")\n\nno date provided - using date with least cloud cover\n\n\n\n\nFigure 9.3: Percentage of soil per pixel estimated by mixture model."
  },
  {
    "objectID": "dc_mixture.html#summary",
    "href": "dc_mixture.html#summary",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "\n9.4 Summary",
    "text": "9.4 Summary\nLinear mixture models (LMM) improve the interpretation of remote sensing images by accounting for mixed pixels and providing a more accurate representation of the Earth’s surface. LMMs provide a more accurate representation of mixed pixels by considering the contributions of multiple land classes within a single pixel. This can lead to improved land cover classification accuracy compared to conventional per-pixel classification methods, which may struggle to accurately classify mixed pixels.\nLMMs also allow for the estimation of the abundances of each land class within a pixel, providing valuable sub-pixel information. This can be especially useful in applications where the spatial resolution of the sensor is not fine enough to resolve individual land cover types, such as monitoring urban growth or studying vegetation dynamics. By considering the sub-pixel composition of land classes, LMMs can provide a more sensitive measure of changes in land cover over time. This can lead to more accurate and precise change detection, particularly in areas with complex land cover patterns or where subtle changes in land cover may occur."
  },
  {
    "objectID": "dc_mixture.html#references",
    "href": "dc_mixture.html#references",
    "title": "\n9  Spectral mixture analysis\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nD. A. Roberts, M. O. Smith, and J. B. Adams, “Green vegetation, nonphotosynthetic vegetation, and soils in AVIRIS data,” Remote Sensing of Environment, vol. 44, no. 2, pp. 255–269, 1993, doi: 10.1016/0034-4257(93)90020-X.\n\n\n[2] \nY. E. Shimabukuro and F. J. Ponzoni, Spectral Mixture for Remote Sensing: Linear Model and Applications. Cham: Springer International Publishing, 2019.\n\n\n[3] \nM. A. Cochrane and C. Souza, “Linear mixture model classification of burned forests in the Eastern Amazon,” International Journal of Remote Sensing, vol. 19, no. 17, pp. 3433–3440, 1998, doi: 10.1080/014311698214109.\n\n\n[4] \nC. M. Souza Jr, D. A. Roberts, and M. A. Cochrane, “Combining spectral and spatial information to map canopy damage from selective logging and forest fires,” Remote Sensing of Environment, vol. 98, no. 2, pp. 329–343, 2005, doi: 10.1016/j.rse.2005.07.013.\n\n\n[5] \nE. L. Bullock, C. E. Woodcock, and C. E. Holden, “Improved change monitoring using an ensemble of time series algorithms,” Remote Sensing of Environment, vol. 238, p. 111165, 2020, doi: 10.1016/j.rse.2019.04.018.\n\n\n[6] \nS. Chen et al., “Monitoring temperate forest degradation on Google Earth Engine using Landsat time series analysis,” Remote Sensing of Environment, vol. 265, p. 112648, 2021, doi: 10.1016/j.rse.2021.112648.\n\n\n[7] \nM. Halabisky, L. M. Moskal, A. Gillespie, and M. Hannam, “Reconstructing semi-arid wetland surface water dynamics through spectral mixture analysis of a time series of Landsat satellite images (1984–2011),” Remote Sensing of Environment, vol. 177, pp. 171–183, 2016, doi: 10.1016/j.rse.2016.02.040.\n\n\n[8] \nC. Wu and A. T. Murray, “Estimating impervious surface distribution by spectral mixture analysis,” Remote sensing of Environment, vol. 84, no. 4, pp. 493–505, 2003.\n\n\n[9] \nV. Franc, V. Hlaváč, and M. Navara, “Sequential Coordinate-Wise Algorithm for the Non-negative Least Squares Problem,” in Computer Analysis of Images and Patterns, 2005, pp. 407–414, doi: 10.1007/11556121_50.\n\n\n[10] \nD. A. Roberts, M. Gardner, R. Church, S. Ustin, G. Scheer, and R. O. Green, “Mapping Chaparral in the Santa Monica Mountains Using Multiple Endmember Spectral Mixture Models,” Remote Sensing of Environment, vol. 65, no. 3, pp. 267–279, 1998, doi: 10.1016/S0034-4257(98)00037-6."
  },
  {
    "objectID": "dc_reduce.html#introduction",
    "href": "dc_reduce.html#introduction",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.1 Introduction",
    "text": "10.1 Introduction\nThere are cases when users want to produce results which combine the values a time series associated to each pixel of a data cube using reduction operators. In the context of time series analysis, a reduction operator is a function that reduces a sequence of data points into a single value or a smaller set of values. This process involves summarizing or aggregating the information from the time series in a meaningful way. Reduction operators are often used to extract key statistics or features from the data, making it easier to analyze and interpret."
  },
  {
    "objectID": "dc_reduce.html#methods",
    "href": "dc_reduce.html#methods",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.2 Methods",
    "text": "10.2 Methods\nTo produce temporal combinations, sits provides sits_reduce, with associated functions:\n\n\nt_max(): maximum value of the series.\n\nt_min(): minimum value of the series\n\nt_mean(): mean of the series.\n\nt_median(): median of the series.\n\nt_sum(): sum of all the points in the series.\n\nt_std(): standard deviation of the series.\n\nt_skewness(): skewness of the series.\n\nt_kurtosis(): kurtosis of the series.\n\nt_amplitude(): difference between maximum and minimum values of the cycle. A small amplitude means a stable cycle.\n\nt_fslope(): maximum value of the first slope of the cycle. Indicates when the cycle presents an abrupt change in the curve. The slope between two values relates the speed of the growth or senescence phases\n\nt_mse(): average spectral energy density. The energy of the time series is distributed by frequency.\n\nt_fqr(): value of the first quartile of the series (0.25).\n\nt_tqr(): value of the third quartile of the series (0.75).\n\nt_iqr(): interquartile range (difference between the third and first quartiles).\n\nThe functions t_sum(), t_std(), t_skewness(), t_kurtosis(), and t_mse() produce values greater than the limit of a two-byte integer. Therefore, we save the images generated by these in floating point format."
  },
  {
    "objectID": "dc_reduce.html#example",
    "href": "dc_reduce.html#example",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.3 Example",
    "text": "10.3 Example\nThe following example shows how to execute a temporal reduction operation.\n\n# define a region of interest to be enclosed in the data cube\nroi &lt;-  c(\"lon_min\" = -55.80259, \"lon_max\" = -55.199, \"lat_min\" = -11.80208, \"lat_max\" = -11.49583)\n# Define a data cube in the MPC repository using NDVI MODIS data\nndvi_cube &lt;- sits_cube(\n    source = \"MPC\",\n    collection  = \"MOD13Q1-6.1\",\n    bands = c(\"NDVI\"),\n    roi = roi,\n    start_date =  \"2018-05-01\",\n    end_date = \"2018-09-30\"\n)\n# Copy the cube to a local file system\nndvi_cube_local &lt;- sits_cube_copy(\n    cube = ndvi_cube,\n    output_dir = tempdir_r,\n    multicores = 4\n)\n\nAfter creating a local data cube based on the contents of the MPC MODIS cube with the NDVI band, we can now compute the maximum NDVI values for each pixel for the images during the period from 2018-05-01 to 2018-09-30.\n\n# create a local directory to store the result of the operation\ntempdir_r_ndvi_max &lt;- file.path(tempdir_r, \"ndvi_max\")\ndir.create(tempdir_r_ndvi_max, showWarnings = FALSE)\n# Calculate the NBR index\nmax_ndvi_cube &lt;- sits_reduce(ndvi_cube_local,\n    NDVIMAX = t_max(NDVI),\n    output_dir = tempdir_r_ndvi_max,\n    multicores = 4,\n    progress = TRUE\n)\nplot(max_ndvi_cube, band = \"NDVIMAX\")\n\n\nknitr::include_graphics(\"./images/dc-reduce-ndvimax.png\")\n\n\n\nFigure 10.1: Maximum NDVI for MPC MODIS cube for period 2018-05-01 to 2018-09-30."
  },
  {
    "objectID": "dc_reduce.html#summary",
    "href": "dc_reduce.html#summary",
    "title": "\n10  Temporal reduction operations\n",
    "section": "\n10.4 Summary",
    "text": "10.4 Summary\nTemporal reduce operations in Earth Observation (EO) data cubes are aggregations or summaries performed along the time dimension. These operations compress temporal information—typically multiple observations of the same location over time—into a single value or summary per pixel or area. They are useful for generating cloud-free composites, trend analyses, and preparing data for classification or modeling."
  },
  {
    "objectID": "dc_texture.html",
    "href": "dc_texture.html",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "",
    "text": "12 Introduction\nGLCM (Gray Level Co-occurrence Matrix) texture measures capture spatial relationships between pixel intensities. Important measures include contrast, dissimilarity, homogeneity, energy, entropy, correlation, variance, and cluster prominence. These measures are useful in image processing and remote sensing, helping with land cover classification, texture recognition, and medical imaging applications. One key benefit is their ability to capture information about pixel arrangement, even under some lighting changes, making them resilient for object classification tasks. Surfaces such as cropland, urban fabric, tree crowns or tumbling water can share similar reflectance values but differ in their arrangement. Injecting GLCM features into a classifier therefore improves separability of spectrally ambiguous classes."
  },
  {
    "objectID": "dc_texture.html#methods",
    "href": "dc_texture.html#methods",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.2 Methods",
    "text": "11.2 Methods\nA GLCM is a 2-D histogram that records how often a pixel with gray level i occurs at a fixed spatial offset (distance d, direction θ) from a pixel with gray level j. It is therefore a second-order statistic: instead of looking at single-pixel values, it captures pairwise relationships. These measures quantify spatial relationships between grey levels in an image and are used to describe texture—patterns of variation in pixel intensity that convey surface properties such as smoothness, roughness, regularity, or granularity. The idea was first formalised by Haralick and colleagues in 1973 [1] and remains one of the most widely used texture descriptors in computer vision, remote sensing and medical imaging.\nThe sits implementation of GLCM measures is follows the guidelines and equations described by Hall-Beyer [2] and uses the following parameters:\n\n\ncube : a data cube\n\nwindow_size : odd number with the size of the sliding window.\n\nangles : a vector indicating the direction angles in radians related to the central pixel and its neighbors (See details). Default is 0.\n\nmemsize: memory available for processing\n\nmulticores: number of cores available for processing\n\noutput_dir: output directory for the resulting cube\n\nprogress: show progress bar?\n\n... : GLCM function (see details).\n\nThe angles parameter captures the spatial relation between the central pixel and its neighbors in radians, where: - 0 : neighbor on right-side. - pi/4 : neighbor on the top-right diagonal - pi/2: neighbor on top of the current pixel - 3*pi/4: neighbor on the top-left diagonal\nOur implementation relies on a symmetric co-occurrence matrix, which considers the opposite directions of an angle. For example, the neighbor pixels based on angle 0 relies on the left and right direction. The neighbor pixels of pi/2 are above and below the central pixel, and so on. If more than one angle is provided, we compute their average.\n\n\nglcm_contrast() : measures the contrast or the amount of local variations present in an image. Low contrast values indicate regions with low spatial frequency.\n\nglcm_homogeneity(): also known as the Inverse Difference Moment, measures image homogeneity by assuming larger values for smaller gray tone differences in pairs.\n\nglcm_asm(): the Angular Second Moment (ASM) measures textural uniformity. High ASM values indicate a constant or a periodic form in the window values.\n\nglcm_energy(): measures textural uniformity. Energy is defined as the square root of the ASM.\n\nglcm_mean(): measures the mean of the probability of co-occurrence of specific pixel values within the neighborhood.\n\nglcm_variance(): measures the heterogeneity and is strongly correlated to first order statistical variables such as standard deviation. Variance values increase as the gray-level values deviate from their mean.\n\nglcm_std(): the standard deviation, which is the square root of the variance.\n\nglcm_correlation(): measures the gray-tone linear dependencies of the image. Low correlation values indicate homogeneous region edges."
  },
  {
    "objectID": "dc_texture.html#example",
    "href": "dc_texture.html#example",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.3 Example",
    "text": "11.3 Example\nAs an example of using GLCM texture measure, we will take an example of a SAR Sentinel-1 image over a small part of the MGRS tile “20LMR”. We will use a cube with a single date to make processing faster. In the case of a multi-date cube, the texture measure will be computed separately for each band and each date.\nWe first select a single date cube for a Sentinel-1 image (band “VH”) for a region in Rondonia, Brasil. We then regularize this cube to match a small region of interest inside tile “20LMR”. Using a small ROI allows a better understanding of the effect of texture measures.\n\n# create an RTC cube from MPC collection for a region in Rondonia, Brazil.\ncube_s1_rtc &lt;-  sits_cube(\n    source = \"MPC\",\n    collection = \"SENTINEL-1-RTC\",\n    bands = c(\"VH\"),\n    orbit = \"descending\",\n    tiles = c(\"20LMR\"),\n    start_date = \"2024-08-01\",\n    end_date = \"2024-08-12\"\n)\n# define a ROI which is part of the cube to reduce processing\nroi &lt;- c(\"lon_min\" = -63.45, \"lon_max\" = -63.3,  \"lat_min\" = -8.7, \"lat_max\" = -8.55)\n\n# create an RTC cube regular\ncube_s1_rtc_20LMR &lt;-  sits_regularize(\n    cube = cube_s1_rtc,\n    period = \"P12D\",\n    res = 20,\n    output_dir = tempdir_r,\n    roi = roi,\n    multicores = 4\n)\nplot(cube_s1_rtc_20LMR, band = \"VH\")\n\n\n\n\n\nFigure 11.1: Original Sentinel-1 image covering part of tile 20LMR.\n\n\n\nThe plot on Figure 11.1 shows a situation of small forest patches inside an area with much deforestation. If also shows that the forest areas have a grainy and unequal texture. There are areas near the center of the image which show higly degraded forest areas. To try to capture these textural differences, we use GLCM Mean and GLCM Contrast, following recommendations by Hall-Beyer [2]. Each measure captures different aspects of image texture.\nThe GLCM Mean represents the average grey level of the pixel pairs in the co-occurrence matrix. For a normalized GLCM matrix \\(P(i,j)\\), the mean is calculated along rows or columns (since in symmetric matrices they are the same):\n\\[\n\\mu = \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} i \\cdot P(i, j)\n\\]\nThis measure detects brightness trends in local regions of the image. It tends to be high for brighter areas, and low for darker areas. The GLCM Mean can distinguish regions based on overall tone, e.g., water vs. dry soil, shadow vs. sunlit areas. It is useful for separating classes that have different average intensity values.\n\n# measure GLMC texture and plot the result\ncube_s1_rtc_20LMR &lt;-  sits_texture(\n    cube = cube_s1_rtc_20LMR,\n    VHMEAN = glcm_mean(VH),\n    window_size = 5,\n    output_dir = tempdir_r,\n    multicores = 4,\n    memsize = 16\n)\nplot(cube_s1_rtc_20LMR, band = \"VHMEAN\")\n\n\n\n\n\nFigure 11.2: GLCM Mean measure for Sentinel-1 image covering part of tile 20LMR.\n\n\n\nFigure 11.2 shows that the GLCM Mean enhances the edges and increases the distinction between forest and non-forest areas. Areas of degraded forest, where trees and clear-cut areas are intermingled, are also stressed to produce an intermediate value between that of the trees and clear-cut areas. In theory, this distinction could help the detection of degraded forests.\nThe GLCM Contrast measures the weighted sum of squared intensity differences between pixel pairs. Contrast quantifies the intensity variation between a pixel and its neighbor, measuring local changes in grey levels. It produces high contrast areas when neighboring pixel pairs have very different grey levels, indicating rough, coarse, or edge-rich texture. Low contrast areas appear when neighboring pixels are similar, indicating smooth, homogeneous regions.\n\\[\n\\text{Contrast} = \\sum_{i=0}^{N-1} \\sum_{j=0}^{N-1} (i - j)^2 \\cdot P(i, j)\n\\]\n\n# measure GLMC texture and plot the result\ncube_s1_rtc_20LMR &lt;-  sits_texture(\n    cube = cube_s1_rtc_20LMR,\n    VHCON = glcm_contrast(VH),\n    window_size = 5,\n    output_dir = tempdir_r,\n    multicores = 4,\n    memsize = 16\n)\nplot(cube_s1_rtc_20LMR, band = \"VHCON\")\n\n\n\n\n\nFigure 11.3: GLCM Contrast measure for Sentinel-1 image covering part of tile 20LMR.\n\n\n\nIn Figure 11.3, one can see that the GLCM Contrast improves our understanding of the internal structure of forest patches. In this region, due to high human activity, many of the forest patches show evidence of degradation. In a dense canopy area, areas of low contrast are to be expected. In low or moderate degradation areas, which is the case in parts of the image, one sees pixels of high and low contrast grouped together. Thus, the GLCM Contrast measure provides further insights on the structure of forests in this area of intense deforestation."
  },
  {
    "objectID": "dc_texture.html#summary",
    "href": "dc_texture.html#summary",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.4 Summary",
    "text": "11.4 Summary\nGLCM features are a way to inject contextual texture cues into image analysis pipelines. When combined with spectral or deep-learning features they consistently boost classification and change-detection accuracy, particularly in heterogeneous landscapes where “what pixels are made of” is not enough—“how they are arranged” makes the difference."
  },
  {
    "objectID": "dc_texture.html#references",
    "href": "dc_texture.html#references",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nR. M. Haralick, K. Shanmugam, and I. Dinstein, “Textural Features for Image Classification,” IEEE Transactions on Systems, Man, and Cybernetics, vol. SMC–3, no. 6, pp. 610–621, 1973, doi: 10.1109/TSMC.1973.4309314.\n\n\n[2] \nM. Hall-Beyer, “Practical guidelines for choosing GLCM textures to use in landscape classification tasks over a range of moderate spatial scales,” International Journal of Remote Sensing, vol. 38, no. 5, pp. 1312–1338, 2017, doi: 10.1080/01431161.2016.1278314."
  },
  {
    "objectID": "dc_texture.html#introduction",
    "href": "dc_texture.html#introduction",
    "title": "\n11  Texture operations in data cubes\n",
    "section": "\n11.1 Introduction",
    "text": "11.1 Introduction\nGLCM (Gray Level Co-occurrence Matrix) texture measures capture spatial relationships between pixel intensities. Important measures include contrast, dissimilarity, homogeneity, energy, entropy, correlation, variance, and cluster prominence. These measures are useful in image processing and remote sensing, helping with land cover classification, texture recognition, and medical imaging applications. One key benefit is their ability to capture information about pixel arrangement, even under some lighting changes, making them resilient for object classification tasks. Surfaces such as cropland, urban fabric, tree crowns or tumbling water can share similar reflectance values but differ in their arrangement. Injecting GLCM features into a classifier therefore improves separability of spectrally ambiguous classes."
  }
]