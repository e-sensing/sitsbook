---
title: "Validation and accuracy measurement"
format: html
---

After producing a classified map from remote sensing imagery, the next essential stage is to validate the results and quantify classification accuracy. This process involves two distinct procedures: cross-validation of training set and assessment of

## Basic measures of accuracy

The metrics of producer’s accuracy, user’s accuracy, and overall accuracy are derived from the confusion matrix (also known as the error matrix), which compares the classified map data to reference (ground truth) data. Each of these metrics provides a different perspective on the quality of classification.

Producer’s accuracy and provides insight into omission errors. User’s accuracy, in contrast, evaluates the reliability of the classification output from the perspective of the map user, capturing commission errors.

Producer’s Accuracy (PA) measures the probability that a reference (true) sample is correctly classified. It measures *omission error* — how often real features of a class are missed in the classification. It is measured the proportion of correctly classified instances relative to the total number of reference samples in a given class. 

$$
    \text{PA}_i = \frac{\text{Correctly classified samples of class } i}{\text{Total reference samples of class } i}
    = \frac{n_{ii}}{\sum_j n_{ji}}
$$
User’s Accuracy (UA) is the probability that a sample classified as a given class actually belongs to that class in the reference data. It measures *commission error* — how often a class on the map includes misclassified pixels.

$$
    \text{UA}_i = \frac{\text{Correctly classified samples of class } i}{\text{Total classified samples as class } i}
    = \frac{n_{ii}}{\sum_j n_{ij}}
$$

Overall Accuracy (OA) is the proportion of all samples that are correctly classified. It provides a general measure of classification success across all classes.

$$
    \text{OA} = \frac{\text{Correctly classified samples of class}}{\text{Total classified samples}} = \frac{\sum_i n_{ii}}{N}
$$

## Example (Simplified Confusion Matrix):

|                 | Reference Class A | Reference Class B | Total (Map) |
|-----------------|-------------------|-------------------|-------------|
| **Map A**       | 40                | 10                | 50          |
| **Map B**       | 5                 | 45                | 50          |
| **Total (Ref)** | 45                | 55                | 100         |

-   **Producer’s Accuracy (A)** = 40 / 45 = 0.89
-   **User’s Accuracy (A)** = 40 / 50 = 0.80
-   **Overall Accuracy** = (40 + 45) / 100 = 0.85

------------------------------------------------------------------------

Would you like these metrics computed on a sample dataset or integrated into a validation report format?

## Cross-validation

The first step is to partition the available data into two distinct subsets: one for training and another for validation. The training set is used to develop the classification model, while the validation set is reserved exclusively for assessing the model’s performance. This separation is crucial to avoid biased accuracy estimates. Ideally, the partitioning should ensure that each land cover class is proportionally represented—this can be achieved through stratified random sampling.

Once the classifier is trained and the image is classified, a reliable set of reference data must be collected or prepared. This reference, or ground truth data, serves as the benchmark for assessing classification quality. It can be derived from field surveys, high-resolution satellite imagery, or expert interpretation, and it must be representative, accurate, and temporally consistent with the classified image.

With the classified map and the reference data in hand, the next step is to construct a confusion matrix (also called an error matrix). This matrix provides a cross-tabulation of predicted class labels against the true class labels from the reference data. Each cell in the matrix reflects the number of pixels (or samples) that fall into the intersection of a predicted class and an actual class.

From the confusion matrix, various accuracy metrics can be derived. The overall accuracy is the simplest and indicates the proportion of correctly classified samples. Producer’s accuracy measures how well a specific class has been correctly classified relative to the reference data, while user’s accuracy evaluates the reliability of the classification from the perspective of the map user. The Kappa coefficient, a widely used index, accounts for the agreement occurring by chance and offers a more robust measure of overall accuracy.

In some cases, additional validation techniques such as k-fold cross-validation or bootstrapping are employed. These methods help estimate the variability and generalizability of the classifier by resampling the data and averaging accuracy over multiple runs.

Beyond numerical metrics, spatial analysis of classification errors can offer valuable insights. Mapping the spatial distribution of misclassifications can reveal patterns or systematic errors—such as consistent confusion between certain land cover types in specific geographic regions.

For studies where estimating the area of land cover types is important, classification errors must be considered when calculating area statistics. This often involves adjusting area estimates using the confusion matrix to correct for misclassifications, following statistically sound approaches recommended in the remote sensing literature.

Finally, a complete validation report should present the confusion matrix, derived accuracy metrics (ideally with confidence intervals), and a critical interpretation of the results. This includes discussing potential sources of error, limitations of the reference data, and any sensor or algorithmic constraints that may have influenced the outcome.

------------------------------------------------------------------------

------------------------------------------------------------------------

Validation and Accuracy Assessment Following the generation of the classified map, a systematic validation process was undertaken to assess the reliability and accuracy of the classification results. This process involved several key steps designed to ensure a rigorous evaluation of model performance.

Initially, the available reference dataset was partitioned into two independent subsets: one designated for training the classification algorithm and the other reserved for validation. This separation is essential to avoid bias in the accuracy estimates and to ensure the independence of the evaluation process. To enhance representativeness and reduce sampling error, stratified random sampling was employed, ensuring that each land cover class was adequately represented in both subsets.

Reference data, serving as ground truth, was obtained from \[insert source—e.g., field surveys, high-resolution imagery, or expert interpretation\]. To ensure compatibility with the classified image, the reference data was carefully selected to be temporally consistent, spatially representative, and thematically reliable.

Accuracy assessment was performed through the construction of a confusion matrix, which cross-tabulates the predicted class labels against the corresponding reference class labels. This matrix provides the foundation for computing standard accuracy metrics. The overall accuracy (OA) was calculated as the proportion of correctly classified samples. In addition, producer’s accuracy and user’s accuracy were computed for each class. Producer’s accuracy indicates the probability that a reference pixel was correctly classified, thereby measuring omission error. Conversely, user’s accuracy measures the probability that a pixel classified into a given category actually represents that category on the ground, reflecting commission error. The Kappa coefficient was also calculated as a measure of agreement adjusted for chance, providing a more robust metric of classification performance.

Where appropriate, cross-validation techniques were applied to evaluate the stability and generalizability of the classification model. Specifically, k-fold cross-validation was used to partition the dataset into multiple subsets, allowing the model to be iteratively trained and validated across different data splits. This approach helped to mitigate overfitting and provided more reliable estimates of classification accuracy.

In addition to quantitative metrics, a spatial analysis of classification errors was conducted. The spatial distribution of misclassified samples was mapped to identify systematic errors and assess their geographic concentration. This step is critical for diagnosing class confusion in heterogeneous or transitional landscapes.

For applications involving area estimation of land cover types, classification errors were accounted for using error-adjusted area estimation techniques based on the confusion matrix, following established guidelines (e.g., Olofsson et al., 2014). This correction ensures that reported land cover statistics reflect both the classification results and their associated uncertainty.

All results were critically interpreted with consideration of potential sources of error, including limitations in the reference data, spectral or temporal ambiguity between land cover types, and the influence of sensor characteristics. The validation process, thus, provided a comprehensive evaluation of classification quality and supported robust inferences regarding the land cover patterns observed.

In remote sensing image classification, **validation and accuracy measurement** are essential steps to assess the performance and reliability of a classification model. Here is a structured outline of the typical steps involved:

## Statistical assessment of classified maps

The accuracy assessment and area estimation procedures in SITS follow the best practices outlined by Olofsson et al. (2014), which provide a statistically rigorous framework for evaluating land cover and land change maps. These guidelines emphasize the importance of using probability sampling, transparent documentation, and unbiased estimation techniques to ensure that both classification accuracy and area statistics are reliable and reproducible.

A key recommendation is the use of probability-based sampling designs—such as stratified random sampling—for the selection of reference data. This approach guarantees that each sample unit has a known, non-zero probability of selection, enabling the derivation of statistically valid and unbiased estimates. Moreover, stratified sampling is particularly effective in improving the precision of area estimates, especially when class distributions are imbalanced.

The reference data used for validation must be collected independently of the classification process and should be carefully labeled according to a set of clear, mutually exclusive, and exhaustive class definitions. These definitions must be consistently applied across both map labels and reference labels to avoid ambiguities during comparison.

Accuracy assessment is conducted through the construction of a confusion matrix, which cross-tabulates classified labels against reference labels. From this matrix, several key metrics are derived, including overall accuracy, producer’s accuracy, and user’s accuracy.

Crucially, Olofsson et al. argue that area estimation should be based on the reference data, not solely on the classified map. This is because map-derived area estimates are often biased due to classification errors. By weighting the confusion matrix according to the sampling design, one can produce unbiased estimates of the area occupied by each land cover class, along with corresponding standard errors and confidence intervals. This correction is particularly important in studies where accurate area estimation informs policy or decision-making processes.

The methodology also calls for the reporting of both accuracy metrics and error-adjusted area estimates, ensuring that readers can assess not only the classification performance but also the precision of the mapped land cover statistics. All elements of the assessment process—including the sampling design, reference data acquisition procedures, labeling protocols, and temporal consistency between reference and map data—should be clearly documented to support reproducibility and transparency.

Finally, Olofsson et al. highlight the importance of temporal consistency between the classification and the reference data. Any discrepancies in acquisition dates can lead to misinterpretations of classification errors that may, in fact, reflect genuine land cover changes. They also advocate for cost-effective sampling strategies, such as allocating more samples to rare or high-priority classes, to improve statistical efficiency without inflating operational costs.

By adhering to these principles, the assessment of classification results not only quantifies accuracy but also supports statistically sound area estimation, enabling robust and policy-relevant applications of remote sensing data.
