<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Machine learning for data cubes | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes</title>
<meta name="author" content="Gilberto Camara">
<meta name="author" content="Rolf Simoes">
<meta name="author" content="Felipe Souza">
<meta name="author" content="Felipe Menino">
<meta name="author" content="Charlotte Pelletier">
<meta name="author" content="Pedro R. Andrade">
<meta name="author" content="Karine Ferreira">
<meta name="author" content="Gilberto Queiroz">
<meta name="description" content="Machine learning classification Machine learning classification is a type of supervised learning in which an algorithm is trained to predict which class an input data point belongs to. The goal of...">
<meta name="generator" content="bookdown 0.39 with bs4_book()">
<meta property="og:title" content="Machine learning for data cubes | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/cover_sits_book.png">
<meta property="og:description" content="Machine learning classification Machine learning classification is a type of supervised learning in which an algorithm is trained to predict which class an input data point belongs to. The goal of...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine learning for data cubes | sits: Satellite Image Time Series Analysis on Earth Observation Data Cubes">
<meta name="twitter:description" content="Machine learning classification Machine learning classification is a type of supervised learning in which an algorithm is trained to predict which class an input data point belongs to. The goal of...">
<meta name="twitter:image" content="/images/cover_sits_book.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/IBM_Plex_Serif-0.4.9/font.css" rel="stylesheet">
<link href="libs/IBM_Plex_Mono-0.4.9/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.8.0/transition.js"></script><script src="libs/bs3compat-0.8.0/tabs.js"></script><script src="libs/bs3compat-0.8.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title=""><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="setup.html">Setup</a></li>
<li><a class="" href="acknowledgements.html">Acknowledgements</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li><a class="" href="earth-observation-data-cubes.html">Earth observation data cubes</a></li>
<li><a class="" href="operations-on-data-cubes.html">Operations on data cubes</a></li>
<li><a class="" href="working-with-time-series.html">Working with time series</a></li>
<li><a class="" href="improving-the-quality-of-training-samples.html">Improving the quality of training samples</a></li>
<li><a class="active" href="machine-learning-for-data-cubes.html">Machine learning for data cubes</a></li>
<li><a class="" href="classification-of-raster-data-cubes.html">Classification of raster data cubes</a></li>
<li><a class="" href="bayesian-smoothing-for-post-processing.html">Bayesian smoothing for post-processing</a></li>
<li><a class="" href="validation-and-accuracy-measurements.html">Validation and accuracy measurements</a></li>
<li><a class="" href="uncertainty-and-active-learning.html">Uncertainty and active learning</a></li>
<li><a class="" href="ensemble-prediction-from-multiple-models.html">Ensemble prediction from multiple models</a></li>
<li><a class="" href="object-based-time-series-image-analysis.html">Object-based time series image analysis</a></li>
<li><a class="" href="data-visualisation-in-sits.html">Data visualisation in sits</a></li>
<li><a class="" href="technical-annex.html">Technical annex</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="machine-learning-for-data-cubes" class="section level1 unnumbered">
<h1>Machine learning for data cubes<a class="anchor" aria-label="anchor" href="#machine-learning-for-data-cubes"><i class="fas fa-link"></i></a>
</h1>
<p><a href="https://www.kaggle.com/code/esensing/machine-learning-for-data-cubes" target="_blank"><img src="https://kaggle.com/static/images/open-in-kaggle.svg"></a></p>
<div id="machine-learning-classification" class="section level2 unnumbered">
<h2>Machine learning classification<a class="anchor" aria-label="anchor" href="#machine-learning-classification"><i class="fas fa-link"></i></a>
</h2>
<p>Machine learning classification is a type of supervised learning in which an algorithm is trained to predict which class an input data point belongs to. The goal of machine learning models is to approximate a function <span class="math inline">\(y = f(x)\)</span> that maps an input <span class="math inline">\(x\)</span> to a class <span class="math inline">\(y\)</span>. A model defines a mapping <span class="math inline">\(y = f(x;\theta)\)</span> and learns the value of the parameters <span class="math inline">\(\theta\)</span> that result in the best function approximation <span class="citation"><a href="references.html#ref-Goodfellow2016">[49]</a></span>. The difference between the different algorithms is their approach to building the mapping that classifies the input data.<br>
In <code>sits</code>, machine learning is used to classify individual time series using the <code>time-first</code> approach. The package includes two kinds of methods for time series classification:</p>
<ul>
<li><p>Machine learning algorithms that do not explicitly consider the temporal structure of the time series. They treat time series as a vector in a high-dimensional feature space, taking each time series instance as independent from the others. They include random forest (<code><a href="https://rdrr.io/pkg/sits/man/sits_rfor.html">sits_rfor()</a></code>), support vector machine (<code><a href="https://rdrr.io/pkg/sits/man/sits_svm.html">sits_svm()</a></code>), extreme gradient boosting (<code><a href="https://rdrr.io/pkg/sits/man/sits_xgboost.html">sits_xgboost()</a></code>), and multilayer perceptron (<code><a href="https://rdrr.io/pkg/sits/man/sits_mlp.html">sits_mlp()</a></code>).</p></li>
<li><p>Deep learning methods where temporal relations between observed values in a time series are taken into account. These models are specifically designed for time series. The temporal order of values in a time series is relevant for the classification model. From this class of models, <code>sits</code> supports 1D convolution neural networks (<code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code>) and temporal attention-based encoders (<code><a href="https://rdrr.io/pkg/sits/man/sits_tae.html">sits_tae()</a></code> and <code><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae()</a></code>).</p></li>
</ul>
<p>Based on experience with <code>sits</code>, random forest, extreme gradient boosting, and temporal deep learning models outperform SVM and multilayer perceptron models. The reason is that some dates provide more information than others in the temporal behavior of land classes. For instance, when monitoring deforestation, dates corresponding to forest removal actions are more informative than earlier or later dates. Similarly, a few dates may capture a large portion of the variation in crop mapping. Therefore, classification methods that consider the temporal order of samples are more likely to capture the seasonal behavior of image time series. Random forest and extreme gradient boosting methods that use individual measures as nodes in decision trees can also capture specific events such as deforestation.</p>
<p>The following examples show how to train machine learning methods and apply them to classify a single time series. We use the set <code>samples_matogrosso_mod13q1</code>, containing time series samples from the Brazilian Mato Grosso state obtained from the MODIS MOD13Q1 product. It has 1,892 samples and nine classes (<code>Cerrado</code>, <code>Forest</code>, <code>Pasture</code>, <code>Soy_Corn</code>, <code>Soy_Cotton</code>, <code>Soy_Fallow</code>, <code>Soy_Millet</code>). Each time series covers 12 months (23 data points) with six bands (NDVI, EVI, BLUE, RED, NIR, MIR). The samples are arranged along an agricultural year, starting in September and ending in August. The dataset was used in the paper “Big Earth observation time series analysis for monitoring Brazilian agriculture” <span class="citation"><a href="references.html#ref-Picoli2018">[50]</a></span>, being available in the R package <code>sitsdata</code>.</p>
</div>
<div id="common-interface-to-machine-learning-and-deep-learning-models" class="section level2 unnumbered">
<h2>Common interface to machine learning and deep learning models<a class="anchor" aria-label="anchor" href="#common-interface-to-machine-learning-and-deep-learning-models"><i class="fas fa-link"></i></a>
</h2>
<p>The <code><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train()</a></code> function provides a standard interface to all machine learning models. This function takes two mandatory parameters: the training data (<code>samples</code>) and the ML algorithm (<code>ml_method</code>). After the model is estimated, it can classify individual time series or data cubes with <code><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify()</a></code>. In what follows, we show how to apply each method to classify a single time series. Then, in Chapter <a href="https://e-sensing.github.io/sitsbook/image-classification-in-data-cubes.html">Image classification in data cubes</a>, we discuss how to classify data cubes.</p>
<p>Since <code>sits</code> is aimed at remote sensing users who are not machine learning experts, it provides a set of default values for all classification models. These settings have been chosen based on testing by the authors. Nevertheless, users can control all parameters for each model. Novice users can rely on the default values, while experienced ones can fine-tune model parameters to meet their needs. Model tuning is discussed at the end of this Chapter.</p>
<p>When a set of time series organized as tibble is taken as input to the classifier, the result is the same tibble with one additional column (<code>predicted</code>), which contains the information on the labels assigned for each interval. The results can be shown in text format using the function <code><a href="https://rdrr.io/pkg/sits/man/sits_show_prediction.html">sits_show_prediction()</a></code> or graphically using <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code>.</p>
</div>
<div id="random-forest" class="section level2 unnumbered">
<h2>Random forest<a class="anchor" aria-label="anchor" href="#random-forest"><i class="fas fa-link"></i></a>
</h2>
<p>Random forest is a machine learning algorithm that uses an ensemble learning method for classification tasks. The algorithm consists of multiple decision trees, each trained on a different subset of the training data and with a different subset of features. To make a prediction, each decision tree in the forest independently classifies the input data. The final prediction is made based on the majority vote of all the decision trees. The randomness in the algorithm comes from the random subsets of data and features used to train each decision tree, which helps to reduce overfitting and improve the accuracy of the model. This classifier measures the importance of each feature in the classification task, which can be helpful in feature selection and data visualization. For an in-depth discussion of the robustness of random forest method for satellite image time series classification, please see Pelletier et al <span class="citation"><a href="references.html#ref-Pelletier2016">[51]</a></span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlrffig"></span>
<img src="images/random_forest.png" alt="Random forest algorithm (Source: Venkata Jagannath in Wikipedia - licenced as CC-BY-SA 4.0)." width="90%"><p class="caption">
Figure 62: Random forest algorithm (Source: Venkata Jagannath in Wikipedia - licenced as CC-BY-SA 4.0).
</p>
</div>
<p><code>sits</code> provides <code><a href="https://rdrr.io/pkg/sits/man/sits_rfor.html">sits_rfor()</a></code>, which uses the R <code>randomForest</code> package <span class="citation"><a href="references.html#ref-Wright2017">[52]</a></span>; its main parameter is <code>num_trees</code>, which is the number of trees to grow with a default value of 100. The model can be visualized using <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot()</a></code>.</p>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Train the Mato Grosso samples with random forest model</span></span>
<span><span class="va">rfor_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train</a></span><span class="op">(</span></span>
<span>  samples <span class="op">=</span> <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  ml_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_rfor.html">sits_rfor</a></span><span class="op">(</span>num_trees <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Plot the most important variables of the model</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">rfor_model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlrformodel"></span>
<img src="images/mlrformodel.png" alt="Most important variables in random forest model (source: authors)." width="90%"><p class="caption">
Figure 63: Most important variables in random forest model (source: authors).
</p>
</div>
<p>The most important explanatory variables are the NIR (near infrared) band on date 17 (2007-05-25) and the MIR (middle infrared) band on date 22 (2007-08-13). The NIR value at the end of May captures the growth of the second crop for double cropping classes. Values of the MIR band at the end of the period (late July to late August) capture bare soil signatures to distinguish between agricultural and natural classes. This corresponds to summertime when the ground is drier after harvesting crops.</p>
<div class="sourceCode" id="cb156"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Classify using random forest model and plot the result</span></span>
<span><span class="va">point_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">point_mt_mod13q1</span>,</span>
<span>  ml_model <span class="op">=</span> <span class="va">rfor_model</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">point_class</span>, bands <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NDVI"</span>, <span class="st">"EVI"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb157"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/include_graphics.html">include_graphics</a></span><span class="op">(</span><span class="st">"./images/mlrforplot.png"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlrforplot"></span>
<img src="images/mlrforplot.png" alt="Classification of time series using random forest (source: authors)." width="90%"><p class="caption">
Figure 64: Classification of time series using random forest (source: authors).
</p>
</div>
<p>The result shows that the area started as a forest in 2000, was deforested from 2004 to 2005, used as pasture from 2006 to 2007, and for double-cropping agriculture from 2009 onwards. This behavior is consistent with expert evaluation of land change process in this region of Amazonia.</p>
<p>Random forest is robust to outliers and can deal with irrelevant inputs <span class="citation"><a href="references.html#ref-Hastie2009">[33]</a></span>. The method tends to overemphasize some variables because its performance tends to stabilize after part of the trees is grown <span class="citation"><a href="references.html#ref-Hastie2009">[33]</a></span>. In cases where abrupt change occurs, such as deforestation mapping, random forest (if properly trained) will emphasize the temporal instances and bands that capture such quick change.</p>
</div>
<div id="support-vector-machine" class="section level2 unnumbered">
<h2>Support vector machine<a class="anchor" aria-label="anchor" href="#support-vector-machine"><i class="fas fa-link"></i></a>
</h2>
<p>The support vector machine (SVM) classifier is a generalization of a linear classifier that finds an optimal separation hyperplane that minimizes misclassification <span class="citation"><a href="references.html#ref-Cortes1995">[53]</a></span>. Since a set of samples with <span class="math inline">\(n\)</span> features defines an n-dimensional feature space, hyperplanes are linear <span class="math inline">\({(n-1)}\)</span>-dimensional boundaries that define linear partitions in that space. If the classes are linearly separable on the feature space, there will be an optimal solution defined by the maximal margin hyperplane, which is the separating hyperplane that is farthest from the training observations <span class="citation"><a href="references.html#ref-James2013">[54]</a></span>. The maximal margin is computed as the smallest distance from the observations to the hyperplane. The solution for the hyperplane coefficients depends only on the samples that define the maximum margin criteria, the so-called support vectors.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlsvmfig"></span>
<img src="images/svm_margin.png" alt="Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. (Source: Larhmam in Wikipedia - licensed as CC-BY-SA-4.0)." width="50%"><p class="caption">
Figure 65: Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. (Source: Larhmam in Wikipedia - licensed as CC-BY-SA-4.0).
</p>
</div>
<p>For data that is not linearly separable, SVM includes kernel functions that map the original feature space into a higher dimensional space, providing nonlinear boundaries to the original feature space. Despite having a linear boundary on the enlarged feature space, the new classification model generally translates its hyperplane to a nonlinear boundary in the original attribute space. Kernels are an efficient computational strategy to produce nonlinear boundaries in the input attribute space; thus, they improve training-class separation. SVM is one of the most widely used algorithms in machine learning applications and has been applied to classify remote sensing data <span class="citation"><a href="references.html#ref-Mountrakis2011">[55]</a></span>.</p>
<p>In <code>sits</code>, SVM is implemented as a wrapper of <code>e1071</code> R package that uses the <code>LIBSVM</code> implementation <span class="citation"><a href="references.html#ref-Chang2011">[56]</a></span>. The <code>sits</code> package adopts the <em>one-against-one</em> method for multiclass classification. For a <span class="math inline">\(q\)</span> class problem, this method creates <span class="math inline">\({q(q-1)/2}\)</span> SVM binary models, one for each class pair combination, testing any unknown input vectors throughout all those models. A voting scheme computes the overall result.</p>
<p>The example below shows how to apply SVM to classify time series using default values. The main parameters are <code>kernel</code>, which controls whether to use a nonlinear transformation (default is <code>radial</code>), <code>cost</code>, which measures the punishment for wrongly-classified samples (default is 10), and <code>cross</code>, which sets the value of the k-fold cross validation (default is 10).</p>
<div class="sourceCode" id="cb158"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Train an SVM model</span></span>
<span><span class="va">svm_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train</a></span><span class="op">(</span></span>
<span>  samples <span class="op">=</span> <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  ml_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_svm.html">sits_svm</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Classify using the SVM model and plot the result</span></span>
<span><span class="va">point_class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">point_mt_mod13q1</span>,</span>
<span>  ml_model <span class="op">=</span> <span class="va">svm_model</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Plot the result</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">point_class</span>, bands <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NDVI"</span>, <span class="st">"EVI"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlsvmplot"></span>
<img src="images/mlsvmplot.png" alt="Classification of time series using SVM (source: authors)." width="90%"><p class="caption">
Figure 66: Classification of time series using SVM (source: authors).
</p>
</div>
<p>The SVM classifier is less stable and less robust to outliers than the random forest method. In this example, it tends to misclassify some of the data. In 2008, it is likely that the correct land class was still <code>Pasture</code> rather than <code>Soy_Millet</code> as produced by the algorithm, while the <code>Soy_Cotton</code> class in 2012 is also inconsistent with the previous and latter classification of <code>Soy_Corn</code>.</p>
</div>
<div id="extreme-gradient-boosting" class="section level2 unnumbered">
<h2>Extreme gradient boosting<a class="anchor" aria-label="anchor" href="#extreme-gradient-boosting"><i class="fas fa-link"></i></a>
</h2>
<p>XGBoost (eXtreme Gradient Boosting) <span class="citation"><a href="references.html#ref-Chen2016">[57]</a></span> is an implementation of gradient boosted decision trees designed for speed and performance. It is an ensemble learning method, meaning it combines the predictions from multiple models to produce a final prediction. XGBoost builds trees one at a time, where each new tree helps to correct errors made by previously trained tree. Each tree builds a new model to correct the errors made by previous models. Using gradient descent, the algorithm iteratively adjusts the predictions of each tree by focusing on instances where previous trees made errors. Models are added sequentially until no further improvements can be made.</p>
<p>Although random forest and boosting use trees for classification, there are significant differences. While random forest builds multiple decision trees in parallel and merges them together for a more accurate and stable prediction, XGBoost builds trees one at a time, where each new tree helps to correct errors made by previously trained tree. XGBoost is often preferred for its speed and performance, particularly on large datasets, and is well-suited for problems where precision is paramount. Random Forest, on the other hand, is simpler to implement, more interpretable, and can be more robust to overfitting, making it a good choice for general-purpose applications.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlxgbfig"></span>
<img src="images/flow_chart_xgboost.png" alt="Flow chart of XGBoost algorithm (Source: Guo et al., Applied Sciences, 2020. - licenced as CC-BY-SA 4.0)." width="90%"><p class="caption">
Figure 67: Flow chart of XGBoost algorithm (Source: Guo et al., Applied Sciences, 2020. - licenced as CC-BY-SA 4.0).
</p>
</div>
<p>The boosting method starts from a weak predictor and then improves performance sequentially by fitting a better model at each iteration. It fits a simple classifier to the training data and uses the residuals of the fit to build a predictor. Typically, the base classifier is a regression tree. Although random forest and boosting use trees for classification, there are significant differences. The performance of random forest generally increases with the number of trees until it becomes stable. Boosting trees apply finer divisions over previous results to improve performance <span class="citation"><a href="references.html#ref-Hastie2009">[33]</a></span>. Some recent papers show that it outperforms random forest for remote sensing image classification <span class="citation"><a href="references.html#ref-Jafarzadeh2021">[58]</a></span>. However, this result is not generalizable since the quality of the training dataset controls actual performance.</p>
<p>In <code>sits</code>, the XGBoost method is implemented by the <code>sits_xbgoost()</code> function, based on <code>XGBoost</code> R package, and has five hyperparameters that require tuning. The <code>sits_xbgoost()</code> function takes the user choices as input to a cross-validation to determine suitable values for the predictor.</p>
<p>The learning rate <code>eta</code> varies from 0.0 to 1.0 and should be kept small (default is 0.3) to avoid overfitting. The minimum loss value <code>gamma</code> specifies the minimum reduction required to make a split. Its default is 0; increasing it makes the algorithm more conservative. The <code>max_depth</code> value controls the maximum depth of the trees. Increasing this value will make the model more complex and likely to overfit (default is 6). The <code>subsample</code> parameter controls the percentage of samples supplied to a tree. Its default is 1 (maximum). Setting it to lower values means that xgboost randomly collects only part of the data instances to grow trees, thus preventing overfitting. The <code>nrounds</code> parameter controls the maximum number of boosting interactions; its default is 100, which has proven to be enough in most cases. To follow the convergence of the algorithm, users can turn the <code>verbose</code> parameter on. In general, the results using the extreme gradient boosting algorithm are similar to the random forest method.</p>
<div class="sourceCode" id="cb159"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Train using  XGBoost</span></span>
<span><span class="va">xgb_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train</a></span><span class="op">(</span></span>
<span>  samples <span class="op">=</span> <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  ml_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_xgboost.html">sits_xgboost</a></span><span class="op">(</span>verbose <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Classify using SVM model and plot the result</span></span>
<span><span class="va">point_class_xgb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">point_mt_mod13q1</span>,</span>
<span>  ml_model <span class="op">=</span> <span class="va">xgb_model</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">point_class_xgb</span>, bands <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NDVI"</span>, <span class="st">"EVI"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlxgbplot"></span>
<img src="images/mlxgbplot.png" alt="Classification of time series using XGBoost (source: authors)." width="90%"><p class="caption">
Figure 68: Classification of time series using XGBoost (source: authors).
</p>
</div>
</div>
<div id="deep-learning-using-multilayer-perceptron" class="section level2 unnumbered">
<h2>Deep learning using multilayer perceptron<a class="anchor" aria-label="anchor" href="#deep-learning-using-multilayer-perceptron"><i class="fas fa-link"></i></a>
</h2>
<p>To support deep learning methods, <code>sits</code> uses the <code>torch</code> R package, which takes the Facebook <code>torch</code> C++ library as a back-end. Machine learning algorithms that use the R <code>torch</code> package are similar to those developed using <code>PyTorch</code>. The simplest deep learning method is multilayer perceptron (MLP), which are feedforward artificial neural networks. An MLP consists of three kinds of nodes: an input layer, a set of hidden layers, and an output layer. The input layer has the same dimension as the number of features in the dataset. The hidden layers attempt to approximate the best classification function. The output layer decides which class should be assigned to the input.</p>
<p>In <code>sits</code>, MLP models can be built using <code><a href="https://rdrr.io/pkg/sits/man/sits_mlp.html">sits_mlp()</a></code>. Since there is no established model for generic classification of satellite image time series, designing MLP models requires parameter customization. The most important decisions are the number of layers in the model and the number of neurons per layer. These values are set by the <code>layers</code> parameter, which is a list of integer values. The size of the list is the number of layers, and each element indicates the number of nodes per layer.</p>
<p>The choice of the number of layers depends on the inherent separability of the dataset to be classified. For datasets where the classes have different signatures, a shallow model (with three layers) may provide appropriate responses. More complex situations require models of deeper hierarchy. Models with many hidden layers may take a long time to train and may not converge. We suggest to start with three layers and test different options for the number of neurons per layer before increasing the number of layers. In our experience, using three to five layers is a reasonable compromise if the training data has a good quality. Further increase in the number of layers will not improve the model.</p>
<p>MLP models also need to include the activation function. The activation function of a node defines the output of that node given an input or set of inputs. Following standard practices <span class="citation"><a href="references.html#ref-Goodfellow2016">[49]</a></span>, we use the <code>relu</code> activation function.</p>
<p>The optimization method (<code>optimizer</code>) represents the gradient descent algorithm to be used. These methods aim to maximize an objective function by updating the parameters in the opposite direction of the gradient of the objective function <span class="citation"><a href="references.html#ref-Ruder2016">[59]</a></span>. Since gradient descent plays a key role in deep learning model fitting, developing optimizers is an important topic of research <span class="citation"><a href="references.html#ref-Bottou2018">[60]</a></span>. Many optimizers have been proposed in the literature, and recent results are reviewed by Schmidt et al. <span class="citation"><a href="references.html#ref-Schmidt2021">[61]</a></span>. The Adamw optimizer provides a good baseline and reliable performance for general deep learning applications <span class="citation"><a href="references.html#ref-Kingma2017">[62]</a></span>. By default, all deep learning algorithms in <code>sits</code> use Adamw.</p>
<p>Another relevant parameter is the list of dropout rates (<code>dropout</code>). Dropout is a technique for randomly dropping units from the neural network during training <span class="citation"><a href="references.html#ref-Srivastava2014">[63]</a></span>. By randomly discarding some neurons, dropout reduces overfitting. Since a cascade of neural nets aims to improve learning as more data is acquired, discarding some neurons may seem like a waste of resources. In practice, dropout prevents an early convergence to a local minimum <span class="citation"><a href="references.html#ref-Goodfellow2016">[49]</a></span>. We suggest users experiment with different dropout rates, starting from small values (10-30%) and increasing as required.</p>
<p>The following example shows how to use <code><a href="https://rdrr.io/pkg/sits/man/sits_mlp.html">sits_mlp()</a></code>. The default parameters have been chosen based on a modified version of <span class="citation"><a href="references.html#ref-Wang2017">[64]</a></span>, which proposes using multilayer perceptron as a baseline for time series classification. These parameters are: (a) Three layers with 512 neurons each, specified by the parameter <code>layers</code>; (b) Using the “relu” activation function; (c) dropout rates of 40%, 30%, and 20% for the layers; (d) the “optimizer_adamw” as optimizer (default value); (e) a number of training steps (<code>epochs</code>) of 100; (f) a <code>batch_size</code> of 64, which indicates how many time series are used for input at a given step; and (g) a validation percentage of 20%, which means 20% of the samples will be randomly set aside for validation.</p>
<p>To simplify the output, the <code>verbose</code> option has been turned off. After the model has been generated, we plot its training history.</p>
<div class="sourceCode" id="cb160"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Train using an MLP model</span></span>
<span><span class="co"># This is an example of how to set parameters</span></span>
<span><span class="co"># First-time users should test default options first</span></span>
<span><span class="va">mlp_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train</a></span><span class="op">(</span></span>
<span>  samples <span class="op">=</span> <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  ml_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_mlp.html">sits_mlp</a></span><span class="op">(</span></span>
<span>    optimizer        <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/torch/man/optim_adamw.html">optim_adamw</a></span>,</span>
<span>    layers           <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">512</span>, <span class="fl">512</span>, <span class="fl">512</span><span class="op">)</span>,</span>
<span>    dropout_rates    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.40</span>, <span class="fl">0.30</span>, <span class="fl">0.20</span><span class="op">)</span>,</span>
<span>    epochs           <span class="op">=</span> <span class="fl">80</span>,</span>
<span>    batch_size       <span class="op">=</span> <span class="fl">64</span>,</span>
<span>    verbose          <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    validation_split <span class="op">=</span> <span class="fl">0.2</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Show training evolution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mlp_model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlmlpmodel"></span>
<img src="images/mlmlpmodel.png" alt="Evolution of training accuracy of MLP model (source: authors)." width="80%"><p class="caption">
Figure 69: Evolution of training accuracy of MLP model (source: authors).
</p>
</div>
<p>Then, we classify a 16-year time series using the multilayer perceptron model.</p>
<div class="sourceCode" id="cb161"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Classify using MLP model and plot the result</span></span>
<span><span class="va">point_mt_mod13q1</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify</a></span><span class="op">(</span><span class="va">mlp_model</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>bands <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NDVI"</span>, <span class="st">"EVI"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlmlpplot"></span>
<img src="images/mlmlpplot.png" alt="Classification of time series using MLP (source: authors)." width="90%"><p class="caption">
Figure 70: Classification of time series using MLP (source: authors).
</p>
</div>
<p>In theory, multilayer perceptron model can capture more subtle changes than random forest and XGBoost In this specific case, the result is similar to theirs. Although the model mixes the <code>Soy_Corn</code> and <code>Soy_Millet</code> classes, the distinction between their temporal signatures is quite subtle. Also, in this case, this suggests the need to improve the number of samples. In this example, the MLP model shows an increase in sensitivity compared to previous models. We recommend to compare different configurations since the MLP model is sensitive to changes in its parameters.</p>
</div>
<div id="temporal-convolutional-neural-network-tempcnn" class="section level2 unnumbered">
<h2>Temporal Convolutional Neural Network (TempCNN)<a class="anchor" aria-label="anchor" href="#temporal-convolutional-neural-network-tempcnn"><i class="fas fa-link"></i></a>
</h2>
<p>Convolutional neural networks (CNN) are deep learning methods that apply convolution filters (sliding windows) to the input data sequentially. The Temporal Convolutional Neural Network (TempCNN) is a neural network architecture specifically designed to process sequential data such as time series. In the case of time series, a 1D CNN applies a moving temporal window to the time series to produce another time series as the result of the convolution.</p>
<p>The TempCNN architecture for satellite image time series classification is proposed by Pelletier et al. <span class="citation"><a href="references.html#ref-Pelletier2019">[65]</a></span>. It has three 1D convolutional layers and a final softmax layer for classification (see Figure <a href="machine-learning-for-data-cubes.html#fig:mltcnnfig">71</a>). The authors combine different methods to avoid overfitting and reduce the vanishing gradient effect, including dropout, regularization, and batch normalization. In the TempCNN reference paper <span class="citation"><a href="references.html#ref-Pelletier2019">[65]</a></span>, the authors favourably compare their model with the Recurrent Neural Network proposed by Russwurm and Körner <span class="citation"><a href="references.html#ref-Russwurm2018">[66]</a></span>. Figure <a href="machine-learning-for-data-cubes.html#fig:mltcnnfig">71</a> shows the architecture of the TempCNN model. TempCNN applies one-dimensional convolutions on the input sequence to capture temporal dependencies, allowing the network to learn long-term dependencies in the input sequence. Each layer of the model captures temporal dependencies at a different scale. Due to its multi-scale approach, TempCNN can capture complex temporal patterns in the data and produce accurate predictions.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mltcnnfig"></span>
<img src="images/tempcnn.png" alt="Structure of tempCNN architecture (Source: Pelletier et al. (2019). Reproduction under fair use doctrine). " width="100%"><p class="caption">
Figure 71: Structure of tempCNN architecture (Source: Pelletier et al. (2019). Reproduction under fair use doctrine).
</p>
</div>
<p>The function <code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code> implements the model. The first parameter is the <code>optimizer</code> used in the backpropagation phase for gradient descent. The default is <code>adamw</code> which is considered as a stable and reliable optimization function. The parameter <code>cnn_layers</code> controls the number of 1D-CNN layers and the size of the filters applied at each layer; the default values are three CNNs with 128 units. The parameter <code>cnn_kernels</code> indicates the size of the convolution kernels; the default is kernels of size 7. Activation for all 1D-CNN layers uses the “relu” function. The dropout rates for each 1D-CNN layer are controlled individually by the parameter <code>cnn_dropout_rates</code>. The <code>validation_split</code> controls the size of the test set relative to the full dataset. We recommend setting aside at least 20% of the samples for validation.</p>
<div class="sourceCode" id="cb162"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/e-sensing/torchopt/">torchopt</a></span><span class="op">)</span></span>
<span><span class="co"># Train using tempCNN</span></span>
<span><span class="va">tempcnn_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train</a></span><span class="op">(</span></span>
<span>  <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn</a></span><span class="op">(</span></span>
<span>    optimizer            <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/torch/man/optim_adamw.html">optim_adamw</a></span>,</span>
<span>    cnn_layers           <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">256</span>, <span class="fl">256</span>, <span class="fl">256</span><span class="op">)</span>,</span>
<span>    cnn_kernels          <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">7</span>, <span class="fl">7</span>, <span class="fl">7</span><span class="op">)</span>,</span>
<span>    cnn_dropout_rates    <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span><span class="op">)</span>,</span>
<span>    epochs               <span class="op">=</span> <span class="fl">80</span>,</span>
<span>    batch_size           <span class="op">=</span> <span class="fl">64</span>,</span>
<span>    validation_split     <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>    verbose              <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Show training evolution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">tempcnn_model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mltcnnmodel"></span>
<img src="images/mltcnnmodel.png" alt="Training evolution of TempCNN model (source: authors)." width="80%"><p class="caption">
Figure 72: Training evolution of TempCNN model (source: authors).
</p>
</div>
<p>Using the TempCNN model, we classify a 16-year time series.</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Classify using TempCNN model and plot the result</span></span>
<span><span class="va">point_mt_mod13q1</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify</a></span><span class="op">(</span><span class="va">tempcnn_model</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>bands <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NDVI"</span>, <span class="st">"EVI"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mltccnplot"></span>
<img src="08-machinelearning_files/figure-html/mltccnplot-1.png" alt="Classification of time series using TempCNN (source: authors)." width="90%"><p class="caption">
Figure 73: Classification of time series using TempCNN (source: authors).
</p>
</div>
<div class="sourceCode" id="cb164"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">knitr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/include_graphics.html">include_graphics</a></span><span class="op">(</span><span class="st">"./images/mltcnnplot.png"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mltcnnplot"></span>
<img src="images/mltcnnplot.png" alt="Classification of time series using TempCNN (source: authors)." width="90%"><p class="caption">
Figure 74: Classification of time series using TempCNN (source: authors).
</p>
</div>
<p>The result has important differences from the previous ones. The TempCNN model indicates the <code>Soy_Cotton</code> class as the most likely one in 2004. While this result is possibly wrong, it shows that the time series for 2004 is different from those of Forest and Pasture classes. One possible explanation is that there was forest degradation in 2004, leading to a signature that is a mix of forest and bare soil. In this case, including forest degradation samples could improve the training data. In our experience, TempCNN models are a reliable way of classifying image time series <span class="citation"><a href="references.html#ref-Simoes2021">[67]</a></span>. Recent work which compares different models also provides evidence that TempCNN models have satisfactory behavior, especially in the case of crop classes <span class="citation"><a href="references.html#ref-Russwurm2020">[68]</a></span>.</p>
</div>
<div id="attention-based-models" class="section level2 unnumbered">
<h2>Attention-based models<a class="anchor" aria-label="anchor" href="#attention-based-models"><i class="fas fa-link"></i></a>
</h2>
<p>Attention-based deep learning models are a class of models that use a mechanism inspired by human attention to focus on specific parts of input during processing. These models have been shown to be effective for various tasks such as machine translation, image captioning, and speech recognition.</p>
<p>The basic idea behind attention-based models is to allow the model to selectively focus on different input parts at different times. This can be done by introducing a mechanism that assigns weights to each element of the input, indicating the relative importance of that element to the current processing step. The model can then use them to compute a weighted sum of the input. The results capture the model’s attention on specific parts of the input.</p>
<p>Attention-based models have become one of the most used deep learning architectures for problems that involve sequential data inputs, e.g., text recognition and automatic translation. The general idea is that not all inputs are alike in applications such as language translation. Consider the English sentence “Look at all the lonely people”. A sound translation system needs to relate the words “look” and “people” as the key parts of this sentence to ensure such link is captured in the translation. A specific type of attention models, called transformers, enables the recognition of such complex relationships between input and output sequences <span class="citation"><a href="references.html#ref-Vaswani2017">[69]</a></span>.</p>
<p>The basic structure of transformers is the same as other neural network algorithms. They have an encoder that transforms textual input values into numerical vectors and a decoder that processes these vectors to provide suitable answers. The difference is how the values are handled internally. In an MLP, all inputs are treated equally at first; based on iterative matching of training and test data, the backpropagation technique feeds information back to the initial layers to identify the most suitable combination of inputs that produces the best output.</p>
<p>Convolutional nets (CNN) combine input values that are close in time (1D) or space (2D) to produce higher-level information that helps to distinguish the different components of the input data. For text recognition, the initial choice of deep learning studies was to use recurrent neural networks (RNN) that handle input sequences.</p>
<p>However, neither MLPs, CNNs, or RNNs have been able to capture the structure of complex inputs such as natural language. The success of transformer-based solutions accounts for substantial improvements in natural language processing.</p>
<p>The two main differences between transformer models and other algorithms are positional encoding and self-attention. Positional encoding assigns an index to each input value, ensuring that the relative locations of the inputs are maintained throughout the learning and processing phases. Self-attention compares every word in a sentence to every other word in the same sentence, including itself. In this way, it learns contextual information about the relation between the words. This conception has been validated in large language models such as BERT <span class="citation"><a href="references.html#ref-Devlin2019">[70]</a></span> and GPT-3 <span class="citation"><a href="references.html#ref-Brown2020">[71]</a></span>.</p>
<p>The application of attention-based models for satellite image time series analysis is proposed by Garnot et al. <span class="citation"><a href="references.html#ref-Garnot2020a">[72]</a></span> and Russwurm and Körner <span class="citation"><a href="references.html#ref-Russwurm2020">[68]</a></span>. A self-attention network can learn to focus on specific time steps and image features most relevant for distinguishing between different classes. The algorithm tries to identify which combination of individual temporal observations is most relevant to identify each class. For example, crop identification will use observations that capture the onset of the growing season, the date of maximum growth, and the end of the growing season. In the case of deforestation, the algorithm tries to identify the dates when the forest is being cut. Attention-based models are a means to identify events that characterize each land class.</p>
<p>The first model proposed by Garnot et al. is a full transformer-based model <span class="citation"><a href="references.html#ref-Garnot2020a">[72]</a></span>. Considering that image time series classification is easier than natural language processing, Garnot et al. also propose a simplified version of the full transformer model <span class="citation"><a href="references.html#ref-Garnot2020b">[73]</a></span>. This simpler model uses a reduced way to compute the attention matrix, reducing time for training and classification without loss of quality of the result.</p>
<p>In <code>sits</code>, the full transformer-based model proposed by Garnot et al. <span class="citation"><a href="references.html#ref-Garnot2020a">[72]</a></span> is implemented using <code><a href="https://rdrr.io/pkg/sits/man/sits_tae.html">sits_tae()</a></code>. The default parameters are those proposed by the authors. The default optimizer is <code>optim_adamw</code>, as also used in the <code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code> function.</p>
<div class="sourceCode" id="cb165"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Train a machine learning model using TAE</span></span>
<span><span class="va">tae_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train</a></span><span class="op">(</span></span>
<span>  <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_tae.html">sits_tae</a></span><span class="op">(</span></span>
<span>    epochs               <span class="op">=</span> <span class="fl">80</span>,</span>
<span>    batch_size           <span class="op">=</span> <span class="fl">64</span>,</span>
<span>    optimizer            <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/torch/man/optim_adamw.html">optim_adamw</a></span>,</span>
<span>    validation_split     <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span>    verbose              <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Show training evolution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">tae_model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mltaemodel"></span>
<img src="images/mltaemodel.png" alt="Training evolution of Temporal Self-Attention model (source: authors)." width="100%"><p class="caption">
Figure 75: Training evolution of Temporal Self-Attention model (source: authors).
</p>
</div>
<p>Then, we classify a 16-year time series using the TAE model.</p>
<div class="sourceCode" id="cb166"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Classify using DL model and plot the result</span></span>
<span><span class="va">point_mt_mod13q1</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify</a></span><span class="op">(</span><span class="va">tae_model</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>bands <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NDVI"</span>, <span class="st">"EVI"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mltaeplot"></span>
<img src="images/mltaeplot.png" alt="Classification of time series using TAE (source: authors)." width="100%"><p class="caption">
Figure 76: Classification of time series using TAE (source: authors).
</p>
</div>
<p>Garnot and co-authors also proposed the Lightweight Temporal Self-Attention Encoder (LTAE) <span class="citation"><a href="references.html#ref-Garnot2020b">[73]</a></span>, which the authors claim can achieve high classification accuracy with fewer parameters compared to other neural network models. It is a good choice for applications where computational resources are limited. The <code><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae()</a></code> function implements this algorithm. The most important parameter to be set is the learning rate <code>lr</code>. Values ranging from 0.001 to 0.005 should produce good results. See also the section below on model tuning.</p>
<div class="sourceCode" id="cb167"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Train a machine learning model using TAE</span></span>
<span><span class="va">ltae_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_train.html">sits_train</a></span><span class="op">(</span></span>
<span>  <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae</a></span><span class="op">(</span></span>
<span>    epochs <span class="op">=</span> <span class="fl">80</span>,</span>
<span>    batch_size <span class="op">=</span> <span class="fl">64</span>,</span>
<span>    optimizer <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/torch/man/optim_adamw.html">optim_adamw</a></span>,</span>
<span>    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.001</span><span class="op">)</span>,</span>
<span>    validation_split <span class="op">=</span> <span class="fl">0.2</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Show training evolution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ltae_model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlltaemodel"></span>
<img src="images/mlltaemodel.png" alt="Training evolution of Lightweight Temporal Self-Attention model (source: authors)." width="80%"><p class="caption">
Figure 77: Training evolution of Lightweight Temporal Self-Attention model (source: authors).
</p>
</div>
<p>Then, we classify a 16-year time series using the LightTAE model.</p>
<div class="sourceCode" id="cb168"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Classify using DL model and plot the result</span></span>
<span><span class="va">point_mt_mod13q1</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_classify.html">sits_classify</a></span><span class="op">(</span><span class="va">ltae_model</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>bands <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NDVI"</span>, <span class="st">"EVI"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mlltaeplot"></span>
<img src="images/mlltaeplot.png" alt="Classification of time series using LightTAE (source: authors)." width="100%"><p class="caption">
Figure 78: Classification of time series using LightTAE (source: authors).
</p>
</div>
<p>The behaviour of both <code><a href="https://rdrr.io/pkg/sits/man/sits_tae.html">sits_tae()</a></code> and <code><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae()</a></code> is similar to that of <code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code>. It points out the possible need for more classes and training data to better represent the transition period between 2004 and 2010. One possibility is that the training data associated with the Pasture class is only consistent with the time series between the years 2005 to 2008. However, the transition from Forest to Pasture in 2004 and from Pasture to Agriculture in 2009-2010 is subject to uncertainty since the classifiers do not agree on the resulting classes. In general, deep learning temporal-aware models are more sensitive to class variability than random forest and extreme gradient boosters.</p>
</div>
<div id="deep-learning-model-tuning" class="section level2 unnumbered">
<h2>Deep learning model tuning<a class="anchor" aria-label="anchor" href="#deep-learning-model-tuning"><i class="fas fa-link"></i></a>
</h2>
<p>Model tuning is the process of selecting the best set of hyperparameters for a specific application. When using deep learning models for image classification, it is a highly recommended step to enable a better fit of the algorithm to the training data. Hyperparameters are parameters of the model that are not learned during training but instead are set prior to training and affect the behavior of the model during training. Examples include the learning rate, batch size, number of epochs, number of hidden layers, number of neurons in each layer, activation functions, regularization parameters, and optimization algorithms.</p>
<p>Deep learning model tuning involves selecting the best combination of hyperparameters that results in the optimal performance of the model on a given task. This is done by training and evaluating the model with different sets of hyperparameters to select the set that gives the best performance.</p>
<p>Deep learning algorithms try to find the optimal point representing the best value of the prediction function that, given an input <span class="math inline">\(X\)</span> of data points, predicts the result <span class="math inline">\(Y\)</span>. In our case, <span class="math inline">\(X\)</span> is a multidimensional time series, and <span class="math inline">\(Y\)</span> is a vector of probabilities for the possible output classes. For complex situations, the best prediction function is time-consuming to estimate. For this reason, deep learning methods rely on gradient descent methods to speed up predictions and converge faster than an exhaustive search <span class="citation"><a href="references.html#ref-Bengio2012">[74]</a></span>. All gradient descent methods use an optimization algorithm adjusted with hyperparameters such as the learning and regularization rates <span class="citation"><a href="references.html#ref-Schmidt2021">[61]</a></span>. The learning rate controls the numerical step of the gradient descent function, and the regularization rate controls model overfitting. Adjusting these values to an optimal setting requires using model tuning methods.</p>
<p>To reduce the learning curve, <code>sits</code> provides default values for all machine learning and deep learning methods, ensuring a reasonable baseline performance. However, refininig model hyperparameters might be necessary, especially for more complex models such as <code><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae()</a></code> or <code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code>. To that end, the package provides the <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning.html">sits_tuning()</a></code> function.</p>
<p>The most straightforward approach to model tuning is to run a grid search; this involves defining a range for each hyperparameter and then testing all possible combinations. This approach leads to a combinatorial explosion and thus is not recommended. Instead, Bergstra and Bengio propose randomly chosen trials <span class="citation"><a href="references.html#ref-Bergstra2012">[75]</a></span>. Their paper shows that randomized trials are more efficient than grid search trials, selecting adequate hyperparameters at a fraction of the computational cost. The <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning.html">sits_tuning()</a></code> function follows Bergstra and Bengio by using a random search on the chosen hyperparameters.</p>
<p>Experiments with image time series show that other optimizers may have better performance for the specific problem of land classification. For this reason, the authors developed the <code>torchopt</code> R package, which includes several recently proposed optimizers, including Madgrad <span class="citation"><a href="references.html#ref-Defazio2021">[76]</a></span>, and Yogi <span class="citation"><a href="references.html#ref-Zaheer2018">[77]</a></span>. Using the <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning.html">sits_tuning()</a></code> function allows testing these and other optimizers available in <code>torch</code> and <code>torch_opt</code> packages.</p>
<p>The <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning.html">sits_tuning()</a></code> function takes the following parameters:</p>
<ul>
<li>
<code>samples</code>: Training dataset to be used by the model.</li>
<li>
<code>samples_validation</code>: Optional dataset containing time series to be used for validation. If missing, the next parameter will be used.</li>
<li>
<code>validation_split</code>: If <code>samples_validation</code> is not used, this parameter defines the proportion of time series in the training dataset to be used for validation (default is 20%).</li>
<li>
<code>ml_method()</code>: Deep learning method (either <code><a href="https://rdrr.io/pkg/sits/man/sits_mlp.html">sits_mlp()</a></code>, <code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code>, <code><a href="https://rdrr.io/pkg/sits/man/sits_tae.html">sits_tae()</a></code> or <code><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae()</a></code>).</li>
<li>
<code>params</code>: Defines the optimizer and its hyperparameters by calling <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning_hparams.html">sits_tuning_hparams()</a></code>, as shown in the example below.</li>
<li>
<code>trials</code>: Number of trials to run the random search.</li>
<li>
<code>multicores</code>: Number of cores to be used for the procedure.</li>
<li>
<code>progress</code>: Show a progress bar?</li>
</ul>
<p>The <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning_hparams.html">sits_tuning_hparams()</a></code> function inside <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning.html">sits_tuning()</a></code> allows defining optimizers and their hyperparameters, including <code>lr</code> (learning rate), <code>eps</code> (controls numerical stability), and <code>weight_decay</code> (controls overfitting). The default values for <code>eps</code> and <code>weight_decay</code> in all <code>sits</code> deep learning functions are 1e-08 and 1e-06, respectively. The default <code>lr</code> for <code><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae()</a></code> and <code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code> is 0.005.</p>
<p>Users have different ways to randomize the hyperparameters, including:</p>
<ul>
<li>
<code>choice()</code> (a list of options);</li>
<li>
<code>uniform</code> (a uniform distribution);</li>
<li>
<code>randint</code> (random integers from a uniform distribution);</li>
<li>
<code>normal(mean, sd)</code> (normal distribution);</li>
<li>
<code>beta(shape1, shape2)</code> (beta distribution);</li>
<li>
<code>loguniform(max, min)</code> (loguniform distribution).</li>
</ul>
<p>We suggest to use the log-uniform distribution to search over a wide range of values that span several orders of magnitude. This is common for hyperparameters like learning rates, which can vary from very small values (e.g., 0.0001) to larger values (e.g., 1.0) in a logarithmic manner. By default, <code><a href="https://rdrr.io/pkg/sits/man/sits_tuning.html">sits_tuning()</a></code> uses a loguniform distribution between 10^-2 and 10^-4 for the learning rate and the same distribution between 10^-2 and 10^-8 for the weight decay.</p>
<div class="sourceCode" id="cb169"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tuned</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_tuning.html">sits_tuning</a></span><span class="op">(</span></span>
<span>  samples <span class="op">=</span> <span class="va">samples_matogrosso_mod13q1</span>,</span>
<span>  ml_method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  params <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/sits/man/sits_tuning_hparams.html">sits_tuning_hparams</a></span><span class="op">(</span></span>
<span>    optimizer <span class="op">=</span> <span class="fu">torch</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/torch/man/optim_adamw.html">optim_adamw</a></span>,</span>
<span>    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>      lr <span class="op">=</span> <span class="fu">loguniform</span><span class="op">(</span><span class="fl">10</span><span class="op">^</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">10</span><span class="op">^</span><span class="op">-</span><span class="fl">4</span><span class="op">)</span>,</span>
<span>      weight_decay <span class="op">=</span> <span class="fu">loguniform</span><span class="op">(</span><span class="fl">10</span><span class="op">^</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">10</span><span class="op">^</span><span class="op">-</span><span class="fl">8</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  trials <span class="op">=</span> <span class="fl">40</span>,</span>
<span>  multicores <span class="op">=</span> <span class="fl">6</span>,</span>
<span>  progress <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The result is a tibble with different values of accuracy, kappa, decision matrix, and hyperparameters. The best results obtain accuracy values between 0.978 and 0.970, as shown below. The best result is obtained by a learning rate of 0.0013 and a weight decay of 3.73e-07. The worst result has an accuracy of 0.891, which shows the importance of the tuning procedure.</p>
<div class="sourceCode" id="cb170"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Obtain accuracy, kappa, lr, and weight decay for the 5 best results</span></span>
<span><span class="co"># Hyperparameters are organized as a list</span></span>
<span><span class="va">hparams_5</span> <span class="op">&lt;-</span> <span class="va">tuned</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="op">]</span><span class="op">$</span><span class="va">opt_hparams</span></span>
<span><span class="co"># Extract learning rate and weight decay from the list</span></span>
<span><span class="va">lr_5</span> <span class="op">&lt;-</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">hparams_5</span>, <span class="kw">function</span><span class="op">(</span><span class="va">h</span><span class="op">)</span> <span class="va">h</span><span class="op">$</span><span class="va">lr</span><span class="op">)</span></span>
<span><span class="va">wd_5</span> <span class="op">&lt;-</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">hparams_5</span>, <span class="kw">function</span><span class="op">(</span><span class="va">h</span><span class="op">)</span> <span class="va">h</span><span class="op">$</span><span class="va">weight_decay</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a tibble to display the results</span></span>
<span><span class="va">best_5</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="fu">::</span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span><span class="op">(</span></span>
<span>  accuracy <span class="op">=</span> <span class="va">tuned</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="op">]</span><span class="op">$</span><span class="va">accuracy</span>,</span>
<span>  kappa <span class="op">=</span> <span class="va">tuned</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="op">]</span><span class="op">$</span><span class="va">kappa</span>,</span>
<span>  lr <span class="op">=</span> <span class="va">lr_5</span>,</span>
<span>  weight_decay <span class="op">=</span> <span class="va">wd_5</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Print the best five combination of hyperparameters</span></span>
<span><span class="va">best_5</span></span></code></pre></div>
<pre><code>#&gt; # A tibble: 5 × 4
#&gt;   accuracy kappa       lr weight_decay
#&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;
#&gt; 1    0.978 0.974 0.00136  0.000000373 
#&gt; 2    0.975 0.970 0.00269  0.0000000861
#&gt; 3    0.973 0.967 0.00162  0.00218     
#&gt; 4    0.970 0.964 0.000378 0.00000868  
#&gt; 5    0.970 0.964 0.00198  0.00000275</code></pre>
<p>For large datasets, the tuning process is time-consuming. Despite this cost, it is recommended to achieve the best performance. In general, tuning hyperparameters for models such as <code><a href="https://rdrr.io/pkg/sits/man/sits_tempcnn.html">sits_tempcnn()</a></code> and <code><a href="https://rdrr.io/pkg/sits/man/sits_lighttae.html">sits_lighttae()</a></code> will result in a slight performance improvement over the default parameters on overall accuracy. The performance gain will be stronger in the less well represented classes, where significant gains in producer’s and user’s accuracies are possible. When detecting change in less frequent classes, tuning can make a substantial difference in the results.</p>
</div>
<div id="considerations-on-model-choice" class="section level2 unnumbered">
<h2>Considerations on model choice<a class="anchor" aria-label="anchor" href="#considerations-on-model-choice"><i class="fas fa-link"></i></a>
</h2>
<p>The results should not be taken as an indication of which method performs better. The most crucial factor for achieving a good result is the quality of the training data <span class="citation"><a href="references.html#ref-Maxwell2018">[31]</a></span>. Experience shows that classification quality depends on the training samples and how well the model matches these samples. For examples of ML for classifying large areas, please see the papers by the authors <span class="citation"><a href="references.html#ref-Ferreira2020a">[7]</a>, <a href="references.html#ref-Picoli2018">[50]</a>, <a href="references.html#ref-Picoli2020a">[78]</a>, <a href="references.html#ref-Simoes2020">[79]</a></span>.</p>
<p>In the specific case of satellite image time series, Russwurm et al. present a comparative study between seven deep neural networks for the classification of agricultural crops, using random forest as a baseline <span class="citation"><a href="references.html#ref-Russwurm2020">[68]</a></span>. The data is composed of Sentinel-2 images over Britanny, France. Their results indicate a slight difference between the best model (attention-based transformer model) over TempCNN and random forest. Attention-based models obtain accuracy ranging from 80-81%, TempCNN gets 78-80%, and random forest obtains 78%. Based on this result and also on the authors’ experience, we make the following recommendations:</p>
<ul>
<li><p>Random forest provides a good baseline for image time series classification and should be included in users’ assessments.</p></li>
<li><p>XGBoost is a worthy alternative to random forest. In principle, XGBoost is more sensitive to data variations at the cost of possible overfitting.</p></li>
<li><p>TempCNN is a reliable model with reasonable training time, which is close to the state-of-the-art in deep learning classifiers for image time series.</p></li>
<li><p>Attention-based models (TAE and LightTAE) can achieve the best overall performance with well-designed and balanced training sets and hyperparameter tuning.</p></li>
</ul>
<p>The best means of improving classification performance is to provide an accurate and reliable training dataset. Accuracy improvements resulting from using deep learning methods instead of random forest of xgboost are on the order of 3-5%, while gains when using good training data improve results by 10-30%. As a basic rule, make sure you have good quality samples before training and classification.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="improving-the-quality-of-training-samples.html">Improving the quality of training samples</a></div>
<div class="next"><a href="classification-of-raster-data-cubes.html">Classification of raster data cubes</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#machine-learning-for-data-cubes">Machine learning for data cubes</a></li>
<li><a class="nav-link" href="#machine-learning-classification">Machine learning classification</a></li>
<li><a class="nav-link" href="#common-interface-to-machine-learning-and-deep-learning-models">Common interface to machine learning and deep learning models</a></li>
<li><a class="nav-link" href="#random-forest">Random forest</a></li>
<li><a class="nav-link" href="#support-vector-machine">Support vector machine</a></li>
<li><a class="nav-link" href="#extreme-gradient-boosting">Extreme gradient boosting</a></li>
<li><a class="nav-link" href="#deep-learning-using-multilayer-perceptron">Deep learning using multilayer perceptron</a></li>
<li><a class="nav-link" href="#temporal-convolutional-neural-network-tempcnn">Temporal Convolutional Neural Network (TempCNN)</a></li>
<li><a class="nav-link" href="#attention-based-models">Attention-based models</a></li>
<li><a class="nav-link" href="#deep-learning-model-tuning">Deep learning model tuning</a></li>
<li><a class="nav-link" href="#considerations-on-model-choice">Considerations on model choice</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong><strong>sits</strong>: Satellite Image Time Series Analysis on Earth Observation Data Cubes</strong>" was written by Gilberto Camara, Rolf Simoes, Felipe Souza, Felipe Menino, Charlotte Pelletier, Pedro R. Andrade, Karine Ferreira, Gilberto Queiroz. It was last built on 2024-08-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
