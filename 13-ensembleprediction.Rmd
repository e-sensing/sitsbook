# Ensemble prediction from multiple models{-}

```{r, echo = FALSE}
source("common.R")
# Create a directory to store files
if (!file.exists("./tempdir/chp13"))
    dir.create("./tempdir/chp13")
```

Ensemble prediction is a powerful technique for combining predictions from multiple models to produce more accurate and robust predictions. In general, ensemble predictions produce better predictions than using a single model. This is because the errors of individual models can cancel out or be reduced when combined with the predictions of other models. As a result, ensemble predictions can lead to better overall accuracy and reduce the risk of overfitting. This can be especially useful when working with complex or uncertain data. By combining the predictions of multiple models, users can identify which features or factors are most important for making accurate predictions. When using ensemble methods, choosing diverse models with different sources of error is important to ensure that the ensemble predictions are more accurate and robust.

The `sits` package provides `sits_combine_predictions()` to estimate ensemble predictions using probability cubes produced by `sits_classify()` and optionally post-processed with `sits_smooth()`. There are two ways to make ensemble predictions from multiple models:

* Averaging: In this approach, the predictions of each model are averaged to produce the final prediction. This method works well when the models have similar accuracy and errors. 

* Uncertainty: Predictions from different models are compared in terms of their uncertainties on a pixel-by-pixel basis; predictions with lower uncertainty are chosen as the more likely ones to be valid. 

In what follows, we will use the same data used in Chapter [Image classification in data cubes](https://e-sensing.github.io/sitsbook/image-classification-in-data-cubes.html) to illustrate how to produce an ensemble prediction. For simplicity, we repeat the steps taken to classify an image in that Chapter: create a data cube, train a model using the lightweight temporal attention encoder algorithm (`sits_lighttae()`), then classify, post-process, and label the data cube. As a starting point, we plot two instances of the data cube at the start and end of the time series. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Color composite image for date 2020-07-06 (Source: Authors)."}
# Files are available in a local directory 
data_dir <- system.file("extdata/Rondonia-20LMR/", package = "sitsdata")
# Read data cube
ro_cube_20LMR <- sits_cube(
    source = "MPC",
    collection = "SENTINEL-2-L2A",
    data_dir = data_dir
)

plot(ro_cube_20LMR, 
    date = "2022-05-13", 
    red = "B11", 
    green = "B8A", 
    blue = "B02")
```

The image from 2020-07-06 shows many areas under deforestation, especially a large one located in the top center of the image. It is helpful to compare to an image one year later, which shows several burned areas resulting from forest removal followed by fire. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Color composite image for date 2021-08-10 (Source: Authors)."}
plot(ro_cube_20LMR, 
    date = "2022-10-04", 
    red = "B11", 
    green = "B8A", 
    blue = "B02")
```

The samples used in the classification are the same as those used in Chapter [Image classification in data cubes](https://e-sensing.github.io/sitsbook/image-classification-in-data-cubes.html). Please refer to that chapter for a more detailed description of the temporal response of the samples. We first reproduce the result obtained in that Chapter using `sits_tempcnn()`. 

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Image Classification using TempCNN model (Source: Authors)."}
# Get the samples from library "sitsdata"
library(sitsdata)
data(samples_prodes_4classes)
# Use only the bands available in the cube
samples_3bands <- sits_select(
    data = samples_prodes_4classes, 
    bands = sits_bands(ro_cube_20LKP))
# Train model using LightTAE algorithm
tcnn_model <- sits_train(
    samples = samples_3bands, 
    ml_method = sits_tempcnn(
        opt_hparams = list(lr = 0.001)))
# Classify data cube
ro_cube_probs_tcnn <- sits_classify(
    data     = ro_cube_20LKP,
    ml_model = tcnn_model,
    output_dir = "./tempdir/chp13",
    version = "tcnn",
    multicores = 4,
    memsize = 12)
# Smooth data cube
ro_cube_bayes_tcnn <- sits_smooth(
    cube    = ro_cube_probs_tcnn,
    output_dir = "./tempdir/chp13",
    version = "tcnn",
    multicores = 4,
    memsize = 12)
# Generate a thematic map
defor_map_tcnn <- sits_label_classification(
    cube = ro_cube_bayes_tcnn,
    multicores = 4,
    memsize = 12,
    output_dir = "./tempdir/chp12",
    version = "tcnn")
plot(defor_map_tcnn)
```

The deforestation map produced by `sits_tempcnn()` has spatial consistency; arguably, it underestimates the burned areas in the right-hand corner of the image. The method tries to model the temporal behavior of the reflectances. For this reason, it sometimes fails to detect changes in the last dates of the time series, as it occurs when areas are burned in August. 

To build a two-member ensemble, we now classify the same image using random forest.  

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Land classification in Rondonia using a random forest algorithm (Source: Authors)."}
# Train model using random forest algorithm
rfor_model <- sits_train(
    samples = samples_3bands, 
    ml_method = sits_rfor())
# Classify the data cube using the tempCNN model
ro_cube_probs_rfor <- sits_classify(
    data = ro_cube_20LKP,
    ml_model = rfor_model,
    output_dir = "./tempdir/chp12/",
    version = "rfor",
    memsize = 16,
    multicores = 4)
# Post-process the probability cube
ro_cube_bayes_rfor <- sits_smooth(
    cube = ro_cube_probs_rfor,
    output_dir = "./tempdir/chp13/",
    version = "rfor",
    memsize = 16,
    multicores = 4)
# Label the post-processed  probability cube
ro_cube_label_rfor <- sits_label_classification(
    cube = ro_cube_bayes_rfor,
    output_dir = "./tempdir/chp13/",
    version = "rfor",
    memsize = 16,
    multicores = 4)
# Plot the random forest version of the classified cube
plot(ro_cube_label_rfor)
```

Comparing the two results, while most of the land areas have been classified equally, there are places of disagreement concerning the places classified as "Burned_Area" and "Highly_Degraded". Since the random forest model is sensitive to the response of images at the end of the period, it tends to be better to distinguish burned areas. However, it tends to reduce the forest areas, classifying some of them as highly degraded. Such misclassification happens because the random forest algorithm disregards the temporal correlation of the input data. Values from a single date are used to distinguish between natural and degraded forest areas. 

Given the differences and complementaries between the two predicted outcomes, combining them using `sits_combine_predictions()` is useful. The first option for ensemble prediction is to take the average of the probability maps to reduce noise.

```{r, tidy = "styler", out.width = "100%", fig.align="center", fig.cap= "Land classification in Rondonia near Samuel dam using the average of the probabilities produced by lightTAE and tempCNN algorithms (Source: Authors)."}
# Combine the two predictions by taking the average of the probabilities for each class
s2_cube_average_probs <- sits_combine_predictions(
  cubes = list(ro_cube_bayes_tcnn, ro_cube_bayes_rfor),
  type = "average",
  output_dir = "./tempdir/chp13/",
  version = "average",
  memsize = 16,
  multicores = 4)

# Label the average probability cube
s2_cube_average_class <- sits_label_classification(
    cube = s2_cube_average_probs,
    output_dir = "./tempdir/chp13/",
    version = "average",
    memsize = 16,
    multicores = 4)

# Plot the second version of the classified cube
plot(s2_cube_average_class)
```
Compared with the initial map, the result has increased the number of pixels classified as burned areas and highly degraded. Not all areas classified as degraded forest by the random forest method have been included in the final map. Only those places where the random forest has high confidence have been included. The average map generally results in a better classification than the individual results.

Overall, ensemble predictions are a powerful tool for improving the accuracy and robustness of machine learning models. By combining the predictions of multiple models, we can reduce errors and uncertainty and gain new insights into the underlying patterns in the data.
